[
    {
        "repo": "PaddlePaddle/Serving",
        "number": 21,
        "title": "kvdb test cases coredump",
        "body": "Running output/bin/db_func, output/bin/db_thread will occasionally coredump:\r\n```\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from KVDBTest\r\n[ RUN      ] KVDBTest.AbstractKVDB_Func_Test\r\n[       OK ] KVDBTest.AbstractKVDB_Func_Test (16 ms)\r\n[----------] 1 test from KVDBTest (16 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (16 ms total)\r\n[  PASSED  ] 1 test.\r\npure virtual method called\r\nterminate called without an active exception\r\nAborted (core dumped)\r\n```",
        "state": "closed",
        "user": "wangguibao",
        "closed_by": "bjjwwang",
        "created_at": "2019-07-05T08:04:21+00:00",
        "updated_at": "2019-10-11T07:37:43+00:00",
        "closed_at": "2019-10-11T07:37:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 88,
        "title": "Build with gcc 5.4.0 error",
        "body": "System: 16.04.6 LTS (Xenial Xerus)\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11) \r\n\r\nError message:\r\n```\r\nIn file included from /usr/include/c++/5/bits/ios_base.h:41:0,\r\n                 from /usr/include/c++/5/ios:42,\r\n                 from /usr/include/c++/5/ostream:38,\r\n                 from /usr/include/c++/5/iterator:64,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/detail/iterator.hpp:54,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/iterator/iterator_categories.hpp:10,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/detail/table.hpp:14,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/detail/equivalent.hpp:10,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/unordered_map.hpp:19,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered_map.hpp:16,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/common.h:26,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/config_manager.h:18,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/src/config_manager.cpp:15:\r\n/usr/include/c++/5/bits/locale_classes.h:284:24: error: reference to ‘basic_string’ is ambiguous\r\n       operator()(const basic_string<_Char, _Traits, _Alloc>& __s1,\r\n                        ^\r\nIn file included from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/functional/hash/extensions.hpp:17:0,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/functional/hash/hash.hpp:477,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/functional/hash.hpp:6,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/unordered_map.hpp:17,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered_map.hpp:16,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/common.h:26,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/config_manager.h:18,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/src/config_manager.cpp:15:\r\n/home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/detail/container_fwd.hpp:61:65: note: candidates are: template<class charT, class traits, class Allocator> class std::basic_string\r\n     template <class charT, class traits, class Allocator> class basic_string;\r\n                                                                 ^\r\nIn file included from /usr/include/c++/5/string:39:0,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /usr/include/c++/5/tuple:39,\r\n                 from /usr/include/c++/5/bits/stl_map.h:63,\r\n                 from /usr/include/c++/5/map:61,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/config_manager.h:16,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/src/config_manager.cpp:15:\r\n/usr/include/c++/5/bits/stringfwd.h:71:11: note:                 template<class _CharT, class _Traits, class _Alloc> class std::__cxx11::basic_string\r\n     class basic_string;\r\n           ^\r\nIn file included from /usr/include/c++/5/bits/ios_base.h:41:0,\r\n                 from /usr/include/c++/5/ios:42,\r\n                 from /usr/include/c++/5/ostream:38,\r\n                 from /usr/include/c++/5/iterator:64,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/detail/iterator.hpp:54,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/iterator/iterator_categories.hpp:10,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/detail/table.hpp:14,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/detail/equivalent.hpp:10,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered/unordered_map.hpp:19,\r\n                 from /home/tangzhiyi01/ctr/Serving/build/third_party/boost/src/extern_boost/boost/unordered_map.hpp:16,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/common.h:26,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/include/config_manager.h:18,\r\n                 from /home/tangzhiyi01/ctr/Serving/sdk-cpp/src/config_manager.cpp:15:\r\n/usr/include/c++/5/bits/locale_classes.h:284:36: error: expected ‘,’ or ‘...’ before ‘<’ token\r\n       operator()(const basic_string<_Char, _Traits, _Alloc>& __s1,\r\n```",
        "state": "closed",
        "user": "wangguibao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2019-10-10T09:19:40+00:00",
        "updated_at": "2024-04-16T09:04:29+00:00",
        "closed_at": "2024-04-16T09:04:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 90,
        "title": "可否出个docker镜像，源码编译了一天，老是中途错误",
        "body": "os  ubuntu 18.04\r\ngo 1.12\r\ncmake 3.10\r\ng++ 7.4.0\r\n\r\n最好出个镜像，学习测试下，不用卡在编译上",
        "state": "closed",
        "user": "3050311118",
        "closed_by": "guru4elephant",
        "created_at": "2019-10-13T09:22:26+00:00",
        "updated_at": "2020-04-05T04:41:42+00:00",
        "closed_at": "2020-04-05T04:41:42+00:00",
        "comments_count": [
            "bjjwwang",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 13,
        "title": "ubuntu18.04 下按照INSTALL.md的说明进行执行报错。",
        "body": "**按照INSTALL.md中的说明，执行如下步骤，最后异步最终报错：**\r\n$ git clone https://github.com/PaddlePaddle/serving.git\r\n$ cd serving\r\n$ mkdir build\r\n$ cd build\r\n$ cmake ..\r\n$ make -j4\r\n$ make install\r\n\r\n**报错信息如下：**\r\n............................................\r\n[ 48%] Completed 'extern_opencv'\r\n[ 48%] Built target extern_opencv\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n",
        "state": "closed",
        "user": "707503331",
        "closed_by": "707503331",
        "created_at": "2019-06-25T00:49:24+00:00",
        "updated_at": "2019-06-27T02:54:59+00:00",
        "closed_at": "2019-06-27T02:54:19+00:00",
        "comments_count": [
            "wangguibao",
            "707503331",
            "wangguibao",
            "luotao1",
            "707503331",
            "707503331",
            "wangguibao",
            "707503331",
            "707503331",
            "707503331",
            "707503331",
            "707503331",
            "707503331",
            "wangguibao",
            "707503331",
            "wangguibao",
            "wangguibao",
            "707503331",
            "wangguibao",
            "707503331",
            "707503331"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 59,
        "title": "Need a configuration option to let user specify whether enable memory optimzation or not",
        "body": "收到反馈，GPU预测引擎在图像分割模型上，EnableMemoryOpt()后会在memory optimizaiton pass fail，具体call stack为：\r\n\r\n<img width=\"1428\" alt=\"6c2512709b2d41827fb25eb19886d2a2\" src=\"https://user-images.githubusercontent.com/1445888/63067876-9a108a00-bf43-11e9-8a38-ae90677b5f08.PNG\">\r\n\r\n希望能够增加一个配置开关，让用户指定是否打开内存优化。",
        "state": "closed",
        "user": "wangguibao",
        "closed_by": "wangguibao",
        "created_at": "2019-08-15T02:02:02+00:00",
        "updated_at": "2019-09-03T09:29:30+00:00",
        "closed_at": "2019-09-03T09:29:30+00:00",
        "comments_count": [
            "wangguibao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 11,
        "title": "Client 与 Server端通过BRPC调用是否支持常链接场景下的负载均衡？",
        "body": "![image](https://user-images.githubusercontent.com/18028216/59431122-b2b2d580-8e16-11e9-97d3-c369848e7684.png)\r\n\r\n关于推理服务部署的时候，Client端与Server端之间通过brpc调用，这个是否支持长连接场景下的负载均衡，如果支持，如何实现？",
        "state": "closed",
        "user": "LaraStuStu",
        "closed_by": "guru4elephant",
        "created_at": "2019-06-13T12:07:58+00:00",
        "updated_at": "2020-04-05T04:44:03+00:00",
        "closed_at": "2020-04-05T04:44:03+00:00",
        "comments_count": [
            "wangguibao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 16,
        "title": "关于编译SDK-CPP部分时的错误",
        "body": "一些环境信息\r\n系统:Ubuntu 16.04.5 LTS\r\ngcc/g++: 5.4.0\r\ncmake: 3.5.1\r\npython: 2.7.12\r\n\r\n出现的情况\r\nCMake过程中没有错误，顺利完成Configure。之后在make的过程中，前面是正常的，但是在编译SDK-CPP部分的时候会报出一些编译错误。下面截取其中主要的一些报错信息(基本都是语法错误)：\r\n1 /usr/include/c++/5/backward/hash_fun.h:136:12: error: previous definition of ‘struct __gnu_cxx::hash<int>’\r\n     struct hash<int>\r\n2 serving/build/third_party/install/brpc/include/butil/strings/string_piece.h:416:70: error: ‘bool butil::operator!=(const StringPiece16&, const StringPiece16&)’ must have an argument of class or enumerated type\r\n inline bool operator!=(const StringPiece16& x, const StringPiece16& y) {\r\n3 serving/build/third_party/install/brpc/include/butil/strings/string_piece.h:378:53: error: ‘string16’ was not declared in this scope\r\n extern template class BUTIL_EXPORT BasicStringPiece<string16>;\r\n4 serving/build/third_party/install/brpc/include/butil/strings/string16.h:180:25: error: ‘basic_string’ is not a class template\r\n class BUTIL_EXPORT std::basic_string<butil::char16, butil::string16_char_traits>;\r\n\r\n个人的想法\r\n个人是觉得是由于自己的环境导致的，但是在CMake自动配置的时候没有报错。所以，想请您给一些建议，或者将您编译成功的环境告知一下(gcc版本，系统版本等)。",
        "state": "closed",
        "user": "joelyang97",
        "closed_by": "joelyang97",
        "created_at": "2019-07-01T11:35:27+00:00",
        "updated_at": "2019-07-24T02:31:19+00:00",
        "closed_at": "2019-07-24T02:31:19+00:00",
        "comments_count": [
            "wangguibao",
            "joelyang97",
            "wangguibao",
            "wangguibao",
            "wangguibao",
            "joelyang97",
            "joelyang97"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 64,
        "title": "cube-agent读取磁盘路径错误",
        "body": "cube/cube-agent/src/agent/define.go中init函数会读取磁盘路径，但是由于有的磁盘名过长，df显示时会换行，使用df -h | grep -E '/home|/ssd'命令获取到的信息不完整，字符串分割出来的数组长度会少一位，获取路径数据时会发生数组越界",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2019-08-30T02:27:15+00:00",
        "updated_at": "2019-09-05T03:19:37+00:00",
        "closed_at": "2019-09-05T03:19:37+00:00",
        "comments_count": [
            "MRXLT",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1,
        "title": "Serving编译没有问题，output中demo目录下serving运行报错Segmentation fault，无日志，无其他提示。",
        "body": "运行命令： bin/serving \r\n系统环境：centos 7.4\r\n\r\n其他说明：client中的demo运行ok。\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "YishengCheng",
        "closed_by": "wangguibao",
        "created_at": "2019-04-08T10:05:28+00:00",
        "updated_at": "2019-04-24T08:20:26+00:00",
        "closed_at": "2019-04-24T08:20:26+00:00",
        "comments_count": [
            "wangguibao",
            "YishengCheng",
            "YishengCheng",
            "YishengCheng",
            "wangguibao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 118,
        "title": "demo-serving下的serving多次启动占用同一个端口不报错",
        "body": "如题，需要添加报错信息。",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-01-14T02:43:24+00:00",
        "updated_at": "2024-04-16T09:04:30+00:00",
        "closed_at": "2024-04-16T09:04:30+00:00",
        "comments_count": [],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 117,
        "title": "Cube和Serving的配置过于复杂",
        "body": "易用性问题反馈：\r\n1. Cube和Serving启动的时候，配置文件复杂难以配合。\r\n2. 缺乏一个Demo模型 （稀疏 + Dense），本地自动加载并能够提供预测服务。\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "guru4elephant",
        "created_at": "2020-01-03T09:10:41+00:00",
        "updated_at": "2020-04-05T04:41:14+00:00",
        "closed_at": "2020-04-05T04:41:14+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 126,
        "title": "make serving server and serving client two wheels",
        "body": "serving server and serving client should have different path.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-04T02:16:57+00:00",
        "updated_at": "2020-04-05T04:39:54+00:00",
        "closed_at": "2020-04-05T04:39:54+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 132,
        "title": "**serving二进制可以自动下载**",
        "body": "针对cpu、gpu版本，将server分成两组whl包，并能够根据包下载不同版本的二进制\r\n预期支持：cpu+mkl+avx；cpu+openblas+avx；cpu+openblas+sse；gpu+mkl+avx",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T01:30:00+00:00",
        "updated_at": "2020-02-15T06:27:42+00:00",
        "closed_at": "2020-02-15T06:27:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 135,
        "title": "** 验证GPU是否能在当前的GeneralServing下使用 **",
        "body": "当前GeneralServing还没有在GPU上进行测试，需要使用典型任务进行效果和性能测试",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-02-07T01:58:12+00:00",
        "updated_at": "2020-03-02T06:15:10+00:00",
        "closed_at": "2020-03-02T06:15:10+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 125,
        "title": "Feature request：make general model config a protobuf",
        "body": "currently, client and server use different general model configs, users may have confusion about this. \r\n\r\nUnify configuration into unify protobuf and make it readable and writable with python API is a requirement.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-04T01:59:09+00:00",
        "updated_at": "2020-04-05T04:39:29+00:00",
        "closed_at": "2020-04-05T04:39:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 134,
        "title": "** Refine General Infer Op **",
        "body": "目前general infer op的输出，直接通过response发出，不适合在infer的结果之上进行二次计算，例如ernie model中的pooling操作，需要将general infer op的输出放到op的output中；此外，增加一个将input放到response中发出的general_response_op，用来负责响应rpc",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T01:55:21+00:00",
        "updated_at": "2020-02-17T11:11:57+00:00",
        "closed_at": "2020-02-17T11:11:57+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 133,
        "title": "**Refine General Reader**",
        "body": "当前的general reader，对lodtensor.name的赋值存在问题，需要通用化；\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/145",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T01:37:12+00:00",
        "updated_at": "2020-02-09T09:21:40+00:00",
        "closed_at": "2020-02-09T09:21:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 127,
        "title": "add go client for paddle serving",
        "body": "add go client for serving",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-06T01:14:17+00:00",
        "updated_at": "2020-04-05T04:42:48+00:00",
        "closed_at": "2020-04-05T04:42:48+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 136,
        "title": "** 统一GeneralServing的Log信息级别 **",
        "body": "统一改为LOGV(2)，方便与inference lib的输出日志做区分",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T02:37:41+00:00",
        "updated_at": "2020-02-07T13:52:10+00:00",
        "closed_at": "2020-02-07T13:52:10+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 137,
        "title": "** Fetch Variable可变 **",
        "body": "Client端可以指定部分fetch var进行单次预测，并且网络服务可以支持返回部分fetch var\r\nhttps://github.com/PaddlePaddle/Serving/pull/145",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T05:35:58+00:00",
        "updated_at": "2020-02-09T09:21:52+00:00",
        "closed_at": "2020-02-09T09:21:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 138,
        "title": "Client端增加predict_with_profiler函数",
        "body": "实现predict_with_profile和batch_predict_with_profile\r\n\r\npredict can be profiled with profile_client and profile_server flags, close this issue",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T05:40:30+00:00",
        "updated_at": "2020-02-23T12:43:23+00:00",
        "closed_at": "2020-02-23T12:43:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 141,
        "title": "add fluid_time_file as default serving configration",
        "body": "currently, we have to add fluid_time_file manually\r\nhttps://github.com/PaddlePaddle/Serving/pull/147",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T08:15:57+00:00",
        "updated_at": "2020-02-09T09:21:13+00:00",
        "closed_at": "2020-02-09T09:21:13+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 142,
        "title": "add go client for GeneralServing",
        "body": "当前的client支持python，我们需要增加一个Go的client",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T08:37:48+00:00",
        "updated_at": "2020-02-17T11:12:01+00:00",
        "closed_at": "2020-02-17T11:12:01+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 143,
        "title": "add criteo ctr task example",
        "body": "we need click through rate example for reference\r\nhttps://github.com/PaddlePaddle/Serving/pull/148",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T13:53:07+00:00",
        "updated_at": "2020-02-09T09:20:47+00:00",
        "closed_at": "2020-02-09T09:20:47+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 140,
        "title": "pip可安装",
        "body": "支持paddle-serving-client的pip可安装\r\n支持paddle-serving-server的cpu/gpu版本可安装，包含不同的lib版本",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-02-07T08:05:47+00:00",
        "updated_at": "2020-02-25T01:48:49+00:00",
        "closed_at": "2020-02-25T01:48:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 144,
        "title": "print  useful client information for failed call of predictor",
        "body": "E0207 22:24:21.136749 181141 general_model.cpp:144] failed call predictor with req: insts\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/209",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-07T14:31:29+00:00",
        "updated_at": "2020-03-02T07:49:29+00:00",
        "closed_at": "2020-03-02T07:49:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 146,
        "title": "gitsubmodule core/general-client/pybind11 seems to be broken",
        "body": "I encountered an error when building PaddleServing in CLIENT_ONLY mode:\r\n\r\n```\r\n[root@my_host_name build_client]# cmake -DCLIENT_ONLY=ON ..\r\n-- Found Paddle host system: centos, version: 6.10\r\n-- Found Paddle host system's CPU: 12 cores\r\n-- CXX compiler: /opt/rh/devtoolset-2/root/usr/bin/c++, version: GNU 4.8.2\r\n-- C compiler: /opt/rh/devtoolset-2/root/usr/bin/cc, version: GNU 4.8.2\r\n-- Do not have AVX2 intrinsics and disabled MKL-DNN\r\n-- BOOST_TAR: boost_1_41_0, BOOST_URL: http://paddlepaddledeps.cdn.bcebos.com/boost_1_41_0.tar.gz\r\n-- Protobuf protoc executable: /home/Serving/build_client/third_party/install/protobuf/bin/protoc\r\n-- Protobuf-lite library: /home/Serving/build_client/third_party/install/protobuf/lib/libprotobuf-lite.a\r\n-- Protobuf library: /home/Serving/build_client/third_party/install/protobuf/lib/libprotobuf.a\r\n-- Protoc library: /home/Serving/build_client/third_party/install/protobuf/lib/libprotoc.a\r\n-- Protobuf version: 3.1\r\n-- ssl:/usr/lib64/libssl.so\r\n-- crypto:/usr/lib64/libcrypto.so\r\npaddle serving source dir: /home/Serving\r\nCMake Error at core/general-client/CMakeLists.txt:2 (add_subdirectory):\r\n  The source directory\r\n\r\n    /home/Serving/core/general-client/pybind11\r\n\r\n  does not contain a CMakeLists.txt file.\r\n\r\n\r\nCMake Error at core/general-client/CMakeLists.txt:3 (pybind11_add_module):\r\n  Unknown CMake command \"pybind11_add_module\".\r\n```\r\n\r\nThe error message says that `/home/Serving/core/general-client/pybind11` directory does not contain a CMakeLists.txt file, and it turns out to be an empty directory. `pybind11` has been changed to a git submodule in commit 546f164088dd5bd8d7d8597af7195b56fba16f03 , but this submodule is not properly configured, which makes `git submodule update` command fail.\r\n\r\nThe folder icon of pybind11 submodule on github is also grey, which indicates that the submodule points to an unreachable location.",
        "state": "closed",
        "user": "Kontinuation",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-08T11:45:52+00:00",
        "updated_at": "2020-02-22T00:41:43+00:00",
        "closed_at": "2020-02-22T00:41:43+00:00",
        "comments_count": [
            "guru4elephant",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 149,
        "title": "setup ci for general serving",
        "body": "we need an automatically verification mechanism for serving",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-09T03:18:21+00:00",
        "updated_at": "2020-03-12T14:11:03+00:00",
        "closed_at": "2020-03-12T14:11:03+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 150,
        "title": "add performance evaluation tool",
        "body": "we need to regress old version of performance when we do optimization on our framework.\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/215\r\n",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-09T05:02:26+00:00",
        "updated_at": "2020-02-29T06:57:56+00:00",
        "closed_at": "2020-02-29T06:57:56+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 153,
        "title": "add multi thread prediction wrapper",
        "body": "currently, client does not support multi thread prediction, but users can do multi process prediction based on python subprocess. we need to wrapper this as a simple function.\r\nhttps://github.com/PaddlePaddle/Serving/pull/156",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-09T09:20:07+00:00",
        "updated_at": "2020-02-10T07:25:02+00:00",
        "closed_at": "2020-02-10T07:25:02+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 152,
        "title": "add auc and acc metric for client",
        "body": "add auc and acc computation for client",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-09T09:19:12+00:00",
        "updated_at": "2020-02-10T06:29:14+00:00",
        "closed_at": "2020-02-10T06:29:14+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 155,
        "title": "add profiler for general server",
        "body": "add profiler for server side",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-10T00:06:06+00:00",
        "updated_at": "2020-02-10T06:29:02+00:00",
        "closed_at": "2020-02-10T06:29:02+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 161,
        "title": "优化client端日志",
        "body": "client端启动时会输出以下不必要的日志对用户产生干扰，需要进行优化\r\n![image](https://user-images.githubusercontent.com/16594411/74295540-4826b080-4d7c-11ea-810f-3a4cca6e87a3.png)\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/202",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T01:46:06+00:00",
        "updated_at": "2020-02-26T22:11:43+00:00",
        "closed_at": "2020-02-26T22:11:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 164,
        "title": "publish imdb performance benchmark",
        "body": "please public prediction benchmark with predict() and batch_predict() interface. \r\nFor imdb task, the model should run on cpu device and model inference time should be profiled.\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/215",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T04:47:28+00:00",
        "updated_at": "2020-02-29T06:59:34+00:00",
        "closed_at": "2020-02-29T06:59:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 154,
        "title": "failed fetch predictor",
        "body": "E0210 06:46:00.630698 101654 predictor_sdk.h:52] Failed fetch predictor:, ep_name: general_model\r\n![image](https://user-images.githubusercontent.com/35550832/74111666-5e443d80-4bd1-11ea-9865-9cc20c17bfb8.png)\r\n\r\n",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-09T22:46:45+00:00",
        "updated_at": "2020-02-26T22:17:40+00:00",
        "closed_at": "2020-02-26T22:17:40+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 165,
        "title": "add dist kv op for server side",
        "body": "add dist kv op and corresponding python configurations\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/296",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T10:31:18+00:00",
        "updated_at": "2020-03-20T11:39:33+00:00",
        "closed_at": "2020-03-20T11:39:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 166,
        "title": "add local kv op for server side",
        "body": "add local kv op with rocks db and corresponding server side\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/296",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T10:31:39+00:00",
        "updated_at": "2020-03-20T11:39:47+00:00",
        "closed_at": "2020-03-20T11:39:47+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 172,
        "title": "fix batch size buf for general reader and general infer op",
        "body": "fix the bug",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-14T15:01:50+00:00",
        "updated_at": "2020-02-17T10:44:34+00:00",
        "closed_at": "2020-02-17T10:44:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 174,
        "title": "add English version readme for paddle serving",
        "body": "For English native users of PaddlePaddle and PaddleServing",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-15T02:16:21+00:00",
        "updated_at": "2020-02-20T15:44:35+00:00",
        "closed_at": "2020-02-20T15:44:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 168,
        "title": "profiling general bert as a service.",
        "body": "",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T10:32:55+00:00",
        "updated_at": "2020-04-05T04:42:20+00:00",
        "closed_at": "2020-04-05T04:42:20+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 167,
        "title": "add a new application with general serving",
        "body": "support bert as a service application with general serving",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-12T10:32:42+00:00",
        "updated_at": "2020-04-05T04:46:30+00:00",
        "closed_at": "2020-04-05T04:46:30+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 179,
        "title": "fix pybind11 submodule problem",
        "body": "core/general-client/pybind11 should be a submodule",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-17T12:05:20+00:00",
        "updated_at": "2020-02-20T15:43:44+00:00",
        "closed_at": "2020-02-20T15:43:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 173,
        "title": "Unify Data Transfer Between Server Op",
        "body": "Currently, data structure is designed manually which is not very extensible. We want to make ops configurable in python API, data structure between ops should be unified and the dependencies of Op can be obtained through resource rather than hard coding",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-15T01:47:19+00:00",
        "updated_at": "2020-02-17T11:12:07+00:00",
        "closed_at": "2020-02-17T11:12:07+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 182,
        "title": "batch predict failed",
        "body": "batch predict failed with general server",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-02-18T01:52:11+00:00",
        "updated_at": "2020-02-20T01:47:38+00:00",
        "closed_at": "2020-02-20T01:47:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 175,
        "title": "pip install failed pip安装失败",
        "body": "whl文件也搜不到\r\nLooking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\r\nCollecting paddle-serving-client\r\n  ERROR: Could not find a version that satisfies the requirement paddle-serving-client (from versions: none)\r\nERROR: No matching distribution found for paddle-serving-client",
        "state": "closed",
        "user": "casolxia",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-15T03:29:14+00:00",
        "updated_at": "2020-02-26T22:15:33+00:00",
        "closed_at": "2020-02-26T22:15:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 187,
        "title": "add profiling from client side to server side with information on each operators",
        "body": "",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-18T13:14:07+00:00",
        "updated_at": "2020-02-20T15:43:52+00:00",
        "closed_at": "2020-02-20T15:43:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 188,
        "title": "add a tool for profile result analysis and visualization",
        "body": "for issue #187, we need a python tool to analyze or visualize the profiling result.\r\nhttps://github.com/PaddlePaddle/Serving/pull/215",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-18T13:21:22+00:00",
        "updated_at": "2020-02-29T06:58:52+00:00",
        "closed_at": "2020-02-29T06:58:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 183,
        "title": "添加使用自编译的二进制文件的方法",
        "body": "让用户可以调用本地编译的server端二进制文件",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-02-18T02:11:07+00:00",
        "updated_at": "2020-02-18T04:41:35+00:00",
        "closed_at": "2020-02-18T04:41:35+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 197,
        "title": "添加client端shape检查功能",
        "body": "",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-21T07:43:17+00:00",
        "updated_at": "2020-03-12T13:35:32+00:00",
        "closed_at": "2020-03-12T13:35:32+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 200,
        "title": "paddle_serving_server一行命令启动时，返回可访问URL",
        "body": "可以增加一个基于HTTP可以访问的demo或者直接给出http post地址",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-02-22T03:43:26+00:00",
        "updated_at": "2024-04-16T09:04:31+00:00",
        "closed_at": "2024-04-16T09:04:31+00:00",
        "comments_count": [],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 192,
        "title": "add available port  check for server",
        "body": "",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-02-20T01:49:25+00:00",
        "updated_at": "2020-02-21T07:45:16+00:00",
        "closed_at": "2020-02-21T07:45:16+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 195,
        "title": "Should have a default gpu id?",
        "body": "  File \"test_gpu_server.py\", line 26, in <module>\r\n    server.run_server()\r\n  File \"/home/users/dongdaxiang/paddle_whls/custom_op/paddle_release_home/python/lib/python2.7/site-packages/paddle_serving_server_gpu/__init__.py\", line 264, in run_server\r\n    self.gpuid,\r\nAttributeError: 'Server' object has no attribute 'gpuid'",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-02-21T03:10:09+00:00",
        "updated_at": "2020-03-17T02:23:36+00:00",
        "closed_at": "2020-03-17T02:23:36+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 199,
        "title": "对server端如何安装，配置各种版本，各种线程或者GPU进行一个说明",
        "body": "对server端如何安装，配置各种版本，各种线程或者GPU进行一个说明，增加到doc中",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-21T13:48:06+00:00",
        "updated_at": "2020-04-05T04:49:20+00:00",
        "closed_at": "2020-04-05T04:49:20+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 203,
        "title": "benchmarking bert as a service with multi-gpu",
        "body": "we need to test the throughputs and latency for new version of serving",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-22T15:42:02+00:00",
        "updated_at": "2020-04-05T04:51:55+00:00",
        "closed_at": "2020-04-05T04:51:55+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 207,
        "title": "support multiple return type",
        "body": "current client api only supports float value return, need to support float/int64\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/209",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-24T23:50:09+00:00",
        "updated_at": "2020-03-02T07:49:54+00:00",
        "closed_at": "2020-03-02T07:49:54+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 210,
        "title": "add server side preprocessing and postprocessing framework with python",
        "body": "https://github.com/PaddlePaddle/Serving/pull/213",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-26T22:15:13+00:00",
        "updated_at": "2020-03-02T07:50:35+00:00",
        "closed_at": "2020-03-02T07:50:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 214,
        "title": "support paddle_serving_server.web_serve launch command for simple http post and debug",
        "body": "https://github.com/PaddlePaddle/Serving/pull/213",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-28T10:04:14+00:00",
        "updated_at": "2020-03-02T07:50:52+00:00",
        "closed_at": "2020-03-02T07:50:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 211,
        "title": "请问现在有官方的dockerfile了吗?",
        "body": "如题，如果有的话，能否提供一下，文档里的链接点进去是404呀~",
        "state": "closed",
        "user": "wwjjy",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-27T07:42:27+00:00",
        "updated_at": "2020-03-20T02:04:33+00:00",
        "closed_at": "2020-03-20T02:04:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "Just-D",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 217,
        "title": "Model Ensemble Example",
        "body": "## Take text classification as an example\r\n- train models with CNN/BOW/LSTM encoder with the same text input.\r\n- build a paddle serving service with three models in one service.\r\n- try to parallelize the inference and get the ensemble model score.\r\n- write publishable documents in English and Chinese for this feature.\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/450",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-29T14:41:04+00:00",
        "updated_at": "2020-04-23T03:50:16+00:00",
        "closed_at": "2020-04-23T03:50:16+00:00",
        "comments_count": [],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 218,
        "title": "A/B Test Example",
        "body": "Take the models trained in https://github.com/PaddlePaddle/Serving/issues/217, and deploy two of the models as A/B test.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "barrierye",
        "created_at": "2020-02-29T14:49:46+00:00",
        "updated_at": "2020-04-01T13:22:47+00:00",
        "closed_at": "2020-04-01T13:22:02+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 219,
        "title": "Criteo CTR model with Dist KV",
        "body": "train a criteo CTR model and serving the model with dist kv op for sparse features.\r\n\r\nMotivation of this tutorial:\r\n- show how to use dist kv op in rpc service\r\n- show how to config built-in operators on server side.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-29T14:51:08+00:00",
        "updated_at": "2020-04-05T04:52:44+00:00",
        "closed_at": "2020-04-05T04:52:44+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 222,
        "title": "Inference of two models with dependencies example",
        "body": "Build a serving example with two models\r\n- the score of model A is the input of model B\r\n- Train model A and model B with Criteo dataset on CTR DNN\r\n- Model A is only trained with dense features\r\n- Model B is trained with sparse features and the score from odel A.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-01T00:21:31+00:00",
        "updated_at": "2020-05-08T13:20:23+00:00",
        "closed_at": "2020-05-08T13:20:23+00:00",
        "comments_count": [],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 220,
        "title": "Add an image representation service Example",
        "body": "add an image representation service with general serving.\r\n- Model: Resnet50 based on Imagenet dataset\r\n- write a publishable tutorial for the serving part given a trained model with paddle.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-29T15:04:07+00:00",
        "updated_at": "2020-04-05T04:53:08+00:00",
        "closed_at": "2020-04-05T04:53:08+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 223,
        "title": "Knowledge Distilling Example",
        "body": "Build a knowledge distilling example, teachers are hosted on server side, and students are training agent.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-01T01:07:53+00:00",
        "updated_at": "2024-04-16T09:04:32+00:00",
        "closed_at": "2024-04-16T09:04:32+00:00",
        "comments_count": [],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 221,
        "title": "LAC as a NLP serving example",
        "body": "add a word segmentation example\r\n- reuse the model from jieba\r\n- a user's input is a Chinese sentence.\r\n- write preprocess logic on web service side.\r\n- serve word segmentation model with RPCService.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-02-29T15:22:39+00:00",
        "updated_at": "2020-04-05T04:39:17+00:00",
        "closed_at": "2020-04-05T04:39:17+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 224,
        "title": "execute a serving server with configs only",
        "body": "server export a config includes transpiled server and client config, as well as workdir.\r\nit will be user-friendly if a server can start up with only config and workdir.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-02T09:01:01+00:00",
        "updated_at": "2024-04-16T09:04:32+00:00",
        "closed_at": "2024-04-16T09:04:32+00:00",
        "comments_count": [],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 226,
        "title": "/doc/DESIGN.md中某段缺少句号，可能让人理解困难",
        "body": "> 3.3 整体设计：\r\n> 用户通过Python Client启动Client和Server，Python API有检查互联和待访问模型是否匹配的功能 Python API背后调用的是Paddle Serving实现的client和server对应功能的pybind，互传的信息通过RPC实现 Client Python API当前有两个简单的功能，load_inference_conf和predict，分别用来执行加载待预测的模型和预测 Server Python API主要负责加载预估模型，以及生成Paddle Serving需要的各种配置，包括engines，workflow，resource等\r\n\r\n建议添加句号或其他更好的排版：\r\n\r\n> 用户通过Python Client启动Client和Server，Python API有检查互联和待访问模型是否匹配的功能。Python API背后调用的是Paddle Serving实现的client和server对应功能的pybind，互传的信息通过RPC实现。Client Python API当前有两个简单的功能，load_inference_conf和predict，分别用来执行加载待预测的模型和预测。Server Python API主要负责加载预估模型，以及生成Paddle Serving需要的各种配置，包括engines，workflow，resource等",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-02T11:45:34+00:00",
        "updated_at": "2020-04-01T13:14:31+00:00",
        "closed_at": "2020-04-01T13:14:31+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 231,
        "title": "/python/examples/criteo_ctr/get_data.sh 文件中URL未解码，导致脚本运行错误",
        "body": "原文件第一行\r\n> wget https://paddle-serving.bj.bcebos.com/data%2Fctr_prediction%2Fctr_data.tar.gz\r\n\r\n应该为\r\n> wget https://paddle-serving.bj.bcebos.com/data/ctr_prediction/ctr_data.tar.gz\r\n\r\n原脚本运行下载文件为：data%2Fctr_prediction%2Fctr_data.tar.gz，无法顺利运行。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-03T07:08:23+00:00",
        "updated_at": "2020-03-26T15:33:44+00:00",
        "closed_at": "2020-03-26T15:33:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 230,
        "title": "Text classification with IMDB",
        "body": "To show how to use user-defined web service to host a text classification model",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-03T05:38:13+00:00",
        "updated_at": "2020-04-05T04:38:49+00:00",
        "closed_at": "2020-04-05T04:38:49+00:00",
        "comments_count": [
            "guru4elephant",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 242,
        "title": "编译时Golang需要>=1.9，原文档写的是>=1.8",
        "body": "编译时候遇到\r\n```bash\r\nundefined: sync.Map\r\n```\r\n> sync.Map is only available in go 1.9 and higher.",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-05T11:23:18+00:00",
        "updated_at": "2020-03-16T02:06:06+00:00",
        "closed_at": "2020-03-16T02:06:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 239,
        "title": "多线程timeline",
        "body": "timeline当前在多线程下无法区分线程级别的数据，需要增加request id信息",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-03-04T11:33:37+00:00",
        "updated_at": "2020-03-06T11:01:52+00:00",
        "closed_at": "2020-03-06T11:01:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 234,
        "title": "/python/examples/fit_a_line 例子在client启动时会出core",
        "body": "复现如下：\r\n1. cd python/examples/fit_a_line && sh ./get_data.sh\r\n2. python test_client.py serving_client_conf/serving_client_conf.prototxt\r\n\r\n进只有一句`segmentation fault`信息\r\n> [1]    46374 segmentation fault  python test_client.py serving_client_conf/serving_client_conf.prototxt\r\n\r\n\r\n在`<python-site-packages-path>/paddle_serving_client/__init__.py`加断点如下：\r\n![image](https://user-images.githubusercontent.com/28446721/75775080-31c4b100-5d8c-11ea-8ada-db0bf3f699ff.png)\r\n报错信息如下：\r\n![image](https://user-images.githubusercontent.com/28446721/75775154-4ef97f80-5d8c-11ea-81e5-448194e6dd51.png)\r\n应该是python端加载serving_client的时候出core",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-03T12:21:06+00:00",
        "updated_at": "2020-04-05T04:50:25+00:00",
        "closed_at": "2020-04-05T04:50:25+00:00",
        "comments_count": [
            "barrierye",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 246,
        "title": "在Travis-CI中编译遇到libbrpc.a链接错误",
        "body": "相关环境：\r\n- ubuntu 14.04.5\r\n- cmake 3.9.2\r\n- gcc 4.8.4\r\n- go 1.14\r\n- python 2.7.17\r\n\r\n部分报错信息：\r\n```bash\r\n[ 60%] Built target general_model_config_py_proto\r\n[ 65%] Built target configure\r\n[ 65%] Built target server_config_py_proto_init\r\nCopy generated python proto into directory paddle_serving_server/proto.\r\n[ 65%] Built target server_config_py_proto\r\n[ 66%] Built target test_configure\r\n[ 68%] Built target pdcodegen\r\n[ 83%] Built target sdk-cpp\r\n[ 93%] Built target pdserving\r\n[ 94%] Built target fluid_cpu_engine\r\n[ 95%] Built target utils\r\n[ 96%] Linking CXX executable serving\r\n../../third_party/install/brpc/lib/libbrpc.a(iobuf.cpp.o): In function `butil::IOBuf::cut_into_SSL_channel(ssl_st*, int*)':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1025: undefined reference to `SSL_write'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1029: undefined reference to `SSL_get_error'\r\n../../third_party/install/brpc/lib/libbrpc.a(iobuf.cpp.o): In function `butil::IOBuf::cut_multiple_into_SSL_channel(ssl_st*, butil::IOBuf* const*, unsigned long, int*)':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1066: undefined reference to `SSL_get_wbio'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1067: undefined reference to `BIO_ctrl'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1068: undefined reference to `BIO_ctrl'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1069: undefined reference to `BIO_fd_non_fatal_error'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1050: undefined reference to `BIO_fd_non_fatal_error'\r\n../../third_party/install/brpc/lib/libbrpc.a(iobuf.cpp.o): In function `butil::IOPortal::append_from_SSL_channel(ssl_st*, int*, unsigned long)':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1719: undefined reference to `SSL_read'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1720: undefined reference to `SSL_get_error'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/butil/iobuf.cpp:1735: undefined reference to `BIO_fd_non_fatal_error'\r\n../../third_party/install/brpc/lib/libbrpc.a(controller.cpp.o): In function `brpc::Controller::AppendServerIdentiy()':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/brpc/controller.cpp:353: undefined reference to `MD5'\r\n../../third_party/install/brpc/lib/libbrpc.a(rtmp_protocol.cpp.o): In function `brpc::policy::adobe_hs::openssl_HMACsha256(void const*, int, void const*, int, void*)':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/brpc/policy/rtmp_protocol.cpp:167: undefined reference to `HMAC'\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/brpc/policy/rtmp_protocol.cpp:158: undefined reference to `EVP_Digest'\r\n../../third_party/install/brpc/lib/libbrpc.a(socket.cpp.o): In function `brpc::Socket::SSLHandshake(int, bool)':\r\n/home/travis/build/barrierye/Serving/build/third_party/brpc/src/extern_brpc/src/brpc/socket.cpp:1892: undefined reference to `SSL_free'\r\n......\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [core/general-server/serving] Error 1\r\nmake[1]: *** [core/general-server/CMakeFiles/serving.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-06T05:48:55+00:00",
        "updated_at": "2020-03-20T02:12:43+00:00",
        "closed_at": "2020-03-20T02:12:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 232,
        "title": "/python/examples/criteo_ctr/local_train.py 基于新版paddle提示警告",
        "body": "在新版paddle中添加了一个auc op相关的警告，见https://github.com/PaddlePaddle/Paddle/pull/19838\r\n> please ensure that you have set the auc states to zeros before saving inference model",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-03T11:29:40+00:00",
        "updated_at": "2020-03-05T14:35:27+00:00",
        "closed_at": "2020-03-05T14:35:27+00:00",
        "comments_count": [
            "barrierye",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 247,
        "title": "/doc/GETTING_STARTED.md \b与现在完全不一致建议直接删除，用/python/examples中各个样例的README替代",
        "body": "文件：https://github.com/barrierye/Serving/blob/develop/doc/GETTING_STARTED.md\r\n目前develop代码在编译时貌似不再支持/doc/INSTALL.md中的`make install`命令：\r\n```bash\r\n/kv_manager.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/include/core/predictor/framework/infer_data.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/include/core/predictor/framework/channel.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/include/core/predictor/framework/dag.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/include/core/predictor/mempool/mempool.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/include/core/predictor/op/op.h\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/demo/serving/bin/serving\r\n-- Up-to-date: /home/barriery/Serving/build-server/output/lib/libfluid_cpu_engine.a\r\n-- Installing: /usr/local/opt/serving_server/share/wheels\r\nCMake Error at python/cmake_install.cmake:36 (file):\r\n  file INSTALL cannot make directory\r\n  \"/usr/local/opt/serving_server/share/wheels\": No such file or directory\r\nCall Stack (most recent call first):\r\n  cmake_install.cmake:39 (include)\r\n\r\n\r\nmake: *** [install] 错误 1\r\n```\r\n这导致本文件在现有代码基础上完全无法复现，建议用/python/examples中各个样例的README替代GETTING_STARTED",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-06T06:40:50+00:00",
        "updated_at": "2020-03-23T08:56:58+00:00",
        "closed_at": "2020-03-23T08:56:57+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 250,
        "title": "serve.py对于GPU直接占用所有卡，每卡一个进程",
        "body": "python -m paddle_serving_server.serve --model bert --gpuids 0,1,2,3 --port 9292",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-06T08:49:03+00:00",
        "updated_at": "2020-03-12T13:35:41+00:00",
        "closed_at": "2020-03-12T13:35:41+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 251,
        "title": "在Travis-CI中编译遇到serving_client.so链接错误",
        "body": "相关环境：\r\n- ubuntu 14.04.5\r\n- cmake 3.9.2\r\n- gcc 4.8.4\r\n- python 2.7.17\r\n- go 1.14\r\n\r\n报错信息如下：\r\n```bash\r\n$ make\r\n[  6%] Built target extern_boost\r\n[ 12%] Built target extern_snappy\r\n[ 18%] Built target extern_leveldb\r\n[ 19%] Built target boost\r\n[ 25%] Built target extern_zlib\r\n[ 31%] Built target extern_gflags\r\n[ 37%] Built target extern_glog\r\n[ 43%] Built target extern_protobuf\r\n[ 49%] Built target extern_brpc\r\n[ 56%] Built target extern_pybind\r\n[ 56%] Built target general_model_config_py_proto_init\r\nCopy generated general_model_config proto file into directory paddle_serving_client/proto.\r\n[ 57%] Built target general_model_config_py_proto\r\n[ 64%] Built target configure\r\n[ 64%] Built target sdk_configure_py_proto_init\r\n[ 65%] Built target test_configure\r\nCopy generated python proto into directory paddle_serving_client/proto.\r\n[ 65%] Built target sdk_configure_py_proto\r\n[ 69%] Built target pdcodegen\r\n[ 95%] Built target sdk-cpp\r\n[ 97%] Built target utils\r\n[ 98%] Linking CXX shared module serving_client.so\r\n/usr/bin/ld: ../../../reliance/python/lib/libpython2.7.a(getbuildinfo.o): relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC\r\n../../../reliance/python/lib/libpython2.7.a(getbuildinfo.o): error adding symbols: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [core/general-client/serving_client.so] Error 1\r\nmake[1]: *** [core/general-client/CMakeFiles/serving_client.dir/all] Error 2\r\nmake: *** [all] Error 2\r\nThe command \"make\" exited with 2.\r\n```\r\n似乎要加上`-fPIC`重新编译，但之前没有遇到过这种情况。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-06T09:22:57+00:00",
        "updated_at": "2020-03-06T09:51:16+00:00",
        "closed_at": "2020-03-06T09:51:16+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 252,
        "title": "dist kv op based ctr auc test.",
        "body": "",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-06T11:04:03+00:00",
        "updated_at": "2020-03-20T11:32:59+00:00",
        "closed_at": "2020-03-20T11:32:59+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 254,
        "title": "support python 3",
        "body": "",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-03-07T14:58:39+00:00",
        "updated_at": "2020-03-17T02:19:07+00:00",
        "closed_at": "2020-03-17T02:19:07+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 253,
        "title": "编译Server代码的时候有时候会报错（概率性出现）：no Go files in /Serving/core/cube/cube-transfer/src/github.com/mipearson/rfw",
        "body": "cmake选项：\r\n```bash\r\ncmake -DPYTHON_INCLUDE_DIR=$PYTHONROOT/include/python2.7/ \\\r\n      -DPYTHON_LIBRARIES=$PYTHONROOT/lib/libpython2.7.a \\\r\n      -DPYTHON_EXECUTABLE=$PYTHONROOT/bin/python \\\r\n      -DCLIENT_ONLY=OFF ..\r\n```\r\n\r\n编译命令：\r\n```bash\r\nmake -j4 > /dev/null\r\n```\r\n\r\n报错信息：\r\n```bash\r\nNote: checking out '1.1.7'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at b02bfa7... Tag open source release 1.1.7.\r\nNote: checking out '1.8.4'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at ddabf50... 1.8.4; soversion=20\r\nNote: checking out 'v2.2.4'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at 9a19306... bump version to 2.2.4\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n    BUILD_TESTING\r\nNote: checking out 'v1.2.8'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at 5089329... zlib 1.2.8\r\nNote: checking out '77592648e3f3be87d6c7123eb81cbad75f9aef5a'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at 7759264... repair wrong namespace problem\r\nNote: checking out '9f75c5aa851cd877fb0d93ccc31b8567a6706546'.\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  git checkout -b new_branch_name\r\nHEAD is now at 9f75c5a... Merge pull request #2337 from sergiocampama/deprecation\r\n/Serving/core/cube/cube-transfer/src/github.com/Badangel/logex/file_logger.go:22:2: no Go files in /Serving/core/cube/cube-transfer/src/github.com/mipearson/rfw\r\nmake[2]: *** [core/cube/cube-transfer/CMakeFiles/logex] Error 1\r\nmake[1]: *** [core/cube/cube-transfer/CMakeFiles/logex.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\nmake: *** [all] Error 2\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-03-07T09:29:46+00:00",
        "updated_at": "2020-03-10T04:39:56+00:00",
        "closed_at": "2020-03-10T04:39:56+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 259,
        "title": "install and run auth do not match",
        "body": "![image](https://user-images.githubusercontent.com/35550832/76198225-a2eee300-6228-11ea-8697-e00c9526b1e7.png)\r\n\r\nA user may install paddle-serving-server with root auth, but run with user auth. The serving binary can not be written on disk in this case, should report an friendly error.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-03-09T09:09:49+00:00",
        "updated_at": "2020-03-17T02:21:30+00:00",
        "closed_at": "2020-03-17T02:21:30+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 258,
        "title": "无法下载paddle-serving-server",
        "body": "pip install paddle-serving-server\r\nCollecting paddle-serving-server\r\n  Could not find a version that satisfies the requirement paddle-serving-server (from versions: )\r\nNo matching distribution found for paddle-serving-server",
        "state": "closed",
        "user": "RockWater",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-09T02:05:30+00:00",
        "updated_at": "2020-03-20T02:10:17+00:00",
        "closed_at": "2020-03-20T02:10:17+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "RockWater",
            "MRXLT",
            "linhandev",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 262,
        "title": "ctriteo ctr client出现如下错",
        "body": "![image](https://user-images.githubusercontent.com/23625746/76236068-131c5980-6267-11ea-8b1c-756eda4a30f5.png)\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-09T16:47:05+00:00",
        "updated_at": "2020-04-05T04:03:33+00:00",
        "closed_at": "2020-04-05T04:03:33+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 264,
        "title": "已经安装了paddle1.7 gpu版本，安装client时仍然强制安装cpu版本的paddle1.7",
        "body": "如题\r\n![image](https://user-images.githubusercontent.com/14270174/76295997-7a322080-62f0-11ea-8a74-09eee27b5bc5.png)\r\n",
        "state": "closed",
        "user": "littletomatodonkey",
        "closed_by": "MRXLT",
        "created_at": "2020-03-10T08:59:30+00:00",
        "updated_at": "2020-03-16T10:00:46+00:00",
        "closed_at": "2020-03-16T10:00:46+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 270,
        "title": "switch between inference pass and no pass",
        "body": "sometimes, an inference pass should be close for debugging, current inference library can not be closed by serving server.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-11T08:50:47+00:00",
        "updated_at": "2024-04-16T09:04:33+00:00",
        "closed_at": "2024-04-16T09:04:33+00:00",
        "comments_count": [],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 269,
        "title": "如何实现fl.equal结果转换数据类型",
        "body": "我想用fl.equal算子，进行计算，得到了一个类似于[True,True,False]的结果，然后我使用fl.reduce_sum计算这个结果的sum([True,True,False])，结果说reduce_sum需要输入不能是bool类型。请问这个要如何转。也就是把[True,false]转为[1,0]，让模型继续可导？能给个示例吗（我使用的是paddle静态图）",
        "state": "closed",
        "user": "leidongfeng",
        "closed_by": "leidongfeng",
        "created_at": "2020-03-11T05:15:57+00:00",
        "updated_at": "2020-03-12T10:21:27+00:00",
        "closed_at": "2020-03-12T10:21:27+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "leidongfeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 272,
        "title": "bug in paddle_serving_server_gpu/web_serve.py",
        "body": "the code \"service.run_server()\" on line 39\r\nshould be \"web_service.run_server()\"",
        "state": "closed",
        "user": "houj04",
        "closed_by": "MRXLT",
        "created_at": "2020-03-12T03:55:51+00:00",
        "updated_at": "2020-03-16T08:17:41+00:00",
        "closed_at": "2020-03-16T08:17:41+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 273,
        "title": "bug in paddle_serving_server_gpu/serve.py",
        "body": "line 66 is: gpus = os.environ[\"CUDA_VISIBLE_DEVICES\"]\r\nit needs \"import os\" first",
        "state": "closed",
        "user": "houj04",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-12T04:04:04+00:00",
        "updated_at": "2020-03-15T04:58:51+00:00",
        "closed_at": "2020-03-15T04:58:51+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 275,
        "title": "missing \"del feed[\"fetch\"]\" in gpu web server",
        "body": "in python/paddle_serving_server_gpu/web_service.py, at line 109, it seems the following two lines are missing, between \"self.preprocess\" and \"client_list[0].predict\":\r\n```\r\nif \"fetch\" in feed:\r\n    del feed[\"fetch\"]\r\n```",
        "state": "closed",
        "user": "houj04",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-12T06:14:48+00:00",
        "updated_at": "2020-03-15T04:58:34+00:00",
        "closed_at": "2020-03-15T04:58:34+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 278,
        "title": "start gpu server failed when CUDA_VISIBLE_DEVICES is not set",
        "body": "if the environment variable of CUDA_VISIBLE_DEVICES is not set, paddle_serving_server_gpu/serve.py will abort at line 67: gpus = os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "state": "closed",
        "user": "houj04",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-12T10:13:57+00:00",
        "updated_at": "2020-03-15T04:58:18+00:00",
        "closed_at": "2020-03-15T04:58:18+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 286,
        "title": "增加对多种CPU指令集的适配",
        "body": "背景：我在一台支持avx2指令集的机器上进行了编译和安装。然后将整个python包分发到其它机器上执行的时候报错\r\nIllegal instruction (core dumped)\r\n推测是分发的那批机器比较古老，cpu不支持avx2指令集。\r\n希望的操作：可以运行时自动检测指令集，或者编译的时候可以增加选项以选择支持的指令集。因为编译的机器确实可能和运行的机器cpu型号不同。",
        "state": "closed",
        "user": "houj04",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-15T10:42:37+00:00",
        "updated_at": "2020-05-08T13:21:48+00:00",
        "closed_at": "2020-05-08T13:21:48+00:00",
        "comments_count": [
            "houj04",
            "MRXLT"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 290,
        "title": "web service not support multi-gpu",
        "body": "当使用export CUDA_VISIBLE_DEVICES=0,1指定多卡时，web service无法实现多卡预测。\r\n原因1\r\n![image](https://user-images.githubusercontent.com/16594411/76817863-fff92300-683e-11ea-91e4-4703ce8cd188.png)\r\n脚本中对于使用多卡的情况做了限制，不支持多卡，仅使用client_list[0]进行预测。\r\n原因2\r\n当修改掉以上代码后，启动预测服务，gpu 0上的预测服务正常，gpu 1上的预测服务挂掉\r\n报错如下\r\n![image](https://user-images.githubusercontent.com/16594411/76817985-5a927f00-683f-11ea-8da8-1cda90809059.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-17T03:06:45+00:00",
        "updated_at": "2020-03-20T02:07:17+00:00",
        "closed_at": "2020-03-20T02:07:17+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 291,
        "title": "gpu web service not support gpu 1",
        "body": "当通过 export CUDA_VISIBLE_DEVICES=1指定使用gpu 1的时候，web service启动时报错\r\n\r\n![image](https://user-images.githubusercontent.com/16594411/76818196-ed331e00-683f-11ea-81a0-081e7f64bec5.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-17T03:07:17+00:00",
        "updated_at": "2020-03-20T02:07:30+00:00",
        "closed_at": "2020-03-20T02:07:30+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 298,
        "title": "服务化部署了人像分割服务之后偶尔会出现500 Internal Server Error 请协助解决",
        "body": "![image](https://user-images.githubusercontent.com/42615458/76968851-8d826300-6964-11ea-93da-ce9c71d91bcc.png)\r\n",
        "state": "closed",
        "user": "lesterhnu",
        "closed_by": "lesterhnu",
        "created_at": "2020-03-18T14:07:27+00:00",
        "updated_at": "2020-03-20T02:01:15+00:00",
        "closed_at": "2020-03-20T02:01:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "lesterhnu",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 305,
        "title": "自己训练的模型，如何转换成serving可以使用的模型",
        "body": "",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "mcl-stone",
        "created_at": "2020-03-20T07:20:04+00:00",
        "updated_at": "2020-03-25T12:24:20+00:00",
        "closed_at": "2020-03-25T12:24:20+00:00",
        "comments_count": [
            "github-actions[bot]",
            "mcl-stone",
            "MRXLT",
            "mcl-stone"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 318,
        "title": "imdb example中get_data.sh脚本提供的模型acc偏低",
        "body": "bow:\r\n```bash\r\n[23726] 0.800673305988 1\r\n[23726] 0.531164646149 0\r\n[23726] 0.516303598881 0\r\n[23726](total: 2084) acc: 0.754318618042\r\n```\r\ncnn:\r\n```bash\r\n[23837] 0.546180129051 1\r\n[23837] 0.540920376778 0\r\n[23837] 0.548328459263 0\r\n[23837](total: 2084) acc: 0.642514395393\r\n```\r\nlstm:\r\n```bash\r\n[23527] 0.499653309584 1\r\n[23527] 0.499804884195 0\r\n[23527] 0.499839872122 0\r\n[23527](total: 2084) acc: 0.49424184261\r\n```\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-23T03:05:38+00:00",
        "updated_at": "2024-04-16T09:04:34+00:00",
        "closed_at": "2024-04-16T09:04:34+00:00",
        "comments_count": [],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 316,
        "title": "web_serve会额外占用一个端口",
        "body": "发现使用python -m paddle_serving_server_gpu.web_serve的时候，除了会使用设置的port以外，还会占用port+1这个端口。\r\n在某些情况下用户无法获得连续的端口，例如在使用某些PaaS平台提供的动态端口的时候。\r\n希望可以让用户自己指定每一个能使用的端口号，而不是目前直接在port后面+1。",
        "state": "closed",
        "user": "houj04",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-23T01:14:32+00:00",
        "updated_at": "2024-03-05T06:48:28+00:00",
        "closed_at": "2024-03-05T06:48:28+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 317,
        "title": "编译文档需要更新，里面仍有旧的CLIENT_ONLY参数",
        "body": "这个commit把CLIENT_ONLY参数移除了，并增加了新的参数。\r\nhttps://github.com/PaddlePaddle/Serving/commit/e8e19b21775ea9aeb350c0a5050f07ffa6535a8a\r\n但是这个文档没有同步更新。\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE.md\r\n",
        "state": "closed",
        "user": "houj04",
        "closed_by": "barrierye",
        "created_at": "2020-03-23T03:01:47+00:00",
        "updated_at": "2020-03-24T09:21:44+00:00",
        "closed_at": "2020-03-24T09:21:43+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 324,
        "title": "Paddle Serving 0.2.0 在Python3.6 下server 和client安装异常",
        "body": "详见  [DLTP-4993](http://newicafe.baidu.com:80/issue/DLTP-4993/show?from=page)\r\n\r\n * BUG复现步骤：\r\n\r\n   *   docker pull hub.baidubce.com/ctr/paddleserving:0.1.3-gpu镜像\r\n\r\n   *   建立docker\r\n\r\n   *   安装anaconda 5.3.1\r\n\r\n   *   建立py3.6环境并 pip install paddlehub\r\n\r\n   *   执行\r\n\r\n       *  (1)  pip install paddle_serving_client\r\n\r\n       *  (2) pip install paddke_serving_server_gpu\r\n\r\n * 期望结果：\r\n\r\n\r\n按照安装逻辑，应该是client支持单独安装，也支持server安装时自动安装client\r\n\r\n上述（1）、（2）应当均可以正常安装\r\n * 实际结果：\r\n   *   上述（1）安装时: \r\n![图片](https://user-images.githubusercontent.com/52739577/77394101-7b2b7d80-6dd9-11ea-9fdb-8c8183ceb07f.png)\r\n   *   上述（2）安装时：\r\n![图片](https://user-images.githubusercontent.com/52739577/77394112-81215e80-6dd9-11ea-9b13-93645a8b4570.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "hysunflower",
        "closed_by": "hysunflower",
        "created_at": "2020-03-24T06:16:02+00:00",
        "updated_at": "2020-04-04T08:30:16+00:00",
        "closed_at": "2020-04-04T08:30:16+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 325,
        "title": "Paddle Serving 0.2.0 基础功能测试 imdb模型 README 文档错误",
        "body": "详见 [DLTP-5021](http://newicafe.baidu.com:80/issue/DLTP-5021/show?from=page)\r\n\r\nimdb 模型 README 页实例描述有误 [imdb](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/imdb)\r\n![图片](https://user-images.githubusercontent.com/52739577/77395257-25a4a000-6ddc-11ea-9ea8-5fd2cfd47307.png)\r\n\r\n",
        "state": "closed",
        "user": "hysunflower",
        "closed_by": "MRXLT",
        "created_at": "2020-03-24T06:31:42+00:00",
        "updated_at": "2020-03-26T05:41:14+00:00",
        "closed_at": "2020-03-26T05:41:14+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 332,
        "title": "示例错误集合",
        "body": "### bert示例\r\nbert_client.py 缺少importBertReader\r\nreadme中获取模型的命令缺少参数\r\n文档中的配置文件夹与实际文件夹名称不一致\r\n执行预测命令文件名与实际不一致，thread参数多余\r\n### ctr文档\r\n没有gpu预测服务的启动方法\r\n### imagenet示例\r\nget_model.sh解压文件名称错误\r\nclient执行预测时配置文件名错误\r\n### imdb示例\r\nreademe中使用的模型未保持一致\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-25T02:58:43+00:00",
        "updated_at": "2020-03-26T05:41:21+00:00",
        "closed_at": "2020-03-26T05:41:21+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 339,
        "title": "bert 示例启动http预测服务时错误",
        "body": "![image](https://user-images.githubusercontent.com/16594411/77526408-75ab6180-6ec5-11ea-84a4-a0fd465a5611.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-25T10:23:02+00:00",
        "updated_at": "2020-04-05T04:04:32+00:00",
        "closed_at": "2020-04-05T04:04:32+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 336,
        "title": "BERT IN 10 MINS DOC ERROR",
        "body": "![image](https://user-images.githubusercontent.com/16594411/77522945-00895d80-6ec0-11ea-9f8c-b80e3fa2a4c3.png)\r\n代码错误\r\n![image](https://user-images.githubusercontent.com/16594411/77522979-0a12c580-6ec0-11ea-85f0-6d0f68dbcfe0.png)\r\n运行时出错",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-25T09:43:04+00:00",
        "updated_at": "2020-03-26T05:41:31+00:00",
        "closed_at": "2020-03-26T05:41:31+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 340,
        "title": "bert 示例执行rpc预测服务时错误",
        "body": "![image](https://user-images.githubusercontent.com/16594411/77526541-bc995700-6ec5-11ea-8b1a-3c9c8f826ec1.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-25T10:23:49+00:00",
        "updated_at": "2020-03-26T06:16:09+00:00",
        "closed_at": "2020-03-26T06:16:09+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 342,
        "title": "client作为子进程时出错退出导致别的子进程Hang住",
        "body": "多个reader子进程连接不同的serving，当其中一个serving出错后（如显存超了），可能导致与之相连的reader子进程出错exit(-1)。\r\nhttps://github.com/PaddlePaddle/Serving/blob/21b998fa5cdb2efed045a7379cb366476366b683/core/general-client/src/general_model.cpp#L364-L367\r\n用多进程共享的Queue、Pipe时，如果子进程使用了锁或者信号量等共享对象，杀掉进程可能导致其它进程死锁。\r\n\r\n------\r\n**具体现象**\r\n如图，某一serving显存超限，挂了。\r\n![image](https://user-images.githubusercontent.com/10208305/77540608-579e2b00-6ede-11ea-8b81-c232236e61c7.png)\r\n\r\n导致与之相连的client获取inference结果出错，则client所在的子进程exit(-1)退出挂了。\r\n![image](https://user-images.githubusercontent.com/10208305/77540729-9502b880-6ede-11ea-9289-2f41a2f4b0a8.png)\r\n\r\n接着导致程序所使用的Queue出错死锁，出现图中的名场面。。\r\n![image](https://user-images.githubusercontent.com/10208305/77540897-db581780-6ede-11ea-8bfb-24a4976d5e53.png)\r\n\r\n**为防止reader一个子进程挂了，程序死锁的情况，需要将上面代码中的exit(-1)改成返回错误码，或者抛出异常，供调用端处理**",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-25T13:29:29+00:00",
        "updated_at": "2020-03-26T15:30:04+00:00",
        "closed_at": "2020-03-26T15:30:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 345,
        "title": "bert模型V100上GPU server 实际占卡和指定的GPU 卡号不一致",
        "body": "启动时指定使用4，5，6，7卡，但实际占用未0，1，2，3\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-26T06:20:03+00:00",
        "updated_at": "2020-03-31T09:07:33+00:00",
        "closed_at": "2020-03-31T09:07:33+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 350,
        "title": "参考编译文档从源码编译gpu版本失败",
        "body": "<img width=\"1104\" alt=\"编译GPU server\" src=\"https://user-images.githubusercontent.com/16594411/77850039-7cf2a800-7202-11ea-978e-20a87b6aaaf5.png\">\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-29T13:16:20+00:00",
        "updated_at": "2020-03-31T05:36:52+00:00",
        "closed_at": "2020-03-31T05:36:52+00:00",
        "comments_count": [
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 351,
        "title": "参考run in docker文档，gpu docker build失败",
        "body": "![image](https://user-images.githubusercontent.com/16594411/77850112-df4ba880-7202-11ea-9acb-2a51d48a8aa5.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "barrierye",
        "created_at": "2020-03-29T13:18:54+00:00",
        "updated_at": "2020-04-01T13:13:33+00:00",
        "closed_at": "2020-04-01T13:13:33+00:00",
        "comments_count": [
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "hysunflower",
            "barrierye",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 355,
        "title": "文档中名词书写不统一",
        "body": "文档中有多处名词不统一的问题\r\npaddle 与 paddlepaddle\r\npaddleserving  与paddle serving",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-29T13:41:04+00:00",
        "updated_at": "2020-04-05T04:38:13+00:00",
        "closed_at": "2020-04-05T04:38:13+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 352,
        "title": "参考run in docker文档，构建cpu镜像失败",
        "body": "Sending build context to Docker daemon 2.048 kB\r\nStep 1 : FROM centos:6.3\r\nPulling repository docker.io/library/centos\r\nCould not reach any registry endpoint",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "barrierye",
        "created_at": "2020-03-29T13:21:10+00:00",
        "updated_at": "2020-03-30T04:52:25+00:00",
        "closed_at": "2020-03-30T04:52:25+00:00",
        "comments_count": [
            "barrierye",
            "hysunflower"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 354,
        "title": "py3版本运行ctr示例失败",
        "body": "![image](https://user-images.githubusercontent.com/16594411/77850341-6fd6b880-7204-11ea-897b-2352cc5fc2f3.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-29T13:30:05+00:00",
        "updated_at": "2020-03-31T09:04:46+00:00",
        "closed_at": "2020-03-31T09:04:46+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 353,
        "title": "py3版本client运行imdb示例失败",
        "body": "\r\n![image](https://user-images.githubusercontent.com/16594411/77850318-4158dd80-7204-11ea-9cd6-98a32c9bc75b.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-03-29T13:28:53+00:00",
        "updated_at": "2024-04-16T09:04:35+00:00",
        "closed_at": "2024-04-16T09:04:35+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 356,
        "title": "多进程使用出错",
        "body": "与paddle共用时，在Process开的子进程里面连接serving服务会出错。在使用线程或者不与paddle共用时没有问题\r\n`client.connect([endpoint])`\r\n![image](https://user-images.githubusercontent.com/10208305/77866414-8d883a00-7265-11ea-9bc9-93d84b95ea46.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-30T01:07:49+00:00",
        "updated_at": "2020-05-08T13:21:12+00:00",
        "closed_at": "2020-05-08T13:21:12+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 360,
        "title": "server端(gpu)开多线程会下载多个 serving-gpu-0.1.3.tar.gz包",
        "body": "![image](https://user-images.githubusercontent.com/28446721/77879077-3c3f7100-728c-11ea-8862-773c8ee3fe74.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-30T05:42:36+00:00",
        "updated_at": "2020-04-04T14:59:34+00:00",
        "closed_at": "2020-04-04T14:59:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 357,
        "title": "【需求】predict支持numpy",
        "body": "现在predict只支持list，效率较低，需要支持numpy\r\nhttps://github.com/PaddlePaddle/Serving/blob/07fb167bb012b98602d589aae7fc57997d7e2f17/core/general-client/src/general_model.cpp#L135-L141",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "wangxicoding",
        "created_at": "2020-03-30T01:11:34+00:00",
        "updated_at": "2020-07-07T09:55:46+00:00",
        "closed_at": "2020-07-07T09:55:46+00:00",
        "comments_count": [
            "guru4elephant",
            "guru4elephant",
            "wangxicoding"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 363,
        "title": "Start bert_as_service fail.",
        "body": "Env:\r\nPaddlePaddle: 1.6.2\r\nPaddle_gpu_serving: 0.8.2\r\nplatform：Linux version 3.10.0_3-0-0-20 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) ) #1 SMP Thu May 24 14:41:54 CST 2018\r\n\r\nWhen execute：\r\n```python\r\nfrom paddle_gpu_serving.run import BertServer\r\nbs = BertServer(with_gpu=False)\r\nbs.with_model(\"ernie\")\r\nbs.run(port=8010)\r\n```\r\n\r\nException info:\r\n![image](https://user-images.githubusercontent.com/28444161/77892251-e62af780-72a4-11ea-93e3-6fdb1484f3b1.png)\r\n",
        "state": "closed",
        "user": "ShenYuhan",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-30T08:39:29+00:00",
        "updated_at": "2020-05-08T13:22:25+00:00",
        "closed_at": "2020-05-08T13:22:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 364,
        "title": "示例代码的py3兼容性问题",
        "body": "在执行py3版本的示例中遇到以下问题\r\nimdb示例\r\n![image](https://user-images.githubusercontent.com/16594411/77893989-3c993580-72a7-11ea-8e87-4f8766d941f9.png)\r\nimagenet示例\r\n![image](https://user-images.githubusercontent.com/16594411/77894035-4cb11500-72a7-11ea-8823-891d2d2176b3.png)\r\nctr示例\r\n![image](https://user-images.githubusercontent.com/16594411/77894060-563a7d00-72a7-11ea-9213-4f67cdc2201f.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-03-30T08:56:20+00:00",
        "updated_at": "2020-03-31T09:03:15+00:00",
        "closed_at": "2020-03-31T09:03:15+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 377,
        "title": "PaddleDetection训练出来的模型，如何转换成PaddlePaddle/Serving可以部署的方式，求例子，弄了好久了还是不行",
        "body": "",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "barrierye",
        "created_at": "2020-04-02T01:15:29+00:00",
        "updated_at": "2020-04-08T13:03:48+00:00",
        "closed_at": "2020-04-08T13:03:48+00:00",
        "comments_count": [
            "guru4elephant",
            "mcl-stone",
            "mcl-stone",
            "bjjwwang",
            "mcl-stone",
            "bjjwwang",
            "bjjwwang",
            "mcl-stone",
            "mcl-stone",
            "mcl-stone",
            "bjjwwang",
            "mcl-stone",
            "bjjwwang",
            "mcl-stone",
            "mcl-stone",
            "barrierye",
            "mcl-stone",
            "barrierye",
            "mcl-stone",
            "barrierye",
            "mcl-stone",
            "barrierye",
            "mcl-stone",
            "barrierye",
            "mcl-stone"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 382,
        "title": "按照编译文档编译Serving时出错",
        "body": "参考https://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE_CN.md\r\n错误如下\r\n![image](https://user-images.githubusercontent.com/16594411/78226323-22a56000-74fe-11ea-845e-21efd26d4c1c.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-02T08:22:56+00:00",
        "updated_at": "2020-04-05T04:37:55+00:00",
        "closed_at": "2020-04-05T04:37:55+00:00",
        "comments_count": [
            "hysunflower",
            "barrierye",
            "barrierye",
            "barrierye",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 366,
        "title": "[bug][需求] 一个进程可以建多个client连同一server",
        "body": "如下代码和图所示，一个进程建多个client连接同一server会出错。 目的是一个用于trainer_reader，在后台线程一直读取训练数据，另外一个client用于读取test数据。\r\n``` python\r\nfrom paddle_serving_client import Client\r\n\r\nclient0 = Client()\r\nclient0.load_client_config('ResNeXt101_32x16d_wsl_client_conf/serving_client_conf.prototxt')\r\nclient0.connect(['127.0.0.1:9999'])\r\n\r\nclient1 = Client()\r\nclient1.load_client_config('ResNeXt101_32x16d_wsl_client_conf/serving_client_conf.prototxt')\r\nclient1.connect(['127.0.0.1:9999'])\r\n```\r\n![image](https://user-images.githubusercontent.com/10208305/77904270-dd432180-72b6-11ea-9376-316d6b335023.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "guru4elephant",
        "created_at": "2020-03-30T10:50:57+00:00",
        "updated_at": "2020-04-03T13:45:51+00:00",
        "closed_at": "2020-04-03T13:45:51+00:00",
        "comments_count": [
            "wangxicoding"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 383,
        "title": "server端不支持centos6 ,ubuntu16,ubuntu 18",
        "body": "centos 6\r\n![image](https://user-images.githubusercontent.com/16594411/78227832-79139e00-7500-11ea-9b53-e9b8d57701c2.png)\r\n\r\n\r\nubuntu 16\r\n![image](https://user-images.githubusercontent.com/16594411/78227541-fdb1ec80-74ff-11ea-86cd-5838a7e6f713.png)\r\nubuntu 18\r\n![image](https://user-images.githubusercontent.com/16594411/78227578-0b677200-7500-11ea-8288-a5ae65908849.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-02T08:39:20+00:00",
        "updated_at": "2024-03-05T06:48:29+00:00",
        "closed_at": "2024-03-05T06:48:28+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 409,
        "title": "enable FP16 when serving.",
        "body": null,
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:32:23+00:00",
        "updated_at": "2024-04-16T09:04:36+00:00",
        "closed_at": "2024-04-16T09:04:36+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 398,
        "title": "GO_CLIENT DOC不能执行",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/IMDB_GO_CLIENT.md\r\n文档无法跑通",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-04T07:15:15+00:00",
        "updated_at": "2020-04-05T04:02:22+00:00",
        "closed_at": "2020-04-05T04:02:22+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 392,
        "title": "开10线程编译时遇到机器内存不足导致编译错误",
        "body": "机器总内存为4045240 kB，可用内存为 2716572 kB\r\n```shell\r\nc++: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://bugzilla.redhat.com/bugzilla> for instructions.\r\nmake[2]: *** [core/sdk-cpp/CMakeFiles/sdk-cpp.dir/bert_service.pb.cc.o] Error 4\r\nmake[2]: *** Waiting for unfinished jobs....\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-03T09:06:27+00:00",
        "updated_at": "2020-04-04T06:20:45+00:00",
        "closed_at": "2020-04-04T06:20:45+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 410,
        "title": "load balancing for multi gpu serving",
        "body": "we should have a load balancing mechanism for gpu serving. gpu memory is limited, it is not acceptable if multiple queries make gpu memory increase out of boundary.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:34:59+00:00",
        "updated_at": "2024-04-16T09:04:37+00:00",
        "closed_at": "2024-04-16T09:04:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 412,
        "title": "async prediction interface",
        "body": "async request should be supported.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:38:20+00:00",
        "updated_at": "2024-04-16T09:04:39+00:00",
        "closed_at": "2024-04-16T09:04:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 411,
        "title": "multiple language client supported.",
        "body": "we should support multiple client language for serving.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:36:18+00:00",
        "updated_at": "2024-04-16T09:04:38+00:00",
        "closed_at": "2024-04-16T09:04:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 413,
        "title": "auto-batching of queries in GPU serving",
        "body": "support auto-batching on server side.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:38:55+00:00",
        "updated_at": "2024-04-16T09:04:40+00:00",
        "closed_at": "2024-04-16T09:04:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 414,
        "title": "support customized operators",
        "body": null,
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T15:40:18+00:00",
        "updated_at": "2024-04-16T09:04:40+00:00",
        "closed_at": "2024-04-16T09:04:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 415,
        "title": "a rpc interface to unload a model",
        "body": null,
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-06T16:00:27+00:00",
        "updated_at": "2024-04-16T09:04:42+00:00",
        "closed_at": "2024-04-16T09:04:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 426,
        "title": "fit a line的web_serve API已废弃，但文档未改动",
        "body": "https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/fit_a_line\r\n\r\n这里的HTTP Service用到的paddle_serving_server.web_serve的API已经被废弃。\r\n文档应当尽速修改",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-08T05:12:49+00:00",
        "updated_at": "2020-04-08T16:29:28+00:00",
        "closed_at": "2020-04-08T16:29:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 416,
        "title": "变长序列输入导致Serving服务挂掉",
        "body": "如图，定义feed_var时，shape为(-1, max_seqlen, 1)也即(-1, 300, 1)。\r\n但在实际输入时seqlen会变化，输入可能是(100, seqlen=101, 1)  (100, seqlen=231, 1) (100, seqlen=300, 1) ...\r\n![image](https://user-images.githubusercontent.com/10208305/78622418-a624cf80-78b7-11ea-86cf-45480ea8efe5.png)\r\n\r\n这种情况下predict会导致Serving服务挂掉\r\n![image](https://user-images.githubusercontent.com/10208305/78623065-5f37d980-78b9-11ea-8ec3-06b80be554c0.png)\r\n\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "wangxicoding",
        "created_at": "2020-04-07T02:21:22+00:00",
        "updated_at": "2020-07-07T10:01:59+00:00",
        "closed_at": "2020-07-07T10:01:59+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 430,
        "title": "Travis-CI中paddlehub下载bert_chinese_L-12_H-768_A-12模型可能会超时",
        "body": "![image](https://user-images.githubusercontent.com/28446721/78794002-be930800-79e5-11ea-81c8-d5caf51b5906.png)\r\n\r\n可以考虑使用bos上保存的模型文件: https://paddle-serving.bj.bcebos.com/paddle_hub_models/text/SemanticModel/bert_chinese_L-12_H-768_A-12.tar.gz (max_seq_len=128)",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-04-08T14:10:33+00:00",
        "updated_at": "2020-04-23T08:09:28+00:00",
        "closed_at": "2020-04-23T08:09:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 424,
        "title": "client 获取fetch_var中的某几个var时出错",
        "body": "保存的模型中fetch var有10个，client预测时fetch参数中只指定第1，3，5个，预测时出现段错误",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-04-08T03:43:11+00:00",
        "updated_at": "2020-04-08T05:53:55+00:00",
        "closed_at": "2020-04-08T05:53:55+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 427,
        "title": "libcrypto.so.10缺少",
        "body": "gpu_ids not set, going to run cpu service.\r\nCUDA Version 10.0.130\r\nmkdir: cannot create directory 'workdir_-1': File exists\r\nGoing to Run Comand\r\n/usr/local/lib/python2.7/dist-packages/paddle_serving_server_gpu/serving-gpu-0.2.0/serving -enable_model_toolkit -inferservice_path workdir_-1 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 10 -port 9292 -reload_interval_s 10 -resource_path workdir_-1 -resource_file resource.prototxt -workflow_path workdir_-1 -workflow_file workflow.prototxt -bthread_concurrency 10 -gpuid 0 \r\n/usr/local/lib/python2.7/dist-packages/paddle_serving_server_gpu/serving-gpu-0.2.0/serving: error while loading shared libraries: libcudart.so.9.0: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "barrierye",
        "created_at": "2020-04-08T08:03:59+00:00",
        "updated_at": "2020-04-08T09:03:05+00:00",
        "closed_at": "2020-04-08T09:03:05+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 432,
        "title": "fetch的变量过大导致出错",
        "body": "batch_size=10。batch_size=1的fetch的数据量约为22.63MB，batch_size=10数据量约为226.3MB，too big data！\r\n![image](https://user-images.githubusercontent.com/10208305/78858436-0609aa80-7a5f-11ea-9217-4bb3fa0c9b7b.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "MRXLT",
        "created_at": "2020-04-09T04:43:33+00:00",
        "updated_at": "2020-04-22T02:52:31+00:00",
        "closed_at": "2020-04-22T02:52:31+00:00",
        "comments_count": [
            "wangxicoding",
            "MRXLT",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 434,
        "title": "导致Serving服务挂掉的场景",
        "body": "1、batch_size过大，显存爆掉\r\n\r\n2、传入空数据\r\n![image](https://user-images.githubusercontent.com/10208305/78865872-014df200-7a71-11ea-830d-0eddd08fc43d.png)\r\n\r\n3、非法、不合规数据？如目前不支持的变长seqlen\r\n\r\n服务需要健壮些，否则用户的某些操作一不小心就导致Serving挂掉了",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-09T06:52:11+00:00",
        "updated_at": "2024-03-05T06:48:29+00:00",
        "closed_at": "2024-03-05T06:48:29+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 433,
        "title": "batch_size为1返回的fetch_map_list为dict",
        "body": "fetch_map_list = client.predict(feed=feed_map_list, fetch=fetch)\r\n\r\n如果feed_map_list的len为1，即batch_size=1，则返回的fetch_map_list为 dict\r\n若feed_map_list的len > 1，即batch_size>1，则返回的fetch_map_list为列表 [dict, ..., dict]\r\n\r\n返回类型不一致，按照多batch时候的代码处理batch_size=1的场景会出错。希望可以把返回类型统一一下\r\n![image](https://user-images.githubusercontent.com/10208305/78864616-8a175e80-7a6e-11ea-90fa-6e362db5a6ba.png)\r\n![image](https://user-images.githubusercontent.com/10208305/78864658-a5826980-7a6e-11ea-81a2-5f3b47afa431.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "MRXLT",
        "created_at": "2020-04-09T06:32:06+00:00",
        "updated_at": "2020-04-22T02:57:49+00:00",
        "closed_at": "2020-04-22T02:57:49+00:00",
        "comments_count": [
            "guru4elephant",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 436,
        "title": "提供的编译docker里没有配置python3",
        "body": " * 如链接 [How to compile PaddleServing](https://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE.md)，，未配置py3\r\n * 在编好的dev 镜像安装anaconda 并创建3.6的环境后，编译过程出现\r\n<img width=\"1158\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/52739577/78876918-a7eebe80-7a82-11ea-990f-72a8ffc0e0a4.png\">\r\n<img width=\"638\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/52739577/78877167-01ef8400-7a83-11ea-850b-f3694a1cd0fa.png\">\r\n\r\n<img width=\"1072\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/52739577/78877058-d8cef380-7a82-11ea-973c-5314c05e110a.png\">\r\n\r\n编的时候除了最后的ERROR 中间没erro\r\n\r\n * 反复尝试后中间包了一次err ModuleNotFoundError: No module named 'google'\r\n，pip 好protobuf 后编过了",
        "state": "closed",
        "user": "hysunflower",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-09T08:55:22+00:00",
        "updated_at": "2020-04-10T04:36:50+00:00",
        "closed_at": "2020-04-10T04:36:50+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 441,
        "title": "Paddle Serving CPU做FasterRCNN检测超时20秒",
        "body": "## 复现步骤\r\n```\r\ndocker run --rm -dit --name pddet hub.baidubce.com/paddlepaddle/serving:0.2.0\r\ndocker exec -it pddet bash\r\nyum install -y libXext libSM libXrender\r\npip install paddlepaddle paddle_serving_server paddle_serving_client opencv-python ujson -i https://pypi.tuna.tsinghua.edu.cn/simple\r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/faster_rcnn_model.tar.gz\r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/paddle_serving_app-0.0.1-py2-none-any.whl\r\npip install -U paddle_serving_app-0.0.1-py2-none-any.whl -i https://pypi.tuna.tsinghua.edu.cn/simple\r\ntar xf faster_rcnn_model.tar.gz\r\ncd faster_rcnn_model\r\nGLOG_v=2 python -m paddle_serving_server.serve --model pddet_serving_model --port 9393\r\npython test_client.py pddet_client_conf/serving_client_conf.prototxt cpp_demo.yml 000000570688.jpg\r\n```\r\n\r\n## 错误日志\r\n### 客户端\r\n```\r\nW0410 05:31:08.978428 10701 predictor.hpp:129] inference call failed, message: [E1008]Reached timeout=20000ms @0.0.0.0:0\r\n```\r\n\r\n#### 服务器端\r\n我增加了输出Tensor的打印\r\n`multiclass_nms` 为 size `[96,6]`的 Tensor\r\n\r\nlog时间信息\r\n```\r\nI0410 05:30:49.026082 10691 op.cpp:158]  general_reader_op_time=[3462]\r\nI0410 05:31:40.752912 10691 op.cpp:158]  general_infer_op_time=[51726798]\r\nI0410 05:31:40.753150 10691 op.cpp:158]  general_response_op_time=[106]\r\nI0410 05:31:40.753165 10691 service.cpp:238] workflow total time: 51732726\r\nI0410 05:31:40.754063 10691 general_model_service.pb.cc:2481]  tc=[51738838]\r\nW0410 05:31:40.754109 10691 baidu_rpc_protocol.cpp:256] Fail to write into fd=9 SocketId=113@127.0.0.1:51334@9393: Unknown error 1014 [1014]\r\n```\r\n\r\n因此可以确认 服务端预测本身 没有问题，但20秒超时限制太短。",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-10T05:36:44+00:00",
        "updated_at": "2024-04-16T09:04:44+00:00",
        "closed_at": "2024-04-16T09:04:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 438,
        "title": "PaddleServing 保存模型 图片变长字段处理",
        "body": "模型是Paddle Detection训练出来的faster_rcnn_r50_1x，出错的流程如下\r\n-----------\r\n\r\n## client发送预测请求\r\n\r\ncv2打开图片转ndarray\r\n前处理为 [[image], [im_shape], [im_info]]\r\n然后把这个作为feed_dict\r\n{'image': image, 'im_shape': im_shape, 'im_info': im_info}\r\n其中 image是 ndarray shape为 (1228800,) 也就是 3，640，640被拉长成一维。\r\nim_shape是 list [640. ,640.  , 1.] \r\nim_info 是 list [480. ,640.   ,1.] \r\n\r\n-----------\r\n## server端处理出错\r\n进入到服务器端之后,开启GLOG_v=2\r\n卡了一段时间程序退出挂掉了，但没有屏幕打印的错误。 serving日志INFO的最后部分如下\r\n```\r\nI0409 08:29:39.147490 25438 dag_view.cpp:68] dag view initialized:\r\nnode id: 1\r\nnode name: general_reader_op\r\nnode type: GeneralReaderOp\r\nI0409 08:29:39.148098 25438 dag_view.cpp:48] stage[1] name: workflow1_1\r\nI0409 08:29:39.148114 25438 dag_view.cpp:49] stage[1] node size: 1\r\nI0409 08:29:39.148133 25438 dag_view.cpp:68] dag view initialized:\r\nnode id: 2\r\nnode name: general_infer_op\r\nnode type: GeneralInferOp\r\nI0409 08:29:39.148145 25438 dag_view.cpp:85] set op pre name:\r\ncurrent op name: general_infer_op previous op name: general_reader_op\r\nI0409 08:29:39.148154 25438 dag_view.cpp:48] stage[2] name: workflow1_2\r\nI0409 08:29:39.148162 25438 dag_view.cpp:49] stage[2] node size: 1\r\nI0409 08:29:39.148185 25438 dag_view.cpp:68] dag view initialized:\r\nnode id: 3\r\nnode name: general_response_op\r\nnode type: GeneralResponseOp\r\nI0409 08:29:39.148484 25438 dag_view.cpp:85] set op pre name:\r\ncurrent op name: general_response_op previous op name: general_infer_op\r\nI0409 08:29:39.148510 25438 general_reader_op.cpp:94] var num: 3\r\nI0409 08:29:39.148519 25438 general_reader_op.cpp:96] start to call load general model_conf op\r\nI0409 08:29:39.148526 25438 general_reader_op.cpp:100] get resource pointer done.\r\nI0409 08:29:39.148535 25438 general_reader_op.cpp:104] print general model config done.\r\nI0409 08:29:39.148545 25438 general_reader_op.cpp:125] var[0] has elem type: 1\r\nI0409 08:29:39.148561 25438 general_reader_op.cpp:143] shape for var[0]: 3\r\nI0409 08:29:39.148568 25438 general_reader_op.cpp:147] var[0] is tensor, capacity: 3\r\nI0409 08:29:39.148582 25438 general_reader_op.cpp:125] var[1] has elem type: 1\r\nI0409 08:29:39.148589 25438 general_reader_op.cpp:143] shape for var[1]: 3\r\nI0409 08:29:39.148597 25438 general_reader_op.cpp:147] var[1] is tensor, capacity: 3\r\nI0409 08:29:39.148605 25438 general_reader_op.cpp:125] var[2] has elem type: 1\r\nI0409 08:29:39.148612 25438 general_reader_op.cpp:143] shape for var[2]: 3\r\nI0409 08:29:39.148619 25438 general_reader_op.cpp:147] var[2] is tensor, capacity: 3\r\nI0409 08:29:39.148628 25438 general_reader_op.cpp:178] var[0] is tensor and capacity=3\r\nI0409 08:29:39.148635 25438 general_reader_op.cpp:178] var[1] is tensor and capacity=3\r\nI0409 08:29:39.148643 25438 general_reader_op.cpp:178] var[2] is tensor and capacity=3\r\n```\r\n--------\r\n## client在serving出错后的反应\r\n客户端直接 failed inference call\r\n```\r\ninference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=3 SocketId=1@127.0.0.1:9292@46642 [R1][E111]Fail to connect SocketId=8589934594@127.0.0.1:9292: Connection refused [R2][E112]Fail to select server from list://127.0.0.1:9292 lb=la\r\n```\r\n后面还有输入的浮点数信息，在此省略\r\n\r\n--------------\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-09T12:44:35+00:00",
        "updated_at": "2024-04-16T09:04:43+00:00",
        "closed_at": "2024-04-16T09:04:43+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 443,
        "title": "Paddle Detection GPU FasterRCNN 报错 CUDA9",
        "body": "## 复现步骤\r\n```\r\ndocker run --rm -dit --name pddet hub.baidubce.com/paddlepaddle/serving:0.2.0-gpu\r\ndocker exec -it pddet bash\r\nyum install -y libXext libSM libXrender\r\npip install paddlepaddle paddle_serving_server_gpu paddle_serving_client opencv-python ujson -i https://pypi.tuna.tsinghua.edu.cn/simple\r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/faster_rcnn_model.tar.gz\r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/paddle_serving_app-0.0.1-py2-none-any.whl\r\npip install -U paddle_serving_app-0.0.1-py2-none-any.whl -i https://pypi.tuna.tsinghua.edu.cn/simple\r\ntar xf faster_rcnn_model.tar.gz\r\ncd faster_rcnn_model\r\nGLOG_v=2 python -m paddle_serving_server_gpu.serve --model pddet_serving_model --port 9393 --gpu_id 0\r\npython test_client.py pddet_client_conf/serving_client_conf.prototxt cpp_demo.yml 000000570688.jpg\r\n```\r\n\r\nCPU版本可以运行成功，但是GPU版本会出现如下报错 CUDA 9.0，就是CUDA版本对不上的错 \r\n![image](https://user-imagpses.githubusercontent.com/23625746/78987650-3174be00-7b61-11ea-9891-7e7a79175b84.png)\r\n\r\n我手动换成了CUDA10，出现如下报错\r\n```\r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/framework.py\", line 2525, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layers/detection.py\", line 2814, in generate_proposals\r\n    'RpnRoiProbs': rpn_roi_probs})\r\n  File \"/PaddleDetection/ppdet/core/workspace.py\", line 150, in partial_apply\r\n    return op(*args, **kwargs_)\r\n  File \"/PaddleDetection/ppdet/modeling/anchor_heads/rpn_head.py\", line 177, in get_proposals\r\n    variances=self.anchor_var)\r\n  File \"/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py\", line 100, in build\r\n    rois = self.rpn_head.get_proposals(body_feats, im_info, mode=mode)\r\n  File \"/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py\", line 248, in test\r\n    return self.build(feed_vars, 'test')\r\n  File \"tools/export_model.py\", line 108, in main\r\n    test_fetches = model.test(feed_vars)\r\n  File \"tools/export_model.py\", line 125, in <module>\r\n    main()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: The index of gather_op should not be empty when the index's rank is 1.\r\n  [Hint: Expected index.dims()[0] > 0, but received index.dims()[0]:0 <= 0:0.] at (/paddle/paddle/fluid/operators/gather.cu.h:82)\r\n  [operator < generate_proposals > error]`\r\n```\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "bjjwwang",
        "created_at": "2020-04-10T11:29:32+00:00",
        "updated_at": "2020-07-01T08:08:39+00:00",
        "closed_at": "2020-07-01T08:08:39+00:00",
        "comments_count": [],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 440,
        "title": "图像分割的预置模型",
        "body": "能否支持 https://github.com/PaddlePaddle/PaddleSeg 下的模型服务，支持多卡远程预测服务等功能",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-10T04:27:45+00:00",
        "updated_at": "2020-05-08T07:52:15+00:00",
        "closed_at": "2020-05-08T07:52:15+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 444,
        "title": "C++端异常退出（段错误）时，python端直接退出不显示C++报错信息",
        "body": "执行相同逻辑遇到段错误，python端没有显示C++的报错信息：\r\n```shell\r\nGLOG_v=2 python server.py\r\n```\r\n![image](https://user-images.githubusercontent.com/28446721/79005515-98a66880-7b89-11ea-9da7-7b085d6731f4.png)\r\n```shell\r\nGLOG_v=2 /data/barriery/Serving/build-server/core/general-server/serving -enable_model_toolkit -inferservice_path work_dir1 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 8080 -reload_interval_s 10 -resource_path work_dir1 -resource_file resource.prototxt -workflow_path work_dir1 -workflow_file workflow.prototxt -bthread_concurrency 4\r\n```\r\n![image](https://user-images.githubusercontent.com/28446721/79005589-c7244380-7b89-11ea-9064-ab6fe0fd0853.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-10T16:17:19+00:00",
        "updated_at": "2024-03-05T06:48:30+00:00",
        "closed_at": "2024-03-05T06:48:30+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 452,
        "title": "add profiler module to paddle_serving_app",
        "body": "we need a profiler that is programable, for example\r\n``` python\r\nwith paddle_serving_app.profiler.profile() as prof:\r\n        fetch_map = client.predict(feed=feed_dict, fetch = fetch_list, profiler=prof)\r\n\r\nprof.print_info()\r\nprof.save_trace_file(\"detection.trace\")\r\n```",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-14T15:49:43+00:00",
        "updated_at": "2024-04-16T09:04:45+00:00",
        "closed_at": "2024-04-16T09:04:45+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 445,
        "title": "Serving性能相关问题",
        "body": "![image](https://user-images.githubusercontent.com/10208305/79088296-c56aa380-7d74-11ea-8549-95ef3b0f1ab8.png)\r\n\r\n见之前issue https://github.com/PaddlePaddle/Serving/issues/432 ，batch_size=1时，fetch的数据量约为22.63MB。profile图中共有三个连接进程，每个batch_size=1。\r\n可能由于fetch数据量较大，`postpro`部分占用时间较长，`client_infer`后面也出现较长的空挡，不知道是否是网络传输的空挡。\r\n进程间不同通信的空挡较大可能是由于fetch数据较大，进程处理数据 + OutPutQueue.put() + InputQueue.get() 耗时较长造成的",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-13T03:01:00+00:00",
        "updated_at": "2024-03-05T06:48:31+00:00",
        "closed_at": "2024-03-05T06:48:31+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 446,
        "title": "client只支持在自己线程内使用？",
        "body": "client在主线程中定义连接server，开启线程池用主线程中定义的client predict，出现如下错误。目前client只支持在定义的线程内使用？\r\n![image](https://user-images.githubusercontent.com/10208305/79100058-16d85a00-7d98-11ea-9d6f-aaed5b2af926.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "wangxicoding",
        "created_at": "2020-04-13T07:08:56+00:00",
        "updated_at": "2020-07-07T10:03:52+00:00",
        "closed_at": "2020-07-07T09:52:18+00:00",
        "comments_count": [
            "MRXLT",
            "wangxicoding"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 453,
        "title": "add traffic monitor for service",
        "body": "given a service, we need to observe the real time traffic of service.",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-14T16:09:01+00:00",
        "updated_at": "2024-04-16T09:04:45+00:00",
        "closed_at": "2024-04-16T09:04:45+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 447,
        "title": "创建多个client似乎没有并发",
        "body": "一个进程中创建多个线程，每个线程启动一个client连接同一个server，出现如下log，各个线程的client共用底层的链接？\r\n![image](https://user-images.githubusercontent.com/10208305/79107507-10051380-7da7-11ea-90d2-bcac56e46c4b.png)\r\n\r\n进行profile，图中开了三个进程，每个进程开两个线程连接同一server，不同进程连接不同server。从图看来同一线程里的多个client没有并发起来。\r\n![image](https://user-images.githubusercontent.com/10208305/79107826-adf8de00-7da7-11ea-8096-c3ea3bf384b3.png)\r\n\r\n速度测试，一个进程一个线程的速度和一个进程两个线程的速度 都为5.79s/step。对同一server开启多个线程连接没有速度上的提升。",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-13T09:04:05+00:00",
        "updated_at": "2021-07-02T08:29:33+00:00",
        "closed_at": "2020-04-17T22:29:24+00:00",
        "comments_count": [
            "barrierye",
            "wangxicoding",
            "barrierye",
            "barrierye",
            "Karenlyw"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 461,
        "title": "SERVER_DAG.md needs to be updated",
        "body": "#450 中加入了op_graph_maker，需要更新SERVER_DAG.md文档。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-17T07:36:14+00:00",
        "updated_at": "2020-04-23T03:06:56+00:00",
        "closed_at": "2020-04-23T03:06:56+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 468,
        "title": "client端C++接口暴露",
        "body": "batch_predict接口的C++版本暴露，方便用户可以联编",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-22T08:53:56+00:00",
        "updated_at": "2024-04-16T09:04:46+00:00",
        "closed_at": "2024-04-16T09:04:46+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 464,
        "title": "PaddleDetection训练好的模型，如何通过Serving部署在服务器上，然后通过http/rpc的方式使用",
        "body": "",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-21T05:45:45+00:00",
        "updated_at": "2020-05-08T13:19:25+00:00",
        "closed_at": "2020-05-08T13:19:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "omtbreak",
            "guru4elephant",
            "omtbreak",
            "guru4elephant",
            "omtbreak",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 486,
        "title": "client缺少python端predict的性能profile",
        "body": "https://github.com/PaddlePaddle/Serving/blob/f54717396f7cc161046e38b117458ccd5729957b/python/paddle_serving_client/__init__.py#L260-L262\r\n目前在predict在Python端的数据preprocess和postprocess缺少profile，需要补上。\r\n更详细可看ISSUE https://github.com/PaddlePaddle/Serving/issues/488",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "wangxicoding",
        "created_at": "2020-04-26T04:09:50+00:00",
        "updated_at": "2020-04-27T04:30:52+00:00",
        "closed_at": "2020-04-27T04:30:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 487,
        "title": "predict接口增加timeline信息打点",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_client/__init__.py#L269\r\n此处代码由于增加了numpy array的构建，性能比较低，需要加入打点进行Profile，方便后续优化",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-26T04:18:37+00:00",
        "updated_at": "2020-05-08T13:16:43+00:00",
        "closed_at": "2020-05-08T13:16:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 488,
        "title": "新版serving client端性能需优化点",
        "body": "新版client采用numpy，从下面的distill_reader的profile结果可以看出，目前reader所占耗时比例很低。主要耗时集中在real_predict也就是`client端`的耗时。\r\n\r\n外层阶段 | 内层阶段 | 耗时 | 耗时占比\r\n-- | -- | -- | --\r\nget_ready（获取就绪memory) |   | 0.117ms | 0.026%\r\npredict（预测阶段，分三阶段）耗时439.54ms | preprocess(准备数据，memory->list) | 0.056ms | 0.013%\r\n<nop> |real_predict | `438.717ms` | `99.77%`\r\n<nop>| postprocess(复制predict到memory) | 0.694ms | 0.157%\r\nput_complete（放回完成memory) |   | 0.062ms | 0.014%\r\n\r\n画出paddle-serving-client的profile图，两个step的数据和`client端`时间吻合。`中间的gap有一大部分时间是client没有profile到的`，见问题https://github.com/PaddlePaddle/Serving/issues/486 \r\n![image](https://user-images.githubusercontent.com/10208305/80297620-54d17700-87b7-11ea-8a44-2bee7f55b652.png)\r\n\r\n分析目前可以优化的点\r\n1. batch_predict增加numpy的接口，减少client python端preprocess和postprocess的numpy与list间的数据拷贝转化。 现batch_predict c++端的接口可以不变，针对Python端再封装一层numpy的c++接口，这样就可以减少python层面的数据转换损耗。\r\n> `preprocess的numpy到list数据转换`(TODO时间占比profile)\r\nhttps://github.com/PaddlePaddle/Serving/blob/f54717396f7cc161046e38b117458ccd5729957b/python/paddle_serving_client/__init__.py#L241\r\n> `batch_predict的input数据list到c++ vector的数据转换，返回数据从vector到list的转换`\r\nhttps://github.com/PaddlePaddle/Serving/blob/f54717396f7cc161046e38b117458ccd5729957b/python/paddle_serving_client/__init__.py#L260-L262\r\n> `返回数据从list到numpy的转换`(TODO时间占比profile)\r\nhttps://github.com/PaddlePaddle/Serving/blob/f54717396f7cc161046e38b117458ccd5729957b/python/paddle_serving_client/__init__.py#L284-L285\r\n2. 从client到serving发送数据和接收数据\r\n可以优化成流式数据，尽量减少gap。\r\n\r\n\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-26T04:25:17+00:00",
        "updated_at": "2024-04-16T09:04:47+00:00",
        "closed_at": "2024-04-16T09:04:47+00:00",
        "comments_count": [],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 482,
        "title": "check_cuda的方式有问题",
        "body": "![image](https://user-images.githubusercontent.com/10208305/80228625-db843800-8681-11ea-8eb1-90134ed5d9f7.png)\r\n不能这样检查是否存在GPU的呀。服务部署挂了一片。。\r\n可以检查环境变量中是否有cuda的so文件，但有so文件不一定有gpu设备，还需要检查是否有/dev/nvidia*\r\n\r\n![image](https://user-images.githubusercontent.com/10208305/80228503-b4c60180-8681-11ea-8aca-6b4f06689c1a.png)\r\n![image](https://user-images.githubusercontent.com/10208305/80228582-cad3c200-8681-11ea-9166-536f0e3721fe.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "wangxicoding",
        "created_at": "2020-04-24T15:24:17+00:00",
        "updated_at": "2020-04-30T02:43:13+00:00",
        "closed_at": "2020-04-30T02:43:13+00:00",
        "comments_count": [
            "guru4elephant",
            "wangxicoding"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 484,
        "title": "AI Studio使用py3找不到serve模块",
        "body": "![image](https://user-images.githubusercontent.com/35550832/80270533-8846cf80-86eb-11ea-8cb6-103103715022.png)\r\nAI Studio使用py3找不到serve模块",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-25T03:54:52+00:00",
        "updated_at": "2020-05-08T13:16:35+00:00",
        "closed_at": "2020-05-08T13:16:35+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 474,
        "title": "paddleServing 安装需要安装docker吗 paddlex发布完成如果通过paddleserving 调用http调用",
        "body": "1.我发布完的paddlex的模型，我想通过http调用进行预测。\r\n2.我文章可以直接安装paddleserving，但文章后面链接到此项目，需要安装docker在安装paddleserving。\r\nhttps://paddlepaddle.org.cn/support/news?action=detail&id=1945\r\n我现在想知道如何快速的将paddleserving安装并发布我的paddlex的模型。\r\n谢谢",
        "state": "closed",
        "user": "wyc880622",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-23T05:54:53+00:00",
        "updated_at": "2024-03-05T06:48:32+00:00",
        "closed_at": "2024-03-05T06:48:32+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "wyc880622",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 473,
        "title": "在Windows下模型保存后，启动服务失败",
        "body": "我按照paddlepaddle微信公众号里介绍的方法进行bert模型上线部署，完全按照步骤保存模型后，启动模型时，终端报错。\r\n源码：\r\n```python\r\nimport paddlehub as hub\r\nimport paddle_serving_client.io as serving_io\r\nmodel_name = 'bert_chinese_L-12_H-768_A-12'\r\nmodule = hub.Module(model_name)\r\ninputs,outputs,program = module.context(trainable=True,max_seq_len=20)\r\nfeed_keys,fetch_keys = [],[]\r\nfor k1 in inputs.keys():\r\n    feed_keys.append(k1)\r\nfor k2 in outputs.keys():\r\n    fetch_keys.append(k2)\r\n    del fetch_keys[2:4]\r\nfeed_dict = dict(zip(feed_keys,[inputs[x] for x in feed_keys]))\r\nfetch_dict = dict(zip(fetch_keys,[outputs[x] for x in fetch_keys]))\r\nserving_io.save_model('bert_seq20_model', 'bert_seq20_client',\r\n                        feed_dict,fetch_dict,program)\r\n```\r\n终端报错如下：\r\n```shell\r\nC:\\Users\\Administrator>python -m paddle_serving_server.serve --model bert_seq20_model --thread 10 --port 9292\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n'cat' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\nFrist time run, downloading PaddleServing components ...\r\n'wget' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\nDownload failed, please check your network or permission of D:\\Python37\\lib\\site-packages\\paddle_serving_server.\r\n```",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "barrierye",
        "created_at": "2020-04-23T04:54:44+00:00",
        "updated_at": "2020-05-08T13:15:02+00:00",
        "closed_at": "2020-05-08T13:15:02+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 489,
        "title": "需要建立python与c++各数据转换的性能baseline，用于指导性能优化",
        "body": "python<-->python转换需测数据：\r\n| 源数据格式 | 目标数据格式 | 转换方法 | 数据大小 |\r\n|--- | --- | --- | --- |\r\n| numpy | list | list(src) | 1MB.. |\r\n| numpy | list | src.tolist() | . |\r\n| ctypes | list | list(src) | . |\r\n| ctypes | list | src[:] | . |\r\n| list | numpy | np.array(list) | . |\r\n| list | ctypes | dst[:] = src[:] | . |\r\n| list | ctypes | cp(*src) | . |\r\n\r\n\r\nc++ <--> python转换需测数据：\r\n| 源数据格式 | 目标数据格式 | 转换方法 | 数据大小 |\r\n|--- | --- | --- | --- |\r\n| list | vector | | 1MB.. |\r\n| numpy | vector | | . |\r\n| ctypes | vector | | . |\r\n| vector | list | | . |\r\n| vector | numpy | | . |\r\n| vector | ctypes | | . |\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-26T05:43:11+00:00",
        "updated_at": "2024-03-05T06:48:33+00:00",
        "closed_at": "2024-03-05T06:48:33+00:00",
        "comments_count": [
            "wangxicoding",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 491,
        "title": "增加图像分割的示例",
        "body": "图像分割的预处理和后处理相对比较麻烦，serving需要在app里支持对应的前后处理，以及对经典分割模型的支持",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-27T02:44:58+00:00",
        "updated_at": "2020-05-08T13:14:26+00:00",
        "closed_at": "2020-05-08T13:14:26+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 499,
        "title": "develop代码rpc预测时，在Client端报Waring",
        "body": "运行fit_a_line rpc版本，client端输出如下：\r\n```shell\r\n...\r\nW0428 14:31:47.450724 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[15.941795] 11.7\r\nW0428 14:31:47.451251 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[13.50612] 11.8\r\nW0428 14:31:47.451813 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[16.184656] 14.3\r\nW0428 14:31:47.452328 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[17.122538] 18.4\r\nW0428 14:31:47.453011 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[20.431929] 29.8\r\nW0428 14:31:47.453507 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[17.51116] 20.0\r\nW0428 14:31:47.454071 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[16.96036] 14.6\r\nW0428 14:31:47.454600 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[22.351873] 20.6\r\nW0428 14:31:47.455077 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[21.23216] 23.7\r\nW0428 14:31:47.455574 16876 stub_impl.hpp:136] Already thread initialized for stub\r\n[14.414518] 12.8\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-28T06:34:13+00:00",
        "updated_at": "2020-04-30T15:41:47+00:00",
        "closed_at": "2020-04-30T15:41:47+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 502,
        "title": "能否通过某种方式控制Serving重新加载workdir的配置",
        "body": "参考TF的模型管理，可以在运行时新增服务或卸载服务（包括版本切换等）。\r\n\r\nServing目前应该可以通过创建多个Workflow来实现同时运行多个服务（接口暂时没开放），如果可以通过某种方式控制Serving重新加载workdir的配置，那么就可以做到运行时新增服务或卸载服务（实现#415 的功能）。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-28T11:48:53+00:00",
        "updated_at": "2024-04-16T09:04:49+00:00",
        "closed_at": "2024-04-16T09:04:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 504,
        "title": "Web Service在预测出错时返回200",
        "body": "![image](https://user-images.githubusercontent.com/28446721/80499200-40e86980-899f-11ea-91b0-8a47abf81ecb.png)\r\n\r\n![image](https://user-images.githubusercontent.com/28446721/80499351-755c2580-899f-11ea-9c01-8434aea71784.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-28T14:27:40+00:00",
        "updated_at": "2024-04-16T09:04:50+00:00",
        "closed_at": "2024-04-16T09:04:50+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 494,
        "title": "已在Windows下安装Docker，我想将训练完成的模型发布到Serving中",
        "body": "我按网址中的去做\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/doc/RUN_IN_DOCKER.md\r\n但在这条命令中让输入用户，我不知道是什么样子的。请问我应该怎么做才可以？\r\ndocker run -p 9292:9292 --name test -dit hub.baidubce.com/paddlepaddle/serving:0.2.0\r\ndocker exec -it test bash\r\n![image](https://user-images.githubusercontent.com/25219834/80337669-09899800-888d-11ea-87c3-fec876ffce55.png)\r\n是不是不在docker中安装也行？",
        "state": "closed",
        "user": "wyc880622",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-27T05:44:03+00:00",
        "updated_at": "2024-03-05T06:48:34+00:00",
        "closed_at": "2024-03-05T06:48:34+00:00",
        "comments_count": [
            "wyc880622",
            "barrierye",
            "wyc880622",
            "barrierye",
            "wyc880622",
            "wyc880622",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 501,
        "title": "senta示例中性能需要优化",
        "body": "使用senta模型示例预测1000条样本需要246.45秒，使用lac做预处理的部分性能较差\r\n![image](https://user-images.githubusercontent.com/16594411/80483340-7f265e80-8988-11ea-9fb7-964254f6bf55.png)\r\n\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-28T11:46:04+00:00",
        "updated_at": "2024-04-16T09:04:48+00:00",
        "closed_at": "2024-04-16T09:04:48+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 503,
        "title": "同进程中启动两个client，预测时会卡住",
        "body": "senta示例中使用lac模型做预处理时，如果将lac client只创建不释放，就会在预测时卡住，复现问题可以将senta示例中的代码做如下改动，此时再通过http访问senta任务就会卡住\r\n![image](https://user-images.githubusercontent.com/16594411/80486915-7f295d00-898e-11ea-8629-4d1cb825765e.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "barrierye",
        "created_at": "2020-04-28T12:27:08+00:00",
        "updated_at": "2020-06-02T15:46:11+00:00",
        "closed_at": "2020-06-02T15:46:11+00:00",
        "comments_count": [
            "barrierye",
            "barrierye",
            "MRXLT",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 505,
        "title": "通过发送rpc请求获取服务端的模型配置",
        "body": "服务端加载模型后，客户端只需要去链接，并用rpc的方式获取配置，降低使用门槛",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-29T00:54:28+00:00",
        "updated_at": "2024-04-16T09:04:51+00:00",
        "closed_at": "2024-04-16T09:04:51+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 506,
        "title": "无代码入门demo",
        "body": "给定一个配置文件，生成一个web服务表单，包括每个字段需要输入的文本框，并生成请求按钮",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-29T00:56:09+00:00",
        "updated_at": "2024-04-16T09:04:52+00:00",
        "closed_at": "2024-04-16T09:04:51+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 508,
        "title": "Http请求的接口有所变更，需要更新develop a new Web service文档",
        "body": "",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-29T08:06:16+00:00",
        "updated_at": "2020-04-30T04:13:28+00:00",
        "closed_at": "2020-04-30T04:13:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 509,
        "title": "Automated code review for Paddle Serving",
        "body": "Hi!\r\nI am a member of the team developing monocodus — a service that performs automatic code review of GitHub pull requests to help organizations ensure a high quality of code.\r\nWe’ve developed some useful features to the moment, and now we’re looking for early users and feedback to find out what we should improve and which features the community needs the most.\r\n\r\nWe ran monocodus on a pre-created fork of your repo on GitHub https://github.com/monocodus-demonstrations/Serving/pulls, and it found some potential issues. I hope that this information will be useful to you and would be happy to receive any feedback here or on my email pavel@monocodus.com.\r\n\r\nIf you want to try our service, feel free to follow the link: https://www.monocodus.com\r\nThe service is entirely free of charge for open source projects. Hope you’ ll like it :)",
        "state": "closed",
        "user": "pavel-ignatovich",
        "closed_by": "guru4elephant",
        "created_at": "2020-04-29T08:56:16+00:00",
        "updated_at": "2020-05-08T13:13:47+00:00",
        "closed_at": "2020-05-08T13:13:47+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 514,
        "title": "docker 内cuda和本机cuda版本不一样如何解决",
        "body": "按照paddle server说明拉了一个docker\r\nnvidia-docker pull hub.baidubce.com/paddlepaddle/serving:0.2.0-gp\r\n部署了tansformer模型，但预测发生错误\r\n\r\n检查发现docker环境是cuda9.0, 而本机安裝的是10.01，所以部署怀疑是cuda版本问题引起，应该如何解决？谢谢",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-30T06:40:21+00:00",
        "updated_at": "2024-04-16T09:04:52+00:00",
        "closed_at": "2024-04-16T09:04:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "dlkht",
            "dlkht"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 515,
        "title": "bert、Resnet50、ResNet101模型的http方法报500 Internal server error",
        "body": "由于http服务为了支持batch预测，导致之前的http服务脚本出错",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-30T08:24:16+00:00",
        "updated_at": "2024-03-05T06:48:36+00:00",
        "closed_at": "2024-03-05T06:48:36+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 516,
        "title": "app模块中的lac_reader不兼容python3",
        "body": "![image](https://user-images.githubusercontent.com/16594411/80689022-4c52a680-8aff-11ea-9c62-eb01d85f944b.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-04-30T08:26:19+00:00",
        "updated_at": "2020-05-12T10:56:36+00:00",
        "closed_at": "2020-05-12T10:56:36+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 523,
        "title": "Verify the Preprocessing steps for Resnet50",
        "body": "seq = Sequential([\r\n    File2Image(), Resize(256), CenterCrop(224), RGB2BGR(), Transpose((2, 0, 1)),\r\n    Div(255), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n])",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-02T01:41:16+00:00",
        "updated_at": "2020-05-08T13:13:24+00:00",
        "closed_at": "2020-05-08T13:13:24+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 517,
        "title": "windows不支持nvidia-docker 安装是这样的吗。",
        "body": "百度大佬您好，我从网上查看windows不支持 nvidia-docker 安装，那paddleserving的部署是不是可以理解, paddleserving 不支持paddleDetection的训练文件部署。\r\n我想运行如下示例  Image Classification\r\nhttps://github.com/PaddlePaddle/Serving\r\n但需要Note: This demo needs paddle-serving-server-gpu.\r\n所以现在的paddleDetection部署问题进行预测有支持GPU的吗？\r\n或者paddleServing的部署利用cpu如何实现paddleDection训练模型的预测。谢谢",
        "state": "closed",
        "user": "wyc880622",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-04-30T08:31:40+00:00",
        "updated_at": "2024-03-05T06:48:37+00:00",
        "closed_at": "2024-03-05T06:48:37+00:00",
        "comments_count": [
            "guru4elephant",
            "wyc880622",
            "guru4elephant",
            "wyc880622",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 518,
        "title": "py2环境client编译失败",
        "body": "![image](https://user-images.githubusercontent.com/16594411/80697308-60040a00-8b0b-11ea-96b7-b1586d79ad60.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-04-30T09:52:59+00:00",
        "updated_at": "2020-05-12T10:55:37+00:00",
        "closed_at": "2020-05-12T10:55:36+00:00",
        "comments_count": [
            "barrierye",
            "MRXLT"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 520,
        "title": "在本机测试paddle serving example模型bert提示错误",
        "body": "error while loading shared libraries: libcrypto.so.10: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-01T04:04:32+00:00",
        "updated_at": "2024-03-05T06:48:38+00:00",
        "closed_at": "2024-03-05T06:48:38+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 524,
        "title": "patchelf导致so文件出错",
        "body": "https://github.com/PaddlePaddle/Serving/blob/f5fd7c9de42a5ff319a9fd32c53c6c1213e06f3e/python/paddle_serving_client/__init__.py#L128\r\n\r\n![image](https://user-images.githubusercontent.com/10208305/80859424-8ce32900-8c93-11ea-9356-5da3d3bc0d98.png)\r\n\r\n多个进程同时跑patchelf时会导致so文件出错，进程挂掉",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "MRXLT",
        "created_at": "2020-05-02T08:41:08+00:00",
        "updated_at": "2020-05-08T07:38:12+00:00",
        "closed_at": "2020-05-08T07:38:12+00:00",
        "comments_count": [
            "wangxicoding",
            "MRXLT"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 525,
        "title": "无法复现检测模型的效果",
        "body": "基于\r\nhttps://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model\r\n进行复现，得到的结果图与readme给出的图不一致",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-03T01:12:19+00:00",
        "updated_at": "2020-05-06T03:29:36+00:00",
        "closed_at": "2020-05-06T03:29:36+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 526,
        "title": "paddle_serving_app中增加教程闭环体验",
        "body": "通过\r\npaddle_serving_app.package --list_model\r\npaddle_serving_app.package --get senta_lstm\r\npaddle_serving_app.package --tutorial senta_lstm\r\n帮助用户用命令行的方式获取已有模型列表，下载模型部署包，运行教程",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-03T06:42:25+00:00",
        "updated_at": "2024-04-16T09:04:53+00:00",
        "closed_at": "2024-04-16T09:04:53+00:00",
        "comments_count": [],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 531,
        "title": "pypi上的whl包版本为0.2.1，但文档还未更新",
        "body": "0.2.1版本比较大的改动是http服务的接口发生变化（兼容batch），这导致原有的文档运行错误。issue #473 反映了这个问题。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-05-06T08:03:23+00:00",
        "updated_at": "2020-05-07T12:21:04+00:00",
        "closed_at": "2020-05-07T12:21:04+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 530,
        "title": "imagenet模型http请求失败",
        "body": "https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/imagenet\r\n环境：\r\nOS： Ubuntu 16.04\r\n\r\n```bash\r\nsh get_model.sh\r\npython image_classification_service.py ResNet50_vd_model workdir 9393 &\r\npython image_http_client.py\r\n```\r\n报错如下\r\n```\r\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\r\n<title>404 Not Found</title>\r\n<h1>Not Found</h1>\r\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\r\n\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0506 07:49:39.612355  8351 config_manager.cpp:217] Not found key in configue: cluster\r\nE0506 07:49:39.612414  8351 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0506 07:49:39.612422  8351 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0506 07:49:39.612442  8351 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0506 07:49:39.612452  8351 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0506 07:49:39.612459  8351 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0506 07:49:39.612466  8351 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0506 07:49:39.612473  8351 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0506 07:49:39.612480  8351 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0506 07:49:39.612486  8351 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0506 07:49:39.612493  8351 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0506 07:49:39.612500  8351 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0506 07:49:39.612507  8351 config_manager.cpp:226] Not found key in configue: protocol\r\nE0506 07:49:39.612519  8351 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0506 07:49:39.612525  8351 config_manager.cpp:228] Not found key in configue: package_size\r\nE0506 07:49:39.612535  8351 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0506 07:49:39.612543  8351 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0506 07:49:39.612550  8351 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0506 07:49:39.616928  8351 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\n127.0.0.1 - - [06/May/2020 07:49:39] \"POST /image/prediction HTTP/1.1\" 404 -\r\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\r\n<title>404 Not Found</title>\r\n<h1>Not Found</h1>\r\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\r\n```",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "bjjwwang",
        "created_at": "2020-05-06T07:55:35+00:00",
        "updated_at": "2020-05-06T08:59:21+00:00",
        "closed_at": "2020-05-06T08:59:21+00:00",
        "comments_count": [
            "barrierye",
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 535,
        "title": "执行from .serving_client import PredictorClient提示错误",
        "body": "错误如下：\r\npatchelf: section header table out of bounds\r\nW0507 18:33:44.715210  3105 init.cc:209] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW0507 18:33:44.715245  3105 init.cc:211] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW0507 18:33:44.715252  3105 init.cc:214] The detail failure signal is:\r\n\r\nW0507 18:33:44.715262  3105 init.cc:217] *** Aborted at 1588847624 (unix time) try \"date -d @1588847624\" if you are using GNU date ***\r\nW0507 18:33:44.717118  3105 init.cc:217] PC: @                0x0 (unknown)\r\nW0507 18:33:44.717303  3105 init.cc:217] *** SIGBUS (@0x7fe203ea3000) received by PID 3105 (TID 0x7fe289dab740) from PID 65679360; stack trace: ***\r\nW0507 18:33:44.719110  3105 init.cc:217]     @     0x7fe289fc6f60 (unknown)\r\nW0507 18:33:44.720683  3105 init.cc:217]     @     0x7fe28a1b1aa5 (unknown)\r\nW0507 18:33:44.722323  3105 init.cc:217]     @     0x7fe28a1b4933 (unknown)\r\nW0507 18:33:44.723898  3105 init.cc:217]     @     0x7fe28a1bf9a4 (unknown)\r\nW0507 18:33:44.725504  3105 init.cc:217]     @     0x7fe28a0e15cf _dl_catch_exception\r\nW0507 18:33:44.726071  3105 init.cc:217]     @     0x7fe28a1bf2a6 (unknown)\r\nW0507 18:33:44.726619  3105 init.cc:217]     @     0x7fe289f5d256 (unknown)\r\nW0507 18:33:44.727196  3105 init.cc:217]     @     0x7fe28a0e15cf _dl_catch_exception\r\nW0507 18:33:44.727761  3105 init.cc:217]     @     0x7fe28a0e165f _dl_catch_error\r\nW0507 18:33:44.728305  3105 init.cc:217]     @     0x7fe289f5da25 (unknown)\r\nW0507 18:33:44.728852  3105 init.cc:217]     @     0x7fe289f5d2e6 dlopen\r\nW0507 18:33:44.729374  3105 init.cc:217]     @     0x7fe288e613cb (unknown)\r\nW0507 18:33:44.729519  3105 init.cc:217]     @           0x5d7b82 _PyMethodDef_RawFastCallKeywords\r\nW0507 18:33:44.729584  3105 init.cc:217]     @           0x554d2d _PyEval_EvalFrameDefault\r\nW0507 18:33:44.729683  3105 init.cc:217]     @           0x54baa2 _PyEval_EvalCodeWithName\r\nW0507 18:33:44.729748  3105 init.cc:217]     @           0x5d9ace _PyFunction_FastCallDict\r\nW0507 18:33:44.729883  3105 init.cc:217]     @           0x590d73 (unknown)\r\nW0507 18:33:44.729977  3105 init.cc:217]     @           0x5d9029 _PyObject_FastCallKeywords\r\nW0507 18:33:44.730113  3105 init.cc:217]     @           0x54b1b1 (unknown)\r\nW0507 18:33:44.730180  3105 init.cc:217]     @           0x55211b _PyEval_EvalFrameDefault\r\nW0507 18:33:44.730273  3105 init.cc:217]     @           0x5d85cc _PyFunction_FastCallKeywords\r\nW0507 18:33:44.730407  3105 init.cc:217]     @           0x54afe0 (unknown)\r\nW0507 18:33:44.730468  3105 init.cc:217]     @           0x55211b _PyEval_EvalFrameDefault\r\nW0507 18:33:44.730564  3105 init.cc:217]     @           0x54baa2 _PyEval_EvalCodeWithName\r\nW0507 18:33:44.730659  3105 init.cc:217]     @           0x5d88c2 _PyFunction_FastCallKeywords\r\nW0507 18:33:44.730792  3105 init.cc:217]     @           0x54afe0 (unknown)\r\nW0507 18:33:44.730854  3105 init.cc:217]     @           0x54f0ba _PyEval_EvalFrameDefault\r\nW0507 18:33:44.730948  3105 init.cc:217]     @           0x5d85cc _PyFunction_FastCallKeywords\r\nW0507 18:33:44.731007  3105 init.cc:217]     @           0x54e3ac _PyEval_EvalFrameDefault\r\nW0507 18:33:44.731103  3105 init.cc:217]     @           0x54baa2 _PyEval_EvalCodeWithName\r\nW0507 18:33:44.731237  3105 init.cc:217]     @           0x558b75 (unknown)\r\nW0507 18:33:44.731360  3105 init.cc:217]     @           0x5d7c03 _PyMethodDef_RawFastCallKeywords\r\n\r\nProcess finished with exit code 135 (interrupted by signal 7: SIGEMT)",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-07T10:45:49+00:00",
        "updated_at": "2024-04-16T09:04:54+00:00",
        "closed_at": "2024-04-16T09:04:54+00:00",
        "comments_count": [
            "dlkht",
            "MRXLT",
            "dlkht",
            "bjjwwang",
            "dlkht",
            "MRXLT",
            "dlkht"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 529,
        "title": "PaddleDetection训练得到的模型权重转换至Serving部署可用的权重类型",
        "body": "PaddleDetection训练得到的模型权重会得到 pdmodel, pdopt, pdparams三个文件\r\n而[提供的例子](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model) 下，/faster_rcnn_model/pddet_serving_model 的提供的用于serving的模型权重是是 _scale, _offset, _weights等类型的文件，希望获得如何转换的方式",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-05T13:52:35+00:00",
        "updated_at": "2020-05-08T08:02:43+00:00",
        "closed_at": "2020-05-08T08:02:43+00:00",
        "comments_count": [
            "guru4elephant",
            "guru4elephant",
            "omtbreak",
            "bjjwwang",
            "bjjwwang",
            "omtbreak"
        ],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 536,
        "title": "部署transformer的情况变化",
        "body": "服务端升级serer到0.2.1，server收到的数据格式处理没问题了（将np.array转成list),但执行batch_predict（）时，client端收到{\"result\":\"Request Value Error\"}\r\nserver端提示：\r\n2020-05-08 11:12:25,176-INFO: 172.17.0.1 - - [08/May/2020 11:12:25] \"POST /transformer/prediction HTTP/1.1\" 200 -\r\n是什么原因？",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-08T11:24:10+00:00",
        "updated_at": "2024-04-16T09:04:55+00:00",
        "closed_at": "2024-04-16T09:04:55+00:00",
        "comments_count": [
            "guru4elephant",
            "dlkht",
            "MRXLT",
            "dlkht",
            "dlkht",
            "MRXLT",
            "dlkht",
            "dlkht",
            "dlkht",
            "MRXLT",
            "MRXLT",
            "dlkht"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 538,
        "title": "预测Ernie模型出错",
        "body": "## 报错信息\r\n```\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: Fully Connected input and weigth size do not match. input width: 768,weight height: 772\r\n  [Hint: Expected in_mat_dims[1] == w_dims0, but received in_mat_dims[1]:768 != w_dims0:772.] at (/paddle/paddle/fluid/operators/fc_op.h:39)\r\n```\r\n## 日志\r\n### GLOG_v = 2日志\r\n```\r\nI0509 12:33:09.808557 15547 dag_view.cpp:68] dag view initialized:\r\nnode id: 1\r\nnode name: general_reader_0\r\nnode type: GeneralReaderOp\r\nI0509 12:33:09.808794 15547 dag_view.cpp:81] op->set_engine_name(general_reader_0)\r\nI0509 12:33:09.808801 15547 dag_view.cpp:48] stage[1] name: workflow1_1\r\nI0509 12:33:09.808807 15547 dag_view.cpp:49] stage[1] node size: 1\r\nI0509 12:33:09.808820 15547 dag_view.cpp:68] dag view initialized:\r\nnode id: 2\r\nnode name: general_infer_0\r\nnode type: GeneralInferOp\r\nI0509 12:33:09.808827 15547 dag_view.cpp:81] op->set_engine_name(general_infer_0)\r\nI0509 12:33:09.808833 15547 dag_view.cpp:91] add op pre name:\r\ncurrent op name: general_infer_0, previous op name: general_reader_0\r\nI0509 12:33:09.808848 15547 dag_view.cpp:48] stage[2] name: workflow1_2\r\nI0509 12:33:09.808854 15547 dag_view.cpp:49] stage[2] node size: 1\r\nI0509 12:33:09.808868 15547 dag_view.cpp:68] dag view initialized:\r\nnode id: 3\r\nnode name: general_response_0\r\nnode type: GeneralResponseOp\r\nI0509 12:33:09.808878 15547 dag_view.cpp:81] op->set_engine_name(general_response_0)\r\nI0509 12:33:09.808884 15547 dag_view.cpp:91] add op pre name:\r\ncurrent op name: general_response_0, previous op name: general_infer_0\r\nI0509 12:33:09.808897 15547 dag_view.cpp:157] vstage->nodes.size(): 1\r\nI0509 12:33:09.809219 15547 general_reader_op.cpp:94] var num: 8\r\nI0509 12:33:09.809270 15547 general_reader_op.cpp:96] start to call load general model_conf op\r\nI0509 12:33:09.809279 15547 general_reader_op.cpp:100] get resource pointer done.\r\nI0509 12:33:09.809288 15547 general_reader_op.cpp:104] print general model config done.\r\nI0509 12:33:09.809296 15547 general_reader_op.cpp:125] var[0] has elem type: 1\r\nI0509 12:33:09.809309 15547 general_reader_op.cpp:143] shape for var[0]: 128\r\nI0509 12:33:09.809316 15547 general_reader_op.cpp:143] shape for var[0]: 1\r\nI0509 12:33:09.809324 15547 general_reader_op.cpp:147] var[0] is tensor, capacity: 128\r\nI0509 12:33:09.809598 15547 general_reader_op.cpp:125] var[1] has elem type: 0\r\nI0509 12:33:09.809633 15547 general_reader_op.cpp:143] shape for var[1]: 128\r\nI0509 12:33:09.809643 15547 general_reader_op.cpp:143] shape for var[1]: 1\r\nI0509 12:33:09.809651 15547 general_reader_op.cpp:147] var[1] is tensor, capacity: 128\r\nI0509 12:33:09.809659 15547 general_reader_op.cpp:125] var[2] has elem type: 0\r\nI0509 12:33:09.809665 15547 general_reader_op.cpp:143] shape for var[2]: 128\r\nI0509 12:33:09.809671 15547 general_reader_op.cpp:143] shape for var[2]: 1\r\nI0509 12:33:09.809676 15547 general_reader_op.cpp:147] var[2] is tensor, capacity: 128\r\nI0509 12:33:09.809684 15547 general_reader_op.cpp:125] var[3] has elem type: 1\r\nI0509 12:33:09.809690 15547 general_reader_op.cpp:143] shape for var[3]: 128\r\nI0509 12:33:09.809696 15547 general_reader_op.cpp:143] shape for var[3]: 1\r\nI0509 12:33:09.809702 15547 general_reader_op.cpp:147] var[3] is tensor, capacity: 128\r\nI0509 12:33:09.809708 15547 general_reader_op.cpp:125] var[4] has elem type: 0\r\nI0509 12:33:09.809715 15547 general_reader_op.cpp:143] shape for var[4]: 128\r\nI0509 12:33:09.809720 15547 general_reader_op.cpp:143] shape for var[4]: 1\r\nI0509 12:33:09.809726 15547 general_reader_op.cpp:147] var[4] is tensor, capacity: 128\r\nI0509 12:33:09.809734 15547 general_reader_op.cpp:125] var[5] has elem type: 0\r\nI0509 12:33:09.809743 15547 general_reader_op.cpp:143] shape for var[5]: 128\r\nI0509 12:33:09.809749 15547 general_reader_op.cpp:143] shape for var[5]: 1\r\nI0509 12:33:09.809758 15547 general_reader_op.cpp:147] var[5] is tensor, capacity: 128\r\nI0509 12:33:09.809764 15547 general_reader_op.cpp:125] var[6] has elem type: 0\r\nI0509 12:33:09.809770 15547 general_reader_op.cpp:143] shape for var[6]: 128\r\nI0509 12:33:09.809777 15547 general_reader_op.cpp:143] shape for var[6]: 1\r\nI0509 12:33:09.809782 15547 general_reader_op.cpp:147] var[6] is tensor, capacity: 128\r\nI0509 12:33:09.809788 15547 general_reader_op.cpp:125] var[7] has elem type: 0\r\nI0509 12:33:09.809793 15547 general_reader_op.cpp:143] shape for var[7]: 128\r\nI0509 12:33:09.809799 15547 general_reader_op.cpp:143] shape for var[7]: 1\r\nI0509 12:33:09.809805 15547 general_reader_op.cpp:147] var[7] is tensor, capacity: 128\r\nI0509 12:33:09.809813 15547 general_reader_op.cpp:178] var[0] is tensor and capacity=128\r\nI0509 12:33:09.809819 15547 general_reader_op.cpp:178] var[1] is tensor and capacity=128\r\nI0509 12:33:09.809828 15547 general_reader_op.cpp:178] var[2] is tensor and capacity=128\r\nI0509 12:33:09.809834 15547 general_reader_op.cpp:178] var[3] is tensor and capacity=128\r\nI0509 12:33:09.809849 15547 general_reader_op.cpp:178] var[4] is tensor and capacity=128\r\nI0509 12:33:09.809854 15547 general_reader_op.cpp:178] var[5] is tensor and capacity=128\r\nI0509 12:33:09.809860 15547 general_reader_op.cpp:178] var[6] is tensor and capacity=128\r\nI0509 12:33:09.809868 15547 general_reader_op.cpp:178] var[7] is tensor and capacity=128\r\nI0509 12:33:09.809876 15547 general_reader_op.cpp:216] output size: 8\r\nI0509 12:33:09.809882 15547 general_reader_op.cpp:224] read data from client success\r\nI0509 12:33:09.809907 15547 op.cpp:159]  general_reader_0_time=[949]\r\nI0509 12:33:09.809918 15547 dag_view.cpp:157] vstage->nodes.size(): 1\r\nI0509 12:33:09.809928 15547 general_infer_op.cpp:39] Going to run inference\r\nI0509 12:33:09.809938 15547 general_infer_op.cpp:50] Get precedent op name: general_reader_0\r\nI0509 12:33:09.810010 15547 general_infer_op.cpp:61] input batch size: 1\r\nI0509 12:33:09.810021 15547 general_infer_op.cpp:65] infer batch size: 1\r\n```\r\n\r\n\r\n## 复现步骤\r\n```\r\nwget ftp://yq01-sys-rpm-ps0579.yq01.baidu.com/home/disk3/fanjun/serving/simnet/l3_posttrain_simnet.tar.gz\r\ntar xf l3_posttrain_simnet.tar.gz\r\ncd l3_posttrain_simnet\r\npython -m paddle_serving_server.serve --model model_server/ --port 9015\r\nhead -n 1 test_qt | python client_test.py\r\n```",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "bjjwwang",
        "created_at": "2020-05-09T04:33:30+00:00",
        "updated_at": "2020-05-09T07:03:22+00:00",
        "closed_at": "2020-05-09T07:03:22+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 540,
        "title": "bert模型预测出错",
        "body": "你好，我今天试着用HTTP和RPC两种方式启动bert的预测，分别遇到了问题。\r\n最开始已经用命令ps -ef | grep \"serving\" | grep -v grep | awk '{print $2}' | xargs kill先杀一遍进程，HTTP的启动命令、报错、日志如下：\r\n```javascript\r\n[root@768de910d24c /]# python -m paddle_serving_server.serve --model bert_seq20_model --port 9292 --thread 4 --name bert &>bert_log.txt &\r\n[1] 1060\r\n[root@768de910d24c /]# curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"words\": \"hello\"}], \"fetch\":[\"pooled_output\"]}' http://0.0.0.0:9292/bert/prediction\r\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\r\n<title>500 Internal Server Error</title>\r\n<h1>Internal Server Error</h1>\r\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\r\n\r\n[root@768de910d24c /]# cat bert_log.txt\r\nweb service address:\r\nhttp://172.17.0.2:9292/bert/prediction\r\nmkdir: cannot create directory 'workdir': File exists\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0509 06:29:50.606771  1066 config_manager.cpp:217] Not found key in configue: cluster\r\nE0509 06:29:50.606863  1066 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0509 06:29:50.606878  1066 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0509 06:29:50.606911  1066 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0509 06:29:50.606927  1066 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0509 06:29:50.606941  1066 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0509 06:29:50.606956  1066 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0509 06:29:50.606971  1066 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0509 06:29:50.606986  1066 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0509 06:29:50.606999  1066 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0509 06:29:50.607236  1066 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0509 06:29:50.607264  1066 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0509 06:29:50.607308  1066 config_manager.cpp:226] Not found key in configue: protocol\r\nE0509 06:29:50.607322  1066 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0509 06:29:50.607331  1066 config_manager.cpp:228] Not found key in configue: package_size\r\nE0509 06:29:50.607340  1066 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0509 06:29:50.607349  1066 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0509 06:29:50.607358  1066 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0509 06:29:50.613191  1066 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:9293\"): added 1\r\ndefault_variant_conf {\r\n  tag: \"default\"\r\n  connection_conf {\r\n    connect_timeout_ms: 2000\r\n    rpc_timeout_ms: 20000\r\n    connect_retry_count: 2\r\n    max_connection_per_host: 100\r\n    hedge_request_timeout_ms: -1\r\n    hedge_fetch_retry_count: 2\r\n    connection_type: \"pooled\"\r\n  }\r\n  naming_conf {\r\n    cluster_filter_strategy: \"Default\"\r\n    load_balance_strategy: \"la\"\r\n  }\r\n  rpc_parameter {\r\n    compress_type: 0\r\n    package_size: 20\r\n    protocol: \"baidu_std\"\r\n    max_channel_per_request: 3\r\n  }\r\n}\r\npredictors {\r\n  name: \"general_model\"\r\n  service_name: \"baidu.paddle_serving.predictor.general_model.GeneralModelService\"\r\n  endpoint_router: \"WeightedRandomRender\"\r\n  weighted_random_render_conf {\r\n    variant_weight_list: \"100\"\r\n  }\r\n  variants {\r\n    tag: \"var1\"\r\n    naming_conf {\r\n      cluster: \"list://0.0.0.0:9293\"\r\n    }\r\n  }\r\n}\r\n\r\n * Serving Flask app \"paddle_serving_server.web_service\" (lazy loading)\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: off\r\n * Running on http://0.0.0.0:9292/ (Press CTRL+C to quit)\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralTextReaderOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralCopyOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralTextResponseOp\r\nI0100 00:00:00.000000  1074 op_repository.h:65] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000  1074 service_manager.h:61] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000  1074 load_general_model_service.pb.h:299] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000  1074 service_manager.h:61] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000  1074 general_model_service.pb.h:1220] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:25] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:31] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:37] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:42] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE in macro!\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:47] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000  1074 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000  1074 fluid_cpu_engine.cpp:53] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR_SIGMOID in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [attention_lstm_fuse_pass]\r\n--- Running IR pass [seqconv_eltadd_relu_fuse_pass]\r\n--- Running IR pass [seqpool_cvm_concat_fuse_pass]\r\n--- Running IR pass [fc_lstm_fuse_pass]\r\n--- Running IR pass [mul_lstm_fuse_pass]\r\n--- Running IR pass [fc_gru_fuse_pass]\r\n--- Running IR pass [mul_gru_fuse_pass]\r\n--- Running IR pass [seq_concat_fc_fuse_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\n--- Running IR pass [repeated_fc_relu_fuse_pass]\r\n--- Running IR pass [squared_mat_sub_fuse_pass]\r\n--- Running IR pass [conv_bn_fuse_pass]\r\n--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\r\n--- Running IR pass [conv_transpose_bn_fuse_pass]\r\n--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]\r\n--- Running IR pass [is_test_pass]\r\n--- Running IR pass [runtime_context_cache_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nW0509 06:29:57.814424  1066 predictor.hpp:129] inference call failed, message: [E-5100]1/1 channels failed, fail_limit=1 [C0][E-5100][172.17.0.2:9293][E-5100]InferService inference failed!\r\nE0509 06:29:57.814563  1066 general_model.cpp:369] failed call predictor with req: insts { } fetch_var_names: \"pooled_output\"\r\n[2020-05-09 06:29:57,815] ERROR in app: Exception on /bert/prediction [POST]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1953, in full_dispatch_request\r\n    return self.finalize_request(rv)\r\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1968, in finalize_request\r\n    response = self.make_response(rv)\r\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 2098, in make_response\r\n    \"The view function did not return a valid response. The\"\r\nTypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.\r\n127.0.0.1 - - [09/May/2020 06:29:57] \"POST /bert/prediction HTTP/1.1\" 500 -\r\nGoing to Run Command\r\n/usr/lib/python2.7/site-packages/paddle_serving_server/serving-cpu-noavx-openblas-0.2.0/serving -enable_model_toolkit -inferservice_path workdir -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 16 -port 8080 -reload_interval_s 10 -resource_path workdir -resource_file resource.prototxt -workflow_path workdir -workflow_file workflow.prototxt -bthread_concurrency 16",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "ClassmateXiaoyu",
        "created_at": "2020-05-09T06:59:39+00:00",
        "updated_at": "2020-05-12T09:10:02+00:00",
        "closed_at": "2020-05-12T09:10:02+00:00",
        "comments_count": [
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 542,
        "title": "imdb example无法用numpy预测",
        "body": "![image](https://user-images.githubusercontent.com/28446721/81469931-7e0fef80-921a-11ea-8995-93feb7c82585.png)\r\n\r\n错误信息中4294967295应该是-1，word_ids里没有-1的值：\r\n```shell\r\n[[  8]\r\n [233]\r\n [ 52]\r\n [601]]\r\n```\r\n\r\nClient端复现代码：\r\n```python\r\nfrom paddle_serving_client import Client\r\nimport numpy as np\r\nfrom imdb_reader import IMDBDataset\r\nimport sys\r\nimport paddle.fluid as fluid\r\nclient = Client()\r\nclient.load_client_config('imdb_bow_client_conf/serving_client_conf.prototxt')\r\nclient.connect([\"127.0.0.1:9292\"])\r\n\r\nimdb_dataset = IMDBDataset()\r\nimdb_dataset.load_resource('imdb.vocab')\r\n\r\nline = \"i am very sad | 0\"\r\nword_ids, label = imdb_dataset.get_words_and_label(line)\r\nword_ids = np.array(word_ids, dtype='int64').reshape((4, 1))\r\nfeed = {\"words\": word_ids}\r\nfetch = [\"prediction\"]\r\nfetch_map = client.predict(feed=feed, fetch=fetch)\r\nprint(fetch_map)\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-09T09:30:20+00:00",
        "updated_at": "2020-05-11T14:47:00+00:00",
        "closed_at": "2020-05-11T14:47:00+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 544,
        "title": "paddle_serving_app无法使用Detection",
        "body": "根据提供的[Faster RCNN示例](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model)，在test_client.py中，\r\n```\r\nfrom paddle_serving_client import Client\r\nimport sys\r\nimport os\r\nimport time\r\nfrom paddle_serving_app.reader.pddet import Detection\r\nimport numpy as np\r\n\r\npy_version = sys.version_info[0]\r\n\r\nfeed_var_names = ['image', 'im_shape', 'im_info']\r\nfetch_var_names = ['multiclass_nms']\r\npddet = Detection(config_path=sys.argv[2], output_dir=\"./output\")\r\nfeed_dict = pddet.preprocess(feed_var_names, sys.argv[3])\r\nclient = Client()\r\nclient.load_client_config(sys.argv[1])\r\nclient.connect(['127.0.0.1:9494'])\r\nfetch_map = client.predict(feed=feed_dict, fetch=fetch_var_names)\r\nouts = fetch_map.values()\r\npddet.postprocess(fetch_map, fetch_var_names)\r\n```\r\n在pycharm或者linux命令行下，都对\r\n```\r\nfrom paddle_serving_app.reader.pddet import Detection\r\n```\r\n报错，找不到serving_app_reader.pddet模块",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-11T03:12:12+00:00",
        "updated_at": "2024-04-16T09:04:56+00:00",
        "closed_at": "2024-04-16T09:04:56+00:00",
        "comments_count": [
            "guru4elephant",
            "omtbreak",
            "MRXLT",
            "omtbreak",
            "MRXLT",
            "omtbreak",
            "omtbreak"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 549,
        "title": "预测库入口使用zero copy",
        "body": "1.7以后，预测库可以省略feed op的操作，直接将输入的tensor copy到初始使用的位置。",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-11T09:53:30+00:00",
        "updated_at": "2024-04-16T09:04:58+00:00",
        "closed_at": "2024-04-16T09:04:58+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 547,
        "title": "百度云CCE集群（K8S）无法pip install paddle-serving-server",
        "body": "## Server端Yaml\r\n以下文件名为`lac_serving.yaml`\r\n```\r\napiVersion: apps/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  name: paddleserving\r\n  labels:\r\n    app: paddleserving\r\nspec:\r\n  replicas: 1\r\n  template:\r\n    metadata:\r\n      name: paddleserving\r\n      labels:\r\n        app: paddleserving\r\n    spec:\r\n      containers:\r\n      - name: paddleserving\r\n        image: hub.baidubce.com/paddlepaddle/serving:0.2.0\r\n        imagePullPolicy: Always\r\n        workingDir: /\r\n        command: ['/bin/bash', '-c']\r\n        args: ['pip install paddle-serving-server \r\n-i https://pypi.tuna.tsinghua.edu.cn/simple && wget --no-check-certificate https://paddle-serving.bj.bcebos.com/lac/lac_model_jieba_web.tar.gz && tar -xzf lac_model_jieba_web.tar.gz && python lac_web_service.py jieba_server_model/ lac_workdir 9292']\r\n        ports:\r\n        - containerPort: 9292\r\n          name: serving\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: paddleserving\r\nspec:\r\n  ports:\r\n    - name: paddleserving\r\n      port: 9292\r\n      targetPort: 9292\r\n  selector:\r\n    app: paddleserving\r\n```\r\n执行 `kubectl apply -f lac_serving.yaml`\r\n在启动容器之后，发现pip install 操作会TIMEOUT。\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-11T07:58:27+00:00",
        "updated_at": "2024-04-16T09:04:57+00:00",
        "closed_at": "2024-04-16T09:04:57+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 548,
        "title": "预测库选项整理",
        "body": "预测库在默认apply pass的时候，可能会存在某些模型上无法work，为了保证用户能够优先使用正确的模型，C++一侧的预测库选项会默认关闭掉所有pass，用户在起服务的时候可以选择开启pass。",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-05-11T09:51:27+00:00",
        "updated_at": "2020-05-12T07:28:42+00:00",
        "closed_at": "2020-05-12T07:28:42+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 555,
        "title": "docker容器中，HTTP和RPC服务进行bert模型预测时报错",
        "body": "基本环境是按照文档pull的hub.baidubce.com/paddlepaddle/serving:0.2.0镜像，Windows7 64位环境下安装的docker。容器中安装的0.2.1版本的的client和server。安装后，uci_housing模型可以正常HTTP和RPC预测。但是用bert模型进行测试时有报错，内容如下。HTTP报错{\"result\":\"Request Value Error\"}可能是由于传入的数据不对，RPC的报错Illegal instruction (core dumped)。\r\n代码如下：\r\n> RPC服务：\r\n```shell\r\n[root@23130a8f27e7 project_bert_seq20]# python -m paddle_serving_server.serve --model bert_seq20_model --port 9292 --thread 4 &>bert_log.txt &\r\n[1] 1358\r\n[root@23130a8f27e7 project_bert_seq20]# cat data-c.txt | python bert_seq20_rpc.py\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0511 07:55:05.645292  1378 config_manager.cpp:217] Not found key in configue: cluster\r\nE0511 07:55:05.646639  1378 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0511 07:55:05.647294  1378 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0511 07:55:05.647946  1378 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0511 07:55:05.648443  1378 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0511 07:55:05.649521  1378 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0511 07:55:05.650112  1378 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0511 07:55:05.650581  1378 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0511 07:55:05.651515  1378 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0511 07:55:05.652343  1378 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0511 07:55:05.652875  1378 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0511 07:55:05.653631  1378 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0511 07:55:05.654289  1378 config_manager.cpp:226] Not found key in configue: protocol\r\nE0511 07:55:05.654405  1378 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0511 07:55:05.654548  1378 config_manager.cpp:228] Not found key in configue: package_size\r\nE0511 07:55:05.654673  1378 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0511 07:55:05.655035  1378 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0511 07:55:05.655126  1378 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0511 07:55:05.664395  1378 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\nIllegal instruction (core dumped)\r\n```\r\n\r\n> HTTP服务：\r\n```shell\r\n[root@23130a8f27e7 project_bert_seq20]# python -m paddle_serving_server.serve --model bert_seq20_model --port 9292 --thread 4 --name bert &>bert_log.txt &\r\n[1] 1404\r\n[root@23130a8f27e7 project_bert_seq20]# curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"words\": \"great\"}], \"fetch\":[\"pooled_output\"]}' http://0.0.0.0:9292/bert/prediction\r\n{\"result\":\"Request Value Error\"}\r\n```",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "ClassmateXiaoyu",
        "created_at": "2020-05-12T09:10:14+00:00",
        "updated_at": "2020-06-12T00:56:33+00:00",
        "closed_at": "2020-06-12T00:56:33+00:00",
        "comments_count": [
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 553,
        "title": "客户端调用 from .serving_client import PredictorClient 报错",
        "body": "本机系统MaxOS，使用的 paddle_serving_client-0.2.2-py3-none-any.whl\r\n```\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport sys\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(640, 640), Transpose((2, 0, 1))\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"/Users/omtbreak/Desktop/untitled1/label_list.txt\", \"output\")\r\n\r\nclient = Client()\r\nclient.load_client_config(\"/Users/omtbreak/Desktop/serving_client_conf.prototxt\")\r\nclient.connect(['122.5.106.226:7788'])\r\n\r\nim = preprocess('/Users/omtbreak/Desktop/000000570688.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms\"])\r\nfetch_map[\"image\"] = '/Users/omtbreak/Desktop/000000570688.jpg'\r\npostprocess(fetch_map)\r\nprint(fetch_map)\r\n```\r\n模型开启在Ubuntu18，客户端调用代码报错\r\n```\r\nFile \"/Users/omtbreak/Desktop/untitled1/test_client.py\", line 15, in <module>\r\n    client.load_client_config(\"/Users/omtbreak/Desktop/serving_client_conf.prototxt\")\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/paddle_serving_client/__init__.py\", line 129, in load_client_config\r\n    from .serving_client import PredictorClient\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/paddle_serving_client/serving_client.so, 2): no suitable image found.  Did find:\r\n\t/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/paddle_serving_client/serving_client.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03\r\n\t/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/paddle_serving_client/serving_client.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03\r\n\r\n```",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-12T01:55:17+00:00",
        "updated_at": "2024-03-05T06:48:39+00:00",
        "closed_at": "2024-03-05T06:48:39+00:00",
        "comments_count": [
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "chengxurensheng666"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 557,
        "title": "criteo_ctr_with_cube目录中Readme的第一步编译源码的目的是什么？",
        "body": "https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/criteo_ctr_with_cube\r\n请问criteo_ctr_with_cube目录中Readme的第一步编译源码的目的是什么？文档里没有写的很明白，是必须要自己先编译源码，才能体验cube? 这个操作感觉很反锁\r\n1.是不是要先安装cmake？\r\n2.是不是必须要用python2.7来编译？",
        "state": "closed",
        "user": "jiangchao123",
        "closed_by": "bjjwwang",
        "created_at": "2020-05-14T01:47:12+00:00",
        "updated_at": "2020-05-15T07:56:00+00:00",
        "closed_at": "2020-05-15T07:56:00+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "jiangchao123",
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 559,
        "title": "paddle serving编译文档支持python3",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE.md\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE_CN.md",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-14T10:18:38+00:00",
        "updated_at": "2020-05-17T08:56:44+00:00",
        "closed_at": "2020-05-17T08:56:44+00:00",
        "comments_count": [
            "bjjwwang",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 560,
        "title": "numpy输入缺少shape检查",
        "body": "numpy输入的shape与模型预期不一致会导致server挂掉，可以在client完成检查",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-14T11:55:22+00:00",
        "updated_at": "2024-03-05T06:48:40+00:00",
        "closed_at": "2024-03-05T06:48:40+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 565,
        "title": "Cant pull serving:0.2.0-gpu",
        "body": "nvidia-docker pull hub.baidubce.com/paddlepaddle/serving:0.2.0-gpu fails to pull all the layer\r\n\r\nTried multiple times , but there is some timeout. ",
        "state": "closed",
        "user": "linux-devil",
        "closed_by": "linux-devil",
        "created_at": "2020-05-16T03:01:52+00:00",
        "updated_at": "2020-05-16T07:18:53+00:00",
        "closed_at": "2020-05-16T07:18:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "linux-devil",
            "barrierye",
            "linux-devil"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 568,
        "title": "Imagenet/Coco等经典数据集的词典、Label信息作为app的api",
        "body": "提供各个数据集的获取方法，以及在应用中直接调用的方法",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-17T10:47:45+00:00",
        "updated_at": "2024-04-16T09:04:59+00:00",
        "closed_at": "2024-04-16T09:04:59+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 562,
        "title": "paddle-serving-client源码问题",
        "body": "paddle-serving-client源码中，调用了.serving_client中的PredictorClient\r\n```\r\n    def load_client_config(self, path):\r\n        from .serving_client import PredictorClient\r\n        from .serving_client import PredictorRes\r\n        model_conf = m_config.GeneralModelConfig()\r\n        f = open(path, 'r')\r\n        model_conf = google.protobuf.text_format.Merge(\r\n            str(f.read()), model_conf)\r\n```\r\n但在使用客户端代码报ImportError\r\n```\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/paddle_serving_client/serving_client.so, 2): no suitable image found.  Did find:\r\n```\r\n查找发现，通过pip3安装的paddle-serving-client中，没有serving_client.py文件，只有serving_client.so\r\n![截屏2020-05-1423 30 20](https://user-images.githubusercontent.com/12478872/81954593-9e391780-963b-11ea-8750-bc92152d3c83.png)\r\n",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-14T15:35:47+00:00",
        "updated_at": "2024-04-16T09:04:58+00:00",
        "closed_at": "2024-04-16T09:04:58+00:00",
        "comments_count": [
            "barrierye",
            "bjjwwang",
            "omtbreak",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "omtbreak"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 566,
        "title": "To find paddle serving model",
        "body": "Where can I find pyramid_box_lite_server_mask to be able to use with  paddle_serving_server.serv",
        "state": "closed",
        "user": "linux-devil",
        "closed_by": "linux-devil",
        "created_at": "2020-05-16T12:10:51+00:00",
        "updated_at": "2020-05-17T04:22:39+00:00",
        "closed_at": "2020-05-17T04:22:39+00:00",
        "comments_count": [
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 567,
        "title": "inference_model_to_serving接口参数顺序需要修改",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_client/io/__init__.py#L106\r\n\r\n`model_filename`和`params_filename`的使用频率低于`serving_server`和`serving_client`，通常可以不写，需要放在后面",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "MRXLT",
        "created_at": "2020-05-17T02:14:33+00:00",
        "updated_at": "2020-05-19T02:39:21+00:00",
        "closed_at": "2020-05-19T02:39:21+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 572,
        "title": "用你们提供的docker源码进行编译gpu版的serving",
        "body": "提供的docker是cuda9，从docker编译出来的版本就只能适用于cuda9吗？我想编译出适用于cuda10的版本该如何编译",
        "state": "closed",
        "user": "miaochunyuan",
        "closed_by": "barrierye",
        "created_at": "2020-05-18T08:21:38+00:00",
        "updated_at": "2020-05-25T07:27:57+00:00",
        "closed_at": "2020-05-25T07:27:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "guru4elephant",
            "miaochunyuan",
            "barrierye",
            "miaochunyuan",
            "MRXLT",
            "miaochunyuan",
            "MRXLT",
            "miaochunyuan",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 575,
        "title": "多进程下和paddle一起使用时出错",
        "body": "https://github.com/PaddlePaddle/Serving/blob/ac1ba932ea66b3fb1cc0bbbdcd94bde91fd679a8/core/sdk-cpp/include/stub_impl.hpp#L330\r\n目前定位到这行代码。把宏手动展开\r\n![image](https://user-images.githubusercontent.com/10208305/82226923-b1a8f300-9959-11ea-8f6b-146ee4400b60.png)\r\n\r\n这个语句chn_options.protocol = var.parameters.protocol.value; 出错了？\r\n已经加了try catch了，是这行语句新开了线程，后台线程出错了，被paddle捕获了？\r\n![image](https://user-images.githubusercontent.com/10208305/82227081-e026ce00-9959-11ea-9db5-f74f481bb41f.png)",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-18T14:54:02+00:00",
        "updated_at": "2020-05-27T12:07:37+00:00",
        "closed_at": "2020-05-27T12:07:37+00:00",
        "comments_count": [
            "wangxicoding",
            "wangxicoding"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 585,
        "title": "运行导出的模型时报错",
        "body": "使用\r\n![image](https://user-images.githubusercontent.com/62418900/82529621-01f89e80-9b6e-11ea-9a9f-179f2ad50b71.png)\r\n这样方法导出模型后，使用python3 -m paddle_serving_server.serve --model inference_model/yolov4_cspdarknet_voc_trush --thread 2 --port 9292运行服务，报如下错误\r\n\r\n![image](https://user-images.githubusercontent.com/62418900/82529729-38ceb480-9b6e-11ea-9dcd-619d41d75543.png)\r\n求解决！",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "mcl-stone",
        "created_at": "2020-05-21T06:20:43+00:00",
        "updated_at": "2020-05-21T07:33:43+00:00",
        "closed_at": "2020-05-21T07:33:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 584,
        "title": "提供的客户端调用示例运行出错",
        "body": "提供的[示例](https://github.com/wangjiawei04/Serving/blob/10fd8cdd3180a8afa0ada144d55203e58b760a61/python/examples/cascade_rcnn/get_data.sh)为将cascade rcnn模型在服务器部署，并使用rpc方式调用。\r\n客户端报错内容为\r\n```\r\nshihao@irecog:~/example$ python test_client.py \r\nsh: patchelf: 未找到命令\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0521 00:10:52.973182  2214 config_manager.cpp:217] Not found key in configue: cluster\r\nE0521 00:10:52.973218  2214 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0521 00:10:52.973223  2214 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0521 00:10:52.973237  2214 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0521 00:10:52.973242  2214 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0521 00:10:52.973246  2214 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0521 00:10:52.973250  2214 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0521 00:10:52.973258  2214 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0521 00:10:52.973263  2214 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0521 00:10:52.973268  2214 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0521 00:10:52.973273  2214 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0521 00:10:52.973278  2214 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0521 00:10:52.973284  2214 config_manager.cpp:226] Not found key in configue: protocol\r\nE0521 00:10:52.973289  2214 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0521 00:10:52.973296  2214 config_manager.cpp:228] Not found key in configue: package_size\r\nE0521 00:10:52.973302  2214 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0521 00:10:52.973307  2214 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0521 00:10:52.973314  2214 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0521 00:10:52.978864  2214 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:8050\"): added 1\r\nW0521 00:11:06.074235  2214 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=3 SocketId=1@127.0.0.1:8050@54934 [R1][E111]Fail to connect SocketId=8589934594@127.0.0.1:8050: Connection refused [R2][E112]Fail to select server from list://127.0.0.1:8050 lb=la\r\nI0521 00:11:06.174870  2393 socket.cpp:2370] Checking SocketId=0@127.0.0.1:8050\r\n```\r\n\r\n服务器端报错内容为\r\n```\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():  \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: Fully Connected input and weigth size do not match. input width: 12544,weight height: 12548\r\n  [Hint: Expected in_mat_dims[1] == w_dims0, but received in_mat_dims[1]:12544 != w_dims0:12548.] at (/paddle/paddle/fluid/operators/fc_op.h:39)\r\n\r\n```",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-21T02:39:09+00:00",
        "updated_at": "2020-05-21T02:41:58+00:00",
        "closed_at": "2020-05-21T02:41:58+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 586,
        "title": "paddle-serving-client使用PredictorClient出现ImportError",
        "body": "使用环境\r\n```\r\nubuntu16.04\r\npython3.6 \r\n通过pip安装的paddle-serving-client   0.2.1\r\n```\r\n运行[示例](https://github.com/wangjiawei04/Serving/blob/10fd8cdd3180a8afa0ada144d55203e58b760a61/python/examples/cascade_rcnn/get_data.sh)的test_client.py时报错\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 12, in <module>\r\n    client.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\n  File \"/home/zhiyang4/.local/lib/python3.6/site-packages/paddle_serving_client/__init__.py\", line 129, in load_client_config\r\n    from .serving_client import PredictorClient\r\nImportError: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory\r\n```",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-21T08:03:33+00:00",
        "updated_at": "2020-05-21T08:28:25+00:00",
        "closed_at": "2020-05-21T08:28:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 580,
        "title": "android手机做客户端，只能用http方式与server通讯吗？",
        "body": "用rpc可行否？",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-20T07:21:26+00:00",
        "updated_at": "2024-04-16T09:05:00+00:00",
        "closed_at": "2024-04-16T09:05:00+00:00",
        "comments_count": [
            "omtbreak",
            "guru4elephant",
            "dlkht"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 583,
        "title": "Client打包上crypto的so文件",
        "body": "在这个issue #573  中，用户遇到找不到crypto的情况：\r\n```shell\r\n# ldd /home/shihao/.local/lib/python3.6/site-packages/paddle_serving_client/serving_client.so | grep libcrypto\r\n/home/shihao/.local/lib/python3.6/site-packages/paddle_serving_client/serving_client.so: /lib/x86_64-linux-gnu/libcrypto.so.10: version `OPENSSL_1.0.1_EC' not found (required by /home/shihao/.local/lib/python3.6/site-packages/paddle_serving_client/serving_client.so)\r\n/home/shihao/.local/lib/python3.6/site-packages/paddle_serving_client/serving_client.so: /lib/x86_64-linux-gnu/libcrypto.so.10: version `libcrypto.so.10' not found (required by /home/shihao/.local/lib/python3.6/site-packages/paddle_serving_client/serving_client.so)\r\n\tlibcrypto.so.10 => /lib/x86_64-linux-gnu/libcrypto.so.10 (0x00007f619feca000)\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-05-20T16:21:00+00:00",
        "updated_at": "2020-07-27T02:42:20+00:00",
        "closed_at": "2020-07-27T02:39:34+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 588,
        "title": "RPC调用方式出现inference call failed 超时错误",
        "body": "两台Ubuntu服务器A和B\r\n情况1\r\n服务器A开启Serving服务（可在本机运行test_client.py 访问127.0.0.1:port获取检测服务）\r\n服务器B远程运行test_client.py程序\r\nB服务器运行报超时错误\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0521 16:47:29.948652  2776 config_manager.cpp:217] Not found key in configue: cluster\r\nE0521 16:47:29.948694  2776 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0521 16:47:29.948699  2776 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0521 16:47:29.948711  2776 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0521 16:47:29.948719  2776 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0521 16:47:29.948727  2776 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0521 16:47:29.948736  2776 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0521 16:47:29.948745  2776 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0521 16:47:29.948752  2776 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0521 16:47:29.948761  2776 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0521 16:47:29.948770  2776 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0521 16:47:29.948778  2776 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0521 16:47:29.948786  2776 config_manager.cpp:226] Not found key in configue: protocol\r\nE0521 16:47:29.948794  2776 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0521 16:47:29.948803  2776 config_manager.cpp:228] Not found key in configue: package_size\r\nE0521 16:47:29.948812  2776 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0521 16:47:29.948820  2776 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0521 16:47:29.948829  2776 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0521 16:47:29.953624  2776 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"122.5.106.226:18050\"): added 1\r\nW0521 16:47:50.181290  2776 predictor.hpp:129] inference call failed, message: [E1008]Reached timeout=20000ms @0.0.0.0:0\r\n```\r\n\r\n情况2\r\n服务器B开启Serving服务\r\n笔记本MacOS下创建docker，docker中运行test_client.py，访问服务器B的检测服务,同样报错报错\r\n```\r\nW0521 07:12:22.125912   185 predictor.hpp:129] inference call failed, message: [E1008]Reached timeout=20000ms @0.0.0.0:0\r\n\r\n```\r\n\r\n",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-21T09:02:51+00:00",
        "updated_at": "2020-05-21T11:43:35+00:00",
        "closed_at": "2020-05-21T11:43:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 587,
        "title": "ERNIE 模型 sequence_unpad op参数shape检查出错",
        "body": "## Serving端报错\r\n```\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: The shape of Input(Length) should be [batch_size].\r\n  [Hint: Expected len_dims.size() == 1, but received len_dims.size():2 != 1:1.] at (/paddle/paddle/fluid/operators/sequence_ops/sequence_unpad_op.cc:41)\r\n  [operator < sequence_unpad > error]\r\n```\r\n报错的原因是 `paddle.fluid.layers. sequence_unpad ( x, length, name=None )` 当中的length字段不能为两个维度，预期是[1]， 实际上是[1 ,1] \r\n此api详情 https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/layers_cn/sequence_unpad_cn.html#sequence-unpad\r\n\r\n### 那么为什么是[1,1]呢？\r\n首先要介绍client端的代码\r\n```\r\nplaceholder_test = [1] * 100\r\neval_placeholder_0 = placeholder_test\r\neval_placeholder_1 = placeholder_test\r\neval_placeholder_2 = placeholder_test\r\neval_placeholder_3 = placeholder_test\r\neval_placeholder_4 = placeholder_test\r\neval_placeholder_5 = placeholder_test\r\neval_placeholder_6 = placeholder_test\r\nlen_test = [100]\r\neval_placeholder_7 = len_test\r\neval_placeholder_8 = len_test\r\neval_placeholder_9 = len_test\r\neval_placeholder_10 = len_test\r\neval_placeholder_11 = len_test\r\neval_placeholder_12 = len_test\r\neval_placeholder_13 = len_test\r\n\r\nfeed_dict = {\"eval_placeholder_0\":eval_placeholder_0, \"eval_placeholder_1\":eval_placeholder_1, \"eval_placeholder_2\":eval_placeholder_2,\r\n        \"eval_placeholder_3\":eval_placeholder_3, \"eval_placeholder_4\":eval_placeholder_4, \"eval_placeholder_5\":eval_placeholder_5, \"eval_placeholder_6\":eval_placeholder_6,\r\n\"eval_placeholder_7\":eval_placeholder_7, \"eval_placeholder_8\":eval_placeholder_8, \"eval_placeholder_9\":eval_placeholder_9,\r\n\"eval_placeholder_10\":eval_placeholder_10, \"eval_placeholder_11\":eval_placeholder_11, \"eval_placeholder_12\":eval_placeholder_12, \"eval_placeholder_13\":eval_placeholder_13}\r\n```\r\n其中`eval_placeholder_7`到`13`就是sequence_unpad的length参数。可以看到这个参数在这里传入的都是`[100]`。在这里`eval_placeholder_7`到`13` shape =1，非lod tensor。\r\n但是在 https://github.com/PaddlePaddle/Serving/blob/develop/core/general-server/op/general_reader_op.cpp#L139\r\n这个位置，general reader op有这样一个操作\r\n```\r\nlod_tensor.shape.push_back(batch_size);\r\n```\r\n### 这一步多append了一个 1，导致和sequence_unpad的length参数输入所需的shape不一样\r\n\r\n再来看下模型是怎么组网的\r\n```\r\n# 可以认为input=text_ids_a 就是 eval_placeholder_0\r\nembed1 = L.embedding(\r\n                input=text_ids_a,\r\n                size=[self.config.vocab_size, self.config.emb_size],\r\n                dtype=self._emb_dtype,\r\n                param_attr=F.ParamAttr(\r\n                    name=self._word_emb_name, initializer=self._param_initializer),\r\n                is_sparse=False)\r\n# 可以认为length = len_a 就是 eval_placeholder_7\r\nemb1 = L.sequence_unpad(embed1, length=len_a)\r\n```\r\n这是出错部分的网络结构，可以发现`eval_placeholder_7`被用来做为api的传参，前面不能加batch_size的维度\r\n\r\n应该如何解决？\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-21T08:36:38+00:00",
        "updated_at": "2024-04-16T09:05:01+00:00",
        "closed_at": "2024-04-16T09:05:01+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 573,
        "title": "docker容器内使用rpc调用目标检测服务报错",
        "body": "### 客户端\r\n#### 客户端环境\r\n客户端系统是MacOS，使用[Docker Run](https://github.com/PaddlePaddle/Serving/blob/develop/doc/RUN_IN_DOCKER_CN.md)中的方式配置了本地的docker容器环境\r\n```\r\npython3.7\r\npaddle-serving-app    0.0.3\r\npaddle-serving-client 0.2.2\r\npaddlepaddle          1.8.0\r\n```\r\n#### rpc调用代码\r\n在容器中运行rpc调用的代码, 根据[Faster RCNN on Serving](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model)进行修改得到的，使用的文件均为容器内路径, 外网访问端口为18050，内网开启端口为8050\r\n```\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(800, 1333), Transpose((2, 0, 1))\r\n])\r\npostprocess = RCNNPostprocess(\"/paddle/label_list.txt\", \"output\")\r\nclient = Client()\r\nclient.load_client_config(\"/paddle/serving_client_conf.prototxt\")\r\nclient.connect(['122.5.106.226:18050'])\r\nim = preprocess('/paddle/000000570688.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = '/paddle/000000570688.jpg'\r\npostprocess(fetch_map)\r\nprint(fetch_map)\r\n```\r\n#### serving_client_conf.prototxt\r\n根据[export serving model](https://github.com/PaddlePaddle/PaddleDetection/pull/605)得到的\r\n![截屏2020-05-1817 19 11](https://user-images.githubusercontent.com/12478872/82196036-c373a180-992b-11ea-94d5-08ff05ed18c1.png)\r\n\r\n### 服务器端\r\n\r\n#### 服务器环境\r\n```\r\nubuntu18\r\npython3.6\r\npaddle-serving-app            0.0.2                 \r\npaddle-serving-client         0.2.1                 \r\npaddle-serving-server         0.2.1 \r\n```\r\n\r\n#### serving_server_conf.prototxt\r\n同样根据[export serving model](https://github.com/PaddlePaddle/PaddleDetection/pull/605)得到的\r\n![截屏2020-05-1817 22 29](https://user-images.githubusercontent.com/12478872/82196415-3bda6280-992c-11ea-8926-6bd5b6ef7477.png)\r\n\r\n#### 启动服务命令 \r\n```\r\nGLOG_v=2 python3 -m paddle_serving_server.serve --model servingExport/cascade_rcnn_r50_fpn_1x/serving_server --port 8050\r\n```\r\n\r\n### 出现的报错\r\n服务器端报错各种维度不匹配，我在客户端代码（以上）的Sequential之前对图片尺寸进行修改大小，Sequential中的图像尺寸仍和模型训练的config中的尺寸一致，会发生不同的维度不匹配问题。\r\n我的困惑是，Sequential会对应该可以将图片转换到模型可以使用的图片尺寸，即800x1333，服务器端部署的模型在训练时也同样使用的是800x1333的图片大小，为什么会出现维度不匹配的问题。\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-18T09:48:53+00:00",
        "updated_at": "2020-06-04T07:31:32+00:00",
        "closed_at": "2020-06-04T07:31:32+00:00",
        "comments_count": [
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "barrierye",
            "barrierye",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "barrierye",
            "omtbreak",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "omtbreak",
            "bjjwwang",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "omtbreak",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 592,
        "title": "TODO paddle serving app的tutorial需要完善一下",
        "body": "```\r\npython -m paddle_serving_app.package --tutorial $MODEL_NAME\r\n```\r\n预期出现对应模型的操作步骤，这块需要添加一下。\r\n相关代码：\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_app/models/model_list.py",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-22T05:34:57+00:00",
        "updated_at": "2024-04-16T09:05:03+00:00",
        "closed_at": "2024-04-16T09:05:03+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 590,
        "title": "Client端从rpc的pb数据转换到C++的ModelRes耗时占比很大",
        "body": "profile：\r\n![image](https://user-images.githubusercontent.com/28446721/82632064-c91dff80-9c29-11ea-805e-59950c2d44ec.png)\r\n对应的代码部分：\r\n![image](https://user-images.githubusercontent.com/28446721/82632051-c28f8800-9c29-11ea-9523-a1dcb5b3e526.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-22T04:43:26+00:00",
        "updated_at": "2024-04-16T09:05:02+00:00",
        "closed_at": "2024-04-16T09:05:02+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 589,
        "title": "serving_client调用rpc预测服务时报错，可服务模型使用paddleNLP的simnet",
        "body": "保存可服务模型时，顺便打印了输入输出变量的名称\r\n```\r\n    inference_model_dir = \"bow_pairwise/exp1\"\r\n\r\n    serving_client_dir = \"temp/serving_client\"\r\n  \r\n    serving_server_dir = \"temp/serving_server\"\r\n   \r\n    feed_var_names, fetch_var_names = inference_model_to_serving(    \r\n        infer_model=inference_model_dir, serving_client=serving_client_dir,      serving_server=serving_server_dir)\r\n\r\n    print(\"feed_names:\",feed_var_names)\r\n \r\n    print(\"fetch_names:\",fetch_var_names)\r\n```\r\n\r\n打印结果为\r\n```\r\nfeed_names: dict_keys(['read_file_0.tmp_0', 'read_file_0.tmp_1'])  \r\nfetch_names: dict_keys(['fc.tmp_1', 'cos_sim_0.tmp_0'])\r\n```\r\n于是我按照这个名称来定义输入、获取输出.\r\n启动好rpc预测服务后，调用client测试\r\n```\r\n    vocab = load_vocab('term2id.dict')    \r\n    str_list = ['你好\\t你好\\t1']    \r\n    left, pos_right = Sim_reader(str_list, vocab)    \r\n    client = Client()    \r\n    client.load_client_config(\"temp/serving_server/serving_client_conf.prototxt\")    \r\n    client.connect([\"127.0.0.1:9292\"])    \r\n    feed={'read_file_0.tmp_0':left,'read_file_0.tmp_1':pos_right}    \r\n    fetch=['fc.tmp_1','cos_sim_0.tmp_0']    \r\n    fetch_res = client.predict(feed=feed, fetch=fetch) \r\n```\r\n其中Sim_reader是模仿源码里面的数据读取方式写的\r\n```\r\ndef Sim_reader(str_list, vocab):    \r\n    simnet_process = SimNetProcessor(str_list, vocab)    \r\n    startup_prog = fluid.Program()    \r\n    get_test_examples = simnet_process.get_reader()    \r\n    batch_data = fluid.io.batch(    \r\n        get_test_examples, 128, drop_last=False)    \r\n    test_prog = fluid.Program()    \r\n    inf_pyreader = fluid.layers.py_reader(    \r\n        capacity=16,    \r\n        shapes=([-1], [-1]),  \r\n        dtypes=('int64', 'int64'),  \r\n        lod_levels=(1, 1),  \r\n        name='test_reader',  \r\n        use_double_buffer=False)  \r\n    inf_pyreader.decorate_paddle_reader(batch_data)  \r\n    left, right = fluid.layers.read_file(inf_pyreader)  \r\n    return left, right\r\n```\r\n其中SimNetProcessor源码里是从文件中一行一行读，我改写成从list中按元素读了.\r\n然后我执行客户端测试，发现报一些错误\r\n\r\n```\r\nfetch_map = client.predict(feed=feed, fetch=fetch)\r\n  File \"/home/lca/.conda/envs/py36-paddle/lib/python3.6/site-packages/paddle_serving_client/__init__.py\", line 296, in predict\r\n    int_feed_names, int_shape, fetch_names, result_batch, self.pid)\r\nValueError: vector::reserve\r\n\r\n```\r\n\r\n我尝试打印过left,pos_right，是variable的类型，里面还有lod_tensor，不知道是不是里面没有实际数据...",
        "state": "closed",
        "user": "lerry-lee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-22T01:53:59+00:00",
        "updated_at": "2024-03-05T06:48:41+00:00",
        "closed_at": "2024-03-05T06:48:41+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "guru4elephant"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 594,
        "title": "部分示例缺少readme",
        "body": "以下示例缺少readme\r\ndeeplabv3\r\nmobilenet\r\nresnet50_v2\r\nunet_for_image_seg",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-22T08:41:13+00:00",
        "updated_at": "2020-06-05T12:31:27+00:00",
        "closed_at": "2020-06-05T12:31:27+00:00",
        "comments_count": [
            "guru4elephant",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 596,
        "title": "faster rcnn 示例错误",
        "body": "1：文档一处错误，test_client.py修改为new_test_client.py。\r\n\r\n2：new_test_client.py中用到sys，未import sys\r\n\r\n3：导入sys后报如下错误：\r\n![image](https://user-images.githubusercontent.com/16594411/82650845-de0c8a00-9c4d-11ea-9f3b-16d11b45b13a.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-22T09:01:32+00:00",
        "updated_at": "2020-05-27T12:01:55+00:00",
        "closed_at": "2020-05-27T12:01:55+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 595,
        "title": "serving_client调用rpc报错(FasterRCNN)",
        "body": "你好，我在测试提供的目标检测模型提供Faster RCNN Model实例 时，可以正常启动serve端，但是client端报错如下：\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0522 08:31:33.885603   161 config_manager.cpp:217] Not found key in configue: cluster\r\nE0522 08:31:33.885659   161 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0522 08:31:33.885700   161 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nE0522 08:31:33.885725   161 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nE0522 08:31:33.885735   161 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nE0522 08:31:33.885742   161 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nE0522 08:31:33.885751   161 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nE0522 08:31:33.885767   161 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nE0522 08:31:33.885776   161 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nE0522 08:31:33.885790   161 config_manager.cpp:212] Not found key in configue: connection_type\r\nE0522 08:31:33.885798   161 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nE0522 08:31:33.885807   161 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nE0522 08:31:33.885814   161 config_manager.cpp:226] Not found key in configue: protocol\r\nE0522 08:31:33.885821   161 config_manager.cpp:227] Not found key in configue: compress_type\r\nE0522 08:31:33.885829   161 config_manager.cpp:228] Not found key in configue: package_size\r\nE0522 08:31:33.885836   161 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nE0522 08:31:33.885844   161 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nE0522 08:31:33.885854   161 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0522 08:31:33.891280   161 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9494\"): added 1\r\n```\r\n测试代码也是采用实例中的test_client.py文件，麻烦看一下什么原因？",
        "state": "closed",
        "user": "wszph",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-22T08:46:39+00:00",
        "updated_at": "2024-03-05T06:48:42+00:00",
        "closed_at": "2024-03-05T06:48:42+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "wszph",
            "bjjwwang",
            "wszph",
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 597,
        "title": "PaddleDetection export得到的模型部署后出现ShapeError",
        "body": "### 前提条件\r\n#### 条件1\r\n在PaddleDetection训练得到的模型权重，由提供的[export工具](https://github.com/PaddlePaddle/PaddleDetection/pull/605)，将其转换得到serving_server、serving_client等文件\r\n\r\nserving_server 中的serving_server_conf.prototxt\r\n![截屏2020-05-2217 25 26](https://user-images.githubusercontent.com/12478872/82653119-4c068080-9c51-11ea-9c40-98fc9aaf89a6.png)\r\n\r\nserving_client中的serving_client_conf.prototxt\r\n![截屏2020-05-2217 27 45](https://user-images.githubusercontent.com/12478872/82653316-97b92a00-9c51-11ea-9cd2-c147f9408a82.png)\r\n\r\n#### 条件2\r\n同时已跑通[部署示例](https://github.com/wangjiawei04/Serving/blob/10fd8cdd3180a8afa0ada144d55203e58b760a61/python/examples/cascade_rcnn/get_data.sh)，此示例是Paddle开发人员训练的cascade rcnn模型，并提供了serving_server和serving_client等文件，在服务器开启服务后，使用rpc方式调用\r\n\r\n### 问题\r\n\r\n在跑通示例后，将我们训练的条件1中的模型，在服务器上开启了部署，本地客户端代码未更改，重新运行得到ShapeError\r\n```\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/home/chenbaicheng/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2525, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/home/chenbaicheng/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/home/chenbaicheng/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1405, in conv2d\r\n    \"data_format\": data_format,\r\n  File \"/home/chenbaicheng/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 162, in _conv_norm\r\n    name=_name + '.conv2d.output.1')\r\n  File \"/home/chenbaicheng/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 430, in c1_stage\r\n    name=_name)\r\n  File \"/home/chenbaicheng/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 451, in __call__\r\n    res = self.c1_stage(res)\r\n  File \"/home/chenbaicheng/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py\", line 98, in build\r\n    body_feats = self.backbone(im)\r\n  File \"/home/chenbaicheng/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py\", line 336, in test\r\n    return self.build(feed_vars, 'test')\r\n  File \"tools/export_serving_model.py\", line 190, in main\r\n    test_fetches = model.test(feed_vars)\r\n  File \"tools/export_serving_model.py\", line 209, in <module>\r\n    main()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: ShapeError: the input of Op(conv) should be 4-D or 5-D Tensor. But received: 2-D Tensor, the shape of input is [1, 3].\r\n  [Hint: Expected in_dims.size() == 4 || in_dims.size() == 5 == true, but received in_dims.size() == 4 || in_dims.size() == 5:0 != true:1.] at (/paddle/paddle/fluid/operators/conv_op.cc:61)\r\n  [operator < conv2d > error]\r\n```",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "omtbreak",
        "created_at": "2020-05-22T09:34:12+00:00",
        "updated_at": "2020-06-04T07:31:15+00:00",
        "closed_at": "2020-06-04T07:31:15+00:00",
        "comments_count": [
            "bjjwwang",
            "omtbreak",
            "qingqing01"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 602,
        "title": "AIStudio，python3无法使用paddle_serving_app",
        "body": "![image](https://user-images.githubusercontent.com/35550832/82734376-442c0680-9d4d-11ea-85d8-91111c1123da.png)\r\n问题如图",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-23T15:29:51+00:00",
        "updated_at": "2024-04-16T09:05:04+00:00",
        "closed_at": "2024-04-16T09:05:04+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 600,
        "title": "自己训练导出的模型，HTTP Service部署后预测失败",
        "body": "![image](https://user-images.githubusercontent.com/62418900/82722968-a230fd80-9cfd-11ea-8931-60186bd1ab85.png)\r\n使用上图示例是可以成功的，自己训练完模型后，按照Paddle Serving部署模型导出也是成功的，使用导出的模型按照示例HTTP Service部署成功后，预测时失败。\r\n![image](https://user-images.githubusercontent.com/62418900/82723000-f89e3c00-9cfd-11ea-845b-e13870738e8c.png)\r\n上图左边是自己导出后的serving_server_conf.prototxt，右边是示例中的文件。使用curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"image\": \"https://paddle-serving.bj.bcebos.com/imagenet-example/daisy.jpg\"}], \"fetch\": [\"score\"]}' http://127.0.0.1:9696/image/prediction命令预测图片，需要改动哪里？fetch:[]参数里面改了下也不行，还有起服务的脚本需要如何修改。求帮助，谢谢。",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-23T06:05:41+00:00",
        "updated_at": "2024-07-02T06:40:40+00:00",
        "closed_at": "2024-07-02T06:40:40+00:00",
        "comments_count": [
            "mcl-stone",
            "mcl-stone",
            "guru4elephant",
            "mcl-stone",
            "bjjwwang",
            "mcl-stone",
            "mcl-stone",
            "mcl-stone",
            "mcl-stone",
            "bjjwwang",
            "JiaoZiLang",
            "ghfblt",
            "Huihuihh",
            "2018-Summer"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 598,
        "title": "python端Client跨线程调用predict报错（numpy输入）",
        "body": "python端client跨线程用numpy预测的时候会偶发性报错（在不报错的情况下预测结果正常），list暂时没遇到这种情况。\r\n可能是GIL的释放导致变量生命周期管理异常（list对应的predict的GIL部分不知道什么时候不见了）\r\n\r\n报错信息：\r\n```shell\r\n# python profile_org_client.py\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0522 18:25:04.466261 28517 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nException in thread Thread-3:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python2.7/threading.py\", line 812, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python2.7/threading.py\", line 765, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"profile_org_client.py\", line 37, in func\r\n    feed={\"x\": x}, fetch=[\"price\"])\r\n  File \"/home/barriery/.local/lib/python2.7/site-packages/paddle_serving_client/__init__.py\", line 342, in predict\r\n    result_map[name].shape = shape\r\nValueError: cannot reshape array of size 0 into shape (1,1)\r\n\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python2.7/threading.py\", line 812, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python2.7/threading.py\", line 765, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"profile_org_client.py\", line 37, in func\r\n    feed={\"x\": x}, fetch=[\"price\"])\r\n  File \"/home/barriery/.local/lib/python2.7/site-packages/paddle_serving_client/__init__.py\", line 342, in predict\r\n    result_map[name].shape = shape\r\nValueError: cannot reshape array of size 0 into shape (1,1)\r\n```\r\n\r\n复现代码：\r\n```python\r\nfrom paddle_serving_client import Client\r\nimport numpy as np\r\nimport sys\r\nimport threading\r\nimport time\r\n\r\nfrom line_profiler import LineProfiler\r\nclient = Client()\r\nclient.load_client_config(\"uci_housing_client/serving_client_conf.prototxt\")\r\nclient.connect([\"127.0.0.1:9393\"])\r\n\r\nimport paddle\r\n\r\nlx = [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584,\r\n        0.6283, 0.4919, 0.1856, 0.0795, -0.0332]\r\nx = np.array(lx, dtype='float')\r\n\r\ndef func(num):\r\n\r\n    for i in range(num):\r\n        fetch_map = client.predict(\r\n            feed={\"x\": x}, fetch=[\"price\"])\r\n        # print(fetch_map)\r\n\r\nthread_num = 3\r\neach_thread = 1000\r\nths = []\r\nstart = time.time()\r\n\r\nfor i in range(thread_num):\r\n    th = threading.Thread(target=func, args=(each_thread, ))\r\n    th.start()\r\n    ths.append(th)\r\n\r\nfor i in range(thread_num):\r\n    ths[i].join()\r\n\r\nend = time.time()\r\nprint(\"time: {}\".format(end - start))\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-05-22T10:21:59+00:00",
        "updated_at": "2020-05-25T02:10:19+00:00",
        "closed_at": "2020-05-25T02:10:19+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 606,
        "title": "使用更改dockerfile构建的cuda10镜像编译后无法在cuda10中使用",
        "body": "修改了dockerfile中的FROM nvidia/cuda:10.0-cudnn7-devel-centos7构建了cuda10编译镜像，编译安装后运行会出现cuda not found的报错：\r\n`GLOG_v=2 python -m paddle_serving_server_gpu.serve --model servingExport/yolov3_darknet_voc/serving_client --port 9293 --gpu_id 5 \r\nmkdir: cannot create directory 'workdir_5': File exists\r\nCUDA not found, please check your environment or use cpu version by \"pip install paddle_serving_server\"`\r\n在docker中cuda是可以被找到并且使用的，但是一直有这样的报错",
        "state": "closed",
        "user": "miaochunyuan",
        "closed_by": "miaochunyuan",
        "created_at": "2020-05-25T01:55:45+00:00",
        "updated_at": "2020-05-25T07:14:30+00:00",
        "closed_at": "2020-05-25T07:14:30+00:00",
        "comments_count": [
            "barrierye",
            "barrierye",
            "miaochunyuan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 610,
        "title": "Paddle Serving的inference_model_to_serving接口转换模型文件的时候报错",
        "body": "代码：\r\n```\r\nimport paddle_serving_client.io as serving_io\r\nserving_io.inference_model_to_serving(\"/home/aistudio/export\", serving_server=\"serving_server\", serving_client=\"serving_client\",  model_filename='__model__', params_filename='__params__')\r\n```\r\n\r\n截图：\r\n![image](https://user-images.githubusercontent.com/15665995/82874033-f93a0b00-9f67-11ea-9a54-91228f1f4b08.png)\r\n![abcd](https://user-images.githubusercontent.com/15665995/82874395-79f90700-9f68-11ea-9743-506053d7b3c6.png)\r\n\r\n",
        "state": "closed",
        "user": "hongenge",
        "closed_by": "TeslaZhao",
        "created_at": "2020-05-26T07:48:27+00:00",
        "updated_at": "2020-11-07T09:51:42+00:00",
        "closed_at": "2020-08-31T14:27:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "BeyondYourself",
            "BeyondYourself",
            "MRXLT",
            "Shixuqing4343"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 613,
        "title": "在docker上构建服务，是否需要root权限",
        "body": "当前准备在linux环境下Docker内构建Paddle Serving，在构建docker服务中是否需要使用到root权限，",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2020-05-27T01:31:02+00:00",
        "updated_at": "2020-05-28T00:32:55+00:00",
        "closed_at": "2020-05-28T00:32:55+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "hongenge",
            "BeyondYourself"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 611,
        "title": "faster_rcnn_model运行不知所措",
        "body": "paddle-serving-app (0.0.2)\r\npaddle-serving-client (0.2.1)\r\npaddle-serving-server (0.2.1)\r\n\r\n安装教程波斯顿，运行正常， 然后我想运行下demo下 的faster_rcnn \r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/faster_rcnn_model.tar.gz\r\nwget https://paddle-serving.bj.bcebos.com/pddet_demo/infer_cfg.yml\r\ntar xf faster_rcnn_model.tar.gz\r\nmv faster_rcnn_model/pddet* ./\r\nGLOG_v=2 python -m paddle_serving_server_gpu.serve --model pddet_serving_model --port 9494 --gpu_id 0\r\n**然后预测就不行了， 需要后面怎么做？ 应该参考哪个教程**？？？？？\r\n[root@9573eac10303 faster]# python3 test_client.py pddet_client_conf/serving_client_conf.prototxt infer_cfg.yml 000000570688.jpg\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 20, in <module>\r\n    preprocess = Sequential([\r\nNameError: name 'Sequential' is not defined\r\n还有后面的RCNNPostprocess  这个是从哪里导入的？？？\r\n\r\n",
        "state": "closed",
        "user": "xiaomujiang",
        "closed_by": "xiaomujiang",
        "created_at": "2020-05-26T09:54:56+00:00",
        "updated_at": "2020-05-29T03:37:38+00:00",
        "closed_at": "2020-05-29T03:37:38+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "MRXLT",
            "xiaomujiang",
            "omtbreak",
            "xiaomujiang",
            "omtbreak",
            "xiaomujiang",
            "MRXLT",
            "xiaomujiang",
            "MRXLT",
            "omtbreak",
            "xiaomujiang",
            "xiaomujiang",
            "MRXLT",
            "guru4elephant",
            "omtbreak",
            "xiaomujiang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 616,
        "title": "增加TRT选项",
        "body": "serving增加TRT选项",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "TeslaZhao",
        "created_at": "2020-05-27T15:54:38+00:00",
        "updated_at": "2020-12-31T01:02:49+00:00",
        "closed_at": "2020-12-31T01:02:49+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "duplicate"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 617,
        "title": "暴露rpc client的timeout设置接口",
        "body": "https://github.com/PaddlePaddle/Serving/issues/611",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "guru4elephant",
        "created_at": "2020-05-28T00:47:21+00:00",
        "updated_at": "2020-06-05T12:31:16+00:00",
        "closed_at": "2020-06-05T12:31:16+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 618,
        "title": "使用Dockerfile.centos6.devel生成的镜像编译cpu server报错",
        "body": "第一次make时失败，再次执行make后编译成功\r\n![image](https://user-images.githubusercontent.com/16594411/83106780-9d9a8980-a0ef-11ea-9831-ae2540b09c77.png)\r\n![image](https://user-images.githubusercontent.com/16594411/83106759-97a4a880-a0ef-11ea-97df-8b64e2a1e2f3.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-05-28T06:29:59+00:00",
        "updated_at": "2020-06-04T02:27:22+00:00",
        "closed_at": "2020-06-04T02:27:22+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 622,
        "title": "hub.baidubce.com/ctr/paddleserving 中 命令 执行 /serving/bin/elastic_serving时，报错Illegal instruction (core dumped)",
        "body": "在模型库 个性化推荐-DeepCTR 中 镜像 hub.baidubce.com/ctr/paddleserving 中 命令 执行 /serving/bin/elastic_serving时，报错 Illegal instruction (core dumped)，有遇到过类似的问题么，求助",
        "state": "closed",
        "user": "HankPeng03",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-05-29T03:00:07+00:00",
        "updated_at": "2024-04-16T09:05:05+00:00",
        "closed_at": "2024-04-16T09:05:05+00:00",
        "comments_count": [
            "HankPeng03",
            "github-actions[bot]",
            "bjjwwang",
            "HankPeng03"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 619,
        "title": "第一次启动http预测服务时会下载两个serving tar包并报错",
        "body": "![image](https://user-images.githubusercontent.com/16594411/83106999-08e45b80-a0f0-11ea-8a9f-3f4d32272fc2.png)\r\n",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-05-28T06:32:26+00:00",
        "updated_at": "2020-06-01T02:31:05+00:00",
        "closed_at": "2020-06-01T02:31:05+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 627,
        "title": "Can't find CUDA when I use CUDA10",
        "body": "when I use CUDA10, it reports that serving can't find CUDA and I use CUDA 9 resolve it!",
        "state": "closed",
        "user": "gongweibao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-01T06:53:27+00:00",
        "updated_at": "2024-03-05T06:48:42+00:00",
        "closed_at": "2024-03-05T06:48:42+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "gongweibao",
            "barrierye",
            "guru4elephant",
            "sevenold",
            "barrierye",
            "sevenold",
            "barrierye",
            "iceriver97"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 625,
        "title": "关于paddle.fluid自训练模型线上部署的问题",
        "body": "我根据AI Studio中的教程在本地电脑用paddle.fluid复写了一个手写数字识别脚本，在本地电脑可以正常保存模型并预测输入图片的结果。之后通过docker运行paddle serving将手写数字识别模型进行线上部署预测，其中通过paddle_serving_client.io.inference_model_to_serving()将fluid.io.save_inference_model()保存的模型进行了转换。随后写了rpc的脚本，通过python -m paddle_serving_server.serve --model 模型server/ --port 9292启动了服务，在client端执行python rpc脚本.py 模型client/serving_client_conf.prototxt时，没有出现在本地环境进行测试预测数来的正确结果。\r\n本地电脑环境：Windows 64、python 3.7、paddle-serving-client 0.2.1、paddle-serving-server 0.2.1、paddlehub 1.6.1、paddlepaddle 1.8.1、Docker version：18.03.0-ce\r\n虚拟环境：使用的镜像hub.baidubce.com/paddlepaddle/serving:latest，内置python 2.7、docker info：Operating System: Boot2Docker 19.03.5 (TCL 10.1)、OSType: linux、Architecture: x86_64。\r\n![QQ图片20200530153355](https://user-images.githubusercontent.com/62195135/83366849-2cefb780-a3e4-11ea-9c32-77e2258810c4.jpg)",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "barrierye",
        "created_at": "2020-05-30T07:35:21+00:00",
        "updated_at": "2020-06-11T13:43:51+00:00",
        "closed_at": "2020-06-11T13:43:50+00:00",
        "comments_count": [
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu",
            "barrierye",
            "ClassmateXiaoyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 628,
        "title": "latest gpu镜像中执行debugger报错",
        "body": "\r\n![image](https://user-images.githubusercontent.com/16594411/83391712-73fe9c80-a426-11ea-983a-f360bed71970.png)\r\n\r\n原因：环境中缺失 libcupti.so\r\n复现脚本\r\nhttps://github.com/PaddlePaddle/Serving/tree/develop/python/paddle_serving_app#debug-tools",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "barrierye",
        "created_at": "2020-06-01T08:40:49+00:00",
        "updated_at": "2020-06-02T06:17:34+00:00",
        "closed_at": "2020-06-02T06:17:34+00:00",
        "comments_count": [
            "barrierye",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 630,
        "title": "使用docker编译得到whl包安装后，启动服务报错CUDA not found",
        "body": "使用docker编译得到paddle_serving_server_gpu-0.2.2-py3-none-any.whl，在docker外进行安装后，启动服务报错：\r\nCUDA not found, please check your environment or use cpu version by \"pip install paddle_serving_server\"\r\n\r\n但在docker内进行安装并启动服务不会报错，启动结果如下：\r\n```\r\nUse local bin : /paddle/ServingCuda10-1/build/core/general-server/serving\r\nGoing to Run Comand\r\n/paddle/ServingCuda10-1/build/core/general-server/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 10 -port 9493 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 10 -gpuid 0 -max_body_size 536870912 \r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralTextReaderOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralTextResponseOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 11626 op_repository.h:65] RAW: Succ regist op: GeneralCopyOp\r\nI0100 00:00:00.000000 11626 service_manager.h:61] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 11626 load_general_model_service.pb.h:299] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 11626 service_manager.h:61] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 11626 general_model_service.pb.h:1473] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:27] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:33] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:39] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:44] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:49] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_gpu_engine.cpp:55] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:25] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:31] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:37] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:42] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:47] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000 11626 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 11626 fluid_cpu_engine.cpp:53] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR_SIGMOID in macro!\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0601 13:35:24.102823 11626 pdserving.cpp:109] use configure[workdir_0/infer_service.prototxt] port[9493] instead of flags\r\nI0601 13:35:24.959201 11626 init.cc:66] Init commandline: dummy dummy --fraction_of_gpu_memory_to_use=0.006855 --cudnn_deterministic=True \r\nI0601 13:35:24.959260 11626 analysis_predictor.cc:84] Profiler is deactivated, and no profiling report will be generated.\r\nI0601 13:35:24.974730 11626 analysis_predictor.cc:833] MODEL VERSION: 1.7.1\r\nI0601 13:35:24.974745 11626 analysis_predictor.cc:835] PREDICTOR VERSION: 1.7.2\r\nI0601 13:35:24.975145 11626 analysis_predictor.cc:440] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\nI0601 13:35:25.131299 11626 ir_params_sync_among_devices_pass.cc:41] Sync params from CPU to GPU\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI0601 13:35:25.299015 11626 analysis_predictor.cc:462] ======= optimize end =======\r\nI0601 13:35:25.302732 11626 fluid_gpu_engine.h:215] create paddle predictor sucess, path: /paddle/PaddleDetection/paddle/paddleDetection/PaddleDetection/servingExport/faster_rcnn_r101_1x/serving_client\r\nW0601 13:35:25.302754 11626 infer.h:487] Succ load common model[0x20a99540], path[/paddle/PaddleDetection/paddle/paddleDetection/PaddleDetection/servingExport/faster_rcnn_r101_1x/serving_client].\r\nW0601 13:35:25.302768 11626 infer.h:185] Succ load model_data_path/paddle/PaddleDetection/paddle/paddleDetection/PaddleDetection/servingExport/faster_rcnn_r101_1x/serving_client\r\nI0601 13:35:25.302775 11626 infer.h:612] FLGS_logtostderr 1\r\n```\r\n-------------------------------------分割线-----------------------------------------\r\n\r\ndocker内编译设置有：Dockerfile.gpu.devel下第一行为 FROM nvidia/cuda:10.0-cudnn7-devel-centos7，docker内部cuda版本为CUDA Version 10.0.130，docker外部cuda版本同样为CUDA Version 10.0.130，剩余操作均遵循文档步骤。\r\n\r\n请问如何解决，感谢~",
        "state": "closed",
        "user": "boceng",
        "closed_by": "barrierye",
        "created_at": "2020-06-01T14:01:27+00:00",
        "updated_at": "2020-06-02T15:43:58+00:00",
        "closed_at": "2020-06-02T15:43:58+00:00",
        "comments_count": [
            "github-actions[bot]",
            "boceng",
            "bjjwwang",
            "barrierye",
            "boceng",
            "boceng",
            "boceng",
            "bjjwwang",
            "barrierye",
            "boceng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 638,
        "title": "latest镜像默认不支持中文显示",
        "body": "![image](https://user-images.githubusercontent.com/16594411/83604230-6a9d3d80-a5a8-11ea-8c96-3a94222ab804.png)\r\n在python3环境处理中文数据时会出现乱码，需要export LANG=en_US.utf8",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-06-03T06:42:56+00:00",
        "updated_at": "2020-06-04T11:14:57+00:00",
        "closed_at": "2020-06-04T11:14:57+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 636,
        "title": "Paddle Serving 还不支持ubuntu16吗",
        "body": "",
        "state": "closed",
        "user": "vinceyzw",
        "closed_by": "MRXLT",
        "created_at": "2020-06-02T12:33:06+00:00",
        "updated_at": "2020-06-30T11:26:05+00:00",
        "closed_at": "2020-06-30T11:26:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 641,
        "title": "自训练yolov3 rpc部署失败",
        "body": "如题，用PaddleDetection 自训练yolov3模型，参考rcnn的serving写rpc的部署，调用失败。（感觉yolov3很常用，可否给一个yolov3的demo指导）",
        "state": "closed",
        "user": "JiaoZiLang",
        "closed_by": "bjjwwang",
        "created_at": "2020-06-03T11:49:56+00:00",
        "updated_at": "2020-07-01T06:14:50+00:00",
        "closed_at": "2020-07-01T06:14:50+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "JiaoZiLang",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 640,
        "title": "monitor未安装依赖",
        "body": "ModuleNotFoundError: No module named 'commands'",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "MRXLT",
        "created_at": "2020-06-03T09:55:24+00:00",
        "updated_at": "2020-06-05T11:05:17+00:00",
        "closed_at": "2020-06-05T11:05:17+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 642,
        "title": "Serving输入类型不支持int32",
        "body": "在部署yolov4的时候，发现一个输入是int32，需要支持。",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "MRXLT",
        "created_at": "2020-06-03T13:12:55+00:00",
        "updated_at": "2020-06-30T06:36:12+00:00",
        "closed_at": "2020-06-30T06:36:12+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 644,
        "title": "RCNN保存模型的fetch变量没有在prototxt文件体现出lod_tensor：true这样的信息",
        "body": "https://github.com/PaddlePaddle/PaddleDetection/issues/855",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-04T02:18:19+00:00",
        "updated_at": "2024-04-16T09:05:06+00:00",
        "closed_at": "2024-04-16T09:05:06+00:00",
        "comments_count": [],
        "labels": [
            "duplicate"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 652,
        "title": "docker环境下，http服务启动 失败",
        "body": "`Traceback (most recent call last):\r\n  File \"OCRService.py\", line 34, in <module>\r\n    image_service.run_web_service()\r\n  File \"/usr/local/lib/python3.7/site-packages/paddle_serving_server/web_service.py\", line 119, in run_web_service\r\n    self.app_instance.run(host=\"0.0.0.0\",\r\nAttributeError: 'OCRService' object has no attribute 'app_instance'`\r\n我的模型已经转换为paddleServing模式；\r\n环境paddle1.8.1  python3.7  docker下启动服务报上面的异常\r\n其中的代码第34行是：\r\n`image_service.run_web_service()`\r\n\r\n\r\n第二个问题：如果使用RPC服务启动的时候总是会出现\r\n File \"OCRService.py\", line 34, in <module>\r\n    image_service.run_rpc_service()\r\n  File \"/usr/local/lib/python3.7/site-packages/paddle_serving_server/web_service.py\", line 97, in run_rpc_service\r\n    localIP = socket.gethostbyname(socket.gethostname())\r\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\r\n\r\n修改了host和各种配置文件还是没有生效，求教一下大家怎么解决",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-05T08:25:32+00:00",
        "updated_at": "2024-04-16T09:05:07+00:00",
        "closed_at": "2024-04-16T09:05:07+00:00",
        "comments_count": [],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 656,
        "title": "debugger支持lod_level=1的预测输入",
        "body": null,
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-07T08:47:12+00:00",
        "updated_at": "2024-04-16T09:05:07+00:00",
        "closed_at": "2024-04-16T09:05:07+00:00",
        "comments_count": [],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 648,
        "title": "使用Detection导出的模型在Serving 预测的效果差异较大",
        "body": "差异比较大\r\n使用的模型：cascade_rcnn_dcn_r101_vd_fpn_1x\r\nDetection预处理如下：\r\nsample_transforms:\r\n  - !DecodeImage\r\n    to_rgb: true\r\n    with_mixup: false\r\n  - !NormalizeImage\r\n    is_channel_first: false\r\n    is_scale: true\r\n    mean: [0.485,0.456,0.406]\r\n    std: [0.229, 0.224,0.225]\r\n  - !ResizeImage\r\n    interp: 1\r\n    max_size: 1333\r\n    target_size: 800\r\n    use_cv2: true\r\n  - !Permute\r\n    channel_first: true\r\n    to_bgr: false\r\n  batch_transforms:\r\n  - !PadBatch\r\n    pad_to_stride: 32\r\n    use_padded_im_info: true\r\n\r\n使用命令进行预测：\r\npython test_client.py serving_server/serving_client_conf.prototxt infer_cfg.yml input/1.jpg\r\n\r\ntest_client.py文件如下：\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport sys\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\nFile2Image(), BGR2RGB(), Div(255.0),\r\nNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\nResize(640, 640), Transpose((2, 0, 1)),PadStride(32)\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"labels.txt\", \"output\")\r\nclient = Client()\r\n\r\nclient.load_client_config(sys.argv[1])\r\nclient.connect(['127.0.0.1:9494'])\r\n\r\nim = preprocess(sys.argv[3])\r\nfetch_map = client.predict(\r\nfeed={\r\n\"image\": im,\r\n\"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n\"im_shape\": np.array(list(im.shape[1:]) + [1.0])\r\n},\r\nfetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = sys.argv[3]\r\npostprocess(fetch_map)\r\n\r\n请问这样预测出来的结果差异较大是test_client.py 预处理不一样导致的吗？\r\n应该怎么修改呢？有没有文档介绍preprocess 部分该怎么填写呢？",
        "state": "closed",
        "user": "niiikolai",
        "closed_by": "niiikolai",
        "created_at": "2020-06-04T10:50:37+00:00",
        "updated_at": "2022-04-25T03:29:11+00:00",
        "closed_at": "2022-04-25T03:29:11+00:00",
        "comments_count": [
            "github-actions[bot]",
            "BeyondYourself"
        ],
        "labels": [
            "example",
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 653,
        "title": "paddle-serving-client 最新的安装包和线上源码不一致",
        "body": "运行最新的包发现源码 save_model(serving_client, serving_server,）参数反了，发现线上源码已更新，能否及时更新安装源的包？\r\n## 安装包代码\r\n![屏幕快照 2020-06-05 下午6 09 16](https://user-images.githubusercontent.com/10573385/83865777-42a10c00-a759-11ea-9d28-b7157f2de308.png)\r\n## 线上代码\r\n![屏幕快照 2020-06-05 下午6 21 25](https://user-images.githubusercontent.com/10573385/83865884-77ad5e80-a759-11ea-9bca-1d3a7b928250.png)\r\n",
        "state": "closed",
        "user": "zoe531",
        "closed_by": "TeslaZhao",
        "created_at": "2020-06-05T10:22:13+00:00",
        "updated_at": "2020-08-31T14:22:03+00:00",
        "closed_at": "2020-08-31T14:22:03+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "zoe531",
            "barrierye"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 662,
        "title": "mem_optim和ir_optim参数指定为False时也会开启对应的功能",
        "body": "",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-06-09T07:13:06+00:00",
        "updated_at": "2020-06-10T12:40:45+00:00",
        "closed_at": "2020-06-10T12:40:45+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 657,
        "title": "feed_vars and fetch_targets for save_model for blazeface",
        "body": "请问我要用save_model来保存blazeface模型，以便在serving上运行，我应该使用什么作为feed_vars 和fetch_targets?\r\n\r\n谢谢！",
        "state": "closed",
        "user": "kgkzhiwen",
        "closed_by": "kgkzhiwen",
        "created_at": "2020-06-08T11:25:49+00:00",
        "updated_at": "2022-09-02T09:05:11+00:00",
        "closed_at": "2020-08-10T06:32:06+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "kgkzhiwen",
            "barrierye",
            "kgkzhiwen",
            "bjjwwang",
            "kgkzhiwen",
            "bjjwwang",
            "taoqinghua"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 658,
        "title": "FasterRCNN例子无法运行",
        "body": "在AiStudio上运行FasterRCNN的[例子](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model), Server端正常运行，使用test_client进行预测时，客户端报错：\r\n```\r\nW0609 09:23:51.480001   257 predictor.hpp:129] inference call failed, message: [E1008]Reached timeout=20000ms @0.0.0.0:0\r\nE0609 09:23:52.275004   257 general_model.cpp:526] failed call predictor with req: insts \r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 40, in <module>\r\n    fetch_map[\"image\"] = sys.argv[3]\r\nTypeError: 'NoneType' object does not support item assignment\r\n```",
        "state": "closed",
        "user": "cqzhao",
        "closed_by": "MRXLT",
        "created_at": "2020-06-09T01:30:23+00:00",
        "updated_at": "2020-06-30T06:34:57+00:00",
        "closed_at": "2020-06-30T06:34:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 660,
        "title": "本地镜像中未成功启动服务端",
        "body": "在服务器的linux系统中（不能连外网），在paddle的镜像中进行服务端的启动，出现以下错误：\r\n![image](https://user-images.githubusercontent.com/12640462/84102029-efbda200-aa41-11ea-94e5-f7317e33987c.png)\r\n，其中镜像中已经安装了paddle-serving-client和server，版本为：\r\npaddle-serving-client 0.2.3\r\npaddle-serving-server 0.2.3\r\n服务器已经安装了，这个是还要重新安装一次？\r\npaddle版本是1.8.1  \r\npython版本3.7.7",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2020-06-09T03:14:21+00:00",
        "updated_at": "2020-06-09T07:57:57+00:00",
        "closed_at": "2020-06-09T07:57:56+00:00",
        "comments_count": [
            "barrierye",
            "BeyondYourself",
            "barrierye",
            "BeyondYourself"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 664,
        "title": "Serving 调用时Out of Memory ",
        "body": "1. 在服务器上使用0号显卡或者1号显卡开启目标检测模型，并且服务器显卡显存充足\r\n![截屏2020-06-0915 51 22](https://user-images.githubusercontent.com/12478872/84120953-16db9a00-aa69-11ea-8be0-34f20b4ca780.png)\r\n\r\n2. 使用RPC方式调用时，服务器端报错out of memory\r\n![截屏2020-06-0915 52 28](https://user-images.githubusercontent.com/12478872/84121062-3e326700-aa69-11ea-834d-8c028a89dada.png)\r\n\r\n并且按照指示设置\r\n```\r\nexport FLAGS_fraction_of_gpu_memory_to_use=0.9\r\n```\r\n同样出现问题，没有解决",
        "state": "closed",
        "user": "omtbreak",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-09T07:53:29+00:00",
        "updated_at": "2024-03-05T06:48:43+00:00",
        "closed_at": "2024-03-05T06:48:43+00:00",
        "comments_count": [
            "MRXLT",
            "omtbreak",
            "omtbreak",
            "MRXLT",
            "omtbreak",
            "omtbreak",
            "bjjwwang",
            "sevenold"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 671,
        "title": "serving什么时候支持int32",
        "body": "自己训练的模型部署使用时报错，因为不支持int32，请问什么时候支持，求支持",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "MRXLT",
        "created_at": "2020-06-10T12:19:26+00:00",
        "updated_at": "2020-06-30T03:44:30+00:00",
        "closed_at": "2020-06-30T03:44:30+00:00",
        "comments_count": [
            "guru4elephant",
            "mcl-stone",
            "mcl-stone",
            "MRXLT",
            "MRXLT"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 672,
        "title": "Client端输入类型为list时，需要展平成一维，不然无法通过shape检查",
        "body": "在 #625 中，用户通过HTTP方式发送 image.reshape(1, 28, 28).tolist() 数据，但因为没有通过shape检查导致预测失败。\r\n\r\nClient中shape检查的相关代码：\r\n```python\r\n218     def shape_check(self, feed, key):\r\n219         if key in self.lod_tensor_set:\r\n220             return\r\n221         if isinstance(feed[key],\r\n222                       list) and len(feed[key]) != self.feed_tensor_len[key]:\r\n223             raise ValueError(\"The shape of feed tensor {} not match.\".format(\r\n224                 key))\r\n225         if type(feed[key]).__module__ == np.__name__ and np.size(feed[\r\n226                 key]) != self.feed_tensor_len[key]:\r\n227             #raise SystemExit(\"The shape of feed tensor {} not match.\".format(\r\n228             #    key))\r\n229             pass\r\n``` ",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-11T06:00:06+00:00",
        "updated_at": "2024-03-05T06:48:44+00:00",
        "closed_at": "2024-03-05T06:48:44+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 674,
        "title": "paddle-serving的RPC和支持多线程调用吗",
        "body": "1 paddle-serving的RPC和支持多线程调用吗\r\n2 服务端在处理的时候支持多线程操作吗，还是只支持多进程\r\n3 服务端部署目前支持分布式部署吗",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-12T00:58:04+00:00",
        "updated_at": "2024-04-16T09:05:08+00:00",
        "closed_at": "2024-04-16T09:05:08+00:00",
        "comments_count": [
            "MRXLT",
            "BeyondYourself",
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 675,
        "title": "lac服务只支持分词吗？",
        "body": "README中给出了lac服务示例，但是只有分词功能，看paddle下的paddlehub的lac可以分词和命名实体实体等，并且lac现在已经出了2.0. 现在的paddleserving下的lac模型是最新的吗？ 是否支持其它如词性标注、命名实体识别等工程。",
        "state": "closed",
        "user": "zhangzhen8230",
        "closed_by": "TeslaZhao",
        "created_at": "2020-06-12T03:38:13+00:00",
        "updated_at": "2020-08-31T14:03:31+00:00",
        "closed_at": "2020-08-31T14:03:31+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 684,
        "title": "paddle-serving启动成功后，相关的日志在哪里设置",
        "body": "1 服务启动成功，客户端调用成功后，有如下警告：`WARNING: Logging before InitGoogleLogging() is written to STDERR`，这个该怎么解决\r\n2 服务端相关启动日志可以从哪里设置，目前没有找到相关配置",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2020-06-16T08:46:12+00:00",
        "updated_at": "2020-08-24T00:38:21+00:00",
        "closed_at": "2020-08-24T00:38:21+00:00",
        "comments_count": [
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 682,
        "title": "在Ubuntu16环境下编译失败",
        "body": "目前是在centos6镜像下编译ubuntu16的包，没有在纯ubuntu16的环境编译过。\r\n用户用ubuntu系统的人数应该不比centos系统的少，如果按照readme进行编译，很可能出问题。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-16T07:10:49+00:00",
        "updated_at": "2024-03-05T06:48:45+00:00",
        "closed_at": "2024-03-05T06:48:45+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 677,
        "title": "模型通过paddle serving部署后的使用疑惑",
        "body": "模型已经通过paddle serving部署成功（RPC和HTTP都已实现）。现在有关于一些使用上的疑惑：\r\n现有一台操作系统为Windows 64的电脑A（以下简称为A），A的环境包括docker（内含paddlepaddle，paddlehub、paddle-serving-server、paddle-serving-client、paddle-serving-app）；一台操作系统同为Windows 64的电脑B（以下简称为B），B的环境同样包括docker（内含paddlepaddle，paddlehub、paddle-serving-server、paddle-serving-client）。\r\n情景1：A做为server端，在A docker中成功的启动了HTTP服务，此时在A docker中，无论是通过运行py脚本或是curl命令，都可以正常的预测数据。在B那边输入网址预测时，预测网址无法访问，那么B应该如何访问A的server？\r\n情景2：A做为server端，B做为client端（已安装paddle-serving-client）。在A docker中成功的启动了RPC服务，此时在A docker中，通过运行py脚本可以正常的预测数据。在B运行脚本也无法访问服务进行数据预测。\r\n注：上述2种情景在A docker中都是正常的，可以访问服务并预测数据。",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "ClassmateXiaoyu",
        "created_at": "2020-06-12T08:45:21+00:00",
        "updated_at": "2020-06-22T01:13:32+00:00",
        "closed_at": "2020-06-22T01:13:32+00:00",
        "comments_count": [
            "MRXLT",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "omtbreak",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu",
            "MRXLT",
            "ClassmateXiaoyu",
            "andy-zhangtao",
            "ClassmateXiaoyu",
            "ClassmateXiaoyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 686,
        "title": "RPC模式下，windows客户端下启动报错",
        "body": "RPC模式下，linux客户端调用服务正常，但是在windows客户端下，直接调用报错，window的paddle-serving-client为0.2版本，paddle为1.8.1  CPU模式下的调用，具体错误如下：\r\n![image](https://user-images.githubusercontent.com/12640462/84846147-b9ef6d80-b080-11ea-8afc-3b2683674114.png)\r\n看了看源码，关于paddle_serving_client.serving_client这部分是so文件，是C的实现方式，这个调用可以改进一下吗，\r\n![image](https://user-images.githubusercontent.com/12640462/84846439-66c9ea80-b081-11ea-8c08-526eec78e6f2.png)\r\n\r\n======================================================\r\nwindows下的paddle-serving-client应该是dll文件编译吧，当前的0.2版本的client是so文件是不是有问题？？",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2020-06-17T02:01:53+00:00",
        "updated_at": "2020-06-18T08:12:39+00:00",
        "closed_at": "2020-06-18T08:12:39+00:00",
        "comments_count": [
            "guru4elephant",
            "barrierye",
            "barrierye",
            "BeyondYourself"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 687,
        "title": "LATEST PACKAGES页面的client包不支持非linux平台安装",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/LATEST_PACKAGES.md",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "MRXLT",
        "created_at": "2020-06-17T02:36:45+00:00",
        "updated_at": "2020-06-18T07:07:49+00:00",
        "closed_at": "2020-06-18T07:07:49+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 690,
        "title": "imdb的模型被更新过，需要更新相应脚本的fetch变量",
        "body": "![image](https://user-images.githubusercontent.com/28446721/85003632-0c1bb600-b189-11ea-854e-acf8124d0bca.png)\r\n目前模型只有prediction一个fetch变量",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-06-18T09:28:27+00:00",
        "updated_at": "2020-07-28T06:07:11+00:00",
        "closed_at": "2020-07-28T06:07:11+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 691,
        "title": "CUBE_QUANT文档中的文件名与实际不符 && quant example不能跑通",
        "body": "1. 文档中为`cube_prepare_quant .sh`，实际为`cube_quant_prepare.sh`\r\n2. quant 例子不能跑通（非quant版本可以跑通）\r\n启动cube：\r\n```shell\r\n# sh cube_quant_prepare.sh&\r\n[1] 20547\r\ngenerate compressed sparse param sequence file\r\nVersion size: 4\r\n[libprotobuf ERROR /data/barriery/Serving/build-server/third_party/protobuf/src/extern_protobuf/src/google/protobuf/message_lite.cc:119] Can't parse message of type \"paddle.framework.proto.VarType.TensorDesc\" because it is missing required fields: (cannot determine missing fields for lite message)\r\nCannot parse tensor desc\r\nE0619 19:07:26.123454 20559 crovl_builder_increment.cpp:176] _data_buf_len == 0\r\nE0619 19:07:26.123708 20559 crovl_builder_increment.cpp:205] _index_buf_len == 0\r\n./\r\nmv: 无法获取\"./cube/data/0_0/test_dict_part0/*\" 的文件状态(stat): 没有那个文件或目录\r\n```\r\n启动server进行预测，server端报错如下：\r\n```shell\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/framework.py\", line 2525, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layers/nn.py\", line 344, in fc\r\n    \"y_num_col_dims\": 1})\r\n  File \"/Serving/python/examples/criteo_ctr_with_cube/network_conf.py\", line 46, in mlp\r\n    scale=1 / math.sqrt(mlp_input.shape[1]))))\r\n  File \"/Serving/python/examples/criteo_ctr_with_cube/network_conf.py\", line 71, in dnn_model\r\n    predict = mlp(mlp_in)\r\n  File \"local_train.py\", line 45, in train\r\n    args.sparse_feature_dim)\r\n  File \"local_train.py\", line 100, in <module>\r\n    train()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: After flatten the input tensor X and Y to 2-D dimensions matrix X1 and Y1, the matrix X1's width must be equal with matrix Y1's height. But received X's shape = [1, 845], X1's shape = [1, 845], X1's width = 845; Y's shape = [273, 400], Y1's shape = [273, 400], Y1's height = 273.\r\n  [Hint: Expected x_mat_dims[1] == y_mat_dims[0], but received x_mat_dims[1]:845 != y_mat_dims[0]:273.] at (/paddle/paddle/fluid/operators/mul_op.cc:90)\r\n  [operator < mul > error]\r\n```",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-06-19T05:30:23+00:00",
        "updated_at": "2020-07-27T02:19:47+00:00",
        "closed_at": "2020-07-27T02:19:47+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 699,
        "title": "paddle环境 brpc建多个client出错",
        "body": "python3 paddle环境另开一个进程，建多个client，出现如下错误。\r\n![image](https://user-images.githubusercontent.com/10208305/85490123-cab15d80-b603-11ea-9c3f-f964bb7bc0ea.png)\r\n\r\npython2 如果装上pathlib，出现一样的错误。\r\n![image](https://user-images.githubusercontent.com/10208305/85490728-de10f880-b604-11ea-9c22-f710a416765e.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-24T02:25:04+00:00",
        "updated_at": "2024-04-16T09:05:09+00:00",
        "closed_at": "2024-04-16T09:05:09+00:00",
        "comments_count": [],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 692,
        "title": "OCR模型 batch predict没跑通",
        "body": "http://gitlab.baidu.com/wangjiawei04/serving-ocr-benchmark",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "bjjwwang",
        "created_at": "2020-06-19T15:56:12+00:00",
        "updated_at": "2020-06-30T08:09:44+00:00",
        "closed_at": "2020-06-30T08:09:44+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 693,
        "title": "windows环境下rpc客户端报错No module named 'paddle_serving_client.serving_client'",
        "body": "![image](https://user-images.githubusercontent.com/48110081/85199496-a06d5080-b322-11ea-963d-5958030cb133.png)\r\n",
        "state": "closed",
        "user": "chengxurensheng666",
        "closed_by": "TeslaZhao",
        "created_at": "2020-06-20T10:20:22+00:00",
        "updated_at": "2020-08-31T14:02:28+00:00",
        "closed_at": "2020-08-31T14:02:28+00:00",
        "comments_count": [
            "github-actions[bot]",
            "chengxurensheng666",
            "guru4elephant",
            "barrierye",
            "chengxurensheng666",
            "guru4elephant",
            "barrierye"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 698,
        "title": "brpc client release会出问题",
        "body": "单个进程创建多个client，最后release的时候会出现下面的错误。\r\n![image](https://user-images.githubusercontent.com/10208305/85489785-1ca5b380-b603-11ea-8fd0-33244d77f8bb.png)\r\n",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-06-24T02:13:03+00:00",
        "updated_at": "2024-03-05T06:48:46+00:00",
        "closed_at": "2024-03-05T06:48:46+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 701,
        "title": "http预测服务框架选择咨询",
        "body": "```\r\n2020-06-24 05:53:05,229-INFO:  * Running on http://0.0.0.0:9393/ (Press CTRL+C to quit)\r\nmkdir: cannot create directory ‘workdir’: File exists\r\nFrist time run, downloading PaddleServing components ...\r\n--2020-06-24 05:53:05--  https://paddle-serving.bj.bcebos.com/bin/serving-cpu-avx-openblas-0.3.0.tar.gz\r\nResolving paddle-serving.bj.bcebos.com (paddle-serving.bj.bcebos.com)... 220.181.33.44, 220.181.33.43\r\nConnecting to paddle-serving.bj.bcebos.com (paddle-serving.bj.bcebos.com)|220.181.33.44|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 52132152 (50M) [application/octet-stream]\r\nSaving to: ‘serving-cpu-avx-openblas-0.3.0.tar.gz’\r\n\r\nserving-cpu-avx-openblas-0.3.0.tar.gz       100%[========================================================================================>]  49.72M  2.30MB/s    in 21s\r\n```\r\n\r\n问下上面的serving二进制`serving-cpu-avx-openblas-0.3.0.tar.gz`有开源吗？ 如果想单独使用这个二进制部署http预测服务是否可行（不用python和flask，担心flask不稳定）？",
        "state": "closed",
        "user": "levinxo",
        "closed_by": "levinxo",
        "created_at": "2020-06-24T08:27:01+00:00",
        "updated_at": "2020-07-06T02:09:52+00:00",
        "closed_at": "2020-07-06T02:09:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guru4elephant",
            "levinxo",
            "barrierye",
            "TeslaZhao",
            "levinxo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 704,
        "title": "PaddleSeRVING",
        "body": "",
        "state": "closed",
        "user": "zhuyingjun-zyj",
        "closed_by": "zhuyingjun-zyj",
        "created_at": "2020-06-29T02:40:32+00:00",
        "updated_at": "2020-06-29T02:46:10+00:00",
        "closed_at": "2020-06-29T02:46:10+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 706,
        "title": "Paddle Serving 、Paddle Inference、PaddleHub Serving三者的区别是什么？有什么样的包含关系？",
        "body": "    问题一：\r\n想用paddles erving做模型的部署，发现文档上有paddle inference也可以部署模型，想知道这两者有什么区别？分别针对什么样的业务场景\r\n  问题二：\r\n我想实现在web端一键控制模型的开始训练、停止训练、预测服务、上传自定义数据集、在web端展示模型训练的实时状态信息，paddleserving、paddleinference、paddlehub serving三个中的哪个框架可以和web端交互实现这种业务场景么？如果可以，能说一下具体的实现方式么？\r\n还希望老师们解答一下，多谢！！\r\n",
        "state": "closed",
        "user": "zhuyingjun-zyj",
        "closed_by": "TeslaZhao",
        "created_at": "2020-06-29T03:06:46+00:00",
        "updated_at": "2024-10-17T02:04:08+00:00",
        "closed_at": "2020-08-31T13:57:56+00:00",
        "comments_count": [
            "guru4elephant",
            "zhuyingjun-zyj",
            "BeyondYourself",
            "minghua-123",
            "minghua-123"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 707,
        "title": "paddle和paddle_serving_client一起导入报错",
        "body": "先导入paddle，再导入client：\r\nimport paddle\r\nfrom paddle_serving_client import Client\r\n报错信息：\r\n```\r\nTraceback (most recent call last):\r\n  File \"*****.py\", line 6, in <module>\r\n    from paddle_serving_client import Client\r\n  File \"/home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle_serving_client/__init__.py\", line 24, in <module>\r\n    from .serving_client import PredictorRes\r\nImportError: /home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle_serving_client/lib/libssl.so.10: symbol private_ossl_minimum_dh_bits, version libcrypto.so.10 not defined in file libcrypto.so.10 with link time reference\r\n```\r\n先导入client，再导入paddle和fluid\r\nfrom paddle_serving_client import Client\r\nimport paddle\r\nimport paddle.fluid as fluid\r\n报错信息：\r\n```\r\nTraceback (most recent call last):\r\n  File \"******.py\", line 6, in <module>\r\n    import paddle.fluid as fluid\r\n  File \"/home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle/fluid/__init__.py\", line 35, in <module>\r\n    from . import framework\r\n  File \"/home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 35, in <module>\r\n    from . import core\r\n  File \"/home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle/fluid/core.py\", line 194, in <module>\r\n    raise e\r\n  File \"/home/ssd6/*****/anaconda3/lib/python3.6/site-packages/paddle/fluid/core.py\", line 167, in <module>\r\n    from .core_avx import *\r\nImportError: dlopen: cannot load any more object with static TLS\r\n```\r\n单独跑paddle和client都是可以的。环境是cuda9 cudnn7.3。\r\n目前我这里升级glibc不熟练。。。一不小心机器容易挂。有没有其他安全点的方法",
        "state": "closed",
        "user": "wang-kangkang",
        "closed_by": "TeslaZhao",
        "created_at": "2020-06-29T07:34:34+00:00",
        "updated_at": "2020-08-31T13:56:47+00:00",
        "closed_at": "2020-08-31T13:56:47+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 713,
        "title": "python -m paddle_serving_app.package --get_model yolov4",
        "body": "python -m paddle_serving_app.package --get_model yolov4\r\n![image](https://user-images.githubusercontent.com/62418900/86236953-7a229d00-bbcd-11ea-93ba-3dba4d7c1c45.png)\r\n找不到model",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "mcl-stone",
        "created_at": "2020-07-01T11:03:10+00:00",
        "updated_at": "2020-07-01T13:47:11+00:00",
        "closed_at": "2020-07-01T13:47:11+00:00",
        "comments_count": [
            "bjjwwang",
            "MRXLT",
            "mcl-stone"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 719,
        "title": "自己编译的cuda10版本serving 运行后预测时报错",
        "body": "显卡T4，因为现在serving不支持cuda10,按照https://github.com/PaddlePaddle/Serving/blob/develop/doc/COMPILE_CN.md这个教程，修改Gpu的dockerfile.gpu.devel文件，仅修改FROM nvidia/cuda:10.1-cudnn7-devel-centos7这个，然后编译成后，安装可以正常运行，运行示例使用https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/yolov4，能正常跑起来，但使用下面脚本进行预测时报错。\r\n环境：\r\n![image](https://user-images.githubusercontent.com/62418900/86506633-c8b97c80-be03-11ea-9a19-d02bcdd836a4.png)\r\nserving端报错\r\n![image](https://user-images.githubusercontent.com/62418900/86506645-e5ee4b00-be03-11ea-9dcf-ad80c4da9096.png)\r\n发送脚本端报错如下：\r\n![image](https://user-images.githubusercontent.com/62418900/86506655-061e0a00-be04-11ea-9a0a-f6f3a09657ab.png)\r\n![image](https://user-images.githubusercontent.com/62418900/86506661-24840580-be04-11ea-8507-71356404d633.png)\r\n模型和代码完全使用上面链接中提供的代码和模型\r\n求解决，谢谢\r\n",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "barrierye",
        "created_at": "2020-07-04T06:39:59+00:00",
        "updated_at": "2020-07-23T08:22:52+00:00",
        "closed_at": "2020-07-23T08:22:52+00:00",
        "comments_count": [
            "mcl-stone",
            "mcl-stone",
            "mcl-stone",
            "barrierye",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 715,
        "title": "Serving GPU镜像编译二进制问题",
        "body": "使用最新的镜像`hub.baidubce.com/paddlepaddle/serving:latest-gpu-devel`作为环境进行编译，编译过程中遇到错误：\r\n`c++: error: unrecognized command line option '-Wimplicit-fallthrough=0'`\r\n升级cmake到3.3.0，gcc到9.x之后解决。\r\n\r\n另外一个错误：\r\n`cc1plus: error: -Werror=parentheses-equality: no option -Wparentheses-equality`\r\n将cmake/flags.cmake里的`-Wparentheses-equality`修改为`-Wparentheses`解决。\r\n\r\n再遇到一个错误：\r\n```\r\n-- Installing: /Serving/server-build-gpu/third_party/install/brpc/lib/pkgconfig/brpc.pc\r\n-- Installing: /Serving/server-build-gpu/third_party/install/brpc/lib/libbrpc.so\r\n-- Installing: /Serving/server-build-gpu/third_party/install/brpc/lib/libbrpc.a\r\n[ 47%] Completed 'extern_brpc'\r\n[ 47%] Built target extern_brpc\r\nmake: *** [all] Error 2\r\n```\r\n\r\n将cmake/flags.cmake里的`set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")`\r\n改为`set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -lpthread\")` 解决\r\n\r\n还有以下错误，安装和升级对应pip库即可：\r\n```\r\n[100%] Generating ../.timestamp\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 24, in <module>\r\n    from paddle_serving_server_gpu.version import serving_server_version\r\n  File \"/jiangjiajun/Serving/server-build-gpu/python/paddle_serving_server_gpu/__init__.py\", line 30, in <module>\r\n    import numpy as np\r\nModuleNotFoundError: No module named 'numpy'\r\n\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 24, in <module>\r\n    from paddle_serving_server_gpu.version import serving_server_version\r\n  File \"/jiangjiajun/Serving/server-build-gpu/python/paddle_serving_server_gpu/__init__.py\", line 32, in <module>\r\n    from .proto import multi_lang_general_model_service_pb2\r\n  File \"/jiangjiajun/Serving/server-build-gpu/python/paddle_serving_server_gpu/proto/multi_lang_general_model_service_pb2.py\", line 21, in <module>\r\n    create_key=_descriptor._internal_create_key,\r\nAttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'\r\n```\r\n\r\n```\r\npip3 install --upgrade pip && \\\r\npip3 uninstall protobuf && \\\r\npip3 install grpcio-tools -i https://mirrors.aliyun.com/pypi/simple/ &&\\\r\npip3 install numpy -i https://mirrors.aliyun.com/pypi/simple/  && \\\r\npip3 install --upgrade protobuf -i https://mirrors.aliyun.com/pypi/simple/ \r\n```\r\n",
        "state": "closed",
        "user": "levinxo",
        "closed_by": "levinxo",
        "created_at": "2020-07-03T05:55:03+00:00",
        "updated_at": "2020-09-01T01:15:18+00:00",
        "closed_at": "2020-09-01T01:15:18+00:00",
        "comments_count": [
            "TeslaZhao",
            "levinxo"
        ],
        "labels": [
            "编译问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 721,
        "title": "pipeline 优化 profile 日志",
        "body": "> 人去看这个日志很费劲\r\n\r\n![image](https://user-images.githubusercontent.com/28446721/86562513-61cbcd00-bf95-11ea-9c78-490e02ca625e.png)\r\n\r\n可以考虑在channel中存日志，传到client端（server端由于dag跨请求，不方便传到client）",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-06T06:31:37+00:00",
        "updated_at": "2024-04-16T09:05:10+00:00",
        "closed_at": "2024-04-16T09:05:10+00:00",
        "comments_count": [
            "cg82616424",
            "barrierye"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 720,
        "title": "ocr web demo跑不通",
        "body": "https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/ocr\r\n```\r\n    rec_idx_tmp = rec_idx_batch[beg:end, 0]\r\nTypeError: list indices must be integers, not tuple \r\n```\r\n",
        "state": "closed",
        "user": "guru4elephant",
        "closed_by": "bjjwwang",
        "created_at": "2020-07-05T07:07:30+00:00",
        "updated_at": "2020-07-17T03:05:38+00:00",
        "closed_at": "2020-07-17T03:05:38+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 724,
        "title": "pipe line 日志统一设计下",
        "body": "1. debug\\trace\\notice\\warning\\fatal日志都应该有哪些，需要设计下",
        "state": "closed",
        "user": "cg82616424",
        "closed_by": "barrierye",
        "created_at": "2020-07-06T09:11:36+00:00",
        "updated_at": "2020-08-17T05:09:43+00:00",
        "closed_at": "2020-08-17T05:09:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "cg82616424"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 726,
        "title": "NEW WEB SERVICE文档外链404",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/NEW_WEB_SERVICE.md\r\n\r\n第一段\r\nThis document will take the image classification service based on the Imagenet data set as an example to introduce how to develop a new web service. The complete code can be visited at [here](https://github.com/PaddlePaddle/Serving/blob/develop/python/examples/imagenet/image_classification_service.py).\r\n\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "MRXLT",
        "created_at": "2020-07-06T10:17:38+00:00",
        "updated_at": "2020-07-14T06:26:24+00:00",
        "closed_at": "2020-07-14T06:26:24+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 728,
        "title": "import paddle_serving_client 时报错，TypeError: __new__() got an unexpected keyword argument 'serialized_options'",
        "body": "<img width=\"1440\" alt=\"截屏2020-07-08下午7 43 19\" src=\"https://user-images.githubusercontent.com/19721227/86914678-48707f80-c153-11ea-97d9-c4ddef806386.png\">\r\n",
        "state": "closed",
        "user": "qjing666",
        "closed_by": "MRXLT",
        "created_at": "2020-07-08T11:43:29+00:00",
        "updated_at": "2020-07-14T06:26:39+00:00",
        "closed_at": "2020-07-14T06:26:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "qjing666",
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 736,
        "title": "部署时报错",
        "body": "使用serving部署，使用debug脚本检测报错如下：\r\n[root@c99a526ec87f paddle]# python3.6 test_client2.py \r\nW0712 09:09:58.147349  1843 analysis_predictor.cc:133] Profiler is activated, which might affect the performance\r\nI0712 09:09:58.785828  1843 analysis_predictor.cc:872] MODEL VERSION: 1.7.1\r\nI0712 09:09:58.785851  1843 analysis_predictor.cc:874] PREDICTOR VERSION: 1.8.2\r\nI0712 09:09:58.786041  1843 analysis_predictor.cc:471] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\nI0712 09:09:58.998006  1843 ir_params_sync_among_devices_pass.cc:41] Sync params from CPU to GPU\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI0712 09:09:59.093183  1843 analysis_predictor.cc:493] ======= optimize end =======\r\nW0712 09:09:59.122561  1843 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 10.2, Runtime API Version: 10.0\r\nW0712 09:09:59.126153  1843 device_context.cc:260] device: 0, cuDNN Version: 7.6.\r\nTraceback (most recent call last):\r\n  File \"test_client2.py\", line 38, in <module>\r\n    fetch=[\"multiclass_nms\"])\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/local_predict.py\", line 127, in predict\r\n    outputs = self.predictor.run(inputs)\r\npaddle.fluid.core_avx.EnforceNotMet: \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)\r\n2   paddle::operators::ConvOp::GetExpectedKernelType(paddle::framework::ExecutionContext const&) const\r\n3   paddle::framework::OperatorWithKernel::ChooseKernel(paddle::framework::RuntimeContext const&, paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const\r\n5   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)\r\n7   paddle::framework::NaiveExecutor::Run()\r\n8   paddle::AnalysisPredictor::Run(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, int)\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/framework.py\", line 2525, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/usr/lib64/python2.7/site-packages/paddle/fluid/layers/nn.py\", line 1405, in conv2d\r\n    \"data_format\": data_format,\r\n  File \"/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 162, in _conv_norm\r\n    name=_name + '.conv2d.output.1')\r\n  File \"/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 430, in c1_stage\r\n    name=_name)\r\n  File \"/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 451, in __call__\r\n    res = self.c1_stage(res)\r\n  File \"/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py\", line 89, in build\r\n    body_feats = self.backbone(im)\r\n  File \"/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py\", line 248, in test\r\n    return self.build(feed_vars, 'test')\r\n  File \"tools/export_model.py\", line 108, in main\r\n    test_fetches = model.test(feed_vars)\r\n  File \"tools/export_model.py\", line 125, in <module>\r\n    main()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: input and filter data type should be consistent\r\n  [Hint: Expected input_data_type == filter_data_type, but received input_data_type:2 != filter_data_type:5.] at (/paddle/paddle/fluid/operators/conv_op.cc:173)\r\n  [operator < conv2d > error]\r\nW0712 09:10:02.100517  1923 device_tracer.cc:53] Invalid timestamp occurred. Please try increasing the FLAGS_multiple_of_cupti_buffer_size.\r\n\r\n------------------------->     Profiling Report     <-------------------------\r\n\r\nPlace: All\r\nTime unit: ms\r\nSorted by total time in descending order in the same thread\r\n\r\nTotal time: 303.444\r\n  Computation time       Total: 201.3       Ratio: 66.3387%\r\n  Framework overhead     Total: 102.143     Ratio: 33.6613%\r\n\r\n-------------------------     GpuMemCpy Summary     -------------------------\r\n\r\nGpuMemcpy                Calls: 172         Total: 83.4565     Ratio: 27.5031%\r\n  GpuMemcpyAsync         Calls: 3           Total: 3.2345      Ratio: 1.06593%\r\n  GpuMemcpySync          Calls: 169         Total: 80.222      Ratio: 26.4372%\r\n\r\n-------------------------       Event Summary       -------------------------\r\n\r\nEvent                                   Calls       Total       CPU Time (Ratio)        GPU Time (Ratio)        Min.        Max.        Ave.        Ratio.      \r\nthread0::load                           169         202.819     202.819352 (1.000000)   0.000000 (0.000000)     0.017028    164.441     1.20011     0.668392    \r\nthread0::GpuMemcpySync:CPU->GPU         169         80.222      40.694908 (0.507279)    39.527102 (0.492721)    0.011679    23.2058     0.474686    0.264372    \r\nthread0::conv2d                         1           17.1275     17.127503 (1.000000)    0.000000 (0.000000)     17.1275     17.1275     17.1275     0.0564438   \r\nthread0::GpuMemcpyAsync:CPU->GPU        3           3.2345      1.820567 (0.562860)     1.413929 (0.437140)     0.011064    3.19608     1.07817     0.0106593   \r\nthread0::feed                           3           0.040244    0.040244 (1.000000)     0.000000 (0.000000)     0.006155    0.025493    0.0134147   0.000132624 \r\n",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "mcl-stone",
        "created_at": "2020-07-12T09:16:24+00:00",
        "updated_at": "2020-07-13T00:57:30+00:00",
        "closed_at": "2020-07-13T00:57:30+00:00",
        "comments_count": [
            "mcl-stone"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 745,
        "title": "tensorRT 支持",
        "body": "需要把 yolov3 darknet53 和yolov3 mobilenetv1 部署到边缘进行推理。\r\n使用 paddle trt 的docker无法驱动GPU（docker pull hub.baidubce.com/paddlepaddle/paddle:1.8.0-gpu-cuda10.0-cudnn7-trt6）https://paddle-inference.readthedocs.io/en/latest/optimize/paddle_trt.html",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-17T14:47:09+00:00",
        "updated_at": "2024-04-16T09:05:11+00:00",
        "closed_at": "2024-04-16T09:05:11+00:00",
        "comments_count": [],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 738,
        "title": "NEW_WEB_SERVICE_CN文档有个死链",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/NEW_WEB_SERVICE_CN.md",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "MRXLT",
        "created_at": "2020-07-13T09:31:11+00:00",
        "updated_at": "2020-07-14T06:26:14+00:00",
        "closed_at": "2020-07-14T06:26:14+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 739,
        "title": "如何优雅的关闭服务",
        "body": "在启动paddle-serving后，如果要停止服务，目前我这边只能执行：`kill -9 进程ID`\r\n但是在查看后台后，发现会有defunct进程出现。该如何恰当的停止服务，有这方面的建议吗，谢谢",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-15T01:08:22+00:00",
        "updated_at": "2024-03-05T06:48:47+00:00",
        "closed_at": "2024-03-05T06:48:47+00:00",
        "comments_count": [
            "MRXLT",
            "BeyondYourself",
            "MRXLT",
            "60999",
            "MRXLT",
            "60999"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 742,
        "title": "runtime Dokerfile 需加入gcc-c++",
        "body": "如题，cube demo 使用时需yum install gcc-c++。从镜像易用性角度建议runtime Dokerfile 需加入gcc-c++",
        "state": "closed",
        "user": "hysunflower",
        "closed_by": "barrierye",
        "created_at": "2020-07-16T04:14:02+00:00",
        "updated_at": "2020-07-16T06:56:12+00:00",
        "closed_at": "2020-07-16T06:56:12+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 744,
        "title": "cuda10支持",
        "body": "from 微信区用户 @海洋",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-07-17T14:43:42+00:00",
        "updated_at": "2020-07-23T08:22:34+00:00",
        "closed_at": "2020-07-23T08:22:34+00:00",
        "comments_count": [
            "mcl-stone",
            "barrierye",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 747,
        "title": "paddle-serving-server 的 Docker 镜像内安装 paddle-serving-server 报错",
        "body": "# 问题描述\r\n进入教程提供的Docker 容器后安装 paddle-serving-server 会报错，是 grpcio 这个包导致的。\r\n根据这个教程来做的  https://github.com/PaddlePaddle/Serving ， 使用的是 CPU 版本的Docker 镜像。\r\n\r\n# 相关信息：\r\n操作系统：macOS 10.14.6/ Ubuntu 16 也有同样问题\r\ndocker desktop version 2.3.0.3\r\nDocker version 19.03.8\r\n\r\n# 重现流程：\r\n安装教程\r\n```\r\ndocker pull hub.baidubce.com/paddlepaddle/serving:latest\r\ndocker run -p 9292:9292 --name test -dit hub.baidubce.com/paddlepaddle/serving:latest\r\ndocker exec -it test bash\r\n```\r\n\r\n在镜像内部执行 \r\n`pip3 install paddle-serving-server -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com`\r\n\r\n# 错误信息：\r\n\r\n> ...\r\n> gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -DOPENSSL_NO_ASM=1 -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=extern \"C\" __attribute__((visibility (\"default\"))) PyObject* -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/abseil-cpp -Ithird_party/address_sorting/include -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -Ithird_party/boringssl-with-bazel/src/include -Ithird_party/upb -Isrc/core/ext/upb-generated -Ithird_party/zlib -I/usr/include/python3.6m -c src/python/grpcio/grpc/_cython/cygrpc.cpp -o python_build/temp.linux-x86_64-3.6/src/python/grpcio/grpc/_cython/cygrpc.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n>     gcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\n>     creating tmp\r\n>     creating tmp/tmptsxr4dqp\r\n>     gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c /tmp/tmptsxr4dqp/a.c -o tmp/tmptsxr4dqp/a.o\r\n>     Traceback (most recent call last):\r\n>       File \"/usr/lib64/python3.6/distutils/unixccompiler.py\", line 127, in _compile\r\n>         extra_postargs)\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/_spawn_patch.py\", line 54, in _commandfile_spawn\r\n>         _classic_spawn(self, command)\r\n>       File \"/usr/lib64/python3.6/distutils/ccompiler.py\", line 909, in spawn\r\n>         spawn(cmd, dry_run=self.dry_run)\r\n>       File \"/usr/lib64/python3.6/distutils/spawn.py\", line 36, in spawn\r\n>         _spawn_posix(cmd, search_path, dry_run=dry_run)\r\n>       File \"/usr/lib64/python3.6/distutils/spawn.py\", line 159, in _spawn_posix\r\n>         % (cmd, exit_status))\r\n>     distutils.errors.DistutilsExecError: command 'gcc' failed with exit status 1\r\n> \r\n>     During handling of the above exception, another exception occurred:\r\n> \r\n>     Traceback (most recent call last):\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/commands.py\", line 262, in build_extensions\r\n>         build_ext.build_ext.build_extensions(self)\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 448, in build_extensions\r\n>         self._build_extensions_serial()\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 473, in _build_extensions_serial\r\n>         self.build_extension(ext)\r\n>       File \"/usr/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 199, in build_extension\r\n>         _build_ext.build_extension(self, ext)\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 533, in build_extension\r\n>         depends=ext.depends)\r\n>       File \"/usr/lib64/python3.6/distutils/ccompiler.py\", line 574, in compile\r\n>         self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/commands.py\", line 246, in new_compile\r\n>         pp_opts)\r\n>       File \"/usr/lib64/python3.6/distutils/unixccompiler.py\", line 129, in _compile\r\n>         raise CompileError(msg)\r\n>     distutils.errors.CompileError: command 'gcc' failed with exit status 1\r\n> \r\n>     During handling of the above exception, another exception occurred:\r\n> \r\n>     Traceback (most recent call last):\r\n>       File \"<string>\", line 1, in <module>\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/setup.py\", line 421, in <module>\r\n>         cmdclass=COMMAND_CLASS,\r\n>       File \"/usr/lib/python3.6/site-packages/setuptools/__init__.py\", line 129, in setup\r\n>         return distutils.core.setup(**attrs)\r\n>       File \"/usr/lib64/python3.6/distutils/core.py\", line 148, in setup\r\n>         dist.run_commands()\r\n>       File \"/usr/lib64/python3.6/distutils/dist.py\", line 955, in run_commands\r\n>         self.run_command(cmd)\r\n>       File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n>         cmd_obj.run()\r\n>       File \"/usr/lib/python3.6/site-packages/setuptools/command/install.py\", line 61, in run\r\n>         return orig.install.run(self)\r\n>       File \"/usr/lib64/python3.6/distutils/command/install.py\", line 556, in run\r\n>         self.run_command('build')\r\n>       File \"/usr/lib64/python3.6/distutils/cmd.py\", line 313, in run_command\r\n>         self.distribution.run_command(command)\r\n>       File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n>         cmd_obj.run()\r\n>       File \"/usr/lib64/python3.6/distutils/command/build.py\", line 135, in run\r\n>         self.run_command(cmd_name)\r\n>       File \"/usr/lib64/python3.6/distutils/cmd.py\", line 313, in run_command\r\n>         self.distribution.run_command(command)\r\n>       File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n>         cmd_obj.run()\r\n>       File \"/usr/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 78, in run\r\n>         _build_ext.run(self)\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 339, in run\r\n>         self.build_extensions()\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/commands.py\", line 267, in build_extensions\r\n>         \"Failed `build_ext` step:\\n{}\".format(formatted_exception))\r\n>     commands.CommandError: Failed `build_ext` step:\r\n>     Traceback (most recent call last):\r\n>       File \"/usr/lib64/python3.6/distutils/unixccompiler.py\", line 127, in _compile\r\n>         extra_postargs)\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/_spawn_patch.py\", line 54, in _commandfile_spawn\r\n>         _classic_spawn(self, command)\r\n>       File \"/usr/lib64/python3.6/distutils/ccompiler.py\", line 909, in spawn\r\n>         spawn(cmd, dry_run=self.dry_run)\r\n>       File \"/usr/lib64/python3.6/distutils/spawn.py\", line 36, in spawn\r\n>         _spawn_posix(cmd, search_path, dry_run=dry_run)\r\n>       File \"/usr/lib64/python3.6/distutils/spawn.py\", line 159, in _spawn_posix\r\n>         % (cmd, exit_status))\r\n>     distutils.errors.DistutilsExecError: command 'gcc' failed with exit status 1\r\n> \r\n>     During handling of the above exception, another exception occurred:\r\n> \r\n>     Traceback (most recent call last):\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/commands.py\", line 262, in build_extensions\r\n>         build_ext.build_ext.build_extensions(self)\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 448, in build_extensions\r\n>         self._build_extensions_serial()\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 473, in _build_extensions_serial\r\n>         self.build_extension(ext)\r\n>       File \"/usr/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 199, in build_extension\r\n>         _build_ext.build_extension(self, ext)\r\n>       File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 533, in build_extension\r\n>         depends=ext.depends)\r\n>       File \"/usr/lib64/python3.6/distutils/ccompiler.py\", line 574, in compile\r\n>         self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)\r\n>       File \"/tmp/pip-build-_r9c2jgo/grpcio/src/python/grpcio/commands.py\", line 246, in new_compile\r\n>         pp_opts)\r\n>       File \"/usr/lib64/python3.6/distutils/unixccompiler.py\", line 129, in _compile\r\n>         raise CompileError(msg)\r\n>     distutils.errors.CompileError: command 'gcc' failed with exit status 1\r\n> \r\n> \r\n>     ----------------------------------------\r\n> Command \"/usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-_r9c2jgo/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-crzaccdf-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-_r9c2jgo/grpcio/\r\n> ",
        "state": "closed",
        "user": "nickyoung889",
        "closed_by": "nickyoung889",
        "created_at": "2020-07-20T08:13:12+00:00",
        "updated_at": "2020-07-23T08:36:32+00:00",
        "closed_at": "2020-07-23T08:36:32+00:00",
        "comments_count": [
            "github-actions[bot]",
            "nickyoung889",
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 748,
        "title": "ocr web server 教程跑不起来",
        "body": "这个是官方教程 https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/ocr\r\n\r\n按照教程安装好依赖，执行\r\n`\r\npython3 rec_web_server.py\r\n`\r\n得到报错\r\n`\r\ncannot import name 'GetRotateCropImage' from 'paddle_serving_app.reader'\r\n`\r\n刚刚看了源码，这个错误是因为没有及时更新 paddle_serving_app 包，教程出来了，代码没跟上。。。。。。\r\n\r\nrec_web_server.py  实际上不需要依赖  paddle_serving_app 这些东西，清除不需要的依赖后，可以跑起来了。\r\n\r\n模拟发送请求\r\n`\r\npython3 rec_web_client.py\r\n`\r\n得到以下错误：\r\n\r\n> 2020-07-20 18:16:56,126-ERROR: Exception on /ocr/prediction [POST]\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 2447, in wsgi_app\r\n>     response = self.full_dispatch_request()\r\n>   File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1945, in full_dispatch_request\r\n>     self.try_trigger_before_first_request_functions()\r\n>   File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1993, in try_trigger_before_first_request_functions\r\n>     func()\r\n>   File \"/usr/local/lib/python3.7/site-packages/paddle_serving_server/web_service.py\", line 108, in init\r\n>     self._launch_web_service()\r\n>   File \"/usr/local/lib/python3.7/site-packages/paddle_serving_server/web_service.py\", line 70, in _launch_web_service\r\n>     self.client = Client()\r\n>   File \"/usr/local/lib/python3.7/site-packages/paddle_serving_client/__init__.py\", line 136, in __init__\r\n>     from .serving_client import PredictorRes\r\n> ImportError: dlopen(/usr/local/lib/python3.7/site-packages/paddle_serving_client/serving_client.so, 2): no suitable image found.  Did find:\r\n> \t/usr/local/lib/python3.7/site-packages/paddle_serving_client/serving_client.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03\r\n> \t/usr/local/lib/python3.7/site-packages/paddle_serving_client/serving_client.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03\r\n> 2020-07-20 18:16:56,127-INFO: 127.0.0.1 - - [20/Jul/2020 18:16:56] \"POST /ocr/prediction HTTP/1.1\" 500 -\r\n\r\n",
        "state": "closed",
        "user": "nickyoung889",
        "closed_by": "nickyoung889",
        "created_at": "2020-07-20T10:19:19+00:00",
        "updated_at": "2020-07-23T08:37:05+00:00",
        "closed_at": "2020-07-23T08:37:05+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 758,
        "title": "cpu docker 安装py3版本的paddle-serving-server时遇到gcc的问题",
        "body": "docker：hub.baidubce.com/paddlepaddle/serving:latest\r\npip命令：pip install paddle-serving-server\r\n报错如下\r\n![image](https://user-images.githubusercontent.com/16594411/88512973-ff0ca500-d019-11ea-808c-a720a9a442de.png)\r\n更多详细信息可以参考\r\nhttps://github.com/PaddlePaddle/Serving/issues/747",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "MRXLT",
        "created_at": "2020-07-27T07:02:20+00:00",
        "updated_at": "2020-07-27T11:49:03+00:00",
        "closed_at": "2020-07-27T11:49:03+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 754,
        "title": " paddle_serving_server 报错",
        "body": " 在servering 启动服务的时候，出现了cuda找不到\r\nPaddleDetection/serving_model# \r\n export CUDA_VISIBLE_DEVICES=0\r\npython -m paddle_serving_server_gpu.serve --model serving_model/ --thread 10 --port 9292 --gpu_ids 0\r\nCUDA not found, please check your environment or use cpu version by \"pip install paddle_serving_server\"\r\n\r\n版本如下\r\npaddle-serving-app        0.1.1\r\npaddle-serving-client     0.3.1\r\npaddle-serving-server     0.3.1\r\npaddle-serving-server-gpu 0.3.1\r\npaddlehub                 1.6.2\r\npaddlepaddle-gpu          1.8.1.post107\r\npython 3.7\r\n查看显卡如下\r\n nvidia-smi\r\nWed Jul 22 17:14:09 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro RTX 4000     Off  | 00000000:5E:00.0  On |                  N/A |\r\n| 30%   36C    P8     2W / 125W |    603MiB /  7979MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     27371      G   /usr/lib/xorg/Xorg                           169MiB |\r\n|    0     28318      G   /usr/bin/gnome-shell                         199MiB |\r\n|    0     33752      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   209MiB |\r\n|    0     36758      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files    22MiB |\r\n+-----------------------------------------------------------------------------+\r\n",
        "state": "closed",
        "user": "xiaomujiang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-22T09:16:16+00:00",
        "updated_at": "2024-03-05T06:48:48+00:00",
        "closed_at": "2024-03-05T06:48:48+00:00",
        "comments_count": [
            "barrierye",
            "xiaomujiang",
            "barrierye"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 765,
        "title": "pipeline支持auto-batching",
        "body": "",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-07-28T05:57:30+00:00",
        "updated_at": "2020-08-17T05:09:43+00:00",
        "closed_at": "2020-08-17T05:09:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 756,
        "title": "能有一个yolov3的demo么？？？",
        "body": "如题，yolov3用的应该还挺多的吧，求各位大大给个yolov3的demo",
        "state": "closed",
        "user": "JiaoZiLang",
        "closed_by": "TeslaZhao",
        "created_at": "2020-07-24T08:22:26+00:00",
        "updated_at": "2020-12-31T01:04:21+00:00",
        "closed_at": "2020-12-31T01:04:21+00:00",
        "comments_count": [
            "JiaoZiLang",
            "guru4elephant",
            "bjjwwang"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 766,
        "title": "添加cuda10.1的镜像",
        "body": "厂内使用cuda10.1，cuda10.2",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-28T05:57:55+00:00",
        "updated_at": "2024-04-16T09:05:12+00:00",
        "closed_at": "2024-04-16T09:05:12+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": [
            "install",
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 763,
        "title": "ubuntu18.04下无法执行推断报错",
        "body": "![image](https://user-images.githubusercontent.com/25734125/88608107-8d813500-d0b3-11ea-9cfc-15ff56f0f48e.png)\r\n\r\n环境为虚拟机ubuntu18.04，检测模块单独可以正常使用，一起使用时就会报这个错误。",
        "state": "closed",
        "user": "coder-sun",
        "closed_by": "TeslaZhao",
        "created_at": "2020-07-28T01:21:51+00:00",
        "updated_at": "2020-08-13T04:50:13+00:00",
        "closed_at": "2020-08-13T04:50:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Steffy-zxf",
            "coder-sun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 767,
        "title": "显卡显存不释放？",
        "body": "随着调用次数的增多， 显存占用率不断增加！！！！\r\n![image](https://user-images.githubusercontent.com/41443157/88653247-62700300-d0fe-11ea-99bf-2268781aaabf.png)\r\n![image](https://user-images.githubusercontent.com/41443157/88653421-a105bd80-d0fe-11ea-9060-ef311752aea0.png)\r\n显存满了后 ，就会报错\r\n![image](https://user-images.githubusercontent.com/41443157/88653539-cdb9d500-d0fe-11ea-8f9c-afa7fccf37e8.png)\r\n然后显存会恢复到最初的占用率\r\n![image](https://user-images.githubusercontent.com/41443157/88653593-e0340e80-d0fe-11ea-9752-a5dcf4e33f8a.png)\r\n需要重启服务，才可以重新开始预测！\r\n不断反复\r\n![image](https://user-images.githubusercontent.com/41443157/88653816-22f5e680-d0ff-11ea-8f41-27b1a0a398de.png)\r\n使用的模型为:https://github.com/PaddlePaddle/PaddleOCR/tree/develop/deploy/pdserving",
        "state": "closed",
        "user": "sevenold",
        "closed_by": "TeslaZhao",
        "created_at": "2020-07-28T10:24:02+00:00",
        "updated_at": "2022-09-23T01:10:41+00:00",
        "closed_at": "2020-08-31T12:03:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "sevenold",
            "MRXLT",
            "sevenold",
            "MRXLT",
            "sevenold",
            "MRXLT",
            "EtachGu",
            "Juruobudong",
            "hq0749a",
            "Juruobudong"
        ],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 768,
        "title": "gcc4.8.4 源码编译出错",
        "body": "![image](https://user-images.githubusercontent.com/6189742/88655883-fc857a80-d101-11ea-814e-422c263def9a.png)\r\n\r\n\r\n```\r\n[100%] Linking CXX executable ../../output/bin/rpc_replay\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(version_set.o): In function `leveldb::VersionSet::MakeInputIterator(leveldb::Compaction*)':\r\nversion_set.cc:(.text+0x18a7): undefined reference to `__cxa_throw_bad_array_new_length'\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(table_builder.o): In function `leveldb::TableBuilder::WriteBlock(leveldb::BlockBuilder*, leveldb::BlockHandle*)':\r\ntable_builder.cc:(.text+0x9f2): undefined reference to `snappy::MaxCompressedLength(unsigned long)'\r\ntable_builder.cc:(.text+0xa30): undefined reference to `snappy::RawCompress(char const*, unsigned long, char*, unsigned long*)'\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(format.o): In function `leveldb::ReadBlock(leveldb::RandomAccessFile*, leveldb::ReadOptions const&, leveldb::BlockHandle const&, leveldb::BlockContents*)':\r\nformat.cc:(.text+0x575): undefined reference to `snappy::GetUncompressedLength(char const*, unsigned long, unsigned long*)'\r\nformat.cc:(.text+0x5f8): undefined reference to `snappy::RawUncompress(char const*, unsigned long, char*)'\r\ncollect2: error: ld returned 1 exit status\r\ntools/parallel_http/CMakeFiles/parallel_http.dir/build.make:97: recipe for target 'output/bin/parallel_http' failed\r\nmake[5]: *** [output/bin/parallel_http] Error 1\r\nCMakeFiles/Makefile2:392: recipe for target 'tools/parallel_http/CMakeFiles/parallel_http.dir/all' failed\r\nmake[4]: *** [tools/parallel_http/CMakeFiles/parallel_http.dir/all] Error 2\r\nmake[4]: *** Waiting for unfinished jobs....\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(version_set.o): In function `leveldb::VersionSet::MakeInputIterator(leveldb::Compaction*)':\r\nversion_set.cc:(.text+0x18a7): undefined reference to `__cxa_throw_bad_array_new_length'\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(table_builder.o): In function `leveldb::TableBuilder::WriteBlock(leveldb::BlockBuilder*, leveldb::BlockHandle*)':\r\ntable_builder.cc:(.text+0x9f2): undefined reference to `snappy::MaxCompressedLength(unsigned long)'\r\ntable_builder.cc:(.text+0xa30): undefined reference to `snappy::RawCompress(char const*, unsigned long, char*, unsigned long*)'\r\n/media/togo/0000678400004823/ocr/code-git/Serving/server-cpu-build/third_party/install/leveldb/lib/libleveldb.a(format.o): In function `leveldb::ReadBlock(leveldb::RandomAccessFile*, leveldb::ReadOptions const&, leveldb::BlockHandle const&, leveldb::BlockContents*)':\r\nformat.cc:(.text+0x575): undefined reference to `snappy::GetUncompressedLength(char const*, unsigned long, unsigned long*)'\r\nformat.cc:(.text+0x5f8): undefined reference to `snappy::RawUncompress(char const*, unsigned long, char*)'\r\ncollect2: error: ld returned 1 exit status\r\ntools/rpc_view/CMakeFiles/rpc_view.dir/build.make:120: recipe for target 'output/bin/rpc_view' failed\r\nmake[5]: *** [output/bin/rpc_view] Error 1\r\nCMakeFiles/Makefile2:473: recipe for target 'tools/rpc_view/CMakeFiles/rpc_view.dir/all' failed\r\n\r\n```\r\n\r\n编译环境： gcc 4.8.4 g++ 4.8.4 cmake 3.16\r\n",
        "state": "closed",
        "user": "fujianhai",
        "closed_by": "fujianhai",
        "created_at": "2020-07-28T10:42:50+00:00",
        "updated_at": "2020-08-06T06:20:16+00:00",
        "closed_at": "2020-08-06T06:20:16+00:00",
        "comments_count": [
            "fujianhai",
            "fujianhai",
            "bjjwwang",
            "fujianhai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 771,
        "title": "请问自己在client端继承web_service类，实现自己的类去进行前置和后置处理，这样的话，serving的执行命令是什么？",
        "body": "如题：使用serving默认的http的服务的命令是这个：python3 -m paddle_serving_server.serve --model fm_serving_model --thread 10 --port 8696 --name fm &>std.log 2>err.log &\r\n\r\n那么，如果我自己实现前置和后置处理，然后执行自己的web_service的命令，应该如何去写？命令里面也能指定线程数吗？\r\n",
        "state": "closed",
        "user": "jiangchao123",
        "closed_by": "TeslaZhao",
        "created_at": "2020-07-28T14:10:57+00:00",
        "updated_at": "2020-08-31T12:01:54+00:00",
        "closed_at": "2020-08-31T12:01:54+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 775,
        "title": "按模型来源重写save a servable model文档",
        "body": "当前部分模型来源：\r\n- 已经保存的模型文件怎么转换成Serving格式文件\r\n- Paddle直接训练如何保存成Serving格式文件\r\n- PaddleHub怎么导出Serving格式文件\r\n- PaddleDetection怎么导出Serving格式文件\r\n- PaddleX怎么导出Serving格式文件\r\n\r\nfrom 微信用户 @快乐发卡机\r\n![image](https://user-images.githubusercontent.com/28446721/89028195-744fe100-d35e-11ea-9f3d-97d4b658cb7c.png)",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-31T10:48:47+00:00",
        "updated_at": "2024-04-16T09:05:13+00:00",
        "closed_at": "2024-04-16T09:05:13+00:00",
        "comments_count": [],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 776,
        "title": "编译文档补充依赖项",
        "body": "例如这些部分\r\n![image](https://user-images.githubusercontent.com/28446721/89143598-c02ca100-d57d-11ea-9a04-9cd3824fb845.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "bjjwwang",
        "created_at": "2020-08-03T03:37:54+00:00",
        "updated_at": "2020-08-03T07:53:03+00:00",
        "closed_at": "2020-08-03T07:53:03+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 772,
        "title": "cuda10-devel 镜像中用paddle detection导出serving可部署的模型报错",
        "body": "来自微信 @海洋\r\nlatest-cuda10-cudnn7-devel 镜像中用paddle detection导出serving可部署的模型报错\r\n![image](https://user-images.githubusercontent.com/28446721/88753267-592e7700-d18e-11ea-8192-eb56c7c4076b.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-07-29T03:27:04+00:00",
        "updated_at": "2020-08-13T04:52:11+00:00",
        "closed_at": "2020-08-13T04:52:11+00:00",
        "comments_count": [
            "mcl-stone",
            "barrierye",
            "mcl-stone",
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 774,
        "title": "文档中有一些死链",
        "body": "from 厂内 @ liyangokay\r\n[死链.xlsx](https://github.com/PaddlePaddle/Serving/files/5001585/default.xlsx)\r\n\r\nid | status_code | link | link_name | page | page_name | version\r\n-- | -- | -- | -- | -- | -- | --\r\n127389 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/python/examples/grpc_impl_example/criteo_ctr_with_cube/../../../doc/CUBE_LOCAL_CN.md | 稀疏参数索引服务Cube单机版使用指南 | /home/work/paddle_repo/Serving/python/examples/grpc_impl_example/criteo_ctr_with_cube/README_CN.md | Serving | default\r\n127397 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/DESIGN.md | Design | /home/work/paddle_repo/Serving/doc/deprecated/INDEX.md | Serving | default\r\n127398 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/INSTALL.md | Installation | /home/work/paddle_repo/Serving/doc/deprecated/INDEX.md | Serving | default\r\n127405 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/BENCHMARKING.md | Benchmarking | /home/work/paddle_repo/Serving/doc/deprecated/INDEX.md | Serving | default\r\n127406 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/GPU_BENCHMARKING.md | GPU Benchmarking | /home/work/paddle_repo/Serving/doc/deprecated/INDEX.md | Serving | default\r\n127409 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../tools/Dockerfile | CPU版本Dockerfile | /home/work/paddle_repo/Serving/doc/deprecated/DOCKER_CN.md | Serving | default\r\n127410 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../tools/Dockerfile.gpu | GPU版本Dockerfile | /home/work/paddle_repo/Serving/doc/deprecated/DOCKER_CN.md | Serving | default\r\n127414 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/pruned-ctr-network.png | Pruned CTR prediction   network | /home/work/paddle_repo/Serving/doc/deprecated/CTR_PREDICTION.md | Serving | default\r\n127420 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/DESIGN.md | 设计文档 | /home/work/paddle_repo/Serving/doc/deprecated/CREATING.md | Serving | default\r\n127423 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../tools/Dockerfile | CPU Version   Dockerfile | /home/work/paddle_repo/Serving/doc/deprecated/DOCKER.md | Serving | default\r\n127424 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../tools/Dockerfile.gpu | GPU Version   Dockerfile | /home/work/paddle_repo/Serving/doc/deprecated/DOCKER.md | Serving | default\r\n127425 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/INSTALL.md | 编译安装说明 | /home/work/paddle_repo/Serving/doc/deprecated/GETTING_STARTED.md | Serving | default\r\n127426 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../demo-client/python/text_classification.py | text_classification.py | /home/work/paddle_repo/Serving/doc/deprecated/HTTP_INTERFACE.md | Serving | default\r\n127427 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/deprecated/../demo-client/php/text_classification.php | text_classification.php | /home/work/paddle_repo/Serving/doc/deprecated/HTTP_INTERFACE.md | Serving | default\r\n127454 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   {} | {} | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING.md | Serving | default\r\n127455 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   1867 | lstm | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING.md | Serving | default\r\n127456 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   217 | bow | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING.md | Serving | default\r\n127465 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/./Distributed_Cube | Distributed Cube User   Guide | /home/work/paddle_repo/Serving/doc/CUBE_LOCAL.md | Serving | default\r\n127511 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   {} | {} | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING_CN.md | Serving | default\r\n127512 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   1867 | lstm | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING_CN.md | Serving | default\r\n127513 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/total:   217 | bow | /home/work/paddle_repo/Serving/doc/ABTEST_IN_PADDLE_SERVING_CN.md | Serving | default\r\n127519 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/CREATING.md | 从零开始写一个预测服务 | /home/work/paddle_repo/Serving/doc/DESIGN_CN.md | Serving | default\r\n127530 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/doc/INSTALL.md | BUILD steps | /home/work/paddle_repo/Serving/doc/CONTRIBUTE.md | Serving | default\r\n127595 | 404 | https://github.com/PaddlePaddle/Serving/blob/default/doc/分布式Cube | 稀疏参数索引服务Cube使用指南 | /home/work/paddle_repo/Serving/doc/CUBE_LOCAL_CN.md | Serving | default\r\n\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "MRXLT",
        "created_at": "2020-07-30T13:49:55+00:00",
        "updated_at": "2020-08-06T02:28:38+00:00",
        "closed_at": "2020-08-06T02:28:38+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 778,
        "title": "python example 报错",
        "body": "from 厂内 @liyangokay\r\npy3的bert和resnet50 http预测报错，python3 ocr rpc 报错。\r\npy2的resnet50 http预测报错  ",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-03T09:24:58+00:00",
        "updated_at": "2020-12-31T01:02:08+00:00",
        "closed_at": "2020-12-31T01:02:08+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 780,
        "title": "No module named 'paddle_serving_client.serving_client'",
        "body": "五月份我提出过相同的issue，当时版本是0.22，现在更新成0.31后，win平台还是报这个错误\r\n![image](https://user-images.githubusercontent.com/48110081/89420355-29223d80-d765-11ea-957a-f72eb69766e3.png)\r\n![image](https://user-images.githubusercontent.com/48110081/89420376-32aba580-d765-11ea-9b4e-8b82a4fb4281.png)\r\n![image](https://user-images.githubusercontent.com/48110081/89420487-5969dc00-d765-11ea-8811-4b2125ff63f7.png)\r\n",
        "state": "closed",
        "user": "chengxurensheng666",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-05T13:48:12+00:00",
        "updated_at": "2022-02-24T08:33:46+00:00",
        "closed_at": "2020-12-31T01:03:47+00:00",
        "comments_count": [
            "barrierye",
            "chengxurensheng666",
            "barrierye",
            "chengxurensheng666",
            "barrierye",
            "yybestw"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 781,
        "title": "关于 c++版本 http 上传pdf文件的例子 ？",
        "body": "您好，serving的性能很好，但是我们需要用c++版本的，请问是否能提供一下c++版本利用http协议上传pdf文件的例子吗？  ",
        "state": "closed",
        "user": "fujianhai",
        "closed_by": "fujianhai",
        "created_at": "2020-08-06T06:17:25+00:00",
        "updated_at": "2020-08-08T15:34:48+00:00",
        "closed_at": "2020-08-08T15:34:48+00:00",
        "comments_count": [
            "fujianhai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 790,
        "title": "输出tensor 有时候需要lod 却没有保存下来的bug",
        "body": "https://github.com/PaddlePaddle/Serving/issues/644",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-11T13:58:45+00:00",
        "updated_at": "2024-04-16T09:05:14+00:00",
        "closed_at": "2024-04-16T09:05:14+00:00",
        "comments_count": [],
        "labels": [
            "enhancement",
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 782,
        "title": "可以开多个线程并行的推理么？小模型在单卡上gpu显存占用比较小，gpu利用率比较低",
        "body": "现在是一个gpu卡部署了一个serving服务，但是gpu利用率不到30%，如果压测的话感觉就积压了，能在一个模型起多个推理线程并行跑么？",
        "state": "closed",
        "user": "jeshxxx",
        "closed_by": "jeshxxx",
        "created_at": "2020-08-06T06:39:16+00:00",
        "updated_at": "2020-08-13T01:58:26+00:00",
        "closed_at": "2020-08-13T01:58:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "jeshxxx",
            "TeslaZhao",
            "jeshxxx",
            "TeslaZhao",
            "jeshxxx",
            "TeslaZhao",
            "jeshxxx"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 797,
        "title": "运行README里的LAC示例报错",
        "body": "如题，按照README的步骤安装paddle serve之后，跑readme里给出的lac示例，到最后这一步时\r\n```\r\ncurl -H Content-Type:application/json -X POST -d '{feed:[{words: 我爱北京天安门}], fetch:[word_seg]}' http://127.0.0.1:9393/lac/prediction\r\n```\r\n报错如下：\r\n```\r\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\">\r\n<html><head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\r\n<title>ERROR: The requested URL could not be retrieved</title>\r\n<style type=\"text/css\"><!--\r\n /*\r\n Stylesheet for Squid Error pages\r\n Adapted from design by Free CSS Templates\r\n http://www.freecsstemplates.org\r\n Released for free under a Creative Commons Attribution 2.5 License\r\n*/\r\n\r\n/* Page basics */\r\n* {\r\n\tfont-family: verdana, sans-serif;\r\n}\r\n\r\nhtml body {\r\n\tmargin: 0;\r\n\tpadding: 0;\r\n\tbackground: #efefef;\r\n\tfont-size: 12px;\r\n\tcolor: #1e1e1e;\r\n}\r\n\r\n/* Page displayed title area */\r\n#titles {\r\n\tmargin-left: 15px;\r\n\tpadding: 10px;\r\n\tpadding-left: 100px;\r\n\tbackground: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat left;\r\n}\r\n\r\n/* initial title */\r\n#titles h1 {\r\n\tcolor: #000000;\r\n}\r\n#titles h2 {\r\n\tcolor: #000000;\r\n}\r\n\r\n/* special event: FTP success page titles */\r\n#titles ftpsuccess {\r\n\tbackground-color:#00ff00;\r\n\twidth:100%;\r\n}\r\n\r\n/* Page displayed body content area */\r\n#content {\r\n\tpadding: 10px;\r\n\tbackground: #ffffff;\r\n}\r\n\r\n/* General text */\r\np {\r\n}\r\n\r\n/* error brief description */\r\n#error p {\r\n}\r\n\r\n/* some data which may have caused the problem */\r\n#data {\r\n}\r\n\r\n/* the error message received from the system or other software */\r\n#sysmsg {\r\n}\r\n\r\npre {\r\n    font-family:sans-serif;\r\n}\r\n\r\n/* special event: FTP / Gopher directory listing */\r\n#dirmsg {\r\n    font-family: courier;\r\n    color: black;\r\n    font-size: 10pt;\r\n}\r\n#dirlisting {\r\n    margin-left: 2%;\r\n    margin-right: 2%;\r\n}\r\n#dirlisting tr.entry td.icon,td.filename,td.size,td.date {\r\n    border-bottom: groove;\r\n}\r\n#dirlisting td.size {\r\n    width: 50px;\r\n    text-align: right;\r\n    padding-right: 5px;\r\n}\r\n\r\n/* horizontal lines */\r\nhr {\r\n\tmargin: 0;\r\n}\r\n\r\n/* page displayed footer area */\r\n#footer {\r\n\tfont-size: 9px;\r\n\tpadding-left: 10px;\r\n}\r\n\r\n\r\nbody\r\n:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya, sans-serif; float: right; }\r\n:lang(he) { direction: rtl; }\r\n --></style>\r\n</head><body id=ERR_ACCESS_DENIED>\r\n<div id=\"titles\">\r\n<h1>ERROR</h1>\r\n<h2>The requested URL could not be retrieved</h2>\r\n</div>\r\n<hr>\r\n\r\n<div id=\"content\">\r\n<p>The following error was encountered while trying to retrieve the URL: <a href=\"http://127.0.0.1:9393/lac/prediction\">http://127.0.0.1:9393/lac/prediction</a></p>\r\n\r\n<blockquote id=\"error\">\r\n<p><b>Access Denied.</b></p>\r\n</blockquote>\r\n\r\n<p>Access control configuration prevents your request from being allowed at this time. Please contact your service provider if you feel this is incorrect.</p>\r\n\r\n<p>Your cache administrator is <a href=\"mailto:webmaster?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%2067701d6a06fd%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2014%20Aug%202020%2007%3A41%3A25%20GMT%0D%0A%0D%0AClientIP%3A%2010.87.145.34%0D%0A%0D%0AHTTP%20Request%3A%0D%0APOST%20%2Flac%2Fprediction%20HTTP%2F1.1%0AHost%3A%20127.0.0.1%3A9393%0D%0AUser-Agent%3A%20curl%2F7.52.1%0D%0AAccept%3A%20*%2F*%0D%0AProxy-Connection%3A%20Keep-Alive%0D%0AContent-Type%3A%20application%2Fjson%0D%0AContent-Length%3A%2057%0D%0A%0D%0A%0D%0A\">webmaster</a>.</p>\r\n<br>\r\n</div>\r\n\r\n<hr>\r\n<div id=\"footer\">\r\n<p>Generated Fri, 14 Aug 2020 07:41:25 GMT by 67701d6a06fd (squid/3.3.8)</p>\r\n<!-- ERR_ACCESS_DENIED -->\r\n</div>\r\n</body></html>\r\n```",
        "state": "closed",
        "user": "xixiaoyao",
        "closed_by": "xixiaoyao",
        "created_at": "2020-08-14T07:53:31+00:00",
        "updated_at": "2020-08-17T07:31:32+00:00",
        "closed_at": "2020-08-17T07:31:31+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 783,
        "title": "Serving缺少logid机制",
        "body": "",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-08-06T07:00:22+00:00",
        "updated_at": "2020-08-17T06:18:52+00:00",
        "closed_at": "2020-08-17T06:18:52+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 792,
        "title": "使用PaddleHub Fine-tune 的模型部署问题",
        "body": "使用PaddleHub Fine-tune 的模型应该如何部署呢？HubServing 好像不支持这种模型",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "iceriver97",
        "created_at": "2020-08-12T08:13:56+00:00",
        "updated_at": "2020-08-12T09:12:32+00:00",
        "closed_at": "2020-08-12T09:12:32+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Steffy-zxf"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 796,
        "title": "README里的LAC示例报错",
        "body": "如题，按照README的步骤安装paddle serve之后，跑readme里给出的lac示例，到最后这一步时\r\n```\r\ncurl -H Content-Type:application/json -X POST -d '{feed:[{words: 我爱北京天安门}], fetch:[word_seg]}' http://127.0.0.1:9393/lac/prediction\r\n```\r\n报错如下：\r\n```\r\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\">\r\n<html><head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\r\n<title>ERROR: The requested URL could not be retrieved</title>\r\n<style type=\"text/css\"><!--\r\n /*\r\n Stylesheet for Squid Error pages\r\n Adapted from design by Free CSS Templates\r\n http://www.freecsstemplates.org\r\n Released for free under a Creative Commons Attribution 2.5 License\r\n*/\r\n\r\n/* Page basics */\r\n* {\r\n\tfont-family: verdana, sans-serif;\r\n}\r\n\r\nhtml body {\r\n\tmargin: 0;\r\n\tpadding: 0;\r\n\tbackground: #efefef;\r\n\tfont-size: 12px;\r\n\tcolor: #1e1e1e;\r\n}\r\n\r\n/* Page displayed title area */\r\n#titles {\r\n\tmargin-left: 15px;\r\n\tpadding: 10px;\r\n\tpadding-left: 100px;\r\n\tbackground: url('http://www.squid-cache.org/Artwork/SN.png') no-repeat left;\r\n}\r\n\r\n/* initial title */\r\n#titles h1 {\r\n\tcolor: #000000;\r\n}\r\n#titles h2 {\r\n\tcolor: #000000;\r\n}\r\n\r\n/* special event: FTP success page titles */\r\n#titles ftpsuccess {\r\n\tbackground-color:#00ff00;\r\n\twidth:100%;\r\n}\r\n\r\n/* Page displayed body content area */\r\n#content {\r\n\tpadding: 10px;\r\n\tbackground: #ffffff;\r\n}\r\n\r\n/* General text */\r\np {\r\n}\r\n\r\n/* error brief description */\r\n#error p {\r\n}\r\n\r\n/* some data which may have caused the problem */\r\n#data {\r\n}\r\n\r\n/* the error message received from the system or other software */\r\n#sysmsg {\r\n}\r\n\r\npre {\r\n    font-family:sans-serif;\r\n}\r\n\r\n/* special event: FTP / Gopher directory listing */\r\n#dirmsg {\r\n    font-family: courier;\r\n    color: black;\r\n    font-size: 10pt;\r\n}\r\n#dirlisting {\r\n    margin-left: 2%;\r\n    margin-right: 2%;\r\n}\r\n#dirlisting tr.entry td.icon,td.filename,td.size,td.date {\r\n    border-bottom: groove;\r\n}\r\n#dirlisting td.size {\r\n    width: 50px;\r\n    text-align: right;\r\n    padding-right: 5px;\r\n}\r\n\r\n/* horizontal lines */\r\nhr {\r\n\tmargin: 0;\r\n}\r\n\r\n/* page displayed footer area */\r\n#footer {\r\n\tfont-size: 9px;\r\n\tpadding-left: 10px;\r\n}\r\n\r\n\r\nbody\r\n:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya, sans-serif; float: right; }\r\n:lang(he) { direction: rtl; }\r\n --></style>\r\n</head><body id=ERR_ACCESS_DENIED>\r\n<div id=\"titles\">\r\n<h1>ERROR</h1>\r\n<h2>The requested URL could not be retrieved</h2>\r\n</div>\r\n<hr>\r\n\r\n<div id=\"content\">\r\n<p>The following error was encountered while trying to retrieve the URL: <a href=\"http://127.0.0.1:9393/lac/prediction\">http://127.0.0.1:9393/lac/prediction</a></p>\r\n\r\n<blockquote id=\"error\">\r\n<p><b>Access Denied.</b></p>\r\n</blockquote>\r\n\r\n<p>Access control configuration prevents your request from being allowed at this time. Please contact your service provider if you feel this is incorrect.</p>\r\n\r\n<p>Your cache administrator is <a href=\"mailto:webmaster?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%2067701d6a06fd%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2014%20Aug%202020%2007%3A41%3A25%20GMT%0D%0A%0D%0AClientIP%3A%2010.87.145.34%0D%0A%0D%0AHTTP%20Request%3A%0D%0APOST%20%2Flac%2Fprediction%20HTTP%2F1.1%0AHost%3A%20127.0.0.1%3A9393%0D%0AUser-Agent%3A%20curl%2F7.52.1%0D%0AAccept%3A%20*%2F*%0D%0AProxy-Connection%3A%20Keep-Alive%0D%0AContent-Type%3A%20application%2Fjson%0D%0AContent-Length%3A%2057%0D%0A%0D%0A%0D%0A\">webmaster</a>.</p>\r\n<br>\r\n</div>\r\n\r\n<hr>\r\n<div id=\"footer\">\r\n<p>Generated Fri, 14 Aug 2020 07:41:25 GMT by 67701d6a06fd (squid/3.3.8)</p>\r\n<!-- ERR_ACCESS_DENIED -->\r\n</div>\r\n</body></html>\r\n```",
        "state": "closed",
        "user": "xixiaoyao",
        "closed_by": "xixiaoyao",
        "created_at": "2020-08-14T07:52:57+00:00",
        "updated_at": "2020-08-17T07:32:41+00:00",
        "closed_at": "2020-08-17T07:32:41+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "xixiaoyao",
            "xixiaoyao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 799,
        "title": "自己训练的模型，部署的时候，如何知道输入参数是什么？",
        "body": "看了PaddleServing的例子，按例子作，很简单。但是部署我自己训练的模型，却不知该如何下手。主要的问题是，我不知道模型的输入参数是什么，客户端调用的时候，该如何给参数？\r\n我是基于PaddleHub作的ERNIE的finetue，作文本分类和序列标注。从训练脚本上面看，也看不出模型的输入参数是什么。难道这个东西是固定的吗？该如何确定模型的输入参数和形式呢？谢谢！\r\n\r\n训练脚本如下，实在是不知道该如何调用：\r\n\r\n# 注意，Reader、Task、context参数中的max_seq_len应该保持一致\r\nreader = hub.reader.SequenceLabelReader(\r\n    dataset = dataset,\r\n    vocab_path = module.get_vocab_path(),\r\n    max_seq_len = max_seq_len)\r\n\r\nstrategy = hub.AdamWeightDecayStrategy(\r\n    weight_decay = 0.01,\r\n    learning_rate = 5e-5,\r\n    warmup_proportion = 0.1)\r\n\r\nconfig = hub.RunConfig(\r\n    use_cuda = use_cuda,\r\n    num_epoch = num_epoch,\r\n    batch_size = batch_size,\r\n    checkpoint_dir = checkpoint_dir,\r\n    eval_interval = 50,\r\n    strategy = strategy)\r\n\r\ntask = hub.SequenceLabelTask(\r\n    data_reader = reader,\r\n    feature = sequence_output,\r\n    feed_list = feed_list,\r\n    add_crf = True,\r\n    max_seq_len = max_seq_len,\r\n    num_classes = dataset.num_labels,\r\n    config = config)",
        "state": "closed",
        "user": "fisher030712",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-16T00:28:18+00:00",
        "updated_at": "2024-03-05T06:48:49+00:00",
        "closed_at": "2024-03-05T06:48:49+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "example",
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 800,
        "title": "强烈建议增加用PaddleHub作精调的模型部署的例子。",
        "body": "被PaddleServing难倒了！\r\n不知道该如何确定自己的模型的输入和输出参数，细节都被Paddlehub隐藏了。\r\n大佬们是如何知道模型的输入输出的呢？还是说，PaddleHub公开的那几个模型，都是固定的输入输出？在哪能查到呢？",
        "state": "closed",
        "user": "fisher030712",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-16T01:04:39+00:00",
        "updated_at": "2024-03-05T06:48:50+00:00",
        "closed_at": "2024-03-05T06:48:50+00:00",
        "comments_count": [
            "fisher030712",
            "fisher030712",
            "bjjwwang"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 803,
        "title": "【疑似bug】框架内部会自动给用户输入升维？",
        "body": "如图，client端的feed dict为如下元素和形状：\r\n![image](https://user-images.githubusercontent.com/24541791/90550473-9486f900-e1c2-11ea-8232-56e6bca5bf2c.png)\r\n\r\n\r\n即包含两个样本的bert输入。\r\n\r\n但是如图所示，\r\n\r\n![image](https://user-images.githubusercontent.com/24541791/90550737-ea5ba100-e1c2-11ea-90b1-3f66df7836a3.png)\r\n\r\nserver端收到输入并经过fluid.embedding之后，本应该得到[2,9,1024]（经过了1024维度的embedding），结果莫名其妙的变成了[1,2,9,1024]，前面不知道从哪里多了一个size为1的维度。这是paddle serving框架内部自动添加的维度吗？",
        "state": "closed",
        "user": "xixiaoyao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-18T18:22:37+00:00",
        "updated_at": "2024-03-05T06:48:51+00:00",
        "closed_at": "2024-03-05T06:48:51+00:00",
        "comments_count": [
            "xixiaoyao",
            "bjjwwang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 805,
        "title": "pyreader 读入数据,并未使用dataset，如何发布服务",
        "body": "我该如何将自定义的 Variable：test 绑定到模型中？我看示例是采用的dataset.set_use_var(),但我并未使用dataset，而是使用的pyreader 读入的数据；下面代码不做绑定，直接这样保存模型是不是错误的做法？\r\n```\r\ntest = fluid.layers.data(name=\"test\", shape=[1], dtype=\"int64\", lod_level=1)\r\nprediction = predict_wrapper_serving(reader, exe, test_prog, test_pyreader, graph_vars,'final', 'final')\r\nserving_io.save_model(\"RE_model\",\"RE_client_conf\", {\"text\": test}, {\"prediction\": prediction},fluid.default_main_program())\r\n```\r\n我如果将自定义的test变量传入网络，应该怎样修改呢？网络之前是使用pyreader读入数据的；代码如下：\r\n```\r\n(src_ids, sent_ids, pos_ids, task_ids, input_mask, labels, seq_lens,\r\n     example_index, tok_to_orig_start_index,\r\n     tok_to_orig_end_index) = fluid.layers.read_file(pyreader)\r\n```\r\n@wangjiawei04  使用 pyreader  异步读取数据的模型是否不能发布serving？有没有类似的例子？\r\n",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-19T03:10:54+00:00",
        "updated_at": "2024-04-16T09:05:15+00:00",
        "closed_at": "2024-04-16T09:05:15+00:00",
        "comments_count": [
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "bjjwwang",
            "bjjwwang",
            "bjjwwang",
            "iceriver97",
            "iceriver97"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 810,
        "title": "cuda10运行镜像中nvidia-smi没有输出（宿主机 GeForce GTX 960，driver 450.57，CUDA version 11.0）",
        "body": "from 微信用户 @ Duncan\r\n![image](https://user-images.githubusercontent.com/28446721/90858836-851dd080-e3b9-11ea-94eb-047641aa7653.png)\r\n![image](https://user-images.githubusercontent.com/28446721/90858823-80591c80-e3b9-11ea-813b-53039629a45f.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-21T06:20:30+00:00",
        "updated_at": "2020-12-31T00:15:58+00:00",
        "closed_at": "2020-12-31T00:15:58+00:00",
        "comments_count": [
            "barrierye",
            "barrierye",
            "barrierye"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 808,
        "title": "opencv-python-4.3.0.38版本安装有问题，降级4.2.0.32可以解决",
        "body": "",
        "state": "closed",
        "user": "liiitleboy",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-20T02:28:03+00:00",
        "updated_at": "2020-08-21T06:17:04+00:00",
        "closed_at": "2020-08-21T06:17:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT",
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 809,
        "title": "CUDA10 镜像无法按Run in docker文档跑pypi上GPU版本的例子",
        "body": "from 微信用户 @ Duncan\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-21T03:58:59+00:00",
        "updated_at": "2020-12-31T00:17:07+00:00",
        "closed_at": "2020-12-31T00:17:07+00:00",
        "comments_count": [
            "barrierye",
            "TeslaZhao"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 811,
        "title": "web_service不兼容python3",
        "body": "from 微信用户 @ Duncan\r\n![image](https://user-images.githubusercontent.com/28446721/90866902-20697280-e3c7-11ea-9f4e-0d2989070f11.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "MRXLT",
        "created_at": "2020-08-21T07:58:26+00:00",
        "updated_at": "2020-08-28T05:01:25+00:00",
        "closed_at": "2020-08-28T05:01:25+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 812,
        "title": "GPU版本使用最新版Shapely报错，降级1.7.0解决",
        "body": "最新版使用1.7.1，会提示缺少so，按照网上提示的安装软件会导致找不到CUDA， 降级Shapely解决。",
        "state": "closed",
        "user": "liiitleboy",
        "closed_by": "barrierye",
        "created_at": "2020-08-21T09:14:15+00:00",
        "updated_at": "2020-08-21T12:08:58+00:00",
        "closed_at": "2020-08-21T12:08:58+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 816,
        "title": "docker: Failed to create Cudnn handle in DeviceContext, 重启容器后正常",
        "body": "from 微信用户 @ Duncan\r\n> 今天服务器端突然就报这个错了，昨天都能预测成功\r\n今天每次预测都会报错\r\n重启docker容器后就正常了\r\n\r\n![image](https://user-images.githubusercontent.com/28446721/91433428-d89a8d80-e895-11ea-8c85-dcca66c8fdd6.png)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-27T10:49:33+00:00",
        "updated_at": "2024-04-16T09:05:16+00:00",
        "closed_at": "2024-04-16T09:05:16+00:00",
        "comments_count": [],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 814,
        "title": "Pipeline Server支持业务error_code返回",
        "body": "现在pipeline sever支持框架error code和业务层error info返回，业务有业务相关的error code，需要返回",
        "state": "closed",
        "user": "TeslaZhao",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-27T08:33:33+00:00",
        "updated_at": "2020-12-31T00:14:59+00:00",
        "closed_at": "2020-12-31T00:14:59+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 818,
        "title": "保存 Serving 模型时出现错误；",
        "body": "保存模型的语句：\r\n`serving_io.save_model(\"RE_model\",\"RE_client_conf\", {\"text\": str_id}, {\"prediction\":new_variable},fluid.default_main_program())`\r\n报错信息：\r\n```\r\nTraceback (most recent call last):\r\n  File \"./ernie/run_duie.py\", line 403, in <module>\r\n    main(args)\r\n  File \"./ernie/run_duie.py\", line 345, in main\r\n    serving_io.save_model(\"RE_model\",\"RE_client_conf\", {\"text\": str_id}, {\"prediction\":new_variable},fluid.default_main_program())\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py\", line 35, in save_model\r\n    feed_var_names = [feed_var_dict[x].name for x in feed_var_dict]\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py\", line 35, in <listcomp>\r\n    feed_var_names = [feed_var_dict[x].name for x in feed_var_dict]\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 639, in name\r\n    return self._ivar.name\r\nAttributeError: 'Variable' object has no attribute '_ivar'\r\n```\r\n其中 str_id，new_variable的类型均为 <class 'paddle.fluid.framework.Variable'>",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "iceriver97",
        "created_at": "2020-09-01T00:35:59+00:00",
        "updated_at": "2020-09-07T00:31:15+00:00",
        "closed_at": "2020-09-07T00:31:14+00:00",
        "comments_count": [
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "MRXLT",
            "iceriver97",
            "MRXLT"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 815,
        "title": "windows下部署serving问题",
        "body": "### 我是在win10用anaconda安装了虚拟的paddle环境，然后把相应的包都安装了；在linux服务器端跑都没问题，运行程序如下：\r\n`from paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.385, 0.386, 0.375], [0.162, 0.159, 0.159], False),\r\n    Resize(1000, 1500), Transpose((2, 0, 1)), PadStride(32)\r\n])\r\nclient = Client()\r\nclient.load_client_config(\"/home/aistudio/PaddleDetection-0.4/output/serving_model/cascade_rcnn_dcn_r50_vd_fpn_3x_server_side/serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\nim = preprocess('/home/aistudio/work/A/test014.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = '/home/aistudio/work/test.jpg'\r\nprint (fetch_map)`\r\n### ，但是在window下运行提示如下：\r\n(paddlepaddle) D:\\Desktop\\serving>python test2.py\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 10, in <module>\r\n    client = Client()\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\paddle_serving_client\\__init__.py\", line 136, in __init__\r\n    from .serving_client import PredictorRes\r\nModuleNotFoundError: No module named 'paddle_serving_client.serving_client'\r\n### 然后，我根据你们提供的新平台支持修改了程序如下：\r\n`from paddle_serving_client import MultiLangClient as Client\r\nfrom paddle_serving_app.reader import *\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.385, 0.386, 0.375], [0.162, 0.159, 0.159], False),\r\n    Resize(1000, 1500), Transpose((2, 0, 1)), PadStride(32)\r\n])\r\nclient = Client()\r\nclient.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\nim = preprocess('/home/aistudio/work/A/test001.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = '/home/aistudio/work/test001.jpg'\r\nprint (fetch_map)`\r\n### 报错如下：\r\n`(paddlepaddle) D:\\Desktop\\serving>python test3.py\r\nTraceback (most recent call last):\r\n  File \"test3.py\", line 12, in <module>\r\n    client.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nAttributeError: 'MultiLangClient' object has no attribute 'load_client_config'`",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "TeslaZhao",
        "created_at": "2020-08-27T08:42:54+00:00",
        "updated_at": "2020-10-08T08:44:31+00:00",
        "closed_at": "2020-10-08T08:44:31+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "barrierye",
            "liangruofei",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "barrierye",
            "liangruofei",
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 824,
        "title": "热加载手册中paddle_serving_server.monitor文件请问有下载链接？",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/INFERENCE_TO_SERVING_CN.md\r\n\r\npython -m **_paddle_serving_server.monitor_** \\\r\n\t--type='hdfs' --hadoop_bin='/hadoop-3.1.2/bin/hadoop' \\\r\n\t--remote_path='/' --remote_model_name='uci_housing.tar.gz' \\\r\n\t--remote_donefile_name='donefile' --local_path='.' \\\r\n\t--local_model_name='uci_housing_model' --local_timestamp_file='fluid_time_file' \\\r\n\t--local_tmp_path='_tmp' --unpacked_filename='uci_housing_model' --debug",
        "state": "closed",
        "user": "zhangyinxia530",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-07T07:32:06+00:00",
        "updated_at": "2024-03-05T06:48:52+00:00",
        "closed_at": "2024-03-05T06:48:51+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 822,
        "title": "centos中利用pip安装后，出现启动项目cuda找不到的情况。",
        "body": "![image](https://user-images.githubusercontent.com/37429336/92207778-642aa480-eebc-11ea-967e-9fabe9d64778.png)\r\n",
        "state": "closed",
        "user": "natureLanguageQing",
        "closed_by": "natureLanguageQing",
        "created_at": "2020-09-04T06:39:36+00:00",
        "updated_at": "2020-09-04T09:00:01+00:00",
        "closed_at": "2020-09-04T09:00:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "natureLanguageQing",
            "natureLanguageQing",
            "natureLanguageQing",
            "natureLanguageQing",
            "natureLanguageQing",
            "natureLanguageQing"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 823,
        "title": "本地ubuntu16.04用conda安装paddle后布置serve报错",
        "body": "卡死到这了：\r\n(paddle) liangruofei@liangruofei-System-Product-Name:~/paddle/serving_model/cascade_rcnn_dcn_r50_vd_fpn_3x_server_side$ python -m paddle_serving_server_gpu.serve --model serving_server --port 9999 --gpu_id 0 --use_multilang\r\nGoing to Run Comand\r\n/home/liangruofei/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle_serving_server_gpu/serving-gpu-cuda9-0.3.2/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 12000 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 536870912 \r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0904 22:27:13.266221  3173 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\nSegmentation fault (core dumped)\r\n",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-04T14:30:29+00:00",
        "updated_at": "2020-12-31T00:14:21+00:00",
        "closed_at": "2020-12-31T00:14:21+00:00",
        "comments_count": [
            "MRXLT",
            "liangruofei",
            "liangruofei",
            "liangruofei",
            "wangqianyi2017",
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 825,
        "title": "如何用多进程启动多个serving实例？",
        "body": "首先感谢研发的辛勤付出！\r\n1、问题背景：\r\n单机单卡（P40）下使用paddel-serving部署了一个yolov3检测模型，命令如下：\r\n![image](https://user-images.githubusercontent.com/34050415/92361210-a2280280-f120-11ea-967d-28992c9242a6.png)\r\n；tornado异步多协程做容器与客户端的通信。在客户端使用了jmeter做压测，吞吐量在10.5左右。\r\n\r\n2、问题描述：\r\n压测过程中，P40显卡的显存使用率在15%~23%之间无法再提高；为了提升GPU利用率，想使用多进程来开多个serving实例，在命令行-help中没有找到相关选项。参考[serving性能测试](https://github.com/PaddlePaddle/Serving/blob/develop/doc/GPU_BENCHMARKING.md)时，有提及到：**单卡多进程：启动N个Serving实例，他们绑定到同一张GPU卡。**这样的描述。\r\n因此想知道目前阶段，如果想要在单卡上开多进程进而启动多个serving实例，该如何操作实现？\r\n",
        "state": "closed",
        "user": "gudufengzhongyipilang",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-07T07:40:40+00:00",
        "updated_at": "2020-10-22T01:51:15+00:00",
        "closed_at": "2020-10-22T01:51:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 826,
        "title": "KVManager::proc_initialize memory leak",
        "body": "```cpp\r\n      auto r = _map.insert(std::make_pair(engine_name, kvinfo));\r\n      if (!r.second) {\r\n        LOG(ERROR) << \"Failed insert item: \" << engine_name;\r\n        return -1;\r\n      }\r\n```\r\nneed to delete 'kvinfo'  that newed above before the return statment",
        "state": "closed",
        "user": "WinwayJia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-07T08:23:21+00:00",
        "updated_at": "2024-03-05T06:48:52+00:00",
        "closed_at": "2024-03-05T06:48:52+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 827,
        "title": "如何获取 fetch参数？",
        "body": "我将模型 freeze 之后，使用 `inference_model_to_serving `将模型保存为 Serving模型；\r\n但我想知道客户端调用 `client.predict()`函数时，fetch参数应该是什么？\r\n不应该是我在freeze模型时使用函数 `save_inference_model` 时指定的 fetch吗？我将其作为参数得到报错信息：\r\n```\r\naistudio@jupyter-380628-755201:~$ python ./client.py 983884_sat.jpg \r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0907 16:32:14.215747  2107 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\nTraceback (most recent call last):\r\n  File \"./client.py\", line 18, in <module>\r\n    fetch_map = client.predict(feed={\"image\": im}, fetch=[\"transpose_1.tmp_0\"])\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_client/__init__.py\", line 274, in predict\r\n    \"Fetch names should not be empty or out of saved fetch list.\")\r\nValueError: Fetch names should not be empty or out of saved fetch list.\r\n```\r\n我无法打开 serving_client_conf.prototxt 文件，是不是在该文件中可以得到我想要的信息呢？\r\n这里是该文件\r\n\r\n[serving_client_conf.zip](https://github.com/PaddlePaddle/Serving/files/5182057/serving_client_conf.zip)\r\n固化模型的代码：\r\n```\r\nfluid.io.save_inference_model(\r\n        cfg.FREEZE.SAVE_DIR,\r\n        feeded_var_names=[image.name],\r\n        target_vars=[logit_out],\r\n        executor=exe,\r\n        main_program=infer_prog,\r\n        model_filename=cfg.FREEZE.MODEL_FILENAME,\r\n        params_filename=cfg.FREEZE.PARAMS_FILENAME)\r\n```\r\n我是不是应该指定client.predict()> fetch = 这里 logit_out.name?",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-07T08:50:54+00:00",
        "updated_at": "2024-04-16T09:05:17+00:00",
        "closed_at": "2024-04-16T09:05:17+00:00",
        "comments_count": [
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "MRXLT",
            "iceriver97",
            "iceriver97",
            "iceriver97"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 829,
        "title": "win10下用conda建立的虚拟paddle环境启动server报错",
        "body": "想在windows下运行serving，报错如下\r\n(paddlepaddle) D:\\Desktop\\serving\\uav\\serving_model\\cascade_rcnn_dcn_r50_vd_fpn_3x_server_side>python -m paddle_serving_server_gpu.serve --thread 10 --model serving_server --port 9292 --gpu_id 0 --use_multilang\r\n子目录或文件 workdir_0 已经存在。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\process.py\", line 297, in _bootstrap\r\n    self.run()\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\process.py\", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\paddle_serving_server_gpu\\serve.py\", line 72, in start_gpu_card_model\r\n    server.run_server()\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\paddle_serving_server_gpu\\__init__.py\", line 739, in run_server\r\n    p_bserver.start()\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'server_configure_pb2.InferServiceConf'>: it's not the same object as server_configure_pb2.InferServiceConf\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\spawn.py\", line 99, in spawn_main\r\n    new_handle = reduction.steal_handle(parent_pid, pipe_handle)\r\n  File \"D:\\Anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\reduction.py\", line 82, in steal_handle\r\n    _winapi.PROCESS_DUP_HANDLE, False, source_pid)\r\nOSError: [WinError 87] 参数错误。",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-09T08:03:28+00:00",
        "updated_at": "2020-12-31T00:09:29+00:00",
        "closed_at": "2020-12-31T00:09:29+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 830,
        "title": "ubunut",
        "body": "serve端执行：\r\n\r\n(paddle) liangruofei@liangruofei-System-Product-Name:~/cascade$ python -m paddle_serving_server_gpu.serve --thread 10 --model serving_server --port 8888 --gpu_id 0 --use_multilang\r\n\r\nmkdir: 无法创建目录\"workdir_0\": 文件已存在\r\n\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0908 20:27:05.811311 10181 general_model.cpp:73] feed var num: 3fetch_var_num: 1\r\nI0908 20:27:05.811334 10181 general_model.cpp:77] feed alias name: image index: 0\r\nI0908 20:27:05.811340 10181 general_model.cpp:80] feed[0] shape:\r\nI0908 20:27:05.811347 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811352 10181 general_model.cpp:87] feed[0] feed type: 1\r\nI0908 20:27:05.811357 10181 general_model.cpp:77] feed alias name: im_info index: 1\r\nI0908 20:27:05.811362 10181 general_model.cpp:80] feed[1] shape:\r\nI0908 20:27:05.811367 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811372 10181 general_model.cpp:87] feed[1] feed type: 1\r\nI0908 20:27:05.811378 10181 general_model.cpp:77] feed alias name: im_shape index: 2\r\nI0908 20:27:05.811383 10181 general_model.cpp:80] feed[2] shape:\r\nI0908 20:27:05.811388 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811393 10181 general_model.cpp:87] feed[2] feed type: 1\r\nI0908 20:27:05.811398 10181 general_model.cpp:95] fetch [0] alias name: multiclass_nms_0.tmp_0\r\nI0908 20:27:05.811477 10181 general_model.cpp:55] Init commandline: dummy /home/liangruofei/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle_serving_server_gpu/serve.py --tryfromenv=profile_client,profile_server,max_body_size \r\nI0908 20:27:05.811640 10181 predictor_sdk.cpp:34] \r\nM\r\ndefault��� d(���������0:pooled\r\nDefaultla\"    baidu_std �\r\ngeneral_model@baidu.paddle_serving.predictor.general_model.GeneralModelServiceWeightedRandomRender\"\r\n100*5\r\ndefault_tag_139745050093072list://0.0.0.0:12000\r\nI0908 20:27:05.811657 10181 predictor_sdk.cpp:28] Succ register all components!\r\nI0908 20:27:05.811672 10181 config_manager.cpp:217] Not found key in configue: cluster\r\nI0908 20:27:05.811678 10181 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0908 20:27:05.811681 10181 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0908 20:27:05.811686 10181 config_manager.cpp:263] split info not set, skip...\r\nI0908 20:27:05.811693 10181 abtest.cpp:55] Succ read weights list: 100, count: 1, normalized: 100\r\nI0908 20:27:05.811698 10181 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nI0908 20:27:05.811702 10181 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nI0908 20:27:05.811705 10181 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nI0908 20:27:05.811709 10181 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nI0908 20:27:05.811713 10181 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nI0908 20:27:05.811717 10181 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nI0908 20:27:05.811720 10181 config_manager.cpp:212] Not found key in configue: connection_type\r\nI0908 20:27:05.811724 10181 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nI0908 20:27:05.811728 10181 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nI0908 20:27:05.811733 10181 config_manager.cpp:226] Not found key in configue: protocol\r\nI0908 20:27:05.811735 10181 config_manager.cpp:227] Not found key in configue: compress_type\r\nI0908 20:27:05.811739 10181 config_manager.cpp:228] Not found key in configue: package_size\r\nI0908 20:27:05.811743 10181 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nI0908 20:27:05.811746 10181 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0908 20:27:05.811750 10181 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0908 20:27:05.811754 10181 config_manager.cpp:263] split info not set, skip...\r\nI0908 20:27:05.811758 10181 config_manager.cpp:186] Succ load one endpoint, name: general_model, count of variants: 1.\r\nI0908 20:27:05.811767 10181 config_manager.cpp:85] Success reload endpoint config file, id: 1\r\nI0908 20:27:05.814249 10181 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\nI0908 20:27:05.814441 10181 stub_impl.hpp:376] Succ create parallel channel, count: 3\r\nI0908 20:27:05.814448 10181 stub_impl.hpp:42] Create stub without tag, ep general_model\r\nI0908 20:27:05.815412 10181 variant.cpp:69] Succ create default debug\r\nI0908 20:27:05.815429 10181 endpoint.cpp:38] Succ create variant: 0, endpoint:general_model\r\nI0908 20:27:05.815438 10181 predictor_sdk.cpp:69] Succ create endpoint instance with name: general_model\r\n然后在client端执行程序：输出\r\n(paddle) liangruofei@liangruofei-System-Product-Name:~$ python test.py \r\n{'serving_status_code': <StatusCode.DEADLINE_EXCEEDED: (4, 'deadline exceeded')>, 'image': '/home/liangruofei/darknet_uav/test.jpg'}\r\n\r\n然而serve端输出显示：\r\n(paddle) liangruofei@liangruofei-System-Product-Name:~/cascade$ python -m paddle_serving_server_gpu.serve --thread 10 --model serving_server --port 8888 --gpu_id 0 --use_multilang\r\nmkdir: 无法创建目录\"workdir_0\": 文件已存在\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0908 20:27:05.811311 10181 general_model.cpp:73] feed var num: 3fetch_var_num: 1\r\nI0908 20:27:05.811334 10181 general_model.cpp:77] feed alias name: image index: 0\r\nI0908 20:27:05.811340 10181 general_model.cpp:80] feed[0] shape:\r\nI0908 20:27:05.811347 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811352 10181 general_model.cpp:87] feed[0] feed type: 1\r\nI0908 20:27:05.811357 10181 general_model.cpp:77] feed alias name: im_info index: 1\r\nI0908 20:27:05.811362 10181 general_model.cpp:80] feed[1] shape:\r\nI0908 20:27:05.811367 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811372 10181 general_model.cpp:87] feed[1] feed type: 1\r\nI0908 20:27:05.811378 10181 general_model.cpp:77] feed alias name: im_shape index: 2\r\nI0908 20:27:05.811383 10181 general_model.cpp:80] feed[2] shape:\r\nI0908 20:27:05.811388 10181 general_model.cpp:84] shape[0]: 3\r\nI0908 20:27:05.811393 10181 general_model.cpp:87] feed[2] feed type: 1\r\nI0908 20:27:05.811398 10181 general_model.cpp:95] fetch [0] alias name: multiclass_nms_0.tmp_0\r\nI0908 20:27:05.811477 10181 general_model.cpp:55] Init commandline: dummy /home/liangruofei/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle_serving_server_gpu/serve.py --tryfromenv=profile_client,profile_server,max_body_size \r\nI0908 20:27:05.811640 10181 predictor_sdk.cpp:34] \r\nM\r\ndefault��� d(���������0:pooled\r\nDefaultla\"    baidu_std �\r\ngeneral_model@baidu.paddle_serving.predictor.general_model.GeneralModelServiceWeightedRandomRender\"\r\n100*5\r\ndefault_tag_139745050093072list://0.0.0.0:12000\r\nI0908 20:27:05.811657 10181 predictor_sdk.cpp:28] Succ register all components!\r\nI0908 20:27:05.811672 10181 config_manager.cpp:217] Not found key in configue: cluster\r\nI0908 20:27:05.811678 10181 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0908 20:27:05.811681 10181 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0908 20:27:05.811686 10181 config_manager.cpp:263] split info not set, skip...\r\nI0908 20:27:05.811693 10181 abtest.cpp:55] Succ read weights list: 100, count: 1, normalized: 100\r\nI0908 20:27:05.811698 10181 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nI0908 20:27:05.811702 10181 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nI0908 20:27:05.811705 10181 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nI0908 20:27:05.811709 10181 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nI0908 20:27:05.811713 10181 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nI0908 20:27:05.811717 10181 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nI0908 20:27:05.811720 10181 config_manager.cpp:212] Not found key in configue: connection_type\r\nI0908 20:27:05.811724 10181 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nI0908 20:27:05.811728 10181 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nI0908 20:27:05.811733 10181 config_manager.cpp:226] Not found key in configue: protocol\r\nI0908 20:27:05.811735 10181 config_manager.cpp:227] Not found key in configue: compress_type\r\nI0908 20:27:05.811739 10181 config_manager.cpp:228] Not found key in configue: package_size\r\nI0908 20:27:05.811743 10181 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nI0908 20:27:05.811746 10181 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0908 20:27:05.811750 10181 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0908 20:27:05.811754 10181 config_manager.cpp:263] split info not set, skip...\r\nI0908 20:27:05.811758 10181 config_manager.cpp:186] Succ load one endpoint, name: general_model, count of variants: 1.\r\nI0908 20:27:05.811767 10181 config_manager.cpp:85] Success reload endpoint config file, id: 1\r\nI0908 20:27:05.814249 10181 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\nI0908 20:27:05.814441 10181 stub_impl.hpp:376] Succ create parallel channel, count: 3\r\nI0908 20:27:05.814448 10181 stub_impl.hpp:42] Create stub without tag, ep general_model\r\nI0908 20:27:05.815412 10181 variant.cpp:69] Succ create default debug\r\nI0908 20:27:05.815429 10181 endpoint.cpp:38] Succ create variant: 0, endpoint:general_model\r\nI0908 20:27:05.815438 10181 predictor_sdk.cpp:69] Succ create endpoint instance with name: general_model\r\nI0908 20:28:07.590495 10250 general_model.cpp:366] batch size: 1\r\nI0908 20:28:07.590517 10250 stub_impl.hpp:149] Succ thread initialize stub impl!\r\nI0908 20:28:07.590524 10250 endpoint.cpp:53] Succ thrd initialize all vars: 1\r\nI0908 20:28:07.590530 10250 predictor_sdk.cpp:129] Succ thrd initialize endpoint:general_model\r\nI0908 20:28:07.590725 10250 general_model.cpp:377] fetch general model predictor done.\r\nI0908 20:28:07.590732 10250 general_model.cpp:378] float feed name size: 3\r\nI0908 20:28:07.590737 10250 general_model.cpp:379] int feed name size: 0\r\nI0908 20:28:07.590744 10250 general_model.cpp:380] max body size : 536870912\r\nI0908 20:28:07.590750 10250 general_model.cpp:388] prepare batch 0\r\nI0908 20:28:07.590759 10250 general_model.cpp:401] batch [0] int_feed_name and float_feed_name prepared\r\nI0908 20:28:07.590764 10250 general_model.cpp:405] tensor_vec size 3 float shape 3\r\nI0908 20:28:07.590770 10250 general_model.cpp:410] prepare float feed image shape size 3\r\nI0908 20:28:07.608628 10250 general_model.cpp:410] prepare float feed im_info shape size 1\r\nI0908 20:28:07.608654 10250 general_model.cpp:410] prepare float feed im_shape shape size 1\r\nI0908 20:28:07.608662 10250 general_model.cpp:462] batch [0] float feed value prepared\r\nI0908 20:28:07.608667 10250 general_model.cpp:545] batch [0] int feed value prepared\r\nW0908 20:28:07.627852 10250 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E111]Fail to connect SocketId=113@0.0.0.0:12000: Connection refused [R1][E112]Fail to select server from list://0.0.0.0:12000 lb=la [R2][E112]Fail to select server from list://0.0.0.0:12000 lb=la\r\nI0908 20:28:07.728153 10200 socket.cpp:2370] Checking SocketId=0@0.0.0.0:12000\r\nE0908 20:28:10.197585 10250 general_model.cpp:567] failed call predictor with req: insts { tensor_array { float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3271606 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.3513678 float_data: 1.355725 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3755751 float_data: 1.3997821 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4239894 float_data: 1.4380295 float_data: 1.4690149 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4912854 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4724039 float_data: 1.4825709 float_data: 1.4966111 float_data: 1.472888 float_data: 1.4786979 float_data: 1.4966111 float_data: 1.4767611 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4951587 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4777296 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4941905 float_data: 1.5009685 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5542245 float_data: 1.5692327 float_data: 1.5919877 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.6021541 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692327 float_data: 1.5692328 float_data: 1.5692329 float_data: 1.5692328 float_data: 1.5692329 float_data: 1.5692328 float_data: 1.5692327 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5726218 float_data: 1.6036071 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176472 float_data: 1.6176474 float_data: 1.6176473 float_data: 1.6176472 float_data: 1.6176473 float_data: 1.6176472 float_data: 1.6190997 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6529899 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6491165 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6549261 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6607361 float_data: 1.6660616 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.690269 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7231903 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7769302 float_data: 1.8079156 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8752124 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.9323409 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.932341 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.932341 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.9323409 float_data: 1.9323411 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.9323409 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9323409 float_data: 1.9323409 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9565482 float_data: 1.9807553 float_data: 1.9807553 float_data: 1.9807553 float_data: 1.9807553 float_data: 1.9807556 float_data: 1.9807553 float_data: 1.9807556 float_data: 1.9807556 float_data: 1.9807553 float_data: 1.9807556 float_data: 1.9807553 float_data: 1.9807553 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 1.9652624 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.994313 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0248127 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0170662 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 1.9851124 float_data: 2.0073829 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0519247 float_data: 2.0775843 float_data: 2.0654807 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0093198 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0170662 float_data: 2.0291698 float_data: 2.0049627 float_data: 1.9885005 float_data: 1.9565482 float_data: 2.0001223 float_data: 2.0136769 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0393364 float_data: 2.0291698 float_data: 2.0320852 float_data: 2.0615807 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0577343 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0732272 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0654807 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0620914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0557973 float_data: 2.0620914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1008222 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0974343 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.0896878 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0732272 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0993712 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1013069 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.1017914 float_data: 2.0896878 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0654807 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0732257 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0775843 float_data: 2.0693545 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0335283 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0480523 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0533772 float_data: 2.0354638 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0170662 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0291698 float_data: 2.0170662 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 2.0049627 float_data: 1.9337947 float_data: 1.9323409 float_data: 1.9323411 float_data: 1.932341 float_data: 1.9371812 float_data: 1.9715565 float_data: 1.9725258 float_data: 1.958002 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 1.9565482 float_data: 2.0291698 float_data: 2.0190017 float_data: 1.9880188 float_data: 1.9807553 float_data: 1.9502542 float_data: 1.9323411 float_data: 1.932341 float_data: 1.932341 float_data: 1.9323409 float_data: 1.9323411 float_data: 1.932341 float_data: 1.932341 float_data: 1.932341 float_data: 1.932341 float_data: 1.932341 float_data: 1.9323411 float_data: 1.9323409 float_data: 1.932341 float_data: 1.932341 float_data: 1.9323411 float_data: 1.9323409 float_data: 1.932341 float_data: 1.932341 float_data: 1.932341 float_data: 1.9323409 float_data: 1.9323409 float_data: 1.932341 float_data: 1.932341 float_data: 1.9086185 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9173326 float_data: 1.9323411 float_data: 1.932341 float_data: 1.932341 float_data: 1.9202373 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.9081337 float_data: 1.8882852 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8839266 float_data: 1.8708538 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597195 float_data: 1.8597193 float_data: 1.8597195 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8597193 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8355122 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.811305 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.811305 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.8113048 float_data: 1.7992013 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7870977 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7628905 float_data: 1.7386833 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7265795 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386832 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7386833 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.7144761 float_data: 1.6747787 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6781652 float_data: 1.6713865 float_data: 1.6433082 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.661703 float_data: 1.6418544 float_data: 1.6597676 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6660616 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418543 float_data: 1.6660616 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.690269 float_data: 1.690269 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6902688 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176473 float_data: 1.6176474 float_data: 1.6176473 float_data: 1.6084484 float_data: 1.59344 float_data: 1.5706867 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692327 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.5692329 float_data: 1.5692328 float_data: 1.5692328 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6418544 float_data: 1.6355603 float_data: 1.6045744 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.5692328 float_data: 1.5813365 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.59344 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.5450256 float_data: 1.532922 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5000005 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.5208182 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4966111 float_data: 1.4816027 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724038 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724038 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4724039 float_data: 1.4481966 \r\n\r\n这样的输出和在aistudio不对啊",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-09T08:04:37+00:00",
        "updated_at": "2024-03-05T06:48:53+00:00",
        "closed_at": "2024-03-05T06:48:53+00:00",
        "comments_count": [
            "TeslaZhao",
            "liangruofei",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 833,
        "title": "使用客户端调用时的疑问：预处理后处理",
        "body": "我想确认两个事情：\r\n1. 对模型输入的预处理与模型输出的后处理，需要自己从原来的脚本中抽离吗？\r\n下面的客户端代码是通用的吗？如果不是我需要修改什么地方？\r\n```\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import Sequential, File2Image, Resize, Transpose, BGR2RGB, SegPostprocess\r\nimport sys\r\nimport cv2\r\nimport numpy as np\r\n\r\nclient = Client()\r\nclient.load_client_config(\"seg_client_conf/serving_client_conf.prototxt\")\r\nclient.connect([\"127.0.0.1:9292\"])\r\n\r\npreprocess = Sequential(\r\n    [File2Image(), Resize(\r\n        (512, 512), interpolation=cv2.INTER_LINEAR)])\r\n\r\npostprocess = SegPostprocess(19)\r\n\r\nfilename = sys.argv[1]\r\nim = preprocess(filename)\r\n\r\nfetch_map = client.predict(feed={\"image\": im}, fetch=[\"arg_max_0.tmp_0\"])\r\nfetch_map[\"filename\"] = filename\r\npostprocess(fetch_map)\r\n```\r\n2. inference_model_to_serving 函数返回的值是不是我在使用 client.predict()时的 feed 与 fetch的值？",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-10T07:38:06+00:00",
        "updated_at": "2024-03-05T06:48:54+00:00",
        "closed_at": "2024-03-05T06:48:54+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao",
            "TeslaZhao",
            "iceriver97",
            "iceriver97",
            "TeslaZhao",
            "bjjwwang",
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "iceriver97",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 835,
        "title": "本地ubuntu16.04复现房产预测报错",
        "body": "serve端用docker实现的没问题：\r\n输出如下：\r\n```\r\n[root@1e9ea907d737 opt]# python3 -m paddle_serving_server_gpu.serve --thread 10 --model uci_housing_model --port 9292 --gpu_id 0 --use_multilang\r\nmkdir: cannot create directory ‘workdir_0’: File exists\r\nGoing to Run Comand\r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/serving-gpu-0.3.1/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 10 -port 12000 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 10 -gpuid 0 -max_body_size 536870912 \r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0913 12:19:05.714517   362 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralTextReaderOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralCopyOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralTextResponseOp\r\nI0100 00:00:00.000000   380 op_repository.h:65] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000   380 service_manager.h:61] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   380 load_general_model_service.pb.h:299] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000   380 service_manager.h:61] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   380 general_model_service.pb.h:1473] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:27] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:33] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:39] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:44] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:49] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_gpu_engine.cpp:55] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:25] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:31] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:37] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:42] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:47] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000   380 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   380 fluid_cpu_engine.cpp:53] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR_SIGMOID in macro!\r\n```\r\nclient端没有输出：\r\nclient端程序为：\r\n```\r\nfrom paddle_serving_client import MultiLangClient as Client\r\n\r\nclient = Client()\r\nclient.connect([\"127.0.0.1:9292\"])\r\ndata = [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727,\r\n        -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]\r\nfetch_map = client.predict(feed={\"x\": data}, fetch=[\"price\"])\r\nprint(fetch_map)\r\n```\r\n配好环境运行结果如下：\r\nSystem-Product-Name:~$ python test2.py \r\nNone\r\n同时在serve端出现：\r\n```\r\nW0913 12:24:20.966691   393 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E111]Fail to connect SocketId=113@0.0.0.0:12000: Connection refused [R1][E112]Fail to select server from list://0.0.0.0:12000 lb=la [R2][E112]Fail to select server from list://0.0.0.0:12000 lb=la\r\nE0913 12:24:20.966754   393 general_model.cpp:561] failed call predictor with req: insts { tensor_array { float_data: 0.0137 float_data: -0.1136 float_data: 0.2553 float_data: -0.0692 float_data: 0.0582 float_data: -0.0727 float_data: -0.1583 float_data: -0.0584 float_data: 0.6283 float_data: 0.4919 float_data: 0.1856 float_data: 0.0795 float_data: -0.0332 elem_type: 1 shape: 13 } } fetch_var_names: \"price\"\r\nI0913 12:24:21.066982   389 socket.cpp:2370] Checking SocketId=0@0.0.0.0:12000\r\n```\r\n请问咋解决",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-13T12:24:38+00:00",
        "updated_at": "2020-12-31T00:07:35+00:00",
        "closed_at": "2020-12-31T00:07:35+00:00",
        "comments_count": [
            "MRXLT",
            "liangruofei",
            "MRXLT"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 834,
        "title": "有没有PaddleSeg到PaddleServing的示例呢？",
        "body": "我在使用Seg套件后想使用Serving套件进行部署，但是碰到了很多问题，所以想看看有没有完整的教程？谢谢",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-11T01:43:16+00:00",
        "updated_at": "2020-10-22T14:02:22+00:00",
        "closed_at": "2020-10-22T02:12:10+00:00",
        "comments_count": [
            "TeslaZhao",
            "iceriver97",
            "TeslaZhao",
            "iceriver97"
        ],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 838,
        "title": "example ocr 服务端启动案例 运行OCR部署代码找不到 对象",
        "body": "在paddle serving app的 0.1.1版本 python example OCR 项目中的reader 找不到GetRotateCropImage, SortedBoxes类。",
        "state": "closed",
        "user": "natureLanguageQing",
        "closed_by": "natureLanguageQing",
        "created_at": "2020-09-14T06:33:58+00:00",
        "updated_at": "2020-09-21T05:26:07+00:00",
        "closed_at": "2020-09-21T05:26:07+00:00",
        "comments_count": [
            "natureLanguageQing",
            "MRXLT",
            "natureLanguageQing"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 840,
        "title": "client_seving显示图像太大(3,225,225)",
        "body": "W0916 11:52:00.469072  6493 redis_protocol.cpp:69] No corresponding PipelinedInfo in socket\r\nE0916 11:52:00.469117  6493 input_messenger.cpp:113] A message from 127.0.0.1:9292(protocol=esp) is bigger than 536870912 bytes, the connection will be closed. Set max_body_size to allow bigger messages\r\nW0916 11:52:00.469156  6493 input_messenger.cpp:276] Close fd=4 SocketId=1@127.0.0.1:9292@51838: too big data\r\nW0916 11:52:00.469290  6460 predictor.hpp:129] inference call failed, message: [E22]1/1 channels failed, fail_limit=1 [C0][E22]Close fd=4 SocketId=1@127.0.0.1:9292@51838: too big data\r\n但是我看别人的也是这么搞得，别人还是(3,640,640)的图片都没报图片太大。为什么我就报错了呢\r\n\r\n! LD_LIBRARY_PATH=/home/aistudio/env/cuda-9.0/lib64:$LD_LIBRARY_PATH && \\\r\npython -m paddle_serving_server_gpu.serve --model uci_car_model --port 9292 --name uci --gpu_id 0（运行serving服务）\r\n\r\nfrom paddle_serving_client import Client\r\nimport cv2\r\nimport numpy as np\r\n\r\nclient = Client()\r\nclient.load_client_config(\"uci_car_client/serving_client_conf.prototxt\")\r\nclient.connect([\"127.0.0.1:9292\"])\r\ndef transform_img(img):\r\n    # 将图片尺寸缩放道 224x224\r\n    img = cv2.resize(img, (224, 224))\r\n    # 读入的图像数据格式是[H, W, C]\r\n    # 使用转置操作将其变成[C, H, W]\r\n    img = np.transpose(img, (2,0,1))\r\n    img = img.astype('float32')\r\n    # 将数据范围调整到[-1.0, 1.0]之间\r\n    img = img / 255.\r\n    img = img * 2.0 - 1.0\r\n    return img\r\ndef read_img(file_name):\r\n    # filepath = os.path.join(datadir, name)\r\n    img = cv2.imread(file_name)\r\n    # print(img)\r\n    img = transform_img(img)\r\n    return img\r\ndata = read_img('6/16c547f-1c20fa9a74ee4ba2acb152fbddd0a2d5-front.jpg')\r\nprint(data.shape,data.dtype,data)\r\nfetch_map = client.predict(feed={\"x\": data}, fetch=[\"type\"])\r\nprint(fetch_map)\r\n（client运行文件）\r\n\r\nfeed_var {\r\n  name: \"img\"\r\n  alias_name: \"x\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 224\r\n  shape: 224\r\n}\r\nfetch_var {\r\n  name: \"fc_2.tmp_2\"\r\n  alias_name: \"type\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 2\r\n}\r\n（serving_client_conf.prototxt文件）\r\n\r\n! LD_LIBRARY_PATH=/home/aistudio/env/cuda-9.0/lib64:$LD_LIBRARY_PATH && \\\r\npython -m paddle_serving_server_gpu.serve --model uci_car_model --port 9292 --name uci --gpu_id 0 --max_body_size 100000000000000000\r\n（设置max_body_size的值，报错依然不变）\r\n",
        "state": "closed",
        "user": "stidk",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-16T04:08:55+00:00",
        "updated_at": "2020-12-31T00:05:53+00:00",
        "closed_at": "2020-12-31T00:05:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 841,
        "title": "没有cudnn时server可以启动，不过预测时闪退",
        "body": "没有cudnn时server可以正常启动。不过在预测时会闪退不报错。serving版本 0.3.1\r\n需要修改点，没有cudnn或者版本错误不能启动~",
        "state": "closed",
        "user": "wangxicoding",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-16T08:54:38+00:00",
        "updated_at": "2020-12-31T00:06:50+00:00",
        "closed_at": "2020-12-31T00:06:49+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 846,
        "title": "有没有类似于websocket这种实时推流的部署方案",
        "body": "我想请教一下，我目前在做车辆检测的部署，场景就是添加一个rtsp之后，对视频流实时的检测车辆，用websocket或socketio这种来返回具体坐标信息，有没有docker这种简单方便的部署方案可以参考。具体应该如何做，新手请多指教，谢谢。",
        "state": "closed",
        "user": "Sunyingbin",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-24T06:21:15+00:00",
        "updated_at": "2020-10-22T02:01:17+00:00",
        "closed_at": "2020-10-22T02:01:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 842,
        "title": "使用java client出现grpc网络异常",
        "body": "我使用了官方文档中的java client文档中的教程测试,在调用最后的请求是出现了网络异常。\r\n我确定服务端是存活的并且在另外一台机器上运行python客户端是正常的。\r\n请问是不是目前不支持windows操作系统的java客户端。\r\n\r\n以下是控制台出现的警告以及错误\r\n\r\n2020-09-18 10:13:56 org.nd4j.linalg.factory.Nd4jBackend load \r\nINFO: Loaded [CpuBackend] backend\r\n2020-09-18 10:13:56 org.nd4j.nativeblas.NativeOpsHolder <init> \r\nINFO: Number of threads used for linear algebra: 4\r\n2020-09-18 10:13:56 org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory createBlas \r\nWARN: *********************************** CPU Feature Check Warning ***********************************\r\n2020-09-18 10:13:56 org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory createBlas \r\nWARN: Warning: Initializing ND4J with Generic x86 binary on a CPU with AVX/AVX2 support\r\n2020-09-18 10:13:56 org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory createBlas \r\nWARN: Using ND4J with AVX/AVX2 will improve performance. See deeplearning4j.org/cpu for more details\r\n2020-09-18 10:13:56 org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory createBlas \r\nWARN: Or set environment variable ND4J_IGNORE_AVX=true to suppress this warning\r\n2020-09-18 10:13:56 org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory createBlas \r\nWARN: *************************************************************************************************\r\n2020-09-18 10:13:56 org.nd4j.nativeblas.Nd4jBlas <init> \r\nINFO: Number of threads used for OpenMP BLAS: 4\r\n2020-09-18 10:13:57 org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner printEnvironmentInformation \r\nINFO: Backend used: [CPU]; OS: [Windows 10]\r\n2020-09-18 10:13:57 org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner printEnvironmentInformation \r\nINFO: Cores: [4]; Memory: [6.7GB];\r\n2020-09-18 10:13:57 org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner printEnvironmentInformation \r\nINFO: Blas vendor: [OPENBLAS]\r\nGet Client config failed: io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason\r\nconnect failed.\r\n\r\nProcess finished with exit code 0\r\n",
        "state": "closed",
        "user": "ltnsace",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-18T02:19:13+00:00",
        "updated_at": "2020-12-30T23:52:19+00:00",
        "closed_at": "2020-12-30T23:52:19+00:00",
        "comments_count": [
            "github-actions[bot]",
            "barrierye",
            "barrierye"
        ],
        "labels": [
            "多语言client"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 847,
        "title": "Java文档中的链接没有更新，是旧版本的sdk",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/doc/JAVA_SDK_CN.md#%E5%AE%89%E8%A3%85",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-24T13:18:09+00:00",
        "updated_at": "2024-04-16T09:05:18+00:00",
        "closed_at": "2024-04-16T09:05:18+00:00",
        "comments_count": [],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 848,
        "title": "server python whl 安装时没有自动安装依赖包",
        "body": "安装好后还需要安装：pip install func_timeout pyyaml",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-24T13:25:52+00:00",
        "updated_at": "2020-10-22T01:59:37+00:00",
        "closed_at": "2020-10-22T01:59:37+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 849,
        "title": "重新安装wheel包时，可能会遇到Couldn't build proto file into descriptor pool!",
        "body": "![311600947824_ pic_hd](https://user-images.githubusercontent.com/28446721/94151481-0c74cd00-fead-11ea-8544-3076f3c1e7f3.jpg)\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "TeslaZhao",
        "created_at": "2020-09-24T13:29:52+00:00",
        "updated_at": "2020-12-31T00:06:15+00:00",
        "closed_at": "2020-12-31T00:06:15+00:00",
        "comments_count": [
            "MRXLT"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 851,
        "title": "serving  OOM",
        "body": "```bash\r\n\r\nexport FLAGS_fraction_of_gpu_memory_to_use=0.9\r\n\r\npython -m paddle_serving_server_gpu.serve --model server_model/server_rec  --port 9294 --gpu_id 2 --mem_optim --thread 2 --max_body_size 1073741824\r\n\r\n```\r\n![image](https://user-images.githubusercontent.com/41443157/94407427-011feb00-01a6-11eb-8860-84df23381d25.png)\r\n\r\n模型是paddleOCR\r\n- ch_ppocr_server_v1.1_det\r\n- ch_ppocr_server_v1.1_rec\r\n\r\n显存11G\r\n\r\n![image](https://user-images.githubusercontent.com/41443157/94407563-2e6c9900-01a6-11eb-8b8c-88bf452bcdf6.png)\r\n\r\n访问多了后，显存就OOM了",
        "state": "closed",
        "user": "sevenold",
        "closed_by": "sevenold",
        "created_at": "2020-09-28T08:19:48+00:00",
        "updated_at": "2020-10-09T02:26:46+00:00",
        "closed_at": "2020-10-09T02:26:46+00:00",
        "comments_count": [
            "MRXLT",
            "sevenold",
            "TeslaZhao",
            "sevenold",
            "MRXLT",
            "sevenold"
        ],
        "labels": [
            "内存问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 857,
        "title": "paddle pgl使用paddle serving进行模型保存的时候，被自动剪枝，导致后面输入的时候模型输入不合法",
        "body": "rt\r\n\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 53, in <module>\r\n    fetch_map = client.predict(feed=feed_dict, fetch=[\"proba\"])\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_client/__init__.py\", line 282, in predict\r\n    raise ValueError(\"Wrong feed name: {}.\".format(key))\r\nValueError: Wrong feed name: sub_graph/edges_dst.",
        "state": "closed",
        "user": "xiuechen",
        "closed_by": "TeslaZhao",
        "created_at": "2020-10-15T03:53:30+00:00",
        "updated_at": "2020-12-30T23:48:52+00:00",
        "closed_at": "2020-12-30T23:48:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "MRXLT"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 858,
        "title": "Check failure stack trace",
        "body": "### 部署paddleOCR脚本\r\n```bash\r\npython -m paddle_serving_server_gpu.serve --model server_model/server_rec  --port 9294 --gpu_id 2  --thread 2 --max_body_size 1073741824\r\n\r\npython -m paddle_serving_server_gpu.serve --model server_model/server_det  --port 9293 --gpu_id 2  --thread 2 --max_body_size 1073741824\r\n\r\n```\r\n\r\n### 版本\r\n- paddle-serving-server-gpu 0.3.2.post10\r\n- Ubuntu 16.10\r\n- docker images paddlepaddle/serving:latest-cuda10.0-cudnn7\r\n\r\n\r\n部署成功后，40万图片经过256线程的并发测试，跑几个小时后，会堵塞住，然后停留在这个错误！！！\r\n```logs\r\n\r\n2020-10-15T03:45:09.161677292Z E1015 11:45:08.967919    14 factory.h:149] RAW: Insert duplicate with tag: WeightedRandomRender\r\n2020-10-15T03:45:09.161723927Z I1015 11:45:08.967919    14 abtest.h:67] RAW: Factory has been registed: WeightedRandomRender->EndpointRouterBase.\r\n2020-10-15T03:45:09.177337994Z E1015 11:45:08.967919    14 factory.h:149] RAW: Insert duplicate with tag: WeightedRandomRender\r\n2020-10-15T03:45:09.177399889Z I1015 11:45:08.967919    14 abtest.h:67] RAW: Factory has been registed: WeightedRandomRender->EndpointRouterBase.\r\n2020-10-15T03:45:10.039389100Z INFO:     192.168.88.35:43330 - \"POST /api/models/predict HTTP/1.1\" 200 OK\r\n2020-10-15T03:45:10.096505252Z E1015 11:45:08.967919    14 factory.h:149] RAW: Insert duplicate with tag: WeightedRandomRender\r\n2020-10-15T03:45:10.096573100Z I1015 11:45:08.967919    14 abtest.h:67] RAW: Factory has been registed: WeightedRandomRender->EndpointRouterBase.\r\n2020-10-15T03:45:10.096589761Z F1015 11:45:10.096339    14 doubly_buffered_data.h:283] Fail to pthread_key_create: Resource temporarily unavailable\r\n2020-10-15T03:45:10.096766297Z *** Check failure stack trace: ***\r\n\r\n```\r\n\r\n### server服务运行正常\r\n\r\n![image](https://user-images.githubusercontent.com/41443157/96076557-33d51d80-0ee0-11eb-9719-f1b853f4c448.png)\r\n",
        "state": "closed",
        "user": "sevenold",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-10-15T04:13:05+00:00",
        "updated_at": "2024-03-05T06:48:55+00:00",
        "closed_at": "2024-03-05T06:48:55+00:00",
        "comments_count": [
            "bjjwwang",
            "sevenold",
            "AnMoran"
        ],
        "labels": [
            "性能",
            "内存问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 860,
        "title": "no module named paddle_serving_client.convert",
        "body": "使用官方帮助文档中保存serving模型的方法，但是提示不存在这个module啊，是不是文档写错了？",
        "state": "closed",
        "user": "iceriver97",
        "closed_by": "iceriver97",
        "created_at": "2020-10-22T14:22:30+00:00",
        "updated_at": "2020-10-23T02:55:51+00:00",
        "closed_at": "2020-10-23T02:55:51+00:00",
        "comments_count": [
            "MRXLT",
            "iceriver97"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 864,
        "title": "关于日志保留时间设置",
        "body": "请教下如何设置日志保留最多天数时间，长时间运行下来日志占用空间太大了",
        "state": "closed",
        "user": "JiaoZiLang",
        "closed_by": "JiaoZiLang",
        "created_at": "2020-11-02T02:48:47+00:00",
        "updated_at": "2020-11-05T02:17:58+00:00",
        "closed_at": "2020-11-05T02:17:58+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao",
            "JiaoZiLang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 865,
        "title": "Ubuntu16.04 用docker部署cascade_rcnn，cpu可以进行，gpu就报错",
        "body": "Ubuntu16.04 用docker部署cascade_rcnn，cpu可以进行，gpu报错：过程如下\r\n1、cpu\r\n服务端：\r\npython3 -m paddle_serving_server_gpu.serve --model serving_server --port 9292 \r\n\r\n显示：\r\nW0100 00:00:00.000000   445 fluid_cpu_engine.cpp:53] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR_SIGMOID in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\n\r\n客户端：执行程序\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(800, 1333), Transpose((2, 0, 1)), PadStride(32)\r\n])\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\nclient.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\nim = preprocess('000000570688.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = '000000570688.jpg'\r\nprint (fetch_map)\r\npostprocess(fetch_map)\r\nprint(fetch_map)\r\n显示结果如下：\r\npython3 test_client.py \r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI1102 17:33:43.807981 16940 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\n{'multiclass_nms_0.tmp_0': array([[1.00000000e+00, 3.89992058e-01, 0.00000000e+00, 5.76865784e+02,\r\n        1.54522171e+01, 6.53119812e+02],\r\n       [1.00000000e+00, 3.77118677e-01, 7.63976440e+02, 5.79579468e+02,\r\n\r\n[5.70000000e+01, 1.32285058e-01, 1.91921631e+02, 6.49584534e+02,\r\n        2.85223114e+02, 6.92049500e+02]], dtype=float32), 'image': '000000570688.jpg', 'multiclass_nms_0.tmp_0.lod': array([  0, 100], dtype=int32)}\r\n\r\n但在gpu下报错\r\n2、gpu\r\n服务端执行：python3 -m paddle_serving_server_gpu.serve --model serving_server --port 9292 --gpu_id 0\r\n\r\n显示结果：\r\nmkdir: cannot create directory ‘workdir_0’: File exists\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\nGoing to Run Comand\r\n/usr/local/python3.5.1/lib/python3.5/site-packages/paddle_serving_server_gpu/serving-gpu-cuda9-0.3.2/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 9292 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 536870912 \r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralTextReaderOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralCopyOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralTextResponseOp\r\nI0100 00:00:00.000000   487 op_repository.h:65] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000   487 service_manager.h:61] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   487 load_general_model_service.pb.h:299] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000   487 service_manager.h:61] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   487 general_model_service.pb.h:1473] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:27] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:33] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:39] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:44] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:49] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidGpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_GPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_gpu_engine.cpp:55] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidGpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_GPU_NATIVE_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:25] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuAnalysisCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:31] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:37] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuAnalysisDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_ANALYSIS_DIR_SIGMOID in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:42] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:47] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<FluidCpuNativeDirCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR in macro!\r\nI0100 00:00:00.000000   487 factory.h:121] RAW: Succ insert one factory, tag: FLUID_CPU_NATIVE_DIR_SIGMOID, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   487 fluid_cpu_engine.cpp:53] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine< FluidCpuNativeDirWithSigmoidCore>->::baidu::paddle_serving::predictor::InferEngine, tag: FLUID_CPU_NATIVE_DIR_SIGMOID in macro!\r\nI1102 09:35:04.759968   487 analysis_predictor.cc:138] Profiler is deactivated, and no profiling report will be generated.\r\nI1102 09:35:04.765663   487 analysis_predictor.cc:875] MODEL VERSION: 1.7.2\r\nI1102 09:35:04.765684   487 analysis_predictor.cc:877] PREDICTOR VERSION: 1.8.4\r\nI1102 09:35:04.765931   487 analysis_predictor.cc:474] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\nI1102 09:35:04.870309   487 ir_params_sync_among_devices_pass.cc:41] Sync params from CPU to GPU\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\nI1102 09:35:04.983223   487 memory_optimize_pass.cc:223] Cluster name : rpn_cls_logits_fpn5.tmp_1  size: 12\r\nI1102 09:35:04.983239   487 memory_optimize_pass.cc:223] Cluster name : im_shape  size: 12\r\nI1102 09:35:04.983242   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_2.tmp_1  size: 4\r\nI1102 09:35:04.983245   487 memory_optimize_pass.cc:223] Cluster name : image  size: 12\r\nI1102 09:35:04.983247   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_0.tmp_1  size: 4\r\nI1102 09:35:04.983249   487 memory_optimize_pass.cc:223] Cluster name : im_info  size: 12\r\nI1102 09:35:04.983253   487 memory_optimize_pass.cc:223] Cluster name : rpn_cls_logits_fpn6.tmp_1  size: 12\r\nI1102 09:35:04.983255   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_0.tmp_0  size: 16\r\nI1102 09:35:04.983259   487 memory_optimize_pass.cc:223] Cluster name : roi_align_2.tmp_0  size: 50176\r\nI1102 09:35:04.983263   487 memory_optimize_pass.cc:223] Cluster name : roi_align_11.tmp_0  size: 50176\r\nI1102 09:35:04.983265   487 memory_optimize_pass.cc:223] Cluster name : roi_align_9.tmp_0  size: 50176\r\nI1102 09:35:04.983269   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_3.tmp_1  size: 4\r\nI1102 09:35:04.983271   487 memory_optimize_pass.cc:223] Cluster name : fpn_res2_sum.tmp_1  size: 1024\r\nI1102 09:35:04.983274   487 memory_optimize_pass.cc:223] Cluster name : concat_2.tmp_0  size: 50176\r\nI1102 09:35:04.983276   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_1.tmp_1  size: 4\r\nI1102 09:35:04.983279   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_1.tmp_0  size: 16\r\nI1102 09:35:04.983281   487 memory_optimize_pass.cc:223] Cluster name : generate_proposals_2.tmp_0  size: 16\r\nI1102 09:35:04.983284   487 memory_optimize_pass.cc:223] Cluster name : roi_align_8.tmp_0  size: 50176\r\nI1102 09:35:04.983289   487 memory_optimize_pass.cc:223] Cluster name : res3d.add.output.5.tmp_0  size: 2048\r\nI1102 09:35:04.983291   487 memory_optimize_pass.cc:223] Cluster name : fpn_res3_sum.tmp_1  size: 1024\r\nI1102 09:35:04.983294   487 memory_optimize_pass.cc:223] Cluster name : fpn_res4_sum.tmp_1  size: 1024\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI1102 09:35:05.009222   487 analysis_predictor.cc:496] ======= optimize end =======\r\nW1102 09:35:05.010625   487 infer.h:487] Succ load common model[0x2ef43e70], path[serving_server].\r\nW1102 09:35:05.010643   487 infer.h:185] Succ load model_data_pathserving_server\r\n\r\n\r\n客户端执行：\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(800, 1333), Transpose((2, 0, 1)), PadStride(32)\r\n])\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\nclient.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\nim = preprocess('000000570688.jpg')\r\nfetch_map = client.predict(feed={\"image\": im, \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n                                 \"im_shape\": np.array(list(im.shape[1:]) + [1.0])}, fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = '000000570688.jpg'\r\nprint (fetch_map)\r\npostprocess(fetch_map)\r\nprint(fetch_map)\r\n\r\n报错如下：\r\n在客户端出现：\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI1102 17:36:08.139057 17125 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\nW1102 17:36:09.307957 17201 socket.cpp:1739] Fail to keep-write into fd=7 SocketId=8589934594@127.0.0.1:9292@33894: Connection reset by peer [104]\r\nW1102 17:36:09.308286 17203 socket.cpp:1739] Fail to keep-write into fd=3 SocketId=226@127.0.0.1:9292@33898: Connection reset by peer [104]\r\nW1102 17:36:09.308418 17125 predictor.hpp:129] inference call failed, message: [E1014]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=3 SocketId=1@127.0.0.1:9292@33890 [R1][E1014]Got EOF of fd=7 SocketId=8589934594@127.0.0.1:9292@33894 [R2][E1014]Got EOF of fd=3 SocketId=226@127.0.0.1:9292@33898\r\nE1102 17:36:10.739296 17125 general_model.cpp:567] failed call predictor with req: insts { tensor_array { float_data: 800 float_data: 1088 float_data: 1 elem_type: 1 shape: 3 } tensor_array { float_data: 0.77617955 float_data: 0.78302467 float_data: 0.79329634 float_data: 0.79330432 float_data: 0.78989387 float_data: 0.77962214 float_data: 0.76935053 float_data: 0.7590788  float_data: 0.7864207 float_data: 0.79669237 float_data: 0.80696404 float_data: 0.76198548 float_data: 0.738412 float_data: 0.75895524 float_data: 0.77270168 float_data: 0.78297329 float_data: 0.76668471 float_data: 0.7556299 float_data: 0.7556299 float_data: 0.77054787 float_data: 0.79314542 float_data: 0.80350375 float_data: 0.80975974 float_data: 0.80770552 float_data: 0.81106305 float_data: 0.817226 float_data: 0.80709565 float_data: 0.80015421 float_data: 0.80015421 float_data: 0.806903 float_data: 0.81717467 float_data: 0.8233794 float_data: 0.82755381 float_data: 0.82755381 float_data: 0.82755387 float_data: 0.82755381 float_data: 0.81740254 float_data: 0.80581164 float_data: 0.79143137 float_data: 0.79988784 float_data: 0.82043105 float_data: 0.824758 float_data: 0.82886666 float_data: 0.83297533 float_data: 0.83038336 float_data: 0.82422036 float_data: 0.80793822 float_data: 0.79542285 float_data: 0.79131418 float_data: 0.77249944 float_data: 0.7457931 float_data: 0.75141692 float_data: 0.75952989 float_data: 0.77185583 float_data: 0.78951657 float_data: 0.81005991 float_data: 0.8104291 float_data: 0.810429 float_data: 0.8104291 float_data: 0.79313254 float_data: 0.76642632 float_data: 0.75180531 float_data: 0.73999935 float_data: 0.73383635 float_data: 0.73563707 float_data: 0.74180007 float_data: 0.76204014 float_data: 0.78066218 float_data: 0.79504251 float_data: 0.80015421 float_data: 0.80015421 float_data: 0.79413086 float_data: 0.79306519 float_data: 0.8033368 float_data: 0.80039978 float_data: 0.79012817  float_data: 0.6700061 float_data: 0.6700061 float_data: 0.67635369 float_data: 0.68662524 float_data: 0.68908411 float_data: 0.70046008 float_data: 0.73538339 float_data: 0.74624741 float_data: 0.74213874 float_data: 0.73608005 float_data: 0.72875828 float_data: 0.71848661 float_data: 0.71074116 float_data: 0.70457816 float_data: 0.69646847 float_data: 0.69055581 float_data: 0.69055581 float_data: 0.68299657 float_data: 0.67067051 float_data: 0.67389327 float_data: 0.67742896 float_data: 0.67948329 float_data: 0.67776763 float_data: 0.67365897 float_data: 0.68119258 float_data: 0.69339806 float_data: 0.71599579 float_data: 0.72480536 float_data: 0.72480536 float_data: 0.72674251 float_data: 0.72653055 float_data: 0.72036767 float_data: 0.70670307 float_data: 0.6882143 float_data: 0.681329 float_data: 0.675166 float_data: 0.669003 float_data: 0.66284 float_data: 0.656677 float_data: 0.65244484 float_data: 0.64945638 float_data: 0.64945638 float_data: 0.65318787 float_data: 0.65935087 float_data: 0.65973127 float_data: 0.66697121 float_data: 0.69367754\r\n float_data: 0.69805253 float_data: 0.68778086 float_data: 0.68713081 float_data: 0.68546975 float_data: 0.67930675 float_data: 0.664482 float_data: 0.64393866 float_data: 0.6426065 float_data: 0.644258 float_data: 0.650421 float_data: 0.65288138 float_data: 0.65288138 float_\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 17, in <module>\r\n    fetch_map[\"image\"] = '000000570688.jpg'\r\nTypeError: 'NoneType' object does not support item assignment\r\n\r\n在服务端显示：\r\n\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():  \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/home/users/wangjiawei04/paddle_release_home/python/lib64/python2.7/site-packages/paddle/fluid/framework.py\", line 2525, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/home/users/wangjiawei04/paddle_release_home/python/lib64/python2.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/home/users/wangjiawei04/paddle_release_home/python/lib64/python2.7/site-packages/paddle/fluid/layers/nn.py\", line 1403, in conv2d\r\n    \"data_format\": data_format,\r\n  File \"/home/users/wangjiawei04/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 181, in _conv_norm\r\n    name=_name + '.conv2d.output.1')\r\n  File \"/home/users/wangjiawei04/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 452, in c1_stage\r\n    name=_name)\r\n  File \"/home/users/wangjiawei04/PaddleDetection/ppdet/modeling/backbones/resnet.py\", line 473, in __call__\r\n    res = self.c1_stage(res)\r\n  File \"/home/users/wangjiawei04/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py\", line 98, in build\r\n    body_feats = self.backbone(im)\r\n  File \"/home/users/wangjiawei04/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py\", line 335, in test\r\n    return self.build(feed_vars, 'test')\r\n  File \"tools/export_serving_model.py\", line 79, in main\r\n    test_fetches = model.test(feed_vars)\r\n  File \"tools/export_serving_model.py\", line 98, in <module>\r\n    main()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError:  Cudnn error, CUDNN_STATUS_EXECUTION_FAILED  at (/paddle/paddle/fluid/operators/conv_cudnn_op.cu:300)\r\n  [operator < conv2d > error]\r\nAborted (core dumped)",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-02T09:39:14+00:00",
        "updated_at": "2024-04-16T09:05:18+00:00",
        "closed_at": "2024-04-16T09:05:18+00:00",
        "comments_count": [
            "liangruofei"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 867,
        "title": "windows部署serving时报错，ModuleNotFoundError: No module named 'fcntl' ",
        "body": "from https://github.com/PaddlePaddle/PaddleDetection/issues/1644",
        "state": "closed",
        "user": "liuhuiCNN",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-02T13:42:43+00:00",
        "updated_at": "2021-04-29T23:39:45+00:00",
        "closed_at": "2020-12-31T00:04:12+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "xialei2821212670",
            "TeslaZhao",
            "duolabmeng6"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 869,
        "title": "C++ client 文档补全",
        "body": "Paddle Serving支持C++ client（基于bRPC），但还没有相关的文档教程。\r\n可以参考[Go client](https://github.com/PaddlePaddle/Serving/blob/develop/doc/IMDB_GO_CLIENT_CN.md)的文档格式制作一个C++ client的文档教程。",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-03T09:47:01+00:00",
        "updated_at": "2024-04-16T09:05:19+00:00",
        "closed_at": "2024-04-16T09:05:19+00:00",
        "comments_count": [],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 868,
        "title": "OCR 两个模型运行在一张卡的时候 其中一个进程会卡住",
        "body": "日志\r\n![image](https://user-images.githubusercontent.com/23625746/97947055-113f7180-1dc7-11eb-8f54-f1224c3fcad9.png)\r\n\r\n运行脚本\r\n![image](https://user-images.githubusercontent.com/23625746/97947061-18ff1600-1dc7-11eb-9774-6689a39692c7.png)\r\n\r\n环境 nvidia-docker\r\n hub.baidubce.com/paddlepaddle/serving:latest-cuda10.0-cudnn7",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-03T03:23:43+00:00",
        "updated_at": "2024-03-05T06:48:56+00:00",
        "closed_at": "2024-03-05T06:48:56+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 870,
        "title": "Serving App数据预处理API文档",
        "body": "Serving App[数据预处理API文档](https://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_app/README_CN.md#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86api)目前比较简单，同时一些新的API没有被记录，需要：\r\n- 补充预处理API详细功能及使用方式\r\n- 补全已有的API的文档",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-03T09:52:32+00:00",
        "updated_at": "2024-04-16T09:05:20+00:00",
        "closed_at": "2024-04-16T09:05:20+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 871,
        "title": "gRPC文档整理",
        "body": "目前gRPC接口相关的文档比较混乱，分散在下面这些文件中：\r\n- [gRPC接口文档](https://github.com/PaddlePaddle/Serving/blob/develop/doc/GRPC_IMPL_CN.md)\r\n- [线性回归预测服务示例文档](https://github.com/PaddlePaddle/Serving/blob/develop/python/examples/grpc_impl_example/fit_a_line/README_CN.md)\r\n- [gRPC接口使用样例](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/grpc_impl_example)\r\n\r\n需要整理成比较清晰的文档（并包含gRPC接口与多语言多平台的关系）",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "barrierye",
        "created_at": "2020-11-03T09:57:41+00:00",
        "updated_at": "2020-12-01T10:35:42+00:00",
        "closed_at": "2020-12-01T10:35:42+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 872,
        "title": "使用paddleserving压测出现一些问题",
        "body": "env：ubuntu1804，显卡1650，千兆网，brpc方式，pp-yolo\r\n1.单线程请求，无报错，qps:5\r\n2.5个线程，错误率18%\r\n全部报错都为\r\n```\r\nE1103 17:58:25.655791  6378 general_model.cpp:561] failed call predictor with req: insts { tensor_array { float_data: -1.278791 float_data: -1.2959157 float_data: -1.3130405 float_data: -1.3301653 float_data: -1.3301653 float_data: -1.34729 float_data: -1.34729 float_data: -1.34729 float_data: -1.34729 float_data: -1.34729 float_data: -1.3301653 float_data: -1.3301653 float_data: -1.34729 float_data: -1.3644147 float_data: -1.3986642 float_data: -1.3130405 float_data: -1.2274168 float_data: -1.1589177 float_data: -1.0732939 float_data: -1.0219196 float_data: -1.0219196 float_data: -1.0390444 float_data: -1.1246682 float_data: -1.1931672 float_data: -1.2616662 .....(此处省略，都是图片数据)\r\n2020-11-03 17:58:25,662-ERROR: Exception on /start [POST]\r\n```\r\n3.10个线程，错误率32%，一段时间后直接崩溃，除了上面的报错，崩溃报错为\r\n```\r\n2020-11-03 17:58:26,181-INFO: 192.168.1.195 - - [03/Nov/2020 17:58:26] \"POST /start HTTP/1.1\" 200 -\r\nW1103 17:58:26.188755 14849 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW1103 17:58:26.188767 14849 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW1103 17:58:26.188771 14849 init.cc:231] The detail failure signal is:\r\n\r\nW1103 17:58:26.188772 14849 init.cc:234] *** Aborted at 1604397506 (unix time) try \"date -d @1604397506\" if you are using GNU date ***\r\nW1103 17:58:26.192802 14849 init.cc:234] PC: @                0x0 (unknown)\r\nW1103 17:58:26.192942 14849 init.cc:234] *** SIGSEGV (@0x18) received by PID 14831 (TID 0x7fdeb28dc700) from PID 24; stack trace: ***\r\nW1103 17:58:26.193902 14849 init.cc:234]     @     0x7fdf19ba48a0 (unknown)\r\nW1103 17:58:26.194393 14849 init.cc:234]     @     0x7fdeb586659e brpc::Controller::Call::OnComplete()\r\nW1103 17:58:26.194905 14849 init.cc:234]     @     0x7fdeb58669a1 brpc::Controller::EndRPC()\r\nW1103 17:58:26.195415 14849 init.cc:234]     @     0x7fdeb5867ea4 brpc::Controller::OnVersionedRPCReturned()\r\nW1103 17:58:26.195947 14849 init.cc:234]     @     0x7fdeb588f08f brpc::policy::ProcessRpcResponse()\r\nW1103 17:58:26.196414 14849 init.cc:234]     @     0x7fdeb582493a brpc::ProcessInputMessage()\r\nW1103 17:58:26.196831 14849 init.cc:234]     @     0x7fdeb5825e7d brpc::InputMessenger::OnNewMessages()\r\nW1103 17:58:26.197309 14849 init.cc:234]     @     0x7fdeb585782d brpc::Socket::ProcessEvent()\r\nW1103 17:58:26.197836 14849 init.cc:234]     @     0x7fdeb5938825 bthread::TaskGroup::task_runner()\r\nW1103 17:58:26.198354 14849 init.cc:234]     @     0x7fdeb5949561 bthread_make_fcontext\r\n```\r\n\r\n",
        "state": "closed",
        "user": "QingYuan-L",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-03T10:29:02+00:00",
        "updated_at": "2024-03-05T06:48:57+00:00",
        "closed_at": "2024-03-05T06:48:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 874,
        "title": "批量分析时，服务中断；如何关闭predict",
        "body": "serving镜像版本：hub.baidubce.com/paddlepaddle/paddle latest-cuda10.0-cudnn7-devel\r\n显卡 2080TI\r\n\r\nserver：\r\n centOS 7.7\r\n cuda 10.0\r\n cudnn 7.6.5\r\n\r\nclient:\r\n Ubuntu 16.04\r\n\r\n问题描述:\r\nE1110 08:51:55.873919   718 factory.h:149] RAW: Insert duplicate with tag: WeightedRandomRender\r\nI1110 08:51:55.873919   718 abtest.h:67] RAW: Factory has been registed: WeightedRandomRender->EndpointRouterBase.\r\nF1110 08:51:57.535298   718 doubly_buffered_data.h:283] Fail to pthread_key_create: Resource temporarily unavailable\r\n*** Check failure stack trace: ***\r\nW1110 08:51:57.535545   718 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW1110 08:51:57.535591   718 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW1110 08:51:57.535596   718 init.cc:231] The detail failure signal is:\r\n\r\nW1110 08:51:57.535603   718 init.cc:234] *** Aborted at 1604969517 (unix time) try \"date -d @1604969517\" if you are using GNU date ***\r\nW1110 08:51:57.538965   718 init.cc:234] PC: @                0x0 (unknown)\r\nW1110 08:51:57.540107   718 init.cc:234] *** SIGABRT (@0x17e) received by PID 382 (TID 0x7f1798df3700) from PID 382; stack trace: ***\r\nW1110 08:51:57.543048   718 init.cc:234]     @     0x7f1a94537390 (unknown)\r\nW1110 08:51:57.545758   718 init.cc:234]     @     0x7f1a9363e428 gsignal\r\nW1110 08:51:57.548385   718 init.cc:234]     @     0x7f1a9364002a abort\r\nW1110 08:51:57.549118   718 init.cc:234]     @     0x7f1935981bb9 google::logging_fail()\r\nW1110 08:51:57.550195   718 init.cc:234]     @     0x7f193598386d google::LogMessage::Fail()\r\nW1110 08:51:57.551098   718 init.cc:234]     @     0x7f193598731c google::LogMessage::SendToLog()\r\nW1110 08:51:57.552357   718 init.cc:234]     @     0x7f1935983393 google::LogMessage::Flush()\r\nW1110 08:51:57.553185   718 init.cc:234]     @     0x7f193598882e google::LogMessageFatal::~LogMessageFatal()\r\nW1110 08:51:57.554028   718 init.cc:234]     @     0x7f1935afe75a butil::DoublyBufferedData<>::DoublyBufferedData()\r\nW1110 08:51:57.554955   718 init.cc:234]     @     0x7f1935afb3de brpc::policy::LocalityAwareLoadBalancer::LocalityAwareLoadBalancer()\r\nW1110 08:51:57.555816   718 init.cc:234]     @     0x7f1935afb5e8 brpc::policy::LocalityAwareLoadBalancer::New()\r\nW1110 08:51:57.556735   718 init.cc:234]     @     0x7f1935aba122 brpc::SharedLoadBalancer::Init()\r\nW1110 08:51:57.557924   718 init.cc:234]     @     0x7f1935aa7c43 brpc::LoadBalancerWithNaming::Init()\r\nW1110 08:51:57.558842   718 init.cc:234]     @     0x7f1935a8beb4 brpc::Channel::Init()\r\nW1110 08:51:57.560153   718 init.cc:234]     @     0x7f1935a472f9 baidu::paddle_serving::sdk_cpp::StubImpl<>::init_channel()\r\nW1110 08:51:57.561209   718 init.cc:234]     @     0x7f1935a4a779 baidu::paddle_serving::sdk_cpp::StubImpl<>::initialize()\r\nW1110 08:51:57.562448   718 init.cc:234]     @     0x7f19359ab708 baidu::paddle_serving::sdk_cpp::Variant::initialize()\r\nW1110 08:51:57.563263   718 init.cc:234]     @     0x7f19359aa817 baidu::paddle_serving::sdk_cpp::Endpoint::initialize()\r\nW1110 08:51:57.564047   718 init.cc:234]     @     0x7f19359b14bd baidu::paddle_serving::sdk_cpp::PredictorApi::create()\r\nW1110 08:51:57.564837   718 init.cc:234]     @     0x7f1935977534 _ZZN8pybind1112cpp_function10initializeIZN5baidu14paddle_serving13general_modelL28pybind11_init_serving_clientERNS_6moduleEEUlRNS4_15PredictorClientERKSsE8_vIS8_SA_EINS_4nameENS_9is_methodENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNESS_.49864\r\nW1110 08:51:57.565630   718 init.cc:234]     @     0x7f1935957db2 pybind11::cpp_function::dispatcher()\r\nW1110 08:51:57.565790   718 init.cc:234]     @           0x5243cb _PyCFunction_FastCallKeywords\r\nW1110 08:51:57.565940   718 init.cc:234]     @           0x57e965 (unknown)\r\nW1110 08:51:57.566080   718 init.cc:234]     @           0x576f9f _PyEval_EvalFrameDefault\r\nW1110 08:51:57.566221   718 init.cc:234]     @           0x57609f (unknown)\r\nW1110 08:51:57.566361   718 init.cc:234]     @           0x57f96b (unknown)\r\nW1110 08:51:57.566501   718 init.cc:234]     @           0x57e8bc (unknown)\r\nW1110 08:51:57.566637   718 init.cc:234]     @           0x576f9f _PyEval_EvalFrameDefault\r\nW1110 08:51:57.566777   718 init.cc:234]     @           0x57f8ad (unknown)\r\nW1110 08:51:57.566916   718 init.cc:234]     @           0x57e8bc (unknown)\r\nW1110 08:51:57.567050   718 init.cc:234]     @           0x576f9f _PyEval_EvalFrameDefault\r\nW1110 08:51:57.567189   718 init.cc:234]     @           0x576736 (unknown)\r\n\r\npaddle serving最大连接数多少；如何关闭predict； 有没有Ubuntu系统的paddleserving镜像；’谢谢",
        "state": "closed",
        "user": "TIan-z",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-10T01:03:34+00:00",
        "updated_at": "2020-12-31T00:01:25+00:00",
        "closed_at": "2020-12-31T00:01:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TIan-z"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 876,
        "title": "怎么关闭paddle client的链接",
        "body": "from paddle_serving_client import Client\r\nclient = Client()\r\nclient.load_client_config(client_config)\r\nclient.connect([])\r\n链接成功使用完毕之后怎么把链接关闭？这里面并没有close()的方法",
        "state": "closed",
        "user": "631961895",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-10T03:17:28+00:00",
        "updated_at": "2020-12-31T00:04:45+00:00",
        "closed_at": "2020-12-31T00:04:45+00:00",
        "comments_count": [
            "bjjwwang",
            "TeslaZhao"
        ],
        "labels": [
            "enhancement",
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 875,
        "title": "Warning: PaddlePaddle catches a failure signal, it may not work properly",
        "body": "为使您的问题得到快速解决，在建立Issues前，请您先通过如下方式搜索是否有相似问题:【搜索issue关键字】【使用labels筛选】【官方文档】\r\n\r\n如果您没有查询到相似问题，为快速解决您的提问，建立issue时请提供如下细节信息：\r\n\r\n标题：简洁、精准概括您的问题\r\n版本、环境信息：\r\n   1）PaddlePaddle版本：\r\npaddle-serving-app 0.1.2\r\npaddle-serving-client 0.3.2\r\npaddle-serving-server-gpu 0.3.2.post10\r\npaddlepaddle 1.8.5\r\n   2）CPU/GPU：NVIDIA-SMI 430.40 Driver Version: 430.40 CUDA Version: 10.1 cudnn 7.6\r\n   3）系统环境：ubuntu 16.04\r\n   4）Python版本号 3.6.10\r\n   5）显存信息\r\n注：您可以通过执行summary_env.py获取以上信息。\r\n复现信息：多线程，多请求\r\n问题描述：\r\n*** Check failure stack trace: ***\r\nW1109 14:13:19.165050 3432 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW1109 14:13:19.165206 3432 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW1109 14:13:19.165244 3432 init.cc:231] The detail failure signal is:\r\n\r\nW1109 14:13:19.165268 3432 init.cc:234] *** Aborted at 1604902399 (unix time) try \"date -d @1604902399\" if you are using GNU date ***\r\nW1109 14:13:19.173751 3432 init.cc:234] PC: @ 0x0 (unknown)\r\nW1109 14:13:19.175218 3432 init.cc:234] *** SIGABRT (@0x3ee00000b64) received by PID 2916 (TID 0x7fdc957fd700) from PID 2916; stack trace: ***\r\nW1109 14:13:19.180501 3432 init.cc:234] @ 0x7fde4b978fd0 (unknown)\r\nW1109 14:13:19.184645 3432 init.cc:234] @ 0x7fde4b978f47 gsignal\r\nW1109 14:13:19.187784 3432 init.cc:234] @ 0x7fde4b97a8b1 abort\r\nW1109 14:13:19.188179 3432 init.cc:234] @ 0x7fdd78ec8bb9 google::logging_fail()\r\nW1109 14:13:19.188958 3432 init.cc:234] @ 0x7fdd78eca86d google::LogMessage::Fail()\r\nW1109 14:13:19.189558 3432 init.cc:234] @ 0x7fdd78ece31c google::LogMessage::SendToLog()\r\nW1109 14:13:19.190533 3432 init.cc:234] @ 0x7fdd78eca393 google::LogMessage::Flush()\r\nW1109 14:13:19.191032 3432 init.cc:234] @ 0x7fdd78ecf82e google::LogMessageFatal::~LogMessageFatal()\r\nW1109 14:13:19.191762 3432 init.cc:234] @ 0x7fdd78f926f7 baidu::paddle_serving::sdk_cpp::StubImpl<>::initialize()\r\nW1109 14:13:19.192704 3432 init.cc:234] @ 0x7fdd78ef2708 baidu::paddle_serving::sdk_cpp::Variant::initialize()\r\nW1109 14:13:19.193222 3432 init.cc:234] @ 0x7fdd78ef1817 baidu::paddle_serving::sdk_cpp::Endpoint::initialize()\r\nW1109 14:13:19.193624 3432 init.cc:234] @ 0x7fdd78ef84bd baidu::paddle_serving::sdk_cpp::PredictorApi::create()\r\nW1109 14:13:19.194011 3432 init.cc:234] @ 0x7fdd78ebe534 ZZN8pybind1112cpp_function10initializeIZN5baidu14paddle_serving13general_modelL28pybind11_init_serving_clientERNS_6moduleEEUlRNS4_15PredictorClientERKSsE8_vIS8_SA_EINS_4nameENS_9is_methodENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNESS.49864\r\nW1109 14:13:19.194411 3432 init.cc:234] @ 0x7fdd78e9edb2 pybind11::cpp_function::dispatcher()\r\nW1109 14:13:19.194532 3432 init.cc:234] @ 0x566ddc _PyCFunction_FastCallDict\r\nW1109 14:13:19.194645 3432 init.cc:234] @ 0x50a783 (unknown)\r\nW1109 14:13:19.194749 3432 init.cc:234] @ 0x50c1f4 _PyEval_EvalFrameDefault\r\nW1109 14:13:19.194857 3432 init.cc:234] @ 0x507f24 (unknown)\r\nW1109 14:13:19.194962 3432 init.cc:234] @ 0x509c50 (unknown)\r\nW1109 14:13:19.195066 3432 init.cc:234] @ 0x50a64d (unknown)\r\nW1109 14:13:19.195168 3432 init.cc:234] @ 0x50c1f4 _PyEval_EvalFrameDefault\r\nW1109 14:13:19.195271 3432 init.cc:234] @ 0x509918 (unknown)\r\nW1109 14:13:19.195377 3432 init.cc:234] @ 0x50a64d (unknown)\r\nW1109 14:13:19.195478 3432 init.cc:234] @ 0x50c1f4 _PyEval_EvalFrameDefault\r\nW1109 14:13:19.195580 3432 init.cc:234] @ 0x507f24 (unknown)\r\nW1109 14:13:19.195647 3432 init.cc:234] @ 0x509202 _PyFunction_FastCallDict\r\nW1109 14:13:19.195760 3432 init.cc:234] @ 0x594b01 (unknown)\r\nW1109 14:13:19.195868 3432 init.cc:234] @ 0x54a17f (unknown)\r\nW1109 14:13:19.195973 3432 init.cc:234] @ 0x5517c1 (unknown)\r\nW1109 14:13:19.196074 3432 init.cc:234] @ 0x5a9eec _PyObject_FastCallKeywords\r\nW1109 14:13:19.196180 3432 init.cc:234] @ 0x50a783 (unknown)\r\nW1109 14:13:19.196280 3432 init.cc:234] @ 0x50c1f4 _PyEval_EvalFrameDefault",
        "state": "closed",
        "user": "631961895",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-10T02:02:24+00:00",
        "updated_at": "2024-03-05T06:48:58+00:00",
        "closed_at": "2024-03-05T06:48:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 879,
        "title": "serving进行预测时，不将结果保存需要如何操作",
        "body": "**不要output输出内容\r\n\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport sys\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(640, 640), Transpose((2, 0, 1))\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\n\r\nclient.load_client_config(sys.argv[1])\r\nclient.connect(['127.0.0.1:9494'])\r\n\r\nim = preprocess(sys.argv[3])\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n        \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n        \"im_shape\": np.array(list(im.shape[1:]) + [1.0])\r\n    },\r\n    fetch=[\"multiclass_nms\"])\r\nfetch_map[\"image\"] = sys.argv[3]\r\npostprocess(fetch_map)",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-11T02:03:28+00:00",
        "updated_at": "2020-12-30T23:58:21+00:00",
        "closed_at": "2020-12-30T23:58:21+00:00",
        "comments_count": [
            "TeslaZhao",
            "liangruofei",
            "TeslaZhao",
            "liangruofei"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 878,
        "title": "paddleserving有做websocket这种服务吗",
        "body": "我现在有一个场景，就是通过接入rtsp视频流，启动一个websocket服务，然后paddle用来识别，将结果实时返回",
        "state": "closed",
        "user": "Sunyingbin",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-10T09:38:41+00:00",
        "updated_at": "2020-12-30T23:59:18+00:00",
        "closed_at": "2020-12-30T23:59:18+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 884,
        "title": "faster_rcnn_model训练自己的模型部署遇到的问题",
        "body": "各位大佬支援一把：\r\n\r\nhttps://github.com/PaddlePaddle/Serving/tree/develop/python/examples/faster_rcnn_model\r\n在本地环境ubuntu18.04，python3.6,paddle1.8.5,cuda10.0部署，没有问题，可以执行；\r\n\r\n但我重新训练成我自己的模型。训练环境是aistudio 的python3.7,paddle1.7.2,cuda9.0，得到的\r\ninfer_cfg.yml如下：\r\nuse_python_inference: false\r\nmode: fluid\r\ndraw_threshold: 0.5\r\nmetric: VOC\r\narch: RCNN\r\nmin_subgraph_size: 40\r\nwith_background: true\r\nPreprocess:\r\n- is_channel_first: false\r\n  is_scale: true\r\n  mean:\r\n  - 0.385\r\n  - 0.385\r\n  - 0.375\r\n  std:\r\n  - 0.162\r\n  - 0.159\r\n  - 0.159\r\n  type: Normalize\r\n- interp: 1\r\n  max_size: 640\r\n  target_size: 640\r\n  type: Resize\r\n  use_cv2: true\r\n- channel_first: true\r\n  to_bgr: false\r\n  type: Permute\r\n- stride: 32\r\n  type: PadStride\r\nlabel_list:\r\n- background\r\n- uav\r\n\r\n在serving端执行没问题。在client端预测时，报错如下：\r\nI1118 18:38:51.513751 12861 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9595\"): added 1\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 39, in <module>\r\n    fetch=[\"multiclass_nms\"])\r\n  File \"/home/uvs/.local/lib/python3.6/site-packages/paddle_serving_client/__init__.py\", line 274, in predict\r\n    \"Fetch names should not be empty or out of saved fetch list.\")\r\nValueError: Fetch names should not be empty or out of saved fetch list.\r\n，求指导\r\n",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-18T10:55:06+00:00",
        "updated_at": "2024-04-16T09:05:21+00:00",
        "closed_at": "2024-04-16T09:05:21+00:00",
        "comments_count": [],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 886,
        "title": "local predict在OCR上 CUDA ERROR（3）",
        "body": "![image](https://user-images.githubusercontent.com/23625746/99641973-ecfdb900-2a85-11eb-8efa-b5605750c2ef.png)\r\n\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "TeslaZhao",
        "created_at": "2020-11-19T08:40:50+00:00",
        "updated_at": "2020-12-30T23:56:02+00:00",
        "closed_at": "2020-12-30T23:56:02+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 885,
        "title": "如何在arm平台部署paddle_serving_client",
        "body": "我在jetson tx2上使用pip 安装 paddle_serving_client 后，初始化Client时显示\r\n![image](https://user-images.githubusercontent.com/35362026/99612809-45669380-2a51-11eb-9663-eace61d340ed.png)\r\n使用file serving_client.so发现该文件是x86下编译的，如果需要重新编译paddle_serving_client的话，我需要怎么做？望解答。",
        "state": "closed",
        "user": "TongHengcheng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-19T02:25:12+00:00",
        "updated_at": "2024-03-05T06:48:58+00:00",
        "closed_at": "2024-03-05T06:48:58+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TongHengcheng",
            "bjjwwang",
            "TongHengcheng",
            "Chambers1994",
            "60999"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 887,
        "title": "这里的or前后顺序写反了",
        "body": "https://github.com/PaddlePaddle/Serving/blob/0849586d38e56574d56b03aacb405a42d969568a/python/paddle_serving_app/reader/image_reader.py#L304\r\n\r\n应该先判断是否为None，然后再判断它的属性",
        "state": "closed",
        "user": "mikeshi80",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-20T06:48:54+00:00",
        "updated_at": "2024-03-05T06:48:59+00:00",
        "closed_at": "2024-03-05T06:48:59+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 891,
        "title": "yolov3检测模型，批量分析时中间报错",
        "body": "容器内起了yolov3的检测模型服务，在批量请求时固定卡在第500哥请求的时候报错（不是图的问题，检查过图没有问题）。\r\n如果一次请求少于500，则没有问题。\r\n----------------------------------------------------\r\n报错信息如下：\r\n![image](https://user-images.githubusercontent.com/20606275/100037547-52162d80-2e3d-11eb-97f7-90a6057d5b56.png)\r\n\r\n检查过容器是否有内存限制等，貌似没有问题\r\n![image](https://user-images.githubusercontent.com/20606275/100037272-bd133480-2e3c-11eb-8143-0b278878f53b.png)\r\n目前请求是这样写的\r\n![image](https://user-images.githubusercontent.com/20606275/100037965-192a8880-2e3e-11eb-9721-aa77c27de30c.png)\r\n\r\n请问如果想批量请求处理的话该怎么修改呢？\r\n500这个限制是固定限制，查找也没找到是哪里做了限制，这个之前有遇到过嘛？\r\n",
        "state": "closed",
        "user": "Jakel21",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-24T02:13:46+00:00",
        "updated_at": "2024-03-05T06:49:01+00:00",
        "closed_at": "2024-03-05T06:49:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 897,
        "title": "local predic能否自动装填lod信息",
        "body": "local predict使用list输入的时候，如果输入为lod_tensor，需要手动填入lod信息，这一步能否做成自动的。",
        "state": "closed",
        "user": "MRXLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-30T14:01:09+00:00",
        "updated_at": "2024-04-16T09:05:23+00:00",
        "closed_at": "2024-04-16T09:05:23+00:00",
        "comments_count": [],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 892,
        "title": "Local deployment debug, package incompatibility",
        "body": "platform : win10\r\nserver-version: 0.3.2\r\n\r\nWhen I run the local prediction(debug)，there seems a lib  incompatible with environment,\r\nThe error message is as follows：\r\n![image](https://user-images.githubusercontent.com/12640462/100174424-60c91700-2f07-11eb-9da7-896f2de7f58c.png)\r\n\r\nI found the lib fcntl was  not compatible with  windows system\r\n![image](https://user-images.githubusercontent.com/12640462/100174674-d3d28d80-2f07-11eb-8a96-c4aca505d6c4.png)\r\n",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2020-11-25T02:21:50+00:00",
        "updated_at": "2020-12-07T02:56:03+00:00",
        "closed_at": "2020-12-07T02:56:03+00:00",
        "comments_count": [
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 893,
        "title": "windows作为client端，ubuntu18.04作为服务端部署问题",
        "body": "各位大神求指教：\r\n\r\n环境：\r\nubuntu18.04：python3.6.9，paddle1.8.5，cuda10.0，cudnn7.6，服务端安装paddle_serving_server_gpu-0.3.2.post10-py3-none-any.whl\r\n\r\nwindow10：python3.6.9，paddle1.8.5，cuda10.0，客户端安装paddle_serving_client-0.3.2-cp36-none-any.whl和paddle_serving_app-0.1.2-py3-none-any.whl\r\n\r\n跑测试样例https://github.com/PaddlePaddle/Serving/blob/release/0.3/python/examples/faster_rcnn_model/README_CN.md，\r\n在ubuntu18.04 运行服务端和客户端可以有效跑结果；\r\n\r\n##问题\r\n\r\n当用grpc，修改程序，不论是将服务端和客户端都在ubuntu运行还是跨平台的windows访问ubuntu都报同样错误：\r\n修改如下：\r\n服务端运行：python3 -m paddle_serving_server_gpu.serve --model pddet_serving_model --thread 10 --port 9494 --use_multilang\r\n客户端运行程序：\r\nfrom paddle_serving_client import MultiLangClient as Client\r\nfrom paddle_serving_app.reader import *\r\nimport sys\r\nimport numpy as np\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Div(255.0),\r\n    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n    Resize(640, 640), Transpose((2, 0, 1))\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\n\r\nclient.connect(['127.0.0.1:9494'])\r\n\r\nim = preprocess('000000570688.jpg')\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n        \"im_info\": np.array(list(im.shape[1:]) + [1.0]),\r\n        \"im_shape\": np.array(list(im.shape[1:]) + [1.0])\r\n    },\r\n    fetch=[\"multiclass_nms\"])\r\nfetch_map[\"image\"] = '000000570688.jpg'\r\nprint(fetch_map)\r\npostprocess(fetch_map)\r\n\r\n## 报错如下\r\n{'serving_status_code': <StatusCode.DEADLINE_EXCEEDED: (4, 'deadline exceeded')>, 'image': '000000570688.jpg'}\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 44, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/home/uvs/.local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/home/uvs/.local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'serving_status_code.lod'\r\n\r\n分析主要错误应该在{'serving_status_code': <StatusCode.DEADLINE_EXCEEDED: (4, 'deadline exceeded')>, 'image': '000000570688.jpg'}此处。\r\n还麻烦大牛解惑（windows端访问也是这个错误，分析应该是通信通了，但在client.predict处理完错误了）\r\n",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "barrierye",
        "created_at": "2020-11-26T07:20:28+00:00",
        "updated_at": "2020-12-01T10:38:20+00:00",
        "closed_at": "2020-12-01T10:38:20+00:00",
        "comments_count": [
            "barrierye"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 895,
        "title": "serving cuda10.1-trt6容器不支持 paddlepaddle-gpu 2.0rc",
        "body": "![image](https://user-images.githubusercontent.com/23625746/100431464-2a3ff780-30d3-11eb-93e4-912af56bd3a1.png)\r\n",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-27T09:08:49+00:00",
        "updated_at": "2024-04-16T09:05:22+00:00",
        "closed_at": "2024-04-16T09:05:22+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 913,
        "title": "保存ERNIE可服务模型，client调用问题",
        "body": "client端代码\r\n```\r\n    from paddle_serving_client import Client\r\n\r\n    client = Client()\r\n    client.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\n    client.connect([\"127.0.0.1:9292\"])\r\n    data_generator = data_preprocess()\r\n    for sample in data_generator():\r\n        src_ids = sample[0]\r\n        sent_ids = sample[1]\r\n        pos_ids = sample[2]\r\n        input_mask = sample[4]\r\n\r\n        data = {\"eval_placeholder_0\": src_ids,\r\n                \"eval_placeholder_1\": sent_ids,\r\n                \"eval_placeholder_2\": pos_ids,\r\n                \"eval_placeholder_3\": input_mask}\r\n\r\n        fetch_map = client.predict(feed=data, fetch=[\"save_infer_model/scale_0.tmp_0\"])\r\n\r\n        print(fetch_map)\r\n```\r\n其中数据预处理data_preprocess是从源码中抽离出来的\r\n```\r\ndef data_preprocess():\r\n    from ernie.reader.task_reader import ClassifyReader\r\n    reader = ClassifyReader(\r\n        vocab_path=\"{}/vocab.txt\".format(ERNIE_PATH),  # ernie配置文件-vocab\r\n        label_map_config=None,\r\n        max_seq_len=128,\r\n        do_lower_case=True,\r\n        in_tokens=False,\r\n        is_inference=True)\r\n\r\n    predict_data_generator = reader.data_generator(\r\n        input_file=\"task_data/test.txt\",\r\n        batch_size=1,\r\n        epoch=1,\r\n        shuffle=False)\r\n\r\n    return predict_data_generator\r\n```\r\n测试数据text.txt为\r\n```\r\ntext_a\ttext_b\tlabel\r\n你好\t你好\t1\r\n```\r\n客户端配置如下\r\n```\r\nfeed_var {\r\n  name: \"eval_placeholder_0\"\r\n  alias_name: \"eval_placeholder_0\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 128\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"eval_placeholder_1\"\r\n  alias_name: \"eval_placeholder_1\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 128\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"eval_placeholder_2\"\r\n  alias_name: \"eval_placeholder_2\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 128\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"eval_placeholder_3\"\r\n  alias_name: \"eval_placeholder_3\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 128\r\n  shape: 1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_0\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 2\r\n}\r\n```\r\n成功启动server端，然后client端执行报错\r\n```\r\nW1209 02:41:42.637696 20343 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=11 SocketId=1@127.0.0.1:9292@34132 [R1][E111]Fail to connect SocketId=8589934594@127.0.0.1:9292: Connection refused [R2][E112]Fail to select server from list://127.0.0.1:9292 lb=la\r\nE1209 02:41:42.637908 20343 general_model.cpp:561] failed call predictor with req: insts { tensor_array { int64_data: 1 int64_data: 6656 int64_data: 17963 int64_data: 1545 int64_data: 2 int64_data: 6656 int64_data: 17963 int64_data: 1801 int64_data: 2 elem_type: 0 shape: 1 shape: 9 shape: 1 } tensor_array { int64_data: 0 int64_data: 0 int64_data: 0 int64_data: 0 int64_data: 0 int64_data: 1 int64_data: 1 int64_data: 1 int64_data: 1 elem_type: 0 shape: 1 shape: 9 shape: 1 } tensor_array { int64_data: 0 int64_data: 1 int64_data: 2 int64_data: 3 int64_data: 4 int64_data: 5 int64_data: 6 int64_data: 7 int64_data: 8 elem_type: 0 shape: 1 shape: 9 shape: 1 } tensor_array { float_data: 1 float_data: 1 float_data: 1 float_data: 1 float_data: 1 float_data: 1 float_data: 1 float_data: 1 float_data: 1 elem_type: 1 shape: 1 shape: 9 shape: 1 } } fetch_var_names: \"save_infer_model/scale_0.tmp_0\"\r\n\r\n```\r\nserver端报错信息为\r\n```\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: The first dimension value of Input(Scale) must equal to be thesecond dimension value of the flattened 2D matrix of Input(X),But received the first dimension value of Input(Scale) is[768], the second dimension value of the flattened 2D matrix of Input(Scale) is [6912].\r\n  [Hint: Expected ctx->GetInputDim(\"Scale\")[0] == right, but received ctx->GetInputDim(\"Scale\")[0]:768 != right:6912.] at (/paddle/paddle/fluid/operators/layer_norm_op.cc:66)\r\n  [operator < layer_norm > error]\r\nAborted (core dumped)\r\n```\r\n请问是什么原因呢\r\n\r\n",
        "state": "closed",
        "user": "lerry-lee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-09T02:55:00+00:00",
        "updated_at": "2024-03-05T06:49:01+00:00",
        "closed_at": "2024-03-05T06:49:01+00:00",
        "comments_count": [
            "TeslaZhao",
            "lerry-lee",
            "TeslaZhao",
            "lerry-lee",
            "Karenlyw",
            "pal-duan",
            "TeslaZhao",
            "lerry-lee",
            "TeslaZhao",
            "TeslaZhao",
            "lerry-lee",
            "lerry-lee",
            "lerry-lee",
            "lerry-lee",
            "TeslaZhao",
            "pal-duan",
            "yangxuan14nlp"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 906,
        "title": "serving-cpu-noavx-openblas-0.3.2/serving: cannot execute binary file",
        "body": "启动服务一直报这个错\r\n![image](https://user-images.githubusercontent.com/26930834/101286935-676d5d80-3828-11eb-9970-fab484c64328.png)\r\n\r\n端口有启动成功，执行\r\n`curl -H \"Content-Type:application/json\" -X POST -d '{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332], \"fetch\":[\"price\"]}' http://127.0.0.1:9292/uci/prediction\r\n`\r\n报500 错误\r\n\r\n![image](https://user-images.githubusercontent.com/26930834/101287001-b9ae7e80-3828-11eb-8193-1c162241d6e3.png)\r\n\r\n",
        "state": "closed",
        "user": "sharlyndb",
        "closed_by": "TeslaZhao",
        "created_at": "2020-12-06T17:08:47+00:00",
        "updated_at": "2020-12-30T23:54:11+00:00",
        "closed_at": "2020-12-30T23:54:11+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "sharlyndb"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 929,
        "title": "paddle-serving-client==0.4.0没有此版本",
        "body": "pip install paddle-serving-client==0.4.0\r\n输出：\r\nERROR: Could not find a version that satisfies the requirement paddle-serving-client==0.4.0\r\nERROR: No matching distribution found for paddle-serving-client==0.4.0\r\n",
        "state": "closed",
        "user": "elanyang",
        "closed_by": "elanyang",
        "created_at": "2020-12-17T03:40:39+00:00",
        "updated_at": "2020-12-22T04:58:39+00:00",
        "closed_at": "2020-12-22T04:58:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 933,
        "title": "how to set batch_size correctly in client",
        "body": "**检测服务的示例中没有给出设置batch_size的方法**，在Fater_RCNN中benckmark.py里：\r\n![image](https://user-images.githubusercontent.com/20606275/102734967-30e12800-437c-11eb-955d-d5101f8f4823.png)\r\n但是benchmark中所有的图都是一张..........\r\n而如果按照batch_feed的方法输送给client.predict()，返回的结果中不区分图片.....所以的结果都是存在一起的：\r\n![image](https://user-images.githubusercontent.com/20606275/102735191-b82e9b80-437c-11eb-9c29-32baea8bee16.png)\r\n**那么我要是输送多张图，结果该怎么对应呢？postprocess怎么做呢？**\r\n其他疑问，希望帮忙解答下哈：\r\n1. RCNNpostprocess中做可视化的部分：\r\n![image](https://user-images.githubusercontent.com/20606275/102735272-ef04b180-437c-11eb-833e-ac8ab6a75800.png)\r\nbenchmark.py中直接给的时图片，不太对应吧....\r\n2.benchmark中batchsize增大qps降低？？\r\n![image](https://user-images.githubusercontent.com/20606275/102735473-866a0480-437d-11eb-9d37-ad1367368e86.png)\r\n\r\n",
        "state": "closed",
        "user": "Jakel21",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-21T03:13:51+00:00",
        "updated_at": "2024-04-16T09:05:24+00:00",
        "closed_at": "2024-04-16T09:05:24+00:00",
        "comments_count": [],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 932,
        "title": "多语言client在batch预测时报错",
        "body": "错误原因：brpc版在batch预测时的接口发生变化，client.predict需要添加`batch=True`参数，而grpc相关接口没有相应做修改（https://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_server/__init__.py#L612 ）\r\n",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-18T13:52:37+00:00",
        "updated_at": "2024-03-05T06:49:02+00:00",
        "closed_at": "2024-03-05T06:49:02+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 939,
        "title": "安装使用psddle",
        "body": "",
        "state": "closed",
        "user": "yuope",
        "closed_by": "TeslaZhao",
        "created_at": "2020-12-23T07:34:55+00:00",
        "updated_at": "2020-12-23T07:58:42+00:00",
        "closed_at": "2020-12-23T07:58:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 940,
        "title": "提示安装成功后使用paddle出错",
        "body": "Python 3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import paddle.fluid as fluid\r\n ** On entry to DGEBAL parameter number  3 had an illegal value\r\n ** On entry to DGEHRD  parameter number  2 had an illegal value\r\n ** On entry to DORGHR DORGQR parameter number  2 had an illegal value\r\n ** On entry to DHSEQR parameter number  4 had an illegal value\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Program\\tools\\python3.8\\lib\\site-packages\\paddle\\__init__.py\", line 18, in <module>\r\n    check_import_scipy(os.name)\r\n  File \"D:\\Program\\tools\\python3.8\\lib\\site-packages\\paddle\\check_import_scipy.py\", line 20, in check_import_scipy\r\n    import scipy.io as scio\r\n  File \"D:\\Program\\tools\\python3.8\\lib\\site-packages\\scipy\\__init__.py\", line 61, in <module>\r\n    from numpy import show_config as show_numpy_config\r\n  File \"D:\\Program\\tools\\python3.8\\lib\\site-packages\\numpy\\__init__.py\", line 305, in <module>\r\n    _win_os_check()\r\n  File \"D:\\Program\\tools\\python3.8\\lib\\site-packages\\numpy\\__init__.py\", line 302, in _win_os_check\r\n    raise RuntimeError(msg.format(__file__)) from None\r\nRuntimeError: The current Numpy installation ('D:\\\\Program\\\\tools\\\\python3.8\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86",
        "state": "closed",
        "user": "yuope",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-23T07:36:51+00:00",
        "updated_at": "2024-04-16T09:05:25+00:00",
        "closed_at": "2024-04-16T09:05:25+00:00",
        "comments_count": [
            "TeslaZhao",
            "yuope"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 943,
        "title": "senta按照文档无法正常运行",
        "body": "python3 -m paddle_serving_server.serve --model lac_model --port 9300\r\npython3 senta_web_service.py",
        "state": "closed",
        "user": "459936800",
        "closed_by": "459936800",
        "created_at": "2020-12-24T07:57:14+00:00",
        "updated_at": "2020-12-25T01:33:55+00:00",
        "closed_at": "2020-12-25T01:33:55+00:00",
        "comments_count": [
            "github-actions[bot]",
            "459936800",
            "459936800"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 948,
        "title": "尝试使用Serving部署手写数字识别模型，预测时出现unknown type",
        "body": "系统环境：deppin v20.1\r\nServing环境：docker搭建\r\n```\r\nhub.baidubce.com/paddlepaddle/serving   latest          825a7fd941ac   3 weeks ago    450MB\r\n```\r\npaddle-serving版本：\r\n```\r\npaddle-serving-client (0.4.0)\r\npaddle-serving-server (0.4.0)\r\n```\r\npaddle 版本：paddle 2.0rc\r\n模型定义：\r\n```python\r\nimport paddle\r\nfrom paddle import fluid\r\nfrom paddle.fluid.dygraph import Linear,Conv2D,Pool2D\r\n\r\nclass MNIST(fluid.dygraph.Layer):\r\n    def __init__(self):\r\n        super(MNIST,self).__init__()\r\n        # 步长是通过计算而来，O=（I-K+2P）/S+1  O为输出大小，I输入大小，K为核大型，P为padding，S为步长\r\n        # 定义卷积层，输出通道数num_filters为5,卷积核大小filter_size为5,步长为1,padding=2,激活函数为relu\r\n        # 输入尺寸为1*28*28，输出尺寸为28*28，个数为20\r\n        self.conv1 = Conv2D(num_channels=1,num_filters=20,filter_size=5,stride=1,padding=2,act='relu')\r\n        # 定义池化层，池化核pool_size=2，池化步长为2，选择最大池化方式\r\n        # 输入尺寸为20*28*28，输出尺寸为20*14*14\r\n        self.pool1 = Pool2D(pool_size=2,pool_stride=2,pool_type='max')\r\n        # 定义卷积层，输出通道数num_filters为5,卷积核大小filter_size为5,步长为1,padding=2,激活函数为relu\r\n        # 输入尺寸为20*14*14，输出尺寸为20*14*14\r\n        self.conv2 = Conv2D(num_channels=20,num_filters=20,filter_size=5,stride=1,padding=2,act='relu')\r\n        # 定义池化层，池化核pool_size=2，池化步长为2，选择最大池化方式\r\n        # 输入尺寸为20*14*14，输出尺寸为20*7*7\r\n        self.pool2 = Pool2D(pool_size=2,pool_stride=2,pool_type='max')\r\n        \r\n        # 修改：全链接层，输出维度为10\r\n        # 输入维度为20*7*7\r\n        self.fc1=Linear(input_dim=980,output_dim=10,act='softmax')\r\n    \r\n    @paddle.jit.to_static\r\n    def forward(self,inputs,labels=None):\r\n        x = self.conv1(inputs)\r\n        x = self.pool1(x)\r\n        x = self.conv2(x)\r\n        x = self.pool2(x)\r\n        # 展平\r\n        x = fluid.layers.reshape(x,[x.shape[0],-1])\r\n        x = self.fc1(x)\r\n        if labels is not None:\r\n            acc = fluid.layers.accuracy(input=x,label=labels)\r\n            return x,acc\r\n        return x\r\n```\r\n保存静态模型文件\r\n```python\r\npaddle.static.save_inference_model()\r\n```\r\n通过paddle_serving保存模型\r\n```python\r\nimport paddle\r\nimport paddle_serving_client.io as serving_io\r\npaddle.enable_static()\r\n\r\nserving_io.inference_model_to_serving(\r\n    './models', # 需要转换的模型文件存储路径，结构文件和参数文件都存在\r\n    serving_server=\"./serving_mnist_model\", # 转换后的模型文件和配置文件的存储路径\r\n    serving_client=\"client_conf\",  # 转换后的客户端配置文件存储路径\r\n    model_filename='mnist_model.pdmodel', # 存储需要转换的模型Inference Program结构的文件名称,默认为__model__\r\n    params_filename='mnist_model.pdiparams' # 存储需要转换的模型所有参数的文件名称,默认为__params__\r\n)\r\n```\r\n启动服务\r\n```\r\n# rpc启动服务\r\npython3 -m paddle_serving_server.serve --model /home/serving_mnist_model --thread 10 --port 9292\r\n```\r\n客户端代码\r\n```\r\n# rpc\r\n# 客户端\r\nfrom paddle_serving_client import Client\r\nclient= Client()\r\n# 加载配置文件\r\nclient.load_client_config('/home/client_conf/serving_client_conf.prototxt')\r\n# 链接服务器\r\nclient.connect([\"127.0.0.1:9292\"])\r\n\r\n# 验证\r\n# 测试\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nimport numpy as np\r\n\r\n# 读取一张本地的样例图片，转变成模型输入的格式\r\ndef load_image(img_path):\r\n    # 从img_path中读取图像，并转为灰度图\r\n    im = Image.open(img_path).convert('L')\r\n    plt.imshow(im)\r\n    im = im.resize((28, 28), Image.ANTIALIAS)\r\n   # 这里如果改为(1,1,28,28)，则会出现float_slot_batch为(1,1,1,28,28)\r\n    im = np.array(im).reshape(1, 28, 28).astype(np.float32)\r\n    # 图像归一化\r\n    im = 1.0 - im / 255.\r\n    im = im.astype(np.float32)\r\n    return im\r\n\r\nstart = time.time()\r\n# 定义预测过程\r\nimg_path = '/home/example_6.jpg'\r\ntensor_img = load_image(img_path)\r\n# 名称可以修改serving_server_conf.prototxt中的alias_name\r\nresults = client.predict(\r\n    feed={\"generated_tensor_0\": tensor_img}, \r\n    fetch=[\"save_infer_model/scale_0.tmp_0\",\"save_infer_model/scale_1.tmp_0\"]\r\n)\r\n#取概率最大的标签作为预测输出\r\n# lab = np.argsort(results.numpy())\r\n# print(\"本次预测的数字是: \", lab[0][-1])\r\n\r\nprint(f\"耗时:{time.time()-start}\")\r\n```\r\n服务器端报错：\r\n```\r\nE1231 05:12:56.182905  1325 analysis_predictor.cc:402] unknown type, only support float32, int64 and int32 now.\r\n```\r\n\r\n客户端报错：\r\n```\r\nW1231 05:12:56.273334  1269 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=25 SocketId=3@127.0.0.1:9292@36402 [R1][E111]Fail to connect SocketId=8589934596@127.0.0.1:9292: Connection refused [R2][E112]Fail to select server from list://127.0.0.1:9292 lb=la\r\nE1231 05:12:56.273932  1269 general_model.cpp:361] failed call predictor with req: insts { tensor_array { float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0.49803919 float_data: 0.86666667 float_data: 0.20392156 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.25490195 float_data: 0.89411765 float_data: 0.85490197 float_data: 0.39607841 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0.054901958 float_data: 0.90980393 float_data: 0.54901958 float_data: 0.015686274 float_data: 0.02352941 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.46274507 float_data: 0.88235295 float_data: 0.086274505 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0.90980393 float_data: 0.54117644 float_data: 0.011764705 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0.039215684 float_data: 0.96470588 float_data: 0.34509802 float_data: 0.0039215684 float_data: 0.0039215684 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.32156861 float_data: 0.95686275 float_data: 0.078431368 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.72941172 float_data: 0.92156863 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.015686274 float_data: 0.81960785 float_data: 0.63921571 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0.0039215684 float_data: 0.011764705 float_data: 0 float_data: 0.25098038 float_data: 0.5960784 float_data: 0.58431375 float_data: 0.54117644 float_data: 0.28235292 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.090196073 float_data: 1 float_data: 0.36078429 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0.015686274 float_data: 0.56470585 float_data: 0.99215686 float_data: 0.86666667 float_data: 0.827451 float_data: 0.69411767 float_data: 0.94117647 float_data: 0.28235292 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.10588235 float_data: 0.98431373 float_data: 0.26274508 float_data: 0 float_data: 0 float_data: 0 float_data: 0.015686274 float_data: 0.63137257 float_data: 0.99215686 float_data: 0.40784311 float_data: 0.02352941 float_data: 0.011764705 float_data: 0.0039215684 float_data: 0.31372547 float_data: 0.86274511 float_data: 0.062745094 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.2235294 float_data: 1 float_data: 0.058823526 float_data: 0 float_data: 0.015686274 float_data: 0 float_data: 0.59215689 float_data: 0.89019608 float_data: 0.25882351 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0.039215684 float_data: 0.89803922 float_data: 0.09803921 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.30196077 float_data: 0.99215686 float_data: 0.047058821 float_data: 0.0078431368 float_data: 0 float_data: 0.10980392 float_data: 0.89019608 float_data: 0.26274508 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0.78431374 float_data: 0.33725488 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.28627449 float_data: 0.99215686 float_data: 0.18039215 float_data: 0 float_data: 0.0039215684 float_data: 0.44313723 float_data: 0.98431373 float_data: 0.027450979 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0.0039215684 float_data: 0.76078433 float_data: 0.39607841 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.058823526 float_data: 0.89411765 float_data: 0.56470585 float_data: 0.015686274 float_data: 0 float_data: 0.23921567 float_data: 1 float_data: 0.27058822 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0.0039215684 float_data: 0 float_data: 0.02352941 float_data: 0.82352942 float_data: 0.32549018 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.8 float_data: 0.90980393 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0.1607843 float_data: 0.99215686 float_data: 0.29019606 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0.4470588 float_data: 0.89803922 float_data: 0.062745094 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.34901959 float_data: 0.972549 float_data: 0.56862748 float_data: 0 float_data: 0.0078431368 float_data: 0.60392153 float_data: 0.89019608 float_data: 0.50196075 float_data: 0.49803919 float_data: 0.47058821 float_data: 0.25490195 float_data: 0.36078429 float_data: 0.97647059 float_data: 0.25490195 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0.66666663 float_data: 0.98431373 float_data: 0.52549016 float_data: 0.094117641 float_data: 0.019607842 float_data: 0.31372547 float_data: 0.39607841 float_data: 0.38823527 float_data: 0.43137252 float_data: 0.55686271 float_data: 0.74901962 float_data: 0.19999999 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.011764705 float_data: 0.5333333 float_data: 0.99215686 float_data: 0.86666667 float_data: 0.55686271 float_data: 0.45490193 float_data: 0.46274507 float_data: 0.74901962 float_data: 0.99215686 float_data: 0.72549021 float_data: 0.18431371 float_data: 0.0078431368 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.015686274 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0.20392156 float_data: 0.56470585 float_data: 0.8 float_data: 0.95686275 float_data: 0.69019604 float_data: 0.50588238 float_data: 0.26666665 float_data: 0.011764705 float_data: 0.0039215684 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0.0078431368 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0 float_data: 0 float_data: 0.0078431368 float_data: 0 float_data: 0.0078431368 float_data: 0.0039215684 float_data: 0 float_data: 0 float_data: 0.0039215684 float_data: 0 float_data: 0.011764705 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 float_data: 0 elem_type: 1 shape: 1 shape: 1 shape: 28 shape: 28 } } fetch_var_names: \"save_infer_model/scale_0.tmp_0\" fetch_var_names: \"save_infer_model/scale_1.tmp_0\" log_id: 0\r\n```",
        "state": "closed",
        "user": "RunningMartin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-31T05:24:20+00:00",
        "updated_at": "2024-03-05T06:49:03+00:00",
        "closed_at": "2024-03-05T06:49:03+00:00",
        "comments_count": [
            "github-actions[bot]",
            "RunningMartin",
            "yeyupiaoling",
            "TeslaZhao",
            "hymanzhu1983"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 950,
        "title": "COMPILE DOC little error",
        "body": "![image](https://user-images.githubusercontent.com/23625746/103538331-f8d7fa00-4ed0-11eb-9dd2-2f9fe981a781.png)\r\n缺失TENSOR RT环境变量的设置",
        "state": "closed",
        "user": "bjjwwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-04T13:08:44+00:00",
        "updated_at": "2024-04-16T09:05:26+00:00",
        "closed_at": "2024-04-16T09:05:26+00:00",
        "comments_count": [],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 949,
        "title": "请求服务出现{\"result\":\"not enough values to unpack (expected 3, got 2)\"}",
        "body": "使用paddleserving的图像分类历程，使用curl H请求远程服务curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"image\": \"https://paddle-serving.bj.bcebos.com/imagenet-example/daisy.jpg\"}], \"fetch\": [\"score\"]}' http://127.0.0.1:9292/image/prediction，完全例子的写法，出现以上{\"result\":\"not enough values to unpack (expected 3, got 2)\"}返回问题。\r\npaddleserving 0.4.0 cuda10 cudnn7\r\n",
        "state": "closed",
        "user": "Kokoing123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-31T07:55:48+00:00",
        "updated_at": "2024-03-05T06:49:04+00:00",
        "closed_at": "2024-03-05T06:49:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 951,
        "title": "这里提示这些API将要抛弃，那我用什么办法提供HTTP请求呢？",
        "body": "这里提示这些API将要抛弃，那我用什么办法提供HTTP请求呢？\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/7ea59bda3f25bccacfbff0cccac79390f0a86017/python/paddle_serving_server_gpu/web_service.py#L262-L271",
        "state": "closed",
        "user": "yeyupiaoling",
        "closed_by": "yeyupiaoling",
        "created_at": "2021-01-05T03:05:36+00:00",
        "updated_at": "2021-01-05T09:01:07+00:00",
        "closed_at": "2021-01-05T09:01:07+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 952,
        "title": "运行fit_a_line例子报错",
        "body": "我使用的是这个了例子，只是将paddle_serving_server 改为 paddle_serving_server_gpu\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/python/examples/fit_a_line/test_server.py\r\n\r\n请求命令\r\n```\r\ncurl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://192.168.88.60:9292/uci/prediction\r\n```\r\n\r\n服务器日志：\r\n```\r\n[root@8a699d9457d2 serving]# python3 test.py \r\nThis API will be deprecated later. Please do not use it\r\nThis API will be deprecated later. Please do not use it\r\nmkdir: cannot create directory ‘workdir’: File exists\r\nThis API will be deprecated later. Please do not use it\r\nweb service address:\r\nhttp://172.17.0.2:9292/uci/prediction\r\nThis API will be deprecated later. Please do not use it\r\n * Serving Flask app \"paddle_serving_server_gpu.web_service\" (lazy loading)\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: off\r\nGoing to Run Comand\r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/serving-gpu-trt-0.4.0/serving -enable_model_toolkit -inferservice_path workdir -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 12000 -reload_interval_s 10 -resource_path workdir -resource_file resource.prototxt -workflow_path workdir -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 67108864 \r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/serving-gpu-trt-0.4.0/serving: error while loading shared libraries: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0105 03:52:53.427336  3670 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:12000\"): added 1\r\n(1, 1, 13)\r\nW0105 03:52:53.429221  3670 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E111]Fail to connect SocketId=1@127.0.0.1:12000: Connection refused [R1][E112]Fail to select server from list://127.0.0.1:12000 lb=la [R2][E112]Fail to select server from list://127.0.0.1:12000 lb=la\r\nE0105 03:52:53.429270  3670 general_model.cpp:361] failed call predictor with req: insts { tensor_array { float_data: 0.0137 float_data: -0.1136 float_data: 0.2553 float_data: -0.0692 float_data: 0.0582 float_data: -0.0727 float_data: -0.1583 float_data: -0.0584 float_data: 0.6283 float_data: 0.4919 float_data: 0.1856 float_data: 0.0795 float_data: -0.0332 elem_type: 1 shape: 1 shape: 1 shape: 13 } } fetch_var_names: \"price\" log_id: 0\r\nThis API will be deprecated later. Please do not use it\r\n[2021-01-05 03:52:53,429] ERROR in app: Exception on /uci/prediction [POST]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/app.py\", line 1952, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/app.py\", line 1821, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/app.py\", line 1950, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/usr/local/lib64/python3.6/site-packages/flask/app.py\", line 1936, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/web_service.py\", line 223, in run\r\n    return self.get_prediction(request)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/web_service.py\", line 193, in get_prediction\r\n    feed=request.json[\"feed\"], fetch=fetch, fetch_map=fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server_gpu/web_service.py\", line 269, in postprocess\r\n    for key in fetch_map:\r\nTypeError: 'NoneType' object is not iterable\r\nI0105 03:52:53.529567  3673 socket.cpp:2370] Checking SocketId=0@127.0.0.1:12000\r\n```",
        "state": "closed",
        "user": "yeyupiaoling",
        "closed_by": "yeyupiaoling",
        "created_at": "2021-01-05T03:55:11+00:00",
        "updated_at": "2021-01-05T06:35:10+00:00",
        "closed_at": "2021-01-05T06:35:10+00:00",
        "comments_count": [
            "yeyupiaoling"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 954,
        "title": "模型预测结果不正确，相差很远",
        "body": "MobileNetV1 和 MobileNetV2 我都试过，模型是通过下面的命令转换得到的\r\n```\r\npython3 -m paddle_serving_client.convert --dirname=./models --model_filename=__model__ --params_filename=__params__\r\n```\r\n\r\n我的测试代码如下，\r\n```python\r\nimport cv2\r\nimport numpy as np\r\nfrom paddle_serving_server_gpu.web_service import WebService\r\n\r\n\r\nclass ClassificationService(WebService):\r\n    def init_classification_service(self):\r\n        with open(\"labels.txt\", 'r', encoding='utf-8') as f:\r\n            self.label_dict = eval(f.read())\r\n            self.label_dict = {v: k for k, v in self.label_dict.items()}\r\n\r\n    def preprocess(self, feed=None, fetch=None):\r\n        img = cv2.imread('images/test.jpg', cv2.IMREAD_COLOR)\r\n        # 转成RGB\r\n        img = img[:, :, ::-1]\r\n        # 缩放大小\r\n        img = cv2.resize(img, (224, 224))\r\n        # 归一化\r\n        img = (img / 255.0 - np.array([0.406, 0.456, 0.485])) / np.array([0.225, 0.224, 0.229])\r\n        # WHC转CWH\r\n        img = img.transpose(2, 0, 1).astype(np.float32)\r\n\r\n        # 打包成输入数据\r\n        feed_batch = {\"feed_image\": img}\r\n        # 指定模型输出名称\r\n        fetch = [\"save_infer_model/scale_0.tmp_0\"]\r\n        return feed_batch, fetch, False\r\n\r\n    def postprocess(self, feed=None, fetch=None, fetch_map=None):\r\n        print(fetch_map)\r\n        # 解析输出结果\r\n        output_data = fetch_map[fetch[0]]\r\n\r\n        # 获取概率最大的label\r\n        output_data = np.squeeze(output_data)\r\n        lab = np.argsort(output_data)[-1]\r\n        # 返回数据\r\n        result = {\"label\": self.label_dict[lab], \"probability\": float(output_data[lab])}\r\n        return result\r\n\r\n\r\nclassification_service = ClassificationService(name=\"Classification\")\r\nclassification_service.load_model_config(\"serving_server\")\r\nclassification_service.init_classification_service()\r\n# 指定使用GPU的ID\r\nclassification_service.set_gpus('0')\r\nclassification_service.prepare_server(workdir=\"workdir\", port=9292)\r\nclassification_service.run_rpc_service()\r\nclassification_service.run_web_service()\r\n```\r\n\r\n输出的结果与没有转换，使用AnalysisConfig相差很大。",
        "state": "closed",
        "user": "yeyupiaoling",
        "closed_by": "yeyupiaoling",
        "created_at": "2021-01-05T07:38:10+00:00",
        "updated_at": "2021-01-06T08:29:51+00:00",
        "closed_at": "2021-01-06T08:29:50+00:00",
        "comments_count": [
            "TeslaZhao",
            "yeyupiaoling"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 957,
        "title": "paddlex训练的model需要怎样转换用于paddleServing？",
        "body": "paddlex训练的model需要怎样用于paddleServing？",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-05T07:54:32+00:00",
        "updated_at": "2024-04-16T09:05:27+00:00",
        "closed_at": "2024-04-16T09:05:27+00:00",
        "comments_count": [
            "TeslaZhao",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 960,
        "title": "win10的docker中 cpu模式运行正常，gpu提示下面描述，",
        "body": "服务器环境：配了一个cuda10.2.141的gpu\r\nhttp://172.17.0.2:9292/image/prediction\r\nThis API will be deprecated later. Please do not use it\r\n * Serving Flask app \"paddle_serving_server_gpu.web_service\" (lazy loading)\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: off\r\nGPU not found, please check your environment or use cpu version by \"pip install paddle_serving_server\"",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "hysunflower",
        "created_at": "2021-01-08T07:32:54+00:00",
        "updated_at": "2021-02-22T11:04:30+00:00",
        "closed_at": "2021-02-22T11:04:30+00:00",
        "comments_count": [
            "TeslaZhao",
            "meishitouzhele",
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 961,
        "title": "在云服务器发布serving格式模型后，调用服务发生报错。",
        "body": "问题描述：在云服务器（CentOS 7.5 64位）上使用RPC模式发布了一个自己的模型（用paddlex训练的分类模型，发布的模型已经转换为了serving格式的）。随后在客户端调用的时候，发生报错ModuleNotFoundError: No module named 'paddle_serving_client.serving_client'。\r\n\r\n云服务器命令：python -m paddle_serving_server.serve --model serving_server --port 9696\r\n客户端命令：python rpc_client.py ./serving_client/serving_client_conf.prototxt\r\n![4148E263-4FF2-452a-92DD-412BBA5D665D](https://user-images.githubusercontent.com/62195135/103998389-ef070d00-51d6-11eb-80fa-a17dd43dd9c4.png)\r\n服务器端服务启动无异常\r\n",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-08T09:27:40+00:00",
        "updated_at": "2024-04-16T09:05:28+00:00",
        "closed_at": "2024-04-16T09:05:28+00:00",
        "comments_count": [
            "TeslaZhao",
            "ClassmateXiaoyu"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 962,
        "title": "服务请求返回 array has incorrect number of dimensions: 3; expected 4",
        "body": "调用OCR服务返回 array has incorrect number of dimensions: 3; expected 4\r\n请问这是什么问题？\r\n",
        "state": "closed",
        "user": "solomance",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-09T11:54:21+00:00",
        "updated_at": "2024-04-16T09:05:28+00:00",
        "closed_at": "2024-04-16T09:05:28+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "solomance",
            "TeslaZhao",
            "solomance",
            "TeslaZhao",
            "solomance",
            "solomance",
            "TeslaZhao",
            "TeslaZhao",
            "solomance",
            "solomance"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 963,
        "title": "No module named 'paddle_serving_server‘",
        "body": "启动RPC服务时报错：\r\n```\r\n$ python -m paddle_serving_server.serve --model serving_server\r\n/home/lca/developEnv/anaconda3/envs/paddle/bin/python: Error while finding module specification for 'paddle_serving_server.serve' (ModuleNotFoundError: No module named 'paddle_serving_server')\r\n```\r\n使用pip list查看当前环境下，是安装了paddle-serving-client和paddle-serving-server的\r\n```\r\n$ pip list\r\nPackage                   Version\r\n------------------------- -------------------\r\nastor                     0.8.1\r\ncertifi                   2020.12.5\r\nchardet                   4.0.0\r\nclick                     7.1.2\r\ncycler                    0.10.0\r\ndecorator                 4.4.2\r\nFlask                     1.1.2\r\nfunc-timeout              4.3.5\r\nfuncsigs                  1.0.2\r\ngast                      0.3.3\r\ngraphviz                  0.16\r\ngrpcio                    1.33.2\r\ngrpcio-tools              1.33.2\r\nidna                      2.10\r\nitsdangerous              1.1.0\r\nJinja2                    2.11.2\r\njoblib                    1.0.0\r\nkiwisolver                1.3.1\r\nMarkupSafe                1.1.1\r\nmatplotlib                3.3.3\r\nnltk                      3.5\r\nnumpy                     1.19.5\r\nobjgraph                  3.5.0\r\nopencv-python             4.2.0.32\r\npaddle-serving-app        0.2.0\r\npaddle-serving-client     0.4.0\r\npaddle-serving-server-gpu 0.4.0.post9\r\npaddlepaddle-gpu          1.8.5.post97\r\npandas                    1.1.5\r\npathlib                   1.0.1\r\nPillow                    8.1.0\r\npip                       20.3.3\r\nprettytable               2.0.0\r\nprotobuf                  3.14.0\r\npyclipper                 1.2.1\r\npyparsing                 2.4.7\r\npython-dateutil           2.8.1\r\npytz                      2020.5\r\nPyYAML                    5.3.1\r\nrarfile                   4.0\r\nregex                     2020.11.13\r\nrequests                  2.25.1\r\nscipy                     1.5.4\r\nsentencepiece             0.1.92\r\nsetuptools                51.0.0.post20201207\r\nsix                       1.15.0\r\ntqdm                      4.56.0\r\nurllib3                   1.26.2\r\nwcwidth                   0.2.5\r\nWerkzeug                  1.0.1\r\nwheel                     0.36.2\r\n```\r\n当前python为3.6",
        "state": "closed",
        "user": "lerry-lee",
        "closed_by": "TeslaZhao",
        "created_at": "2021-01-11T13:35:21+00:00",
        "updated_at": "2024-07-10T09:32:18+00:00",
        "closed_at": "2021-03-01T08:24:09+00:00",
        "comments_count": [
            "TeslaZhao",
            "oodm"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 964,
        "title": "基于pd开发的模型是否能加密部署？",
        "body": "你好！我有两个问题：\r\n1、本项目和paddlehub里的[部署方案](https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.0.0-rc/docs/docs_en/tutorial/serving_en.md)有什么区别？如果我使用pd自己开发的模型需要部署，应该使用哪种方案？\r\n2、基于serving的部署方案（无论本项目还是paddlehub），是否支持模型加密？我搜索到[类似问题](https://github.com/PaddlePaddle/Serving/pull/689)，不知是否表明已经具备加密部署能力，如果具备是否能提供使用说明文档？",
        "state": "closed",
        "user": "grapefruitL",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-12T03:35:40+00:00",
        "updated_at": "2024-03-05T06:49:05+00:00",
        "closed_at": "2024-03-05T06:49:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 965,
        "title": "确定高性能、生产可用？官方Benchmark显示性能比较低",
        "body": "Repo简介里写着高性能，可是Benchmark数据显示性能不太能接受：https://github.com/PaddlePaddle/Serving/blob/v0.4.0/doc/BENCHMARKING.md",
        "state": "closed",
        "user": "readthecodes",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-14T16:55:30+00:00",
        "updated_at": "2024-03-05T06:49:06+00:00",
        "closed_at": "2024-03-05T06:49:06+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 968,
        "title": "Paddle Serving 是否支持单进程中的多线程执行预测",
        "body": "Hi,\r\n\r\n感谢作者开发Paddle Serving，我打算使用Paddle Serving去部署model。不过这里有一个疑问：Paddle Serving 是否支持单进程中的多线程执行预测？\r\n\r\n有这个疑问的原因是，做Python Web application开发的时候，一般是通过多进程的方式去部署从而增加服务端的性能（比如Django/Flask，因为python 有GIL所以单进程没有办法利用CPU所有的核）。但是由于有的时候Model模型文件会很大，所以多进程会导致内存占用翻个好几倍。那Paddle Serving也是通过多进程（像Django或者Flask）的方式去增加服务端的性能的吗？Paddle Serving 是否支持单进程中的多线程执行预测 ？或者说Paddle Serving是否支持单进程中利用CPU所有核（绕开Pythond的GIL）去执行预测 ？\r\n\r\nThank you !\r\n\r\n\r\n",
        "state": "closed",
        "user": "zhfkt",
        "closed_by": "TeslaZhao",
        "created_at": "2021-01-16T12:31:36+00:00",
        "updated_at": "2021-03-01T08:23:45+00:00",
        "closed_at": "2021-03-01T08:23:45+00:00",
        "comments_count": [
            "github-actions[bot]",
            "xiao4er",
            "TeslaZhao",
            "HexToString",
            "zhfkt"
        ],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 973,
        "title": "windows原生系统下部署serving问题",
        "body": "参考https://github.com/PaddlePaddle/Serving/blob/v0.4.0/doc/WINDOWS_TUTORIAL_CN.md\r\n运行ocr实例出现错误：\r\n环境是python3.8 \r\n执行命令：python ocr_debugger_server.py cpu\r\n 错误： \r\nTraceback (most recent call last):\r\n  File \"ocr_debugger_server.py\", line 28, in <module>\r\n    from paddle_serving_server.web_service import WebService\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 24, in <module>\r\n    from paddle_serving_server import pipeline\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\paddle_serving_server\\pipeline\\__init__.py\", line 15, in <module>\r\n    from .operator import Op, RequestOp, ResponseOp\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\paddle_serving_server\\pipeline\\operator.py\", line 19, in <module>\r\n    from paddle_serving_client import MultiLangClient, Client\r\nImportError: cannot import name 'MultiLangClient' from 'paddle_serving_client' (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\paddle_serving_client\\__init__.py)",
        "state": "closed",
        "user": "suntao2015005848",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-19T05:44:45+00:00",
        "updated_at": "2024-03-05T06:49:07+00:00",
        "closed_at": "2024-03-05T06:49:07+00:00",
        "comments_count": [
            "github-actions[bot]",
            "suntao2015005848",
            "bjjwwang",
            "bjjwwang",
            "suntao2015005848",
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 975,
        "title": "Serving pipeline的ocr例子只能返回一个识别字符串",
        "body": "直接按照官方的这个例子跑Serving pipeline的OCR，永远只能返回一个识别字符串\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.4.0/python/examples/pipeline/ocr/README_CN.md\r\n\r\n启动 - \r\n\r\npython web_service.py\r\n\r\n请求requests - \r\n\r\npython pipeline_http_client.py\r\n\r\n输出 - \r\n\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['没有吃饱只有一个']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['管理员的认证--使用JT']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['当mybatis-plus所提供的接口不能满足业务需要的时候']\"]}\r\n\r\n\r\n测试图片：\r\n\r\n--------------------------\r\n\r\n图片1：\r\n![7](https://user-images.githubusercontent.com/4969804/105021349-69f7df80-5a83-11eb-8b23-cc92d9b99a5d.jpg)\r\n\r\n图片2：\r\n![test](https://user-images.githubusercontent.com/4969804/105021369-6ebc9380-5a83-11eb-95ef-f7eb6813ca6f.png)\r\n\r\n图片3：\r\n![3](https://user-images.githubusercontent.com/4969804/105021710-d1ae2a80-5a83-11eb-9430-f859d4dd2b55.png)\r\n\r\n\r\n--------------------------\r\n\r\n\r\n\r\n稍微调试了一下，发现DetOp https://github.com/PaddlePaddle/Serving/blob/54999920b674cd3f370ede4bea43fff1dbd9ca10/python/examples/pipeline/ocr/web_service.py#L30 可以识别多个文本框boxes，但是在 RecOp 阶段https://github.com/PaddlePaddle/Serving/blob/54999920b674cd3f370ede4bea43fff1dbd9ca10/python/examples/pipeline/ocr/web_service.py#L69 只能输出一个文本，而不是多个文本。不知道是否是Pipeline框架的bug，或者是本身web_service.py有问题。请看一下这个bug，谢谢！\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "zhfkt",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-19T10:33:09+00:00",
        "updated_at": "2024-03-05T06:49:08+00:00",
        "closed_at": "2024-03-05T06:49:08+00:00",
        "comments_count": [
            "zhfkt",
            "zhfkt",
            "TeslaZhao",
            "zengqi0730",
            "TeslaZhao",
            "TeslaZhao",
            "zengqi0730",
            "TeslaZhao"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 977,
        "title": "模型转换出错",
        "body": "使用paddlex训练的yolov3_darknet53 模型放置serving部署。\r\n需要做转换，在做转换的时候报错了\r\n转化的命令\r\n`python -m paddle_serving_client.convert --dirname ./model/best_model/ --model_filename=\"model.pdmodel\" --params_filename=\"model.pdparams\"`\r\n模型文件\r\n![image](https://user-images.githubusercontent.com/2733701/105117876-43c85300-5b08-11eb-8444-c5e4bdde192b.png)\r\n\r\n报错信息如下\r\n`TensorRT dynamic library (libnvinfer.so) that Paddle depends on is not configured correctly. (error code is libnvinfer.so: cannot open shared object file: No such file or directory)\r\n  Suggestions:\r\n  1. Check if TensorRT is installed correctly and its version is matched with paddlepaddle you installed.\r\n  2. Configure TensorRT dynamic library environment variables as follows:\r\n  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`\r\n  - Windows: set PATH by `set PATH=XXX;/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/executor.py:1070: UserWarning: The following exception is not an EOF exception.\r\n  \"The following exception is not an EOF exception.\")\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/paddle-serve/env/lib/python3.6/site-packages/paddle_serving_client/convert.py\", line 66, in <module>\r\n    params_filename=args.params_filename)\r\n  File \"/home/paddle-serve/env/lib/python3.6/site-packages/paddle_serving_client/io/__init__.py\", line 120, in inference_model_to_serving\r\n    fluid.io.load_inference_model(dirname=dirname, executor=exe, model_filename=model_filename, params_filename=params_filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 1402, in load_inference_model\r\n    load_persistables(executor, load_dirname, program, params_filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 928, in load_persistables\r\n    filename=filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 751, in load_vars\r\n    filename=filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 805, in load_vars\r\n    executor.run(load_prog)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/executor.py\", line 1071, in run\r\n    six.reraise(*sys.exc_info())\r\n  File \"/home/paddle-serve/env/lib/python3.6/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/executor.py\", line 1066, in run\r\n    return_merged=return_merged)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/executor.py\", line 1154, in _run_impl\r\n    use_program_cache=use_program_cache)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/executor.py\", line 1229, in _run_program\r\n    fetch_var_name)\r\npaddle.fluid.core_avx.EnforceNotMet: \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)\r\n2   paddle::framework::DeserializeFromStream(std::istream&, paddle::framework::LoDTensor*, paddle::platform::DeviceContext const&)\r\n3   paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>::LoadParamsFromBuffer(paddle::framework::ExecutionContext const&, paddle::platform::Place const&, std::istream*, bool, std::vector<std::string, std::allocator<std::string> > const&) const\r\n4   paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const\r\n5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CPUPlace, false, 0ul, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, double>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, int>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, signed char>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, long> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)\r\n6   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const\r\n7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n8   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)\r\n9   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)\r\n10  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)\r\n11  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 803, in load_vars\r\n    'model_from_memory': vars_from_memory\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 751, in load_vars\r\n    filename=filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 928, in load_persistables\r\n    filename=filename)\r\n  File \"/home/paddle-serve/env/lib64/python3.6/site-packages/paddle/fluid/io.py\", line 1402, in load_inference_model\r\n    load_persistables(executor, load_dirname, program, params_filename)\r\n  File \"/home/paddle-serve/env/lib/python3.6/site-packages/paddle_serving_client/io/__init__.py\", line 120, in inference_model_to_serving\r\n    fluid.io.load_inference_model(dirname=dirname, executor=exe, model_filename=model_filename, params_filename=params_filename)\r\n  File \"/home/paddle-serve/env/lib/python3.6/site-packages/paddle_serving_client/convert.py\", line 66, in <module>\r\n    params_filename=args.params_filename)\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: tensor version 1904018048 is not supported, Only version 0 is supported\r\n  [Hint: Expected version == 0U, but received version:1904018048 != 0U:0.] at (/paddle/paddle/fluid/framework/lod_tensor.cc:287)\r\n  [operator < load_combine > error]`\r\n\r\n环境基于centos7.6 python3.6 serving0.4 和cuda9.0b版本",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "ponycloud235",
        "created_at": "2021-01-20T02:14:37+00:00",
        "updated_at": "2021-01-21T06:26:31+00:00",
        "closed_at": "2021-01-21T06:26:31+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ponycloud235"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 978,
        "title": "pipleline模式下 ，在存在local_service_conf的情况下，config.yml中的concurrency无法设定线程数",
        "body": "\r\npipleline模式下 ，在存在local_service_conf的情况下，config.yml中的concurrency无法设定线程数\r\n\r\n\r\n对于这个下面的这个例子：\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.4.0/python/examples/pipeline/ocr/config.yml\r\n\r\n改变 concurrency 无法改变并发的线程数，线程数一直为2。\r\n\r\n原因在于 存在local_service_conf的情况下，需要同时设置 thread_num 才行，否则就一直是2。可能出问题的代码在这里。\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/54999920b674cd3f370ede4bea43fff1dbd9ca10/python/pipeline/operator.py#L155  \r\n\r\nthread_num 覆盖了上面的 concurrency 的值。如果没有设置 thread_num 的情况下，默认thread_num = 2，也会 覆盖了上面的 concurrency的值。\r\n\r\n应该变成 在没有设定 thread_num  的情况下， 不要覆盖 concurrency 的值。\r\n\r\nPlease check this bug\r\n\r\nThank you !\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "zhfkt",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-20T05:23:25+00:00",
        "updated_at": "2024-04-16T09:05:29+00:00",
        "closed_at": "2024-04-16T09:05:29+00:00",
        "comments_count": [
            "TeslaZhao",
            "zhfkt",
            "zhfkt",
            "TeslaZhao",
            "TeslaZhao",
            "TeslaZhao",
            "zhfkt"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 982,
        "title": "readme文档中有一处小错误",
        "body": "![image](https://user-images.githubusercontent.com/2733701/105269702-6d987d00-5bcf-11eb-9afd-7b701ab1757c.png)\r\n\r\n这里在头部漏了一句\r\n`import numpy as np`\r\n\r\n不引入，执行\r\n`fetch_map = client.predict(feed={\"x\": np.array(data).reshape(1,13,1)}, fetch=[\"price\"])` \r\n报错\r\n`NameError: name 'np' is not defined`",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "TeslaZhao",
        "created_at": "2021-01-21T02:01:04+00:00",
        "updated_at": "2021-03-01T08:30:44+00:00",
        "closed_at": "2021-03-01T08:30:44+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 985,
        "title": "Pipeline模式下，设置为 线程模型（is_thread_op: True）之后，在有多个请求的情况下，predict无法利用CPU所有核。",
        "body": "直接按照官方的这个例子跑Serving pipeline的OCR。\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.4.0/python/examples/pipeline/ocr/README_CN.md\r\n\r\n在有多个请求的情况下，当设置为进程模型（在config.yml中is_thread_op: False）的时候可以利用CPU所有核心去predict。但是设置为 线程模型（is_thread_op: True）之后，predict无法利用CPU所有核去predict，只能利用local predictor中的thread_num个CPU核心数去predict\r\n\r\n\r\n复现步骤:\r\n\r\n1. 先按照 https://github.com/PaddlePaddle/Serving/blob/v0.4.0/python/examples/pipeline/ocr/README_CN.md 配置。\r\n\r\n2. 设置config.yml中的字段\r\na. worker_num: 10\r\nb. op的det和rec下 concurrency: 8\r\n\r\n3. 从 https://github.com/PaddlePaddle/PaddleOCR/blob/develop/doc/doc_ch/models_list.md 下载大模型 https://paddleocr.bj.bcebos.com/20-09-22/server/rec/ch_ppocr_server_v1.1_rec_infer.tar  （下载大模型的原因是一张图片predict可以跑30秒左右）\r\n\r\n4. 通过 https://github.com/PaddlePaddle/PaddleOCR/blob/develop/deploy/pdserving/inference_to_serving.py 脚本转换模型到Serving格式\r\n\r\n5.  设置config.yml中 rec模型路径 op -> rec下的model_config为新的转换后Serving格式的大模型路径\r\nmodel_config: inference/ch_ppocr_server_v1.1_rec_infer/serving_server_dir\r\n\r\n6. 进程模式下（config.yml设置 is_thread_op: False），在两个独立窗口中同时执行 脚本 python pipeline_http_client.py（模拟同时有两个请求），会发现cpu可以利用4个核心去跑（因为local predictor中的thread_num默认值为2，现在有两个请求，生成两个进程 -> 2个进程*2个thread = 会利用CPU4个核心）\r\n下图中显示了，在htop中多进程会利用4个CPU核心跑满100%：\r\n\r\n![2](https://user-images.githubusercontent.com/4969804/105337068-ef5ec980-5c14-11eb-95f3-0f8cb4d8d1f3.png)\r\n\r\n7. 但是在线程模式下（config.yml设置 is_thread_op: True），在两个独立窗口中同时执行 脚本 python pipeline_http_client.py（模拟同时有两个请求），会发现cpu只能用2个核心去跑\r\n下图中显示了，在htop中多线程模式只能利用2个（local predictor中的thread_num默认值）CPU核心跑满100%：\r\n\r\n![1](https://user-images.githubusercontent.com/4969804/105337028-e79f2500-5c14-11eb-81d7-307247cf4681.png)\r\n\r\n观察发现，在线程模式下，只能利用local predictor中的thread_num个CPU核心数去predict，在有多个请求的情况下（同时发送一个相同的request，在这里是在两个窗口中同时执行 python pipeline_http_client.py），无法利用CPU所有核。仿佛参数 concurrency 和 worker_num 没有使用一样。\r\n\r\nPls review whether it is a bug or not. \r\n\r\nThank you !\r\n\r\n",
        "state": "closed",
        "user": "zhfkt",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-21T10:27:20+00:00",
        "updated_at": "2024-04-16T09:05:30+00:00",
        "closed_at": "2024-04-16T09:05:30+00:00",
        "comments_count": [],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 983,
        "title": "部署检测模型,特定图片导致rpc服务崩溃",
        "body": "# 问题\r\n使用paddleDetection训练了一个检测模型,部署到serving中,使用其他图片都可以正常使用,但是有个别一两张图片,导致**rpc服务异常崩溃**,后续预测无法继续.经过对比,导致错误的图片和其他图片大小,通道等没有什么大的区别,\r\n# 建议\r\nrpc服务内部是否可以添加全局异常处理,只返回错误而不至于导致服务崩溃\r\n# 日志\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0121 07:12:52.103834   355 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"0.0.0.0:12000\"): added 1\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():\r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)\r\n2   paddle::AnalysisPredictor::GetFetch(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, paddle::framework::Scope*)\r\n3   paddle::AnalysisPredictor::Run(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, int)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: Tensor not initialized yet when Tensor::type() is called.\r\n  [Hint: holder_ should not be null.] at (/paddle/paddle/fluid/framework/tensor.h:140)\r\n\r\nW0121 07:12:55.967315   355 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=10 SocketId=1@0.0.0.0:12000@45966 [R1][E111]Fail to connect SocketId=8589934594@0.0.0.0:12000: Connection refused [R2][E112]Fail to select server from list://0.0.0.0:12000 lb=la\r\nI0121 07:12:56.067745   361 socket.cpp:2370] Checking SocketId=0@0.0.0.0:12000\r\nE0121 07:12:56.965327   355 general_model.cpp:361] failed call predictor with req: insts { tensor_array { float_data: -2.1179039 float_data: -2.1179039 float_data: -2.1179039 float_data: -2.1179039 float_data: -2.1179039 float_data: -2.1179039 float_data: 0.62205678 float_data: 0.41655976 float_data: 0.4336845 float_data: 0.45080924 float_data: 0.467934 float_data: 0.45080924 float_data: 0.467934 float_data: 0.5021835 float_data: 0.51930827 float_data: 0.48505875 float_data: 0.41655976 float_data: 0.34806073 float_data: 0.262437 float_data: 0.38231024 float_data: 0.38231024 float_data: 0.33093598 float_data: 0.38231024 float_data: 0.5021835 float_data: 0.467934 float_data: 0.45080924 float_data: 0.45080924 float_data: 0.48505875 float_data: 0.48505875 float_data: 0.5021835 float_data: 0.5021835 float_data: 0.5021835 float_data: 0.467934 float_data: 0.4336845 float_data: 0.4336845 float_data: 0.4336845 float_data: 0.4336845 float_data: 0.467934 float_data: 0.27956173 float_data: 0.3651855 float_data: 0.41655976 float_data: 0.39943498 float_data: 0.3651855 float_data: 0.4336845 float_data: 0.45080924 float_data: 0.39943498 float_data: 0.41.........................略\r\n```",
        "state": "closed",
        "user": "nblib",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-21T07:17:35+00:00",
        "updated_at": "2024-03-05T06:49:08+00:00",
        "closed_at": "2024-03-05T06:49:08+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "nblib",
            "qjinshanq",
            "xealml",
            "ponycloud235",
            "MrTsien",
            "ponycloud235",
            "MrTsien",
            "WangRongsheng",
            "WangRongsheng",
            "ponycloud235"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 984,
        "title": "gpu模型部署不成功，无报错，如何排除？",
        "body": "\r\n运行\r\n![image](https://user-images.githubusercontent.com/2733701/105324614-99832500-5c06-11eb-9bc0-2d5d127f4378.png)\r\n相关命令，后一直卡着，然后就没了？ 可以有什么办法排除吗\r\n`python -m paddle_serving_server_gpu.serve --model yolov3_server --port 9292 --gpu_ids 0`\r\n\r\n相关环境：\r\n![image](https://user-images.githubusercontent.com/2733701/105324594-94be7100-5c06-11eb-8aa4-031f7d0a4e2f.png)\r\npython3.6.8版本 \r\ncentos7.6\r\ncuda9.0\r\n依赖包版本信息\r\n![image](https://user-images.githubusercontent.com/2733701/105324873-ee26a000-5c06-11eb-9b1a-c97d3a8d3e3b.png)\r\n",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-21T08:37:28+00:00",
        "updated_at": "2024-03-05T06:49:09+00:00",
        "closed_at": "2024-03-05T06:49:09+00:00",
        "comments_count": [
            "ponycloud235",
            "ponycloud235",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 986,
        "title": "Pipeline模式下 ，设置local_service_conf中的参数thread_num无论多大，都只能利用CPU的4个核心。",
        "body": "pipleline模式下 ，设置local_service_conf中的参数thread_num无论多大，都只能利用CPU的4个核心。\r\n\r\n我试了下thread_num设置成了10，但是似乎只用到了4个核（观察一个长时间的predict任务发现只有4个cpu核会跑到100%），是否对thread_num的上限有做了什么设置？(BLAS相关？)",
        "state": "closed",
        "user": "zhfkt",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-21T10:47:20+00:00",
        "updated_at": "2024-05-28T06:38:51+00:00",
        "closed_at": "2024-05-28T06:38:51+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao",
            "zhfkt",
            "huaxiangsiyi",
            "huaxiangsiyi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 989,
        "title": "Paddle Serving 启动lac服务的tags词性分析需要什么参数",
        "body": "lac模型的 tags分析如何启用啊",
        "state": "closed",
        "user": "suntao2015005848",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-01-22T05:16:04+00:00",
        "updated_at": "2024-03-05T06:49:10+00:00",
        "closed_at": "2024-03-05T06:49:10+00:00",
        "comments_count": [
            "TeslaZhao",
            "weixiu00",
            "bjjwwang"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 990,
        "title": "文档有处小错误",
        "body": "https://github.com/PaddlePaddle/Serving/blob/v0.4.0/doc/SAVE_CN.md\r\n![image](https://user-images.githubusercontent.com/2733701/105667423-30f1bc00-5f16-11eb-9ef3-6757dcc1616f.png)\r\n少了m，应该是params_filename",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "TeslaZhao",
        "created_at": "2021-01-25T06:04:26+00:00",
        "updated_at": "2021-03-01T08:45:48+00:00",
        "closed_at": "2021-03-01T08:45:48+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1009,
        "title": "yolov3 http部署方式",
        "body": "使用grpc方式部署成功了，使用http部署一直无法调用成功，我的问题：\r\n   1. 使用官网的方式uci方式的入参和出参是如何定义的？\r\n用的是yolov3的模型， python的代码如下\r\n\r\n因项目需使用http方式，方便接入\r\n\r\n`import sys\r\nimport numpy as np\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport cv2\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Resize(\r\n        (608, 608), interpolation=cv2.INTER_LINEAR), Div(255.0), Transpose(\r\n            (2, 0, 1))\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\", [608, 608])\r\nclient = Client()\r\n\r\nclient.load_client_config(\"yolov3_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\n\r\nim = preprocess(sys.argv[1])\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n        \"im_size\": np.array(list(im.shape[1:])),\r\n    },\r\n    fetch=[\"save_infer_model/scale_0.tmp_0\"],\r\n    batch=False)\r\nfetch_map[\"image\"] = sys.argv[1]\r\npostprocess(fetch_map)`",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-01T01:22:04+00:00",
        "updated_at": "2024-04-16T09:05:31+00:00",
        "closed_at": "2024-04-16T09:05:31+00:00",
        "comments_count": [
            "TeslaZhao",
            "ponycloud235",
            "ponycloud235"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1016,
        "title": "Serving/README_CN.md 文档错误处建议修改",
        "body": "[Serving/README_CN.md ](https://github.com/PaddlePaddle/Serving/blob/v0.4.0/README_CN.md)\r\n![image](https://user-images.githubusercontent.com/16701660/106745901-ba169a80-665c-11eb-993c-b6af28f5b206.png)\r\n错误命令：\r\n`python resnet50_imagenet_classify.py resnet50_serving_model &`\r\n正确命令：\r\n`python resnet50_imagenet_classify.py resnet_v2_50_imagenet_model`",
        "state": "closed",
        "user": "GreenValue",
        "closed_by": "GreenValue",
        "created_at": "2021-02-03T12:18:33+00:00",
        "updated_at": "2021-02-04T06:47:03+00:00",
        "closed_at": "2021-02-04T06:46:58+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1011,
        "title": "paddle_serving_server_gpu 部署时的错误。如运行错误 libcudart.so.10.1: cannot open shared object file: No such file or directory",
        "body": "- 操作：PaddleDetection在线Serving部署\r\n\r\n[https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.0-beta/deploy/serving](url)\r\n\r\n- docker中执行\r\n\r\n```\r\npython -m paddle_serving_server_gpu.serve --model serving_server --port 9393 --gpu_ids 0\r\n```\r\n输出\r\n```\r\nmkdir: cannot create directory ‘workdir_0’: File exists\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\nGoing to Run Comand\r\n/usr/local/python2.7.15/lib/python2.7/site-packages/paddle_serving_server_gpu/serving-gpu-trt-0.4.0/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 9393 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 536870912 \r\n/usr/local/python2.7.15/lib/python2.7/site-packages/paddle_serving_server_gpu/serving-gpu-trt-0.4.0/serving: error while loading shared libraries: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n```\r\n\r\n- docker环境\r\nnvidia-docker pull hub.baidubce.com/paddlepaddle/**paddle:1.8.5-gpu-cuda10.0-cudnn7**\r\n```\r\nlocate libcudart.so\r\n\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n```",
        "state": "closed",
        "user": "GreenValue",
        "closed_by": "TeslaZhao",
        "created_at": "2021-02-01T12:26:08+00:00",
        "updated_at": "2021-03-01T08:21:15+00:00",
        "closed_at": "2021-03-01T08:21:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "GreenValue",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "TeslaZhao",
            "GreenValue",
            "GreenValue",
            "GreenValue",
            "GreenValue"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1015,
        "title": "Serving example中OCR 服务，报错",
        "body": "文档：https://github.com/PaddlePaddle/Serving/blob/develop/python/examples/ocr/README_CN.md\r\n启动服务， port应该是9292，文档上是9293\r\n`python -m paddle_serving_server_gpu.serve --model ocr_det_model --port 9292 --gpu_id 0`\r\n\r\n执行命令\r\nroot@my_ubuntu:/paddle/Serving/python/examples/ocr# `python ocr_web_client.py`\r\n```\r\n<Response [404]>\r\nTraceback (most recent call last):\r\n  File \"ocr_web_client.py\", line 40, in <module>\r\n    print(r.json())\r\n  File \"/usr/local/python2.7.15/lib/python2.7/site-packages/requests/models.py\", line 898, in json\r\n    return complexjson.loads(self.text, **kwargs)\r\n  File \"/usr/local/python2.7.15/lib/python2.7/json/__init__.py\", line 339, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/usr/local/python2.7.15/lib/python2.7/json/decoder.py\", line 364, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/usr/local/python2.7.15/lib/python2.7/json/decoder.py\", line 380, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\nValueError: Expecting ',' delimiter: line 1 column 7 (char 6)\r\n```\r\n\r\n",
        "state": "closed",
        "user": "GreenValue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-03T07:25:58+00:00",
        "updated_at": "2024-03-05T06:49:11+00:00",
        "closed_at": "2024-03-05T06:49:11+00:00",
        "comments_count": [
            "GreenValue",
            "GreenValue",
            "GreenValue",
            "TeslaZhao",
            "TeslaZhao",
            "GreenValue",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1017,
        "title": "如何在Paddle Serving使用c++ Client？",
        "body": "[如何在Paddle Serving使用Go Client](https://github.com/PaddlePaddle/Serving/blob/v0.4.0/doc/IMDB_GO_CLIENT_CN.md)\r\n有Go，怎么没有C++的呀？",
        "state": "closed",
        "user": "GreenValue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-03T12:40:19+00:00",
        "updated_at": "2024-04-16T09:05:32+00:00",
        "closed_at": "2024-04-16T09:05:32+00:00",
        "comments_count": [
            "TeslaZhao",
            "GreenValue",
            "GreenValue",
            "TeslaZhao",
            "GreenValue"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1023,
        "title": "使用http方式GPU环境推理模型调用报错",
        "body": "环境docker 基于 hub.baidubce.com/paddlepaddle/serving   latest-cuda10.0-cudnn7 \r\n同样代码在cpu环境下没有问题\r\n其他版本信息\r\n`paddle-serving-app (0.2.0)\r\npaddle-serving-client (0.4.0)\r\npaddle-serving-server-gpu (0.4.0.post10)\r\npaddlepaddle-gpu (1.8.5.post107)\r\n`\r\n报错信息如下：\r\n![image](https://user-images.githubusercontent.com/2733701/107140875-2b599480-6960-11eb-96b5-2981a64cd9b5.png)\r\n\r\n相关运行代码\r\n`python3 uwsgi.py`\r\n`\r\ncurl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"multiclass_nms_0.tmp_0\"]}' http://127.0.0.1:9393/uci/prediction\r\n`\r\n\r\n`#coding=utf-8\r\nfrom paddle_serving_server_gpu.web_service import WebService\r\nfrom paddle_serving_app.reader import *\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport sys\r\nimport base64\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Resize(\r\n        (608, 608), interpolation=cv2.INTER_LINEAR), Div(255.0), Transpose(\r\n            (2, 0, 1))\r\n])\r\n\r\nclass UciService(WebService):\r\n    def preprocess(self, feed=[], fetch=[]):\r\n        print(feed)\r\n        #data = base64.b64decode(feed[0][\"image\"].encode('utf8'))\r\n        #data = np.fromstring(data, np.uint8)\r\n        #im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n        #调用本地图片进行测试\r\n        car_im = preprocess(\"./22.jpg\")\r\n        is_batch = True\r\n        print(car_im.shape)\r\n        print(np.array(list(car_im.shape[1:])).shape)\r\n        feed = [{\r\n            \"image\": car_im,\r\n            \"im_size\": np.array(list(car_im.shape[1:]))\r\n        }]\r\n        return feed, fetch, is_batch\r\n    def postprocess(self, feed={}, fetch=[], fetch_map=None):\r\n        print(fetch)\r\n        print(fetch_map)\r\n        return fetch\r\n\r\n#配置预测服务\r\nuci_service = UciService(name = \"uci\")\r\nuci_service.load_model_config(\"./car_server\")\r\nuci_service.prepare_server(workdir=\"./workdir\", port=9393, device=\"gpu\", gpuid=0)\r\nuci_service.run_rpc_service()\r\nuci_service.run_web_service()\r\n#获取flask服务\r\napp_instance = uci_service.get_app_instance()\r\n`",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-07T08:23:44+00:00",
        "updated_at": "2024-03-05T06:49:12+00:00",
        "closed_at": "2024-03-05T06:49:12+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1038,
        "title": "请问有没有详细的工程设计文档？",
        "body": "想基于paddler serving做二次开发，但是在源码阅读过程中，缺乏可以参考的设计文档和代码注释，导致进展缓慢。能否贡献一下项目的设计文档、相关流程的解析（初始化流程、IO流程）以及各数据结构间的层次关系",
        "state": "closed",
        "user": "jinmin527",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-17T07:53:32+00:00",
        "updated_at": "2024-04-16T09:05:33+00:00",
        "closed_at": "2024-04-16T09:05:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "jinmin527",
            "TeslaZhao",
            "jinmin527"
        ],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1056,
        "title": "怎么在生产环境部署",
        "body": "看了文档，还是部署很清楚。\r\n在自己服务器部署倒是基本知道，但是运行时cpu基本占满，\r\n想部署在云上的函数计算中，但是不清楚该怎么做",
        "state": "closed",
        "user": "guojiahuiEmily",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-22T09:48:54+00:00",
        "updated_at": "2024-03-05T06:49:13+00:00",
        "closed_at": "2024-03-05T06:49:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1040,
        "title": "PaddleServing依赖的openssl是什么版本？1.0版本么？",
        "body": "基于`nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04`，操作如下\r\n\r\n`pip3 isntall paddle-serving-client`竟然只是0.2.1版本😓，只好`pip3 install -i https://mirror.baidu.com/pypi/simple https://paddle-serving.bj.bcebos.com/whl/paddle_serving_client-0.0.0-cp38-none-any.whl`重新安装（对了，这个cp38，还不在https://github.com/PaddlePaddle/Serving/blob/v0.4.0/doc/LATEST_PACKAGES.md 显示呢）。\r\n然后，\r\n```\r\nTraceback (most recent call last):\r\n  File \"text_classify_service.py\", line 17, in <module>\r\n    from paddle_serving_app.reader.imdb_reader import IMDBDataset\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_app/reader/__init__.py\", line 15, in <module>\r\n    from .image_reader import ImageReader, File2Image, URL2Image, Sequential, Normalize, Base64ToImage\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_app/reader/image_reader.py\", line 17, in <module>\r\n    import cv2\r\n  File \"/usr/local/lib/python3.8/dist-packages/cv2/__init__.py\", line 3, in <module>\r\n    from .cv2 import *\r\nImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory\r\n```\r\n 😓\r\n按照https://github.com/baidu/lac/issues/108 使用`apt-get install -y libglib2.0-0 libsm6 libxext6 libxrender-dev`安装必要依赖后，又遇到\r\n```\r\nTraceback (most recent call last):\r\n  File \"text_classify_service.py\", line 17, in <module>\r\n    from paddle_serving_app.reader.imdb_reader import IMDBDataset\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_app/reader/__init__.py\", line 15, in <module>\r\n    from .image_reader import ImageReader, File2Image, URL2Image, Sequential, Normalize, Base64ToImage\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_app/reader/image_reader.py\", line 24, in <module>\r\n    from shapely.geometry import Polygon\r\nModuleNotFoundError: No module named 'shapely'\r\n```\r\n继续使用`pip3 install shapely  -i https://mirror.baidu.com/pypi/simple`安装依赖后，总算运行起来了 👏🏻\r\n执行`curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"words\": \"i am very sad | 0\"}], \"fetch\":[\"prediction\"]}' http://127.0.0.1:9292/imdb/prediction`，😖服务端又又又又\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 1945, in full_dispatch_request\r\n    self.try_trigger_before_first_request_functions()\r\n  File \"/usr/local/lib/python3.8/dist-packages/flask/app.py\", line 1993, in try_trigger_before_first_request_functions\r\n    func()\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_server_gpu/web_service.py\", line 217, in init\r\n    self._launch_web_service()\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_server_gpu/web_service.py\", line 167, in _launch_web_service\r\n    self.client = Client()\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_client/__init__.py\", line 139, in __init__\r\n    from .serving_client import PredictorRes\r\nImportError: libcrypto.so.10: cannot open shared object file: No such file or directory\r\n```\r\n\r\n但是ubuntu已经安装了openssl，有libcrypt.so.1.1.0文件的。版本不一致，先软链下试一试\r\n```bash\r\n/usr/lib/x86_64-linux-gnu/\r\nln -s libcrypto.so.1.1 libcrypto.so.10\r\nln -s libssl.so.1.1 libssl.so.10\r\n```\r\n还是遇到错误\r\n```\r\nImportError: /lib/x86_64-linux-gnu/libcrypto.so.1.1: version `OPENSSL_1.0.1_EC' not found (required by /usr/local/lib/python3.8/dist-packages/paddle_serving_client/serving_client.so)\r\n```\r\n这个，看来还是需要openssl 1.0版本的。\r\n\r\nubuntu16上才是1.0版本的openssl，但是ubuntu16又难以安装paddle2.0.0，这就😓了。",
        "state": "closed",
        "user": "wadefelix",
        "closed_by": "TeslaZhao",
        "created_at": "2021-02-18T10:43:26+00:00",
        "updated_at": "2021-03-17T02:54:13+00:00",
        "closed_at": "2021-03-17T02:54:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "wadefelix",
            "bjjwwang",
            "bjjwwang",
            "TeslaZhao"
        ],
        "labels": [
            "dependencies"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1061,
        "title": "请问paddlehub里的模型怎么转为serving的模型",
        "body": "paddlehub的藏头诗模型，怎么转为可以serving的模型？\r\n\r\n另外预测时，怎么调用，需要做什么处理才能输出汉字的结果",
        "state": "closed",
        "user": "guojiahuiEmily",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-22T12:28:40+00:00",
        "updated_at": "2024-03-05T06:49:15+00:00",
        "closed_at": "2024-03-05T06:49:15+00:00",
        "comments_count": [
            "TeslaZhao",
            "guojiahuiEmily",
            "TeslaZhao"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1060,
        "title": "模型转为serving时报错要enable_static，怎么解决",
        "body": "python -m paddle_serving_client.convert --dirname . --model_filename dygraph_model.pdmodel --params_filename dygraph_model.pdiparams --serving_server serving_server --serving_client serving_client\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n\r\nAssertionError: In PaddlePaddle 2.x, we turn on dynamic graph mode by default, and 'load_inference_model()' is only supported in static graph mode. So if you want to use this api, please call 'paddle.enable_static()' before this api to enter static graph mode.",
        "state": "closed",
        "user": "guojiahuiEmily",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-22T12:07:10+00:00",
        "updated_at": "2024-03-05T06:49:14+00:00",
        "closed_at": "2024-03-05T06:49:14+00:00",
        "comments_count": [
            "TeslaZhao",
            "muyu1944",
            "HexToString"
        ],
        "labels": [
            "模型保存与转换"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1066,
        "title": "paddlex导出的模型，trans_model.py转换后部署到paddle serving，feed_var的im_size参数怎么赋值？",
        "body": "paddlex导出的图像分类检测模型，用trans_model.py转换后部署到paddle serving。\r\n转换后生成的serving_client_conf.prototxt 内容\r\n=======================================\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_size\"\r\n  alias_name: \"im_size\"\r\n  is_lod_tensor: false\r\n  feed_type: 2\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_0\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n================================\r\n推理客户端代码：\r\nfeed_dict={}\r\nfeed_dict[\"image\"]=im\r\n#feed_dict[\"im_size\"]=\r\nfetch_map = client.predict(feed=feed_dict, fetch=[\"save_infer_model/scale_0.tmp_0\"])\r\n其中的im_size参数怎么赋值？",
        "state": "closed",
        "user": "zwei-joe",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-23T01:48:46+00:00",
        "updated_at": "2024-04-16T09:05:34+00:00",
        "closed_at": "2024-04-16T09:05:34+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "zwei-joe"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1068,
        "title": "请问这个项目中的模型怎么导出为serving格式",
        "body": "这里面没有param\r\nhttps://aistudio.baidu.com/aistudio/projectdetail/1182749?channelType=0&channel=0",
        "state": "closed",
        "user": "guojiahuiEmily",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-23T08:17:52+00:00",
        "updated_at": "2024-03-05T06:49:16+00:00",
        "closed_at": "2024-03-05T06:49:16+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": [
            "模型保存与转换"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1071,
        "title": "serving example中pipeline ocr里坐标不一致",
        "body": "example/pipline/ocr 里Det Op输出的out_dict里获取的dt_boxes 与 Rec_Op收到的input_dicts里 dt_boxes不一致，前者是原图的坐标，后者貌似是整个剪切后的图片坐标。 那我client如何通过serving服务返回文字在图片中的坐标呢？",
        "state": "closed",
        "user": "zengqi0730",
        "closed_by": "hysunflower",
        "created_at": "2021-02-24T08:50:32+00:00",
        "updated_at": "2021-02-26T06:01:36+00:00",
        "closed_at": "2021-02-26T06:01:36+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "zengqi0730"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1072,
        "title": "sentencepiece0.1.83版本过低在python3.8中安装失败",
        "body": "如题,paddle-serving-app==0.3.0依赖的sentencepiece版本在python3.8中安装失败\r\n* python: 3.8.5 \r\n* paddle-serving-app: 0.3.0\r\n* sentencepiece: 0.1.83\r\n````\r\nCollecting sentencepiece==0.1.83\r\n  Using cached sentencepiece-0.1.83.tar.gz (497 kB)\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /home/hewe/miniconda3/envs/python3.8/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-2jsz91ft/sentencepiece_00334083bc7d4ada8f5348fb8ad91b80/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-2jsz91ft/sentencepiece_00334083bc7d4ada8f5348fb8ad91b80/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-ie2fbv1p\r\n         cwd: /tmp/pip-install-2jsz91ft/sentencepiece_00334083bc7d4ada8f5348fb8ad91b80/\r\n    Complete output (7 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-2jsz91ft/sentencepiece_00334083bc7d4ada8f5348fb8ad91b80/setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"/home/hewe/miniconda3/envs/python3.8/lib/python3.8/codecs.py\", line 905, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n    ----------------------------------------\r\n```",
        "state": "closed",
        "user": "nblib",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-02-24T11:52:54+00:00",
        "updated_at": "2024-03-05T06:49:17+00:00",
        "closed_at": "2024-03-05T06:49:16+00:00",
        "comments_count": [
            "TeslaZhao",
            "bjjwwang"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1075,
        "title": "Bert-As-Service例子运行报错",
        "body": "首先按照官方教程在Docker内构建Paddle Serving\r\n\r\n- 获取镜像：docker pull registry.baidubce.com/paddlepaddle/serving:latest-cuda10.0-cudnn7-devel\r\n- 创建容器：nvidia-docker run -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:latest-cuda10.0-cudnn7-devel\r\n- 进入容器：nvidia-docker exec -it test bash\r\n- 安装PaddleServing（环境py27，cuda10.0）：\r\n1. pip install paddle-serving-client==0.5.0 \r\n2. pip install paddle-serving-app==0.3.0 \r\n3. pip install paddle-serving-server-gpu==0.5.0.post10\r\n- 安装paddle：pip install https://paddle-wheel.bj.bcebos.com/2.0.0-gpu-cuda10-cudnn7-mkl/paddlepaddle_gpu-2.0.0.post100-cp27-cp27mu-linux_x86_64.whl\r\n\r\n然后按照Bert-As-Service教程\r\n#下载模型：\r\nwget https://paddle-serving.bj.bcebos.com/paddle_hub_models/text/SemanticModel/bert_chinese_L-12_H-768_A-12.tar.gz\r\ntar -xzf bert_chinese_L-12_H-768_A-12.tar.gz\r\nmv bert_chinese_L-12_H-768_A-12_model bert_seq128_model\r\nmv bert_chinese_L-12_H-768_A-12_client bert_seq128_client\r\n#获取词典和样例数据：\r\nsh get_data.sh \r\n#启动GPU预测服务\r\npython bert_web_service_gpu.py bert_seq128_model/ 9292\r\n \r\n执行预测：curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"words\": \"hello\"}], \"fetch\":[\"pooled_output\"]}' http://127.0.0.1:9292/bert/prediction\r\n\r\n结果报错：\r\n![image](https://user-images.githubusercontent.com/24710595/109646990-c1fe2780-7b93-11eb-916b-bb95c220bd25.png)\r\n",
        "state": "closed",
        "user": "HJYgotoPLAY",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-02T12:13:53+00:00",
        "updated_at": "2024-03-05T06:49:18+00:00",
        "closed_at": "2024-03-05T06:49:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1077,
        "title": "基于paddlehub训练的文本分类模型，使用paddle_serving_client RPC调用失败",
        "body": "安装了以下包\r\npaddlepaddle-gpu==1.8.5.post97\r\npaddlehub==1.7.1 \r\npaddle_serving_client==0.4.0 \r\npaddle_serving_server==0.4.0\r\npaddle_serving_app==0.3.0 \r\n\r\n基于https://github.com/PaddlePaddle/PaddleHub/blob/release/v1.8/demo/text_classification/text_cls.py训练了一个文本分类模型\r\n使用paddlepaddle serving生成模型并启动RPC服务成功\r\npython -m paddle_serving_client.convert --dirname ckpt_bert/best_model\r\npython -m paddle_serving_server.serve --model serving_server --thread 2 --port 9292 --name test\r\n\r\n但是调用RPC服务时失败。\r\nimport sys\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_client.utils import benchmark_args\r\nfrom paddle_serving_app.reader import ChineseBertReader\r\nimport numpy as np\r\n\r\nmax_seq_len=64\r\nreader = ChineseBertReader({\"max_seq_len\": max_seq_len})\r\nfetch = [\"save_infer_model/scale_0.tmp_1\"]\r\nendpoint_list = ['127.0.0.1:9292']\r\nclient = Client()\r\nclient.load_client_config(\"text_classification/serving_client/serving_client_conf.prototxt\")\r\nclient.connect(endpoint_list)\r\n\r\nfeed_batch = []\r\nfor line in ['abc','testid']:\r\n    feed_dict = reader.process(line)\r\n    for key in feed_dict.keys():\r\n        feed_dict[key] = np.array(feed_dict[key]).reshape((max_seq_len, 1))\r\n    feed_batch.append(feed_dict)\r\nresult = client.predict(feed=feed_batch, fetch=fetch, batch=True)\r\nprint(result)\r\n\r\n错误日志为：\r\nERROR 2021-03-05 10:06:29,572 [_internal.py:113] 127.0.0.1 - - [05/Mar/2021 10:06:29] code 400, message Bad request syntax ('PRPC\\x00\\x00\\x06L\\x00\\x00\\x00Y')\r\nINFO 2021-03-05 10:06:29,573 [_internal.py:113] 127.0.0.1 - - [05/Mar/2021 10:06:29] \"PRPCLY\" HTTPStatus.BAD_REQUEST -\r\nERROR 2021-03-05 10:07:05,628 [_internal.py:113] 127.0.0.1 - - [05/Mar/2021 10:07:05] code 400, message Bad request syntax ('PRPC\\x00\\x00\\x06M\\x00\\x00\\x00Z')\r\nINFO 2021-03-05 10:07:05,629 [_internal.py:113] 127.0.0.1 - - [05/Mar/2021 10:07:05] \"PRPCMZ\" HTTPStatus.BAD_REQUEST -\r\n\r\n基于http服务调用失败：\r\ncurl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"input_ids\": [101,3844,6407,671,678,704,3152,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"position_ids\":[0,1,2,3,4,5,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"segment_ids\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"input_mask\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"seq_len\":64}], \"fetch\":[\"save_infer_model/scale_0.tmp_1\"]}' http://127.0.0.1:9292/test/prediction\r\n返回错误：{\"result\":\"Please make sure all of your inputs are numpy array\"}\r\n\r\n---------serving_client/serving_client_conf.prototxt\r\nfeed_var {\r\n  name: \"input_ids\"\r\n  alias_name: \"input_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 64\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"position_ids\"\r\n  alias_name: \"position_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 64\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"segment_ids\"\r\n  alias_name: \"segment_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 64\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"input_mask\"\r\n  alias_name: \"input_mask\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 64\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"seq_len\"\r\n  alias_name: \"seq_len\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_1.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_1.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 0\r\n  shape: -1\r\n}\r\n\r\n---------------------------------------------------------------\r\n切换到环境下部署\r\npaddlepaddle==2.0.0\r\npaddle_serving_client==0.5.0 \r\npaddle_serving_server==0.5.0\r\npaddle_serving_app==0.3.0 \r\n\r\n上面的基于http服务调用的请求成功，能正确返回预测值。\r\n\r\n上面到调用RPC服务的代码依然失败。\r\nERROR 2021-03-05 10:15:56,782 [_internal.py:113] 192.168.1.52 - - [05/Mar/2021 10:15:56] code 400, message Bad request syntax ('PRPC\\x00\\x00\\x06M\\x00\\x00\\x00Z')\r\nINFO 2021-03-05 10:15:56,782 [_internal.py:113] 192.168.1.52 - - [05/Mar/2021 10:15:56] \"PRPCMZ\" HTTPStatus.BAD_REQUEST -\r\n\r\n---------------------------------------------------------------\r\n请问有PaddleHub的文本分类模型如何通过RPC调用PaddleServing的例子吗？\r\n目前能找到的只有这个bert获取embedding的例子，用处不大\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/python/examples/bert/bert_client.py\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "baibaiw5",
        "closed_by": "baibaiw5",
        "created_at": "2021-03-05T02:16:35+00:00",
        "updated_at": "2021-03-09T02:51:27+00:00",
        "closed_at": "2021-03-09T02:51:27+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "baibaiw5"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1078,
        "title": "转换OCR rec模型后调用失败",
        "body": "转换rec模型之后，启动rec_debugger_server调用后报以下错：\r\n\r\nRuntimeError: (Unavailable) Load operator fail to open file D:\\Serving-0.5.0\\serving_rec_server/bn3c_branch2b_scale, please check whether the model file is complete or damaged.\r\n  [Hint: Expected static_cast<bool>(fin) == true, but received static_cast<bool>(fin):0 != true:1.] (at D:\\v2.0.0\\paddle\\paddle/fluid/operators/load_op.h:41)\r\n  [operator < load > error]\r\n\r\ndet模型转换以后调用没问题，Windows10平台",
        "state": "closed",
        "user": "xiahongjin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-08T08:15:05+00:00",
        "updated_at": "2024-03-05T06:49:19+00:00",
        "closed_at": "2024-03-05T06:49:19+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "xiahongjin",
            "TeslaZhao",
            "HexToString",
            "xiahongjin",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1090,
        "title": "Aistudio教程未公开",
        "body": "https://aistudio.baidu.com/aistudio/projectdetail/1550674\r\n显示项目未公开",
        "state": "closed",
        "user": "paopjian",
        "closed_by": "paopjian",
        "created_at": "2021-03-17T06:51:09+00:00",
        "updated_at": "2021-03-23T07:58:49+00:00",
        "closed_at": "2021-03-23T07:58:49+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1085,
        "title": "使用CPU教程运行失败",
        "body": "命令:\r\n\r\ndocker pull registry.baidubce.com/paddlepaddle/serving:0.5.0-devel\r\ndocker run -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:0.5.0-devel\r\ndocker exec -it test bash\r\n\r\n报错:\r\nError response from daemon: Container 7ac0574a7f984b2fc6ce67ef2f07ec59772ff3aa3066182f0bac379d0712b2d1 is not running\r\n查看日志:\r\n/bin/sh: 1: source: not found\r\n\r\n",
        "state": "closed",
        "user": "paopjian",
        "closed_by": "paopjian",
        "created_at": "2021-03-15T06:22:30+00:00",
        "updated_at": "2021-03-17T01:58:15+00:00",
        "closed_at": "2021-03-17T01:58:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "paopjian",
            "TeslaZhao",
            "paopjian",
            "bjjwwang"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1091,
        "title": "java教程服务端启动执行错误",
        "body": "https://github.com/PaddlePaddle/Serving/blob/develop/java/README_CN.md\r\n已完成\r\n```\r\ndocker pull registry.baidubce.com/paddlepaddle/serving:0.5.0-java\r\ndocker run --rm -dit --name java_serving registry.baidubce.com/paddlepaddle/serving:0.5.0-java\r\ndocker exec -it java_serving bash\r\ncd Serving/java\r\nmvn compile\r\nmvn install\r\ncd examples\r\nmvn compile\r\nmvn install\r\npip install paddle-serving-client==0.5.0\r\npip install paddle-serving-server==0.5.0 # CPU\r\npip install paddle-serving-app==0.3.0\r\ncd ../../python/examples/fit_a_line\r\nsh get_data.sh\r\n```\r\n执行\r\npython -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9393 --use_multilang &\r\n发生错误\r\n![image](https://user-images.githubusercontent.com/20377352/111436256-acdbd980-86f9-11eb-955a-2e9838d2a9cb.png)\r\n",
        "state": "closed",
        "user": "paopjian",
        "closed_by": "paopjian",
        "created_at": "2021-03-17T08:21:56+00:00",
        "updated_at": "2021-03-18T07:02:16+00:00",
        "closed_at": "2021-03-18T07:02:16+00:00",
        "comments_count": [
            "HexToString"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1094,
        "title": "raspberry 4b 树莓派4B docker下编译出错",
        "body": "因为下载的cpu镜像架构不同无法运行，所以使用文件 Dockerfile 构建镜像\r\n参考：https://github.com/PaddlePaddle/Serving/blob/develop/doc/DOCKER_IMAGES_CN.md \r\n`\r\npi@raspberrypi:~/Serving/tools $ mkdir armv7\r\npi@raspberrypi:~/Serving/tools $ cp Dockerfile.devel ./armv7/ \r\npi@raspberrypi:~/Serving/tools $ cd armv7/ \r\n`\r\n\r\n修改Dockerfile.devel\r\nENV WITH_GPU=${WITH_GPU:-OFF}\r\nENV WITH_AVX=${WITH_AVX:-OFF}\r\n\r\n`\r\npi@raspberrypi:~/Serving/tools $ sudo docker build -f Dockerfile.devel -t paddle-serving.armv7-pi4:latest .\r\n`\r\n\r\n`\r\nSending build context to Docker daemon  11.78kB\r\nStep 1/71 : FROM hub.baidubce.com/ctr/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n ---> 5978609a60c5\r\nStep 2/71 : MAINTAINER PaddlePaddle Authors <paddle-dev@baidu.com>\r\n ---> Using cache\r\n ---> fa640de9e332\r\nStep 3/71 : ARG WITH_GPU\r\n ---> Using cache\r\n ---> 7110618d9a45\r\nStep 4/71 : ARG WITH_AVX\r\n ---> Using cache\r\n ---> 9cdb4b048a19\r\nStep 5/71 : ENV WITH_GPU=${WITH_GPU:-ON}\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm/v7) and no specific platform was requested\r\n ---> Running in c8fafac96d1f\r\nRemoving intermediate container c8fafac96d1f\r\n ---> 1d4ac76be9ef\r\nStep 6/71 : ENV WITH_AVX=${WITH_AVX:-ON}\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm/v7) and no specific platform was requested\r\n ---> Running in 1e9182cbb019\r\nRemoving intermediate container 1e9182cbb019\r\n ---> e2add42a6d84\r\nStep 7/71 : ENV HOME /root\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm/v7) and no specific platform was requested\r\n ---> Running in eeea6d370e01\r\nRemoving intermediate container eeea6d370e01\r\n ---> 7c42f998b26f\r\nStep 8/71 : COPY tools/dockerfile/scripts/root/ /root/\r\nCOPY failed: file not found in build context or excluded by .dockerignore: stat tools/dockerfile/scripts/root/: file does not exist\r\n`\r\n请问如何继续构建docker镜像呢？",
        "state": "closed",
        "user": "60999",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-21T18:47:16+00:00",
        "updated_at": "2024-03-05T06:49:20+00:00",
        "closed_at": "2024-03-05T06:49:20+00:00",
        "comments_count": [
            "github-actions[bot]",
            "60999",
            "TeslaZhao"
        ],
        "labels": [
            "硬件适配"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1095,
        "title": "jetson nano JetPack 4.5启动 paddle-serving Docker镜像失败",
        "body": "参考\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/README_CN.md#%E5%AE%89%E8%A3%85\r\n\r\npython3 summary_env.py\r\n****************************************\r\nPaddle version: None\r\nPaddle With CUDA: None\r\n\r\nOS: Ubuntu 18.04\r\nPython version: 3.6.9\r\n\r\nCUDA version: None\r\ncuDNN version: None.None.None\r\nNvidia driver version: None\r\n\r\nR32 (release), REVISION: 5.0, GCID: 25531747, BOARD: t210ref, EABI: aarch64, DATE: Fri Jan 15 22:55:35 UTC 2021\r\n\r\n$ dpkg -l | grep TensorRT\r\nii  graphsurgeon-tf                            7.1.3-1+cuda10.2                                 arm64        GraphSurgeon for TensorRT package\r\nii  libnvinfer-bin                             7.1.3-1+cuda10.2                                 arm64        TensorRT binaries\r\nii  libnvinfer-dev                             7.1.3-1+cuda10.2                                 arm64        TensorRT development libraries and headers\r\nii  libnvinfer-doc                             7.1.3-1+cuda10.2                                 all          TensorRT documentation\r\nii  libnvinfer-plugin-dev                      7.1.3-1+cuda10.2                                 arm64        TensorRT plugin libraries\r\nii  libnvinfer-plugin7                         7.1.3-1+cuda10.2                                 arm64        TensorRT plugin libraries\r\nii  libnvinfer-samples                         7.1.3-1+cuda10.2                                 all          TensorRT samples\r\nii  libnvinfer7                                7.1.3-1+cuda10.2                                 arm64        TensorRT runtime libraries\r\nii  libnvonnxparsers-dev                       7.1.3-1+cuda10.2                                 arm64        TensorRT ONNX libraries\r\nii  libnvonnxparsers7                          7.1.3-1+cuda10.2                                 arm64        TensorRT ONNX libraries\r\nii  libnvparsers-dev                           7.1.3-1+cuda10.2                                 arm64        TensorRT parsers libraries\r\nii  libnvparsers7                              7.1.3-1+cuda10.2                                 arm64        TensorRT parsers libraries\r\nii  nvidia-container-csv-tensorrt              7.1.3.0-1+cuda10.2                               arm64        Jetpack TensorRT CSV file\r\nii  python-libnvinfer                          7.1.3-1+cuda10.2                                 arm64        Python bindings for TensorRT\r\nii  python-libnvinfer-dev                      7.1.3-1+cuda10.2                                 arm64        Python development package for TensorRT\r\nii  python3-libnvinfer                         7.1.3-1+cuda10.2                                 arm64        Python 3 bindings for TensorRT\r\nii  python3-libnvinfer-dev                     7.1.3-1+cuda10.2                                 arm64        Python 3 development package for TensorRT\r\nii  tensorrt                                   7.1.3.0-1+cuda10.2                               arm64        Meta package of TensorRT\r\nii  uff-converter-tf                           7.1.3-1+cuda10.2                                 arm64        UFF converter for TensorRT package\r\n\r\n\r\n`# 启动 GPU Docker\r\nsudo nvidia-docker pull registry.baidubce.com/paddlepaddle/serving:0.5.0-cuda10.2-cudnn8-devel\r\nsudo nvidia-docker run -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:0.5.0-cuda10.2-cudnn8-devel bash`\r\n\r\n输出：\r\nsrc: /usr/lib/aarch64-linux-gnu/libcublasLt.so, src_lnk: libcublasLt.so.10, dst: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/lib/aarch64-linux-gnu/libcublasLt.so, dst_lnk: libcublasLt.so.10\r\nsrc: /usr/lib/aarch64-linux-gnu/libnvblas.so, src_lnk: libnvblas.so.10, dst: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/lib/aarch64-linux-gnu/libnvblas.so, dst_lnk: libnvblas.so.10\r\nsrc: /usr/lib/aarch64-linux-gnu/libcublas.so, src_lnk: libcublas.so.10, dst: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/lib/aarch64-linux-gnu/libcublas.so, dst_lnk: libcublas.so.10\r\nsrc: /usr/lib/aarch64-linux-gnu/libcublasLt.so.10, src_lnk: libcublasLt.so.10.2.2.89, dst: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/lib/aarch64-linux-gnu/libcublasLt.so.10, dst_lnk: libcublasLt.so.10.2.2.89\r\nsrc: /usr/lib/aarch64-linux-gnu/libcublas.so.10, src_lnk: libcublas.so.10.2.2.89, dst: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/lib/aarch64-linux-gnu/libcublas.so.10, dst_lnk: libcublas.so.10.2.2.89\r\n, stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda>=10.2 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 --pid=8066 /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged]\r\nnvidia-container-cli: mount error: stat failed: /var/lib/docker/overlay2/db61f16ed7640ec030f082603d17482379694357e008b0101493615a0b0e4d69/merged/usr/local/cuda-10.2/compat/libcuda.so.440.118.02: no such file or directory: unknown.\r\n\r\n启动也失败，编译也失败，请问能否提供一个镜像或者Dockerfile，谢谢!\r\n",
        "state": "closed",
        "user": "60999",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-21T19:12:12+00:00",
        "updated_at": "2024-04-16T09:05:35+00:00",
        "closed_at": "2024-04-16T09:05:35+00:00",
        "comments_count": [
            "TeslaZhao",
            "60999"
        ],
        "labels": [
            "硬件适配"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1096,
        "title": "jetson nano  docker下编译出错",
        "body": "$ docker build -f Dockerfile.cuda10.2-cudnn8.devel -t paddle-serving.armv8-nano:latest .\r\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile.cuda10.2-cudnn8.devel&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&shmsize=0&t=paddle-serving.armv8-nano%3Alatest&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied\r\nfai@fai-desktop:~/Serving/tools/armv8$ sudo docker build -f Dockerfile.cuda10.2-cudnn8.devel -t paddle-serving.armv8-nano:latest .\r\nSending build context to Docker daemon  8.704kB\r\nStep 1/62 : FROM hub.baidubce.com/ctr/cuda:10.2-cudnn8-devel-ubuntu16.04\r\n10.2-cudnn8-devel-ubuntu16.04: Pulling from ctr/cuda\r\nbe8ec4e48d7f: Already exists \r\n33b8b485aff0: Already exists \r\nd887158cc58c: Already exists \r\n05895bb28c18: Already exists \r\n7cc9fc34ed5e: Already exists \r\nc001a8e57e73: Already exists \r\n993a4be22922: Already exists \r\nc4187999c3bc: Already exists \r\n6a0cbff9712e: Already exists \r\nec7d68937b6b: Already exists \r\nDigest: sha256:3bdcf059a133ecf41d064239f821afdd7981570b697727ddc59ed378fad5c442\r\nStatus: Downloaded newer image for hub.baidubce.com/ctr/cuda:10.2-cudnn8-devel-ubuntu16.04\r\n ---> 49868fe0d389\r\nStep 2/62 : MAINTAINER PaddlePaddle Authors <paddle-dev@baidu.com>\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in 8749a9c6dfb5\r\nRemoving intermediate container 8749a9c6dfb5\r\n ---> daed91230b87\r\nStep 3/62 : ARG WITH_GPU\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in be2ad1defb14\r\nRemoving intermediate container be2ad1defb14\r\n ---> 4c6fa83de4db\r\nStep 4/62 : ARG WITH_AVX\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in 4239f079a906\r\nRemoving intermediate container 4239f079a906\r\n ---> 50fefa529989\r\nStep 5/62 : ENV WITH_GPU=${WITH_GPU:-ON}\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in a3c13d94b232\r\nRemoving intermediate container a3c13d94b232\r\n ---> e4dbfedf9776\r\nStep 6/62 : ENV WITH_AVX=${WITH_AVX:-ON}\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in f957e3d17d2c\r\nRemoving intermediate container f957e3d17d2c\r\n ---> 2dd4dcd6ae71\r\nStep 7/62 : ENV HOME /root\r\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n ---> Running in 9e0134bed03c\r\nRemoving intermediate container 9e0134bed03c\r\n ---> 426de4b55cfe\r\nStep 8/62 : COPY tools/dockerfile/scripts/root/ /root/\r\nCOPY failed: file not found in build context or excluded by .dockerignore: stat tools/dockerfile/scripts/root/: file does not exist\r\n\r\n看起来是架构不同，请问如何解决呢？",
        "state": "closed",
        "user": "60999",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-21T19:17:10+00:00",
        "updated_at": "2024-03-05T06:49:20+00:00",
        "closed_at": "2024-03-05T06:49:20+00:00",
        "comments_count": [
            "TeslaZhao",
            "60999",
            "ted8201"
        ],
        "labels": [
            "硬件适配"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1097,
        "title": "paddle_serving_client.Client()  创建实例失败",
        "body": "在执行以下代码时报错\r\nfrom paddle_serving_client import Client\r\n**_client = Client()_**\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_client/__init__.py\", line 139, in __init__\r\n    from .serving_client import PredictorRes\r\nImportError: libcrypto.so.10: cannot open shared object file: No such file or directory",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-03-22T02:05:51+00:00",
        "updated_at": "2021-05-21T08:14:43+00:00",
        "closed_at": "2021-05-21T08:14:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zouxiaoshi",
            "zouxiaoshi",
            "TeslaZhao",
            "zouxiaoshi",
            "TeslaZhao",
            "zouxiaoshi",
            "TeslaZhao",
            "zouxiaoshi",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1103,
        "title": "serving上部署自己的模型，跑服务器端没报错，HTTP调用的时候报错",
        "body": "serving版本：\r\npaddle-serving-app                 0.3.0\r\npaddle-serving-client              0.5.0\r\npaddle-serving-server              0.5.0\r\n\r\n服务器端错误：\r\nD:\\ProgramData\\Anaconda3\\lib\\site-packages\\setuptools\\depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\nW0326 16:10:42.209364  7192 analysis_predictor.cc:1145] Deprecated. Please use CreatePredictor instead.\r\nI0326 16:10:42.209364  7192 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0326 16:10:42.227408  7192 analysis_predictor.cc:574] ir_optim is turned off, no IR pass will be executed\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\n[2021-03-26 16:10:42,242] ERROR in app: Exception on / [POST]\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1945, in full_dispatch_request\r\n    self.try_trigger_before_first_request_functions()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1993, in try_trigger_before_first_request_functions\r\n    func()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 188, in init\r\n    self._launch_local_predictor()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 202, in _launch_local_predictor\r\n    \"{}\".format(self.model_config), use_gpu=False)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_app\\local_predict.py\", line 138, in load_model_config\r\n    self.predictor = create_paddle_predictor(config)\r\nRuntimeError: (Unavailable) Load operator fail to open file yolov3_mobilenet_v1_menghuan/detection_server//conv2_1_sep_bn_variance, please check whether the model file is complete or damaged.\r\n  [Hint: Expected static_cast<bool>(fin) == true, but received static_cast<bool>(fin):0 != true:1.] (at D:\\v2.0.1\\paddle\\paddle/fluid/operators/load_op.h:41)\r\n  [operator < load > error]\r\n客户端错误：\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 331, in _error_catcher\r\n    yield\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 413, in read\r\n    data = self._fp.read(amt)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 447, in read\r\n    n = self.readinto(b)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 491, in readinto\r\n    n = self.fp.readinto(b)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 589, in readinto\r\n    return self._sock.recv_into(b)\r\nConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 750, in generate\r\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 465, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 430, in read\r\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 349, in _error_catcher\r\n    raise ProtocolError('Connection broken: %r' % e, e)\r\nurllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)\", ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Detection_Client.py\", line 24, in <module>\r\n    r = requests.post(url=url, headers=headers, data=json.dumps(data))\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 116, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 60, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 524, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 677, in send\r\n    r.content\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 828, in content\r\n    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 753, in generate\r\n    raise ChunkedEncodingError(e)\r\nrequests.exceptions.ChunkedEncodingError: (\"Connection broken: ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)\", ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))",
        "state": "closed",
        "user": "pipishaw",
        "closed_by": "pipishaw",
        "created_at": "2021-03-26T08:28:29+00:00",
        "updated_at": "2021-03-26T08:30:05+00:00",
        "closed_at": "2021-03-26T08:30:05+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1099,
        "title": "win10启动报错",
        "body": "(paddlepaddle) G:\\ML Projects\\Serving\\python\\examples\\grpc_impl_example\\fit_a_line>python -m paddle_serving_server_gpu.serve --model uci_housing_model --thread 10 --port 9393 --gpu_id 0 --name uci\r\nThis API will be deprecated later. Please do not use it\r\nThis API will be deprecated later. Please do not use it\r\nThis API will be deprecated later. Please do not use it\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\nThis API will be deprecated later. Please do not use it\r\nweb service address:\r\nhttp://192.168.1.2:9393/uci/prediction\r\nTraceback (most recent call last):\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\paddle_serving_server_gpu\\serve.py\", line 244, in <module>\r\n    web_service.run_rpc_service()\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\paddle_serving_server_gpu\\web_service.py\", line 239, in run_rpc_service\r\n    p.start()\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'server_configure_pb2.InferServiceConf'>: it's not the same object as server_configure_pb2.InferServiceConf\r\n\r\n(paddlepaddle) G:\\ML Projects\\Serving\\python\\examples\\grpc_impl_example\\fit_a_line>Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\spawn.py\", line 99, in spawn_main\r\n    new_handle = reduction.steal_handle(parent_pid, pipe_handle)\r\n  File \"C:\\anaconda3\\envs\\paddlepaddle\\lib\\multiprocessing\\reduction.py\", line 82, in steal_handle\r\n    _winapi.PROCESS_DUP_HANDLE, False, source_pid)\r\nOSError: [WinError 87] 参数错误。",
        "state": "closed",
        "user": "claudehotline",
        "closed_by": "claudehotline",
        "created_at": "2021-03-24T15:07:28+00:00",
        "updated_at": "2021-03-26T13:19:23+00:00",
        "closed_at": "2021-03-26T13:19:23+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1098,
        "title": "训练时用save_persistables保存模型，如何进行serving模型转换？",
        "body": "请问我用paddle训练时，是用save_persistables进行全部持久性变量保存，可以用paddle serving进行吗？我看训练好的模型都和用save_inference_model的不一样。 ",
        "state": "closed",
        "user": "bultiful",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-23T07:45:52+00:00",
        "updated_at": "2024-04-16T09:05:35+00:00",
        "closed_at": "2024-04-16T09:05:35+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "bultiful",
            "bjjwwang",
            "bjjwwang",
            "bultiful"
        ],
        "labels": [
            "模型保存与转换"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1100,
        "title": "ernie_tiny部署paddle serving后，curl返回 request that this server could not understand",
        "body": "paddler serving部署文本匹配的ernie_tiny后，curl -H \"Content-Type:application/json\" -X POST -d '{\"Data\":[['世界上什么东西最小', '世界上什么东西最小？']] , \"fetch\":[\"prediction\"]}' http://172.0.0.1:9292/ernie/prediction，返回\r\n  400 Bad Request\r\n  Bad Request\r\n  The browser (or proxy) sent a request that this server could not understand.\r\n\r\n请问是传入参数格式不对吗？还是操作步骤不正确呢？谢谢！\r\n完整的操作步骤如下：\r\n1. 获取docker 镜像 registry.baidubce.com/paddlepaddle/serving:0.5.0-cuda10.2-cudnn8-devel\r\n2. 容器内依次安装\tpaddle-serving-server-gpu==0.5.0.post102，paddlepaddle-gpu==2.0.0 和paddle_serving_client\r\n3. 将[ernin_tiny](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_matching/sentence_transformers)的训练模型转为paddle serving模型，ernie_tiny_model包含如下文件\r\n      total 350716\r\n      drwxrwxr-x 2 media media      4096 Mar 24 09:43 ./\r\n      drwxrwxr-x 4 media media      4096 Mar 24 09:44 ../\r\n      -rw-rw-r-- 1 media media    656906 Mar 24 09:43 __model__\r\n      -rw-rw-r-- 1 media media 358454636 Mar 24 09:43 __params__\r\n      -rw-rw-r-- 1 media media       356 Mar 24 09:43 serving_server_conf.prototxt\r\n      -rw-rw-r-- 1 media media       147 Mar 24 09:43 serving_server_conf.stream.prototxt\r\n\r\n4. 将步骤#3中生成的ernie_tiny_model和ernie_tiny_client拷贝到docker/home/workspace/Serving/python/examples/ernie_tiny 中\r\n5. 使用uwsgi启动服务\r\n   from paddle_serving_server.web_service import WebService\r\n  uci_service = WebService(name=\"ernie\")\r\n  uci_service.load_model_config(\"./ernie_tiny_model\")\r\n  uci_service.prepare_server(workdir=\"./workdir\", port=int(9500), device=\"cpu\")\r\n  uci_service.run_rpc_service()\r\n  app_instance = uci_service.get_app_instance()\r\n6. 启动服务 uwsgi --http :9292 --module uwsgi_service:app_instance\r\n",
        "state": "closed",
        "user": "Nicole1983",
        "closed_by": "Nicole1983",
        "created_at": "2021-03-25T01:35:02+00:00",
        "updated_at": "2021-03-26T04:25:40+00:00",
        "closed_at": "2021-03-26T04:25:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Nicole1983",
            "Nicole1983"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1102,
        "title": "windows环境，serving上部署自己的模型，跑服务器端没报错，HTTP调用的时候报错",
        "body": "serving版本：\r\npaddle-serving-app                 0.3.0\r\npaddle-serving-client              0.5.0\r\npaddle-serving-server              0.5.0\r\n\r\n服务器端错误：\r\nD:\\ProgramData\\Anaconda3\\lib\\site-packages\\setuptools\\depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\nW0326 16:10:42.209364  7192 analysis_predictor.cc:1145] Deprecated. Please use CreatePredictor instead.\r\nI0326 16:10:42.209364  7192 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0326 16:10:42.227408  7192 analysis_predictor.cc:574] ir_optim is turned off, no IR pass will be executed\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\n[2021-03-26 16:10:42,242] ERROR in app: Exception on / [POST]\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1945, in full_dispatch_request\r\n    self.try_trigger_before_first_request_functions()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1993, in try_trigger_before_first_request_functions\r\n    func()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 188, in init\r\n    self._launch_local_predictor()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 202, in _launch_local_predictor\r\n    \"{}\".format(self.model_config), use_gpu=False)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddle_serving_app\\local_predict.py\", line 138, in load_model_config\r\n    self.predictor = create_paddle_predictor(config)\r\nRuntimeError: (Unavailable) Load operator fail to open file yolov3_mobilenet_v1_menghuan/detection_server//conv2_1_sep_bn_variance, please check whether the model file is complete or damaged.\r\n  [Hint: Expected static_cast<bool>(fin) == true, but received static_cast<bool>(fin):0 != true:1.] (at D:\\v2.0.1\\paddle\\paddle/fluid/operators/load_op.h:41)\r\n  [operator < load > error]\r\n客户端错误：\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 331, in _error_catcher\r\n    yield\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 413, in read\r\n    data = self._fp.read(amt)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 447, in read\r\n    n = self.readinto(b)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 491, in readinto\r\n    n = self.fp.readinto(b)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 589, in readinto\r\n    return self._sock.recv_into(b)\r\nConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 750, in generate\r\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 465, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 430, in read\r\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 349, in _error_catcher\r\n    raise ProtocolError('Connection broken: %r' % e, e)\r\nurllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)\", ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Detection_Client.py\", line 24, in <module>\r\n    r = requests.post(url=url, headers=headers, data=json.dumps(data))\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 116, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 60, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 524, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 677, in send\r\n    r.content\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 828, in content\r\n    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 753, in generate\r\n    raise ChunkedEncodingError(e)\r\nrequests.exceptions.ChunkedEncodingError: (\"Connection broken: ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)\", ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None))",
        "state": "closed",
        "user": "pipishaw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-26T08:25:37+00:00",
        "updated_at": "2024-04-16T09:05:36+00:00",
        "closed_at": "2024-04-16T09:05:36+00:00",
        "comments_count": [
            "github-actions[bot]",
            "pipishaw",
            "TeslaZhao",
            "meishitouzhele",
            "pipishaw"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1104,
        "title": "AiStudio报错",
        "body": "is:issue is:open /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_server/serving-cpu-avx-openblas-0.5.0/serving: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.11' not found (required by /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_server/serving-cpu-avx-openblas-0.5.0/serving) /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_server/serving-cpu-avx-openblas-0.5.0/serving: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_server/serving-cpu-avx-openblas-0.5.0/serving)",
        "state": "closed",
        "user": "lliony",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-28T06:28:30+00:00",
        "updated_at": "2024-03-05T06:49:21+00:00",
        "closed_at": "2024-03-05T06:49:21+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "教程"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1107,
        "title": "使用Paddle Serving部署Ernie-tiny返回结果精度不理想",
        "body": "在本地测试文本相似度匹配，准确度挺不错的；\r\n但相同的模型部署之后，模型计算出的特征值和部署前稍有不同，准确度和原来差很多。\r\n\r\n我使用的是以下命令导出模型：\r\n`\r\npython3 -m paddle_serving_client.convert --dirname . --model_filename model.pdmodel --params_filename model.pdiparams --serving_server serving_server --serving_client serving_client\r\n`\r\n导出文件：\r\n![image](https://user-images.githubusercontent.com/76727820/112817921-8f255180-90b5-11eb-96e9-b0ebb8a82441.png)\r\n\r\n模型导出后，对配置文件做了如下改动：\r\n![image](https://user-images.githubusercontent.com/76727820/112816174-b5e28880-90b3-11eb-8a07-b3579d91d3ad.png)\r\n\r\n通过uwsgi启动服务\r\n`uwsgi --http :9292 --module nlp_service:app_instance`\r\n\r\n最后通过模型输出特征进行二分类返回结果。\r\n\r\n请问是哪里不正确或有遗漏呢，非常感谢！\r\n",
        "state": "closed",
        "user": "deep-rooteddz",
        "closed_by": "deep-rooteddz",
        "created_at": "2021-03-29T09:39:23+00:00",
        "updated_at": "2021-04-09T02:23:39+00:00",
        "closed_at": "2021-04-09T02:23:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "deep-rooteddz",
            "bjjwwang",
            "deep-rooteddz",
            "deep-rooteddz"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1109,
        "title": "部署HTTP服务后图片的post格式是什么？",
        "body": "请问部署好HTTP服务后，我要进行POST的数据里有图片，那么图片的数据格式是base64还是其他什么呢？能不能给个示例呢？官方给的示例是post json的。麻烦解答谢谢。\r\n\r\n`curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://127.0.0.1:9292/uci/prediction`\r\n这个示例要怎么改才能上传图片？",
        "state": "closed",
        "user": "thugbobby",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-31T08:34:09+00:00",
        "updated_at": "2024-03-05T06:49:22+00:00",
        "closed_at": "2024-03-05T06:49:22+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1110,
        "title": "win10环境 docker 安装 paddle serving，是否能使用T4 Cuda11 驱动？",
        "body": "win10环境 docker 安装 paddle serving，是否能使用T4 Cuda11  GPU 驱动？",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-01T04:00:52+00:00",
        "updated_at": "2024-04-16T09:05:37+00:00",
        "closed_at": "2024-04-16T09:05:37+00:00",
        "comments_count": [
            "TeslaZhao",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1115,
        "title": "win10原生系统安装paddle serving使用案例的时候提示wget问题，需要怎么设置才能下载？",
        "body": null,
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-06T10:02:57+00:00",
        "updated_at": "2024-03-05T06:49:23+00:00",
        "closed_at": "2024-03-05T06:49:23+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1118,
        "title": "win10 部署serving后，运行examples下的ocr案例没有问题，想运行图像分类案例imagenet报错如下",
        "body": "  File \".\\resnet50_web_service.py\", line 78\r\n    image_service.run_web_service()\r\n                ^\r\nSyntaxError: invalid syntax\r\n能提供一个运行examples下图像分类的过程吗？，想运行通过后 发布下自己的图像分类model（在centos的docker下能正常的model，想在win10下测试）感谢",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-07T04:00:57+00:00",
        "updated_at": "2024-04-16T09:05:38+00:00",
        "closed_at": "2024-04-16T09:05:38+00:00",
        "comments_count": [
            "meishitouzhele",
            "HexToString",
            "meishitouzhele",
            "meishitouzhele",
            "meishitouzhele",
            "meishitouzhele",
            "HexToString",
            "meishitouzhele",
            "HexToString",
            "HexToString",
            "meishitouzhele",
            "HexToString",
            "meishitouzhele",
            "meishitouzhele",
            "meishitouzhele",
            "HexToString",
            "meishitouzhele",
            "HexToString",
            "meishitouzhele"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1116,
        "title": "PaddlePaddleServing是否支持TLS connection between parameter servers and workers",
        "body": "你好，根据https://github.com/PaddlePaddle/Serving/blob/develop/doc/DESIGN_DOC_CN.md 此文档可知 C++ Serving设计模型用better-rpc进行底层的通信，Python Pipeline Serving模型采用gRPC和gPRC gateway通信，而better-rpc和gRPC都支持TLS加密，那是否等同于说明PaddlePaddleServing是支持TLS加密连接的呢？如果PaddleServing有关于TLS连接的介绍文档，可否贴一下？谢谢！",
        "state": "closed",
        "user": "YueWang1996",
        "closed_by": "YueWang1996",
        "created_at": "2021-04-06T10:23:20+00:00",
        "updated_at": "2021-04-12T08:56:57+00:00",
        "closed_at": "2021-04-12T08:56:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "YueWang1996"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1121,
        "title": "咨询一下，如何在paddleserving里面设置并发执行的模块？",
        "body": "如题，没找到相关的并发处理的位置，感谢解答。",
        "state": "closed",
        "user": "allenxln",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-08T12:43:50+00:00",
        "updated_at": "2024-03-05T06:49:24+00:00",
        "closed_at": "2024-03-05T06:49:24+00:00",
        "comments_count": [
            "github-actions[bot]",
            "meishitouzhele",
            "TeslaZhao",
            "meishitouzhele",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1122,
        "title": "请问如何修改 batch size 参数和引擎实例数？",
        "body": "就类似于 Triton 的 dynamic batch 和 instance group",
        "state": "closed",
        "user": "zh0ngtian",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-09T02:51:16+00:00",
        "updated_at": "2024-03-05T06:49:25+00:00",
        "closed_at": "2024-03-05T06:49:25+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "zh0ngtian",
            "TeslaZhao",
            "zh0ngtian",
            "TeslaZhao",
            "zh0ngtian",
            "BraveLii"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1123,
        "title": "执行yolov3模型推理时报错",
        "body": "利用paddledetection训练了yolov3的模型，然后转化为paddle serving的模型\r\n模型链接：https://pan.baidu.com/s/1iETMZ8b6hLppFaT33Oc3pg \r\n提取码：14ea \r\n\r\n`test_client.py`文件\r\n```\r\nimport sys\r\nimport numpy as np\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport cv2\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Resize(\r\n        (608, 608), interpolation=cv2.INTER_LINEAR), Div(255.0), Transpose(\r\n            (2, 0, 1))\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\", [608, 608])\r\nclient = Client()\r\n\r\nclient.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9393'])\r\n\r\nim = preprocess(sys.argv[1])\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n        \"im_size\": np.array(list(im.shape[1:])),\r\n    },\r\n    fetch=[\"multiclass_nms_0.tmp_0\"])\r\nfetch_map[\"image\"] = sys.argv[1]\r\npostprocess(fetch_map)\r\n```\r\n报错信息\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0409 04:27:36.250483 29111 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nTraceback (most recent call last):\r\n  File \"../../deploy/serving/test_client.py\", line 40, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/root/miniconda3/lib/python3.7/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/root/miniconda3/lib/python3.7/site-packages/paddle_serving_app/reader/image_reader.py\", line 350, in _get_bbox_result\r\n    bbox_results = self._bbox2out([result], clsid2catid, is_bbox_normalized)\r\n  File \"/root/miniconda3/lib/python3.7/site-packages/paddle_serving_app/reader/image_reader.py\", line 313, in _bbox2out\r\n    catid = (clsid2catid[int(clsid)])\r\nKeyError: 5\r\n```",
        "state": "closed",
        "user": "Huihuihh",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-09T08:41:11+00:00",
        "updated_at": "2024-03-05T06:49:26+00:00",
        "closed_at": "2024-03-05T06:49:26+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "Huihuihh",
            "HexToString",
            "Huihuihh",
            "HexToString",
            "Huihuihh",
            "HexToString"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1129,
        "title": "示例里基本上都是rpc服务的，有没有http服务的？",
        "body": "使用paddleDetection训练了ppyolo，，请问怎么使用Serving部署成http服务呢？",
        "state": "closed",
        "user": "Lie-huo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-12T16:31:56+00:00",
        "updated_at": "2024-03-05T06:49:27+00:00",
        "closed_at": "2024-03-05T06:49:27+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Lie-huo",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1125,
        "title": "paddle_serving java遇到的问题",
        "body": "我根据java文档https://github.com/PaddlePaddle/Serving/tree/develop/java\r\n放在win中idea打开，并且maven导入了\r\n![image](https://user-images.githubusercontent.com/48110081/114295645-1d97cb00-9ad9-11eb-91c0-6ee0b9a8a425.png)\r\n但是在执行的时候报错，错误提示是\r\n![image](https://user-images.githubusercontent.com/48110081/114295669-3dc78a00-9ad9-11eb-9367-f98ee07765ac.png)\r\n\r\n![image](https://user-images.githubusercontent.com/48110081/114295683-559f0e00-9ad9-11eb-8586-c357eb9e39f8.png)\r\n",
        "state": "closed",
        "user": "chengxurensheng666",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-11T07:19:43+00:00",
        "updated_at": "2024-03-05T06:49:27+00:00",
        "closed_at": "2024-03-05T06:49:27+00:00",
        "comments_count": [
            "HexToString",
            "chengxurensheng666",
            "HexToString",
            "chengxurensheng666",
            "TeslaZhao",
            "ycw4713"
        ],
        "labels": [
            "多语言client"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1134,
        "title": "在Pipeline Serving模式下加载自己用Paddleocr训练的模型无法启动服务",
        "body": "已跑通paddle/examples/pieline/ocr官方样例的ocr pieline的部署。\r\n然后在此基础上，将自己训练的的预测模型，通过paddle_serving_client.convert的模块完成转换。\r\n预测模型文件如下\r\n![image](https://user-images.githubusercontent.com/31469777/114691401-3824ab00-9d4a-11eb-9809-a81b2a193f37.png)\r\n转换后的serving文件列表如下\r\n![image](https://user-images.githubusercontent.com/31469777/114691553-61453b80-9d4a-11eb-9e37-2afaf8e3d439.png)\r\n最后开启服务是显示是如下结果\r\n![image](https://user-images.githubusercontent.com/31469777/114691662-7d48dd00-9d4a-11eb-954d-27e84139ae75.png)\r\n请问是哪的问题",
        "state": "closed",
        "user": "huangch1024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-14T09:55:04+00:00",
        "updated_at": "2024-04-16T09:05:39+00:00",
        "closed_at": "2024-04-16T09:05:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bytes-lost",
            "huangch1024"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1135,
        "title": "部署服务时出现，unrecognized arguments: --gpu_ids 0",
        "body": "在ai studio上运行python/examples/detection/ppyolo_r50vd_dcn_1x_coco时，执行python -m paddle_serving_server.serve --model serving_server --port 9494 --gpu_ids 0出现serve: error: unrecognized arguments: --gpu_ids 0，这是什么问题",
        "state": "closed",
        "user": "Lie-huo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-14T10:21:03+00:00",
        "updated_at": "2024-03-05T06:49:28+00:00",
        "closed_at": "2024-03-05T06:49:28+00:00",
        "comments_count": [
            "HexToString"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1137,
        "title": "热更新",
        "body": "模型文件结构如下：\r\n|-serving_server\r\n|-- __model__\r\n|-- __params__\r\n|-- serving_server_conf.protetxt\r\n|-- serving_server_conf.stream.protetxt\r\n通过如下方式启动模型：\r\npython -m paddle_serving_server_gpu.serve --model serving_server --port 9393 --gpu_ids 0\r\n看了 paddle_server_server monitor中的代码，热更新实际上是 \r\n1. 替换模型文件 serving_server 2. 在serving_server中创建新的 fluid_time_stamp。\r\n所以，核心还是通过time_stamp来判断是否应该更新模型。\r\n我在serving_server文件夹下尝试创建新的fluid_time_stamp后，并没有发生更新行为，不知是为什么。",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-04-15T10:38:55+00:00",
        "updated_at": "2021-04-30T07:15:57+00:00",
        "closed_at": "2021-04-30T07:15:57+00:00",
        "comments_count": [
            "TeslaZhao",
            "zouxiaoshi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1140,
        "title": "unrecognized arguments: --gpu_id 0",
        "body": "使用这两个文档启动发生失败，发生错误的在第二个文档的步骤\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/README_CN.md\r\nhttps://github.com/PaddlePaddle/Serving/tree/develop/python/examples/ocr\r\n\r\n(ocr) [root@jj ocr]$ python3 -m paddle_serving_server.serve --model ocr_det_model --port 9293 --gpu_id 0\r\nusage: serve [-h] [--thread THREAD] [--model MODEL] [--port PORT]\r\n             [--name NAME] [--workdir WORKDIR] [--device DEVICE]\r\n             [--mem_optim_off] [--ir_optim] [--use_mkl]\r\n             [--max_body_size MAX_BODY_SIZE] [--use_encryption_model]\r\n             [--use_multilang] [--product_name PRODUCT_NAME]\r\n             [--container_id CONTAINER_ID]\r\nserve: error: unrecognized arguments: --gpu_id 0",
        "state": "closed",
        "user": "xealml",
        "closed_by": "xealml",
        "created_at": "2021-04-16T06:39:07+00:00",
        "updated_at": "2021-04-19T09:23:01+00:00",
        "closed_at": "2021-04-19T09:23:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "xealml"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1142,
        "title": "panddle Serving 部署的api地址在哪设置呢？",
        "body": "paddle serving的api接口地址改在哪设置呢？我看见目前給出来的例子，都是默认的本地127的地址，也没有写在哪设置这个地址呢？",
        "state": "closed",
        "user": "huangch1024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-17T09:08:52+00:00",
        "updated_at": "2024-03-05T06:49:29+00:00",
        "closed_at": "2024-03-05T06:49:29+00:00",
        "comments_count": [
            "zhangjun",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1141,
        "title": "找不到 libnvinfer.so.7",
        "body": "```bash\r\n启动pd serving 时报错\r\npython -m paddle_serving_server_gpu.serve \\\r\n--model serving_server \\\r\n-port 9393 --gpu_ids 0\r\n\r\n\r\nmkdir: cannot create directory ‘workdir_0’: File exists\r\nGoing to Run Comand\r\n/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/serving-gpu-102-0.5.0/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 9393 -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 536870912 \r\n/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/serving-gpu-102-0.5.0/serving: error while loading shared libraries: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n```\r\n\r\n### 说明：\r\n官方镜像体积太大了，后来自己了个镜像。\r\n- BASE IMAGE： nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04\r\n- 其他package如下:\r\npaddle-serving-app        0.2.0              \r\npaddle-serving-client     0.5.0              \r\npaddle-serving-server-gpu 0.5.0.post102      \r\npaddlepaddle-gpu          2.0.1.post100 \r\n\r\n通过 find / libnv*   找了下，容器里确实没有libnvinfer.so.7，\r\n可以给个这个文件的下载链接吗",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-04-16T07:24:58+00:00",
        "updated_at": "2021-04-19T04:27:30+00:00",
        "closed_at": "2021-04-19T04:27:30+00:00",
        "comments_count": [
            "TeslaZhao",
            "zouxiaoshi"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1148,
        "title": "ppyolo test_client 结果不对",
        "body": "执行[ppyo_r50vd_dcn_1x_coco](https://github.com/PaddlePaddle/Serving/tree/develop/python/examples/detection/ppyolo_r50vd_dcn_1x_coco) 里的示例，但是推理后的结果很差。\r\n推理如果如下：\r\n\r\n![000000570688 (1)](https://user-images.githubusercontent.com/44261675/115193836-41818f00-a11f-11eb-8067-1ac786bd9188.jpg)\r\n\r\n\r\n版本信息如下：\r\npaddle-serving-app        0.2.0              \r\npaddle-serving-client     0.5.0              \r\npaddle-serving-server-gpu 0.5.0.post10       \r\npaddlepaddle-gpu          2.0.1.post100    ",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-19T06:55:47+00:00",
        "updated_at": "2024-03-05T06:49:30+00:00",
        "closed_at": "2024-03-05T06:49:30+00:00",
        "comments_count": [
            "Nick198903"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1149,
        "title": "启动报错",
        "body": "File \"/usr/local/lib/python3.8/site-packages/paddle_serving_server_gpu/web_service.py\", line 230, in run_rpc_service\r\n    localIP = socket.gethostbyname(socket.gethostname())\r\nsocket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\n\r\n启动命令是\r\npython3 -m paddle_serving_server_gpu.serve --model uci_housing_model --thread 10 --port 9393 --gpu_id 0 --name uci",
        "state": "closed",
        "user": "yangwenshuo",
        "closed_by": "yangwenshuo",
        "created_at": "2021-04-19T07:54:00+00:00",
        "updated_at": "2021-04-21T07:12:19+00:00",
        "closed_at": "2021-04-21T07:12:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1158,
        "title": "docker里面编译老是失败,baidu内网",
        "body": "fatal: unable to access 'https://github.com/google/snappy/': Unknown SSL protocol error in connection to github.com:443\r\nfatal: unable to access 'https://github.com/gflags/gflags.git/': Unknown SSL protocol error in connection to github.com:443\r\nCloning into 'extern_snappy'...\r\nCloning into 'extern_gflags'...\r\nfatal: unable to access 'https://github.com/open-source-parsers/jsoncpp/': Unknown SSL protocol error in connection to github.com:443\r\nfatal: unable to access 'https://github.com/pybind/pybind11.git/': Unknown SSL protocol error in connection to github.com:443\r\nCloning into 'extern_pybind'...\r\nfatal: unable to access 'https://github.com/google/protobuf.git/': Unknown SSL protocol error in connection to github.com:443\r\n\r\n\r\n请问有什么好的解决办法么",
        "state": "closed",
        "user": "yangwenshuo",
        "closed_by": "yangwenshuo",
        "created_at": "2021-04-21T07:15:41+00:00",
        "updated_at": "2021-04-21T09:00:14+00:00",
        "closed_at": "2021-04-21T09:00:14+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1160,
        "title": "\u000e\u000e编译CPU版本Server报错",
        "body": "所执行的命令是\r\n\r\nmkdir server-build-cpu && cd server-build-cpu\r\ncmake -DPYTHON_INCLUDE_DIR=$PYTHON_INCLUDE_DIR/ \\\r\n    -DPYTHON_LIBRARIES=$PYTHON_LIBRARIES \\\r\n    -DPYTHON_EXECUTABLE=$PYTHON_EXECUTABLE \\\r\n    -DSERVER=ON ..\r\nmake -j10\r\n\r\n\r\n报错的信息是\r\n\r\nc++: fatal error: Killed signal terminated program cc1plus\r\ncompilation terminated.\r\ncore/sdk-cpp/CMakeFiles/sdk-cpp.dir/build.make:292: recipe for target 'core/sdk-cpp/CMakeFiles/sdk-cpp.dir/dense_service.pb.cc.o' failed\r\nmake[2]: *** [core/sdk-cpp/CMakeFiles/sdk-cpp.dir/dense_service.pb.cc.o] Error 1\r\n\r\nc++: fatal error: Killed signal terminated program cc1plus\r\ncompilation terminated.\r\ncore/predictor/CMakeFiles/pdserving.dir/build.make:184: recipe for target 'core/predictor/CMakeFiles/pdserving.dir/op/op.cpp.o' failed\r\nmake[2]: *** [core/predictor/CMakeFiles/pdserving.dir/op/op.cpp.o] Error 1\r\n",
        "state": "closed",
        "user": "yangwenshuo",
        "closed_by": "yangwenshuo",
        "created_at": "2021-04-21T09:30:34+00:00",
        "updated_at": "2021-04-21T10:40:19+00:00",
        "closed_at": "2021-04-21T10:40:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1159,
        "title": "python3启动paddleserving报错",
        "body": "  image_process_service.run_service()\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/web_service.py\", line 60, in run_service\r\n    self._server.run_server()\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 297, in run_server\r\n    http_port=self._http_port)  # start grpc_gateway\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 124, in _run_grpc_gateway\r\n    if http_port <= 0:\r\nTypeError: '<=' not supported between instances of 'NoneType' and 'int'\r\n\r\n错误信息如上",
        "state": "closed",
        "user": "yangwenshuo",
        "closed_by": "yangwenshuo",
        "created_at": "2021-04-21T07:16:57+00:00",
        "updated_at": "2021-04-21T09:00:23+00:00",
        "closed_at": "2021-04-21T09:00:23+00:00",
        "comments_count": [
            "TeslaZhao",
            "yangwenshuo",
            "yangwenshuo"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1162,
        "title": "paddle sering更换新的proto文件编译失败",
        "body": "--grpc-gateway_out: no field \"name\" found in NormalMessageReq\r\n\r\n[ 60%] Building CXX object core/cube/cube-api/CMakeFiles/cube-api.dir/src/meta.cpp.o\r\npython/CMakeFiles/paddle_python.dir/build.make:67: recipe for target '.timestamp' failed\r\nmake[2]: *** [.timestamp] Error 1\r\nCMakeFiles/Makefile2:1271: recipe for target 'python/CMakeFiles/paddle_python.dir/all' failed\r\nmake[1]: *** [python/CMakeFiles/paddle_python.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n\r\n错误信息如上图,NormalMessageReq是我们定义的请求格式,看错误信息是没有name这个字段,我应该去改哪里把 这个用name字段的地方去掉呢",
        "state": "closed",
        "user": "yangwenshuo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-22T02:54:57+00:00",
        "updated_at": "2024-03-05T06:49:31+00:00",
        "closed_at": "2024-03-05T06:49:31+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1177,
        "title": "Serving部署pipline ocr自己的模型时，自己模型的rec模型的fetch_list是[‘save_infer_model/scale_0.tmp_1’]，而demo为[\"ctc_greedy_decoder_0.tmp_0\", \"softmax_0.tmp_0\"],导致无法部署自己的模型 ",
        "body": "example/pipeling/ocr中的样例模型rec模型",
        "state": "closed",
        "user": "huangch1024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-25T11:56:14+00:00",
        "updated_at": "2024-04-16T09:05:40+00:00",
        "closed_at": "2024-04-16T09:05:40+00:00",
        "comments_count": [
            "TeslaZhao",
            "huangch1024"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1173,
        "title": "uci_housing_client测试报错",
        "body": "python3.6 test_client.py uci_housing_client/serving_client_conf.prototxt\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0425 03:51:49.812734  3687 general_model.cpp:73] feed var num: 1fetch_var_num: 1\r\nI0425 03:51:49.812860  3687 general_model.cpp:77] feed alias name: x index: 0\r\nI0425 03:51:49.812870  3687 general_model.cpp:80] feed[0] shape:\r\nI0425 03:51:49.812887  3687 general_model.cpp:84] shape[0]: 13\r\nI0425 03:51:49.812906  3687 general_model.cpp:87] feed[0] feed type: 1\r\nI0425 03:51:49.812930  3687 general_model.cpp:95] fetch [0] alias name: price\r\nI0425 03:51:49.813046  3687 general_model.cpp:55] Init commandline: dummy test_client.py --tryfromenv=profile_client,profile_server,max_body_size \r\nI0425 03:51:49.813273  3687 predictor_sdk.cpp:34] \r\nM\r\ndefault??? d(?????????0:pooled\r\nDefaultla\"      baidu_std ?\r\ngeneral_model@baidu.paddle_serving.predictor.general_model.GeneralModelServiceWeightedRandomRender\"\r\n100*6\r\nefault_tag_140558042823816list://127.0.0.1:9393\r\nI0425 03:51:49.813410  3687 predictor_sdk.cpp:28] Succ register all components!\r\nI0425 03:51:49.813446  3687 config_manager.cpp:217] Not found key in configue: cluster\r\nI0425 03:51:49.813465  3687 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0425 03:51:49.813484  3687 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0425 03:51:49.813504  3687 config_manager.cpp:263] split info not set, skip...\r\nI0425 03:51:49.813534  3687 abtest.cpp:55] Succ read weights list: 100, count: 1, normalized: 100\r\nI0425 03:51:49.813549  3687 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nI0425 03:51:49.813560  3687 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nI0425 03:51:49.813578  3687 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nI0425 03:51:49.813591  3687 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nI0425 03:51:49.813604  3687 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nI0425 03:51:49.813614  3687 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nI0425 03:51:49.813629  3687 config_manager.cpp:212] Not found key in configue: connection_type\r\nI0425 03:51:49.813642  3687 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nI0425 03:51:49.813652  3687 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nI0425 03:51:49.813663  3687 config_manager.cpp:226] Not found key in configue: protocol\r\nI0425 03:51:49.813673  3687 config_manager.cpp:227] Not found key in configue: compress_type\r\nI0425 03:51:49.813694  3687 config_manager.cpp:228] Not found key in configue: package_size\r\nI0425 03:51:49.813701  3687 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nI0425 03:51:49.813722  3687 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0425 03:51:49.813733  3687 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0425 03:51:49.813750  3687 config_manager.cpp:263] split info not set, skip...\r\nI0425 03:51:49.813768  3687 config_manager.cpp:186] Succ load one endpoint, name: general_model, count of variants: 1.\r\nI0425 03:51:49.813784  3687 config_manager.cpp:85] Success reload endpoint config file, id: 1\r\nI0425 03:51:49.818302  3687 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0425 03:51:49.818620  3687 stub_impl.hpp:376] Succ create parallel channel, count: 3\r\nI0425 03:51:49.818637  3687 stub_impl.hpp:42] Create stub without tag, ep general_model\r\nI0425 03:51:49.819711  3687 variant.cpp:69] Succ create default debug\r\nI0425 03:51:49.819731  3687 endpoint.cpp:38] Succ create variant: 0, endpoint:general_model\r\nI0425 03:51:49.819742  3687 predictor_sdk.cpp:69] Succ create endpoint instance with name: general_model\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0425 03:51:50.029362  3687 dynamic_loader.cc:128] Set paddle lib path : /usr/local/lib/python3.6/site-packages/paddle/libs\r\nI0425 03:51:51.851876  3687 init.cc:85] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=check_nan_inf,fast_check_nan_inf,benchmark,eager_delete_scope,fraction_of_cpu_memory_to_use,initial_cpu_memory_in_mb,init_allocated_mem,paddle_num_threads,dist_threadpool_size,eager_delete_tensor_gb,fast_eager_deletion_mode,memory_fraction_of_eager_deletion,allocator_strategy,reader_queue_speed_test_mode,print_sub_graph_dir,pe_profile_fname,inner_op_parallelism,enable_parallel_graph,fuse_parameter_groups_size,multiple_of_cupti_buffer_size,fuse_parameter_memory_size,tracer_profile_fname,dygraph_debug,use_system_allocator,enable_unused_var_check,free_idle_chunk,free_when_no_cache_hit,call_stack_level,sort_sum_gradient,max_inplace_grad_add,use_pinned_memory,cpu_deterministic,use_mkldnn,tracer_mkldnn_ops_on,tracer_mkldnn_ops_off,fraction_of_gpu_memory_to_use,initial_gpu_memory_in_mb,reallocate_gpu_memory_in_mb,cudnn_deterministic,enable_cublas_tensor_op_math,conv_workspace_size_limit,cudnn_exhaustive_search,selected_gpus,sync_nccl_allreduce,cudnn_batchnorm_spatial_persistent,gpu_allocator_retry_time,local_exe_sub_scope_limit,gpu_memory_limit_mb \r\nI0425 03:51:51.852031  3687 init.cc:92] After Parse: argc is 1\r\ndata (13,)\r\ndata (1, 13)\r\nI0425 03:51:51.937012  3687 general_model.cpp:154] batch size: 1\r\nI0425 03:51:51.937045  3687 stub_impl.hpp:149] Succ thread initialize stub impl!\r\nI0425 03:51:51.937055  3687 endpoint.cpp:53] Succ thrd initialize all vars: 1\r\nI0425 03:51:51.937067  3687 predictor_sdk.cpp:129] Succ thrd initialize endpoint:general_model\r\nI0425 03:51:51.937382  3687 general_model.cpp:165] fetch general model predictor done.\r\nI0425 03:51:51.937393  3687 general_model.cpp:166] float feed name size: 1\r\nI0425 03:51:51.937431  3687 general_model.cpp:167] int feed name size: 0\r\nI0425 03:51:51.937441  3687 general_model.cpp:168] max body size : 536870912\r\nI0425 03:51:51.937449  3687 general_model.cpp:176] prepare batch 0\r\nI0425 03:51:51.937465  3687 general_model.cpp:189] batch [0] int_feed_name and float_feed_name prepared\r\nI0425 03:51:51.937479  3687 general_model.cpp:193] tensor_vec size 1 float shape 1\r\nI0425 03:51:51.937496  3687 general_model.cpp:198] prepare float feed x shape size 2\r\nI0425 03:51:51.937513  3687 general_model.cpp:253] batch [0] float feed value prepared\r\nI0425 03:51:51.937539  3687 general_model.cpp:339] batch [0] int feed value prepared\r\nW0425 03:51:51.940088  3737 redis_protocol.cpp:69] No corresponding PipelinedInfo in socket\r\nE0425 03:51:51.940167  3737 input_messenger.cpp:113] A message from 127.0.0.1:9393(protocol=esp) is bigger than 536870912 bytes, the connection will be closed. Set max_body_size to allow bigger messages\r\nW0425 03:51:51.940189  3737 input_messenger.cpp:276] Close fd=11 SocketId=1@127.0.0.1:9393@46672: too big data\r\nW0425 03:51:51.940340  3687 predictor.hpp:129] inference call failed, message: [E22]1/1 channels failed, fail_limit=1 [C0][E22]Close fd=11 SocketId=1@127.0.0.1:9393@46672: too big data\r\nE0425 03:51:51.940464  3687 general_model.cpp:361] failed call predictor with req: insts { tensor_array { float_data: 0.38269556 float_data: -0.11363637 float_data: 0.25525004 float_data: -0.069169961 float_data: 0.25577149 float_data: -0.015833376 float_data: 0.10427496 float_data: -0.17569885 float_data: 0.62828666 float_data: 0.49191383 float_data: 0.18558154 float_data: -0.851919 float_data: 0.051515914 elem_type: 1 shape: 1 shape: 13 } } fetch_var_names: \"price\" log_id: 0\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 37, in <module>\r\n    print('ok', fetch_map[\"price\"])\r\nTypeError: 'NoneType' object is not subscriptable\r\nI0425 03:51:51.940901  3687 mmap_allocator.cc:119] PID: 3687, MemoryMapFdSet: set size - 0\r\nI0425 03:51:52.038947  3687 mmap_allocator.cc:119] PID: 3687, MemoryMapFdSet: set size - 0\r\nI0425 03:51:52.040920  3736 socket.cpp:2370] Checking SocketId=0@127.0.0.1:9393",
        "state": "closed",
        "user": "bluestinger",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-25T07:08:57+00:00",
        "updated_at": "2024-03-05T06:49:32+00:00",
        "closed_at": "2024-03-05T06:49:32+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bluestinger",
            "HexToString",
            "bluestinger",
            "bluestinger",
            "HexToString"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1178,
        "title": "ernie 1.0模型用bert测试报错",
        "body": "WARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0426 03:29:11.977586  7611 general_model.cpp:73] feed var num: 5fetch_var_num: 2\r\nI0426 03:29:11.977638  7611 general_model.cpp:77] feed alias name: input_mask index: 0\r\nI0426 03:29:11.977648  7611 general_model.cpp:80] feed[0] shape:\r\nI0426 03:29:11.977654  7611 general_model.cpp:84] shape[0]: 512\r\nI0426 03:29:11.977661  7611 general_model.cpp:84] shape[1]: 1\r\nI0426 03:29:11.977667  7611 general_model.cpp:87] feed[0] feed type: 1\r\nI0426 03:29:11.977675  7611 general_model.cpp:77] feed alias name: position_ids index: 1\r\nI0426 03:29:11.977782  7611 general_model.cpp:80] feed[1] shape:\r\nI0426 03:29:11.977788  7611 general_model.cpp:84] shape[0]: 512\r\nI0426 03:29:11.977797  7611 general_model.cpp:84] shape[1]: 1\r\nI0426 03:29:11.977802  7611 general_model.cpp:87] feed[1] feed type: 0\r\nI0426 03:29:11.977810  7611 general_model.cpp:77] feed alias name: input_ids index: 2\r\nI0426 03:29:11.977816  7611 general_model.cpp:80] feed[2] shape:\r\nI0426 03:29:11.977841  7611 general_model.cpp:84] shape[0]: 512\r\nI0426 03:29:11.977872  7611 general_model.cpp:84] shape[1]: 1\r\nI0426 03:29:11.977880  7611 general_model.cpp:87] feed[2] feed type: 0\r\nI0426 03:29:11.977898  7611 general_model.cpp:77] feed alias name: segment_ids index: 3\r\nI0426 03:29:11.977905  7611 general_model.cpp:80] feed[3] shape:\r\nI0426 03:29:11.977911  7611 general_model.cpp:84] shape[0]: 512\r\nI0426 03:29:11.977917  7611 general_model.cpp:84] shape[1]: 1\r\nI0426 03:29:11.977923  7611 general_model.cpp:87] feed[3] feed type: 0\r\nI0426 03:29:11.977960  7611 general_model.cpp:77] feed alias name: segment_ids index: 4\r\nI0426 03:29:11.977993  7611 general_model.cpp:80] feed[4] shape:\r\nI0426 03:29:11.978008  7611 general_model.cpp:84] shape[0]: 1\r\nI0426 03:29:11.978015  7611 general_model.cpp:87] feed[4] feed type: 0\r\nI0426 03:29:11.978030  7611 general_model.cpp:95] fetch [0] alias name: pooled_output\r\nI0426 03:29:11.978039  7611 general_model.cpp:95] fetch [1] alias name: sequence_output\r\nI0426 03:29:11.978149  7611 general_model.cpp:55] Init commandline: dummy ernie_client.py --tryfromenv=profile_client,profile_server,max_body_size \r\nI0426 03:29:11.978436  7611 predictor_sdk.cpp:34] \r\nM\r\ndefault??? d(?????????0:pooled\r\nDefaultla\"      baidu_std ?\r\ngeneral_model@baidu.paddle_serving.predictor.general_model.GeneralModelServiceWeightedRandomRender\"\r\n100*6\r\nefault_tag_140065490880496list://127.0.0.1:9293\r\nI0426 03:29:11.978503  7611 predictor_sdk.cpp:28] Succ register all components!\r\nI0426 03:29:11.978541  7611 config_manager.cpp:217] Not found key in configue: cluster\r\nI0426 03:29:11.978554  7611 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0426 03:29:11.978564  7611 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0426 03:29:11.978586  7611 config_manager.cpp:263] split info not set, skip...\r\nI0426 03:29:11.978610  7611 abtest.cpp:55] Succ read weights list: 100, count: 1, normalized: 100\r\nI0426 03:29:11.978628  7611 config_manager.cpp:202] Not found key in configue: connect_timeout_ms\r\nI0426 03:29:11.978650  7611 config_manager.cpp:203] Not found key in configue: rpc_timeout_ms\r\nI0426 03:29:11.978665  7611 config_manager.cpp:205] Not found key in configue: hedge_request_timeout_ms\r\nI0426 03:29:11.978682  7611 config_manager.cpp:207] Not found key in configue: connect_retry_count\r\nI0426 03:29:11.978706  7611 config_manager.cpp:209] Not found key in configue: hedge_fetch_retry_count\r\nI0426 03:29:11.978716  7611 config_manager.cpp:211] Not found key in configue: max_connection_per_host\r\nI0426 03:29:11.978725  7611 config_manager.cpp:212] Not found key in configue: connection_type\r\nI0426 03:29:11.978734  7611 config_manager.cpp:219] Not found key in configue: load_balance_strategy\r\nI0426 03:29:11.978744  7611 config_manager.cpp:221] Not found key in configue: cluster_filter_strategy\r\nI0426 03:29:11.978751  7611 config_manager.cpp:226] Not found key in configue: protocol\r\nI0426 03:29:11.978762  7611 config_manager.cpp:227] Not found key in configue: compress_type\r\nI0426 03:29:11.978772  7611 config_manager.cpp:228] Not found key in configue: package_size\r\nI0426 03:29:11.978781  7611 config_manager.cpp:230] Not found key in configue: max_channel_per_request\r\nI0426 03:29:11.978788  7611 config_manager.cpp:234] Not found key in configue: split_tag_name\r\nI0426 03:29:11.978804  7611 config_manager.cpp:235] Not found key in configue: tag_candidates\r\nI0426 03:29:11.978816  7611 config_manager.cpp:263] split info not set, skip...\r\nI0426 03:29:11.978823  7611 config_manager.cpp:186] Succ load one endpoint, name: general_model, count of variants: 1.\r\nI0426 03:29:11.978838  7611 config_manager.cpp:85] Success reload endpoint config file, id: 1\r\nI0426 03:29:11.982506  7611 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9293\"): added 1\r\nI0426 03:29:11.982806  7611 stub_impl.hpp:376] Succ create parallel channel, count: 3\r\nI0426 03:29:11.982821  7611 stub_impl.hpp:42] Create stub without tag, ep general_model\r\nI0426 03:29:11.983860  7611 variant.cpp:69] Succ create default debug\r\nI0426 03:29:11.983878  7611 endpoint.cpp:38] Succ create variant: 0, endpoint:general_model\r\nI0426 03:29:11.983889  7611 predictor_sdk.cpp:69] Succ create endpoint instance with name: general_model\r\nI0426 03:29:11.984738  7611 general_model.cpp:154] batch size: 1\r\nI0426 03:29:11.984761  7611 stub_impl.hpp:149] Succ thread initialize stub impl!\r\nI0426 03:29:11.984771  7611 endpoint.cpp:53] Succ thrd initialize all vars: 1\r\nI0426 03:29:11.984781  7611 predictor_sdk.cpp:129] Succ thrd initialize endpoint:general_model\r\nI0426 03:29:11.985100  7611 general_model.cpp:165] fetch general model predictor done.\r\nI0426 03:29:11.985112  7611 general_model.cpp:166] float feed name size: 1\r\nI0426 03:29:11.985123  7611 general_model.cpp:167] int feed name size: 3\r\nI0426 03:29:11.985131  7611 general_model.cpp:168] max body size : 536870912\r\nI0426 03:29:11.985143  7611 general_model.cpp:176] prepare batch 0\r\nI0426 03:29:11.985162  7611 general_model.cpp:189] batch [0] int_feed_name and float_feed_name prepared\r\nI0426 03:29:11.985174  7611 general_model.cpp:193] tensor_vec size 4 float shape 1\r\nI0426 03:29:11.985191  7611 general_model.cpp:198] prepare float feed input_mask shape size 3\r\nI0426 03:29:11.985209  7611 general_model.cpp:253] batch [0] float feed value prepared\r\nI0426 03:29:11.985241  7611 general_model.cpp:270] prepare int feed input_ids shape size 3\r\nI0426 03:29:11.985260  7611 general_model.cpp:270] prepare int feed position_ids shape size 3\r\nSegmentation fault (core dumped)",
        "state": "closed",
        "user": "bluestinger",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-26T03:44:35+00:00",
        "updated_at": "2024-03-05T06:49:33+00:00",
        "closed_at": "2024-03-05T06:49:33+00:00",
        "comments_count": [
            "bluestinger",
            "bluestinger",
            "HexToString",
            "HexToString",
            "bluestinger",
            "HexToString",
            "bluestinger",
            "bluestinger",
            "bluestinger",
            "bluestinger",
            "bluestinger",
            "bluestinger",
            "HexToString"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1189,
        "title": "Http方式部署thread参数不起作用",
        "body": "使用以下命令部署模型是，输入的thread参数不起作用，cpu使用率不超过400%：\r\n\r\n> python -m paddle_serving_server.serve --model my_model --thread 10 --port 9292 --name my_model\r\n\r\n原因是文件：\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/python/paddle_serving_server/serve.py\r\n中第433行中processes=4，并没有更新为用户的输入参数；\r\n",
        "state": "closed",
        "user": "CH7auAI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-28T08:54:09+00:00",
        "updated_at": "2024-03-05T06:49:34+00:00",
        "closed_at": "2024-03-05T06:49:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "yafeisong"
        ],
        "labels": [
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1196,
        "title": "docker 容器无法启动",
        "body": "我用的是ubuntu20.04系统，按照https://github.com/PaddlePaddle/Serving/blob/develop/doc/RUN_IN_DOCKER_CN.md教程输入一下指令，\r\n$ docker run --gpus all -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:latest-cuda10.2-cudnn8-devel\r\nf2ad919b0f20069c696073ea35f6fd743771c0b9bcb9ebd5adba2969482bb952\r\n$ docker exec -it test bash\r\nError response from daemon: Container f2ad919b0f20069c696073ea35f6fd743771c0b9bcb9ebd5adba2969482bb952 is not running报出了容器没有运行，然后我使用docker的stop和start命令也还是报出容器没有运行，\r\n想请教一下如何解决啊？",
        "state": "closed",
        "user": "Amanda-Barbara",
        "closed_by": "hysunflower",
        "created_at": "2021-04-30T02:39:35+00:00",
        "updated_at": "2021-05-07T03:41:39+00:00",
        "closed_at": "2021-05-07T03:41:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "Amanda-Barbara"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1195,
        "title": "docker serarch 报404",
        "body": "我想要尝试拉取docker镜像的内容，但是在访问了对应的链接之后，却是提示我404找不到资源\r\n```\r\n[root@iz20d333q744p1z ~]# docker search registry.baidubce.com/paddlepaddle/serving\r\nError response from daemon: Unexpected status code 404\r\n[root@iz20d333q744p1z ~]# docker search hub.baidubce.com/paddlepaddle/serving\r\nError response from daemon: Unexpected status code 404\r\n```\r\n",
        "state": "closed",
        "user": "JunKuangKuang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-30T00:04:53+00:00",
        "updated_at": "2024-03-05T06:49:34+00:00",
        "closed_at": "2024-03-05T06:49:34+00:00",
        "comments_count": [
            "github-actions[bot]",
            "JunKuangKuang",
            "hysunflower"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1200,
        "title": "Pipeline Serving 的RPC服务同时启动一个web服务器和RPC服务器，请问这里的web是uwsgi还是其他的什么？",
        "body": null,
        "state": "closed",
        "user": "huangch1024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-30T07:45:50+00:00",
        "updated_at": "2024-03-05T06:49:35+00:00",
        "closed_at": "2024-03-05T06:49:35+00:00",
        "comments_count": [
            "huangch1024",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1201,
        "title": "请问可以部署ocr到阿里云函数计算吗 我这边提示错误不知道怎么解决",
        "body": "···\r\n{\r\n  \"errorMessage\": \"Unable to import module 'index'\",\r\n  \"errorType\": \"ImportError\",\r\n  \"stackTrace\": [\r\n    \"ImportError: /code/.fun/root/usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /code/.fun/python/lib/python3.6/site-packages/paddle/fluid/core_avx.so)\"\r\n  ]\r\n}\r\n···\r\n\r\n安装环境\r\n···\r\nfun install --runtime python3 --package-type pip install paddle-serving-client==0.5.0\r\nfun install --runtime python3 --package-type pip install paddle-serving-server==0.5.0 # CPU\r\nfun install --runtime python3 --package-type pip install paddle-serving-app==0.3.0 \r\nfun install --runtime python3 --package-type pip install paddlepaddle==2.0.0\r\n···\r\n\r\n然后运行 ocr_debugger_server.py\r\n\r\n就提示GLIBCXX_3.4.22",
        "state": "closed",
        "user": "duolabmeng6",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-30T08:02:33+00:00",
        "updated_at": "2024-04-16T09:05:41+00:00",
        "closed_at": "2024-04-16T09:05:41+00:00",
        "comments_count": [
            "github-actions[bot]",
            "duolabmeng6",
            "TeslaZhao",
            "duolabmeng6"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1216,
        "title": "OCR的例子直接跑会报错，还有文档的一点小错误",
        "body": "环境：Win10 X64，paddle2.0.2\r\n1. 运行`python ocr_debugger_server.py gpu`会报错，因为例子中判断参数为GPU后仍旧引入了CPU库。。。\r\n```\r\nTraceback (most recent call last):\r\n  File \"ocr_debugger_server.py\", line 109, in <module>\r\n    ocr_service.set_gpus(\"0\")\r\nAttributeError: 'OCRService' object has no attribute 'set_gpus'\r\n```\r\n`ocr_debugger_server.py `代码\r\n```\r\nif sys.argv[1] == 'gpu':\r\n    from paddle_serving_server.web_service import WebService\r\nelif sys.argv[1] == 'cpu':\r\n    from paddle_serving_server.web_service import WebService\r\n```\r\n2. 程序支持paddle2.0，但提示某api即将弃用，且没有说api名字，建议更换api；另外Win10下提示touch命令出错，需要加上判断用其他命令，或者提前安装touch（`npm install touch-cli -g`）\r\n```\r\nThis API will be deprecated later. Please do not use it\r\nThis API will be deprecated later. Please do not use it\r\n子目录或文件 workdir 已经存在。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\n```\r\n3. OCR的readme中运行CPU和GPU版本用的都是同一个命令。。\r\n>  https://github.com/PaddlePaddle/Serving/blob/develop/python/examples/ocr/README_CN.md\r\n这个里面有两处“启动服务”中命令都是这个问题，并没有使用GPU",
        "state": "closed",
        "user": "erma0",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-12T13:44:59+00:00",
        "updated_at": "2024-03-05T06:49:36+00:00",
        "closed_at": "2024-03-05T06:49:36+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1202,
        "title": "使用0.3.1跑plato模型出core报错。",
        "body": "用paddle1.8.1训的model\r\npaddle-serving版本是0.3.1\r\n\r\n1.构造的数据的shape\r\n这份数据在原生的python 预测端是可以正常进行。\r\n![image](https://user-images.githubusercontent.com/548443/117394972-c0513700-af29-11eb-979d-dca56fb67c68.png)\r\n\r\n报错信息\r\n  File \"/home/hupenglong/py37/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/home/hupenglong/py37/lib/python3.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/home/hupenglong/py37/lib/python3.7/site-packages/paddle/fluid/layers/tensor.py\", line 375, in concat\r\n    type='concat', inputs=inputs, outputs={'Out': [out]}, attrs=attrs)\r\n  File \"/home/hupenglong/Knover/models/unified_transformer.py\", line 127, in _gen_input\r\n    emb_out = layers.concat([aux_emb, emb_out], axis=1)\r\n  File \"/home/hupenglong/Knover/models/unified_transformer.py\", line 179, in _generation_network\r\n    token_ids, type_ids, pos_ids, generation_mask, aux_emb=aux_emb)\r\n  File \"/home/hupenglong/Knover/models/plato.py\", line 198, in forward\r\n    gather_idx=inputs.get(\"parent_idx\", None),\r\n  File \"/home/hupenglong/Knover/models/model_base.py\", line 92, in _build_programs\r\n    outputs = self.forward(inputs, is_infer=True)\r\n  File \"/home/hupenglong/Knover/models/model_base.py\", line 74, in __init__\r\n    self._build_programs()\r\n  File \"/home/hupenglong/Knover/models/unified_transformer.py\", line 94, in __init__\r\n    super(UnifiedTransformer, self).__init__(args, place)\r\n  File \"/home/hupenglong/Knover/models/plato.py\", line 50, in __init__\r\n    super(Plato, self).__init__(args, place)\r\n  File \"/home/hupenglong/Knover/models/__init__.py\", line 50, in create_model\r\n    return MODEL_REGISTRY[args.model](args, place)\r\n  File \"save_inference_model.py\", line 54, in save\r\n    model = models.create_model(args, place)\r\n  File \"save_inference_model.py\", line 63, in <module>\r\n    save(args)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: ShapeError: Dimension 2 in inputs' shapes must be equal. But recevied input[0]'s shape = [1, 1, 20, 1024], input[1]'s shape = [1, 20, 7, 1024].\r\n  [Hint: Expected inputs_dims[0][j] == inputs_dims[i][j], but received inputs_dims[0][j]:20 != inputs_dims[i][j]:7.] at (/paddle/paddle/fluid/operators/concat_op.h:55)\r\n  [operator < concat > error]\r\nAborted (core dumped)\r\n\r\n有几个概念不太清楚：\r\n1）传给client.predict中的feed的格式，如果是batch_size>1需要传list,每个list是一个字典；还是像我这样传与训练代码feed_dict一致就是合法的?\r\n2）报错信息表示似乎给input shape多增加了一维信息\r\n     model定义的shape应该是\r\n     aux 20,1,1024,\r\n     emb_out 20,7,1024\r\n3)  我把数据变成batch_size=1,报错信息也类似。多增加了一个维度。\r\n4) 怀疑是否跟对lod tensor支持不好有关呢？有几个feed变量是lod tensor\r\n2. client端代码\r\nhttps://github.com/lonelydancer/algorithm/blob/master/client.py\r\nclient的配置\r\nhttps://github.com/lonelydancer/algorithm/blob/master/serving_client_conf.prototxt\r\n3. 对应的plato feed_dict\r\nhttps://github.com/PaddlePaddle/Knover/blob/plato-2/models/plato.py\r\n_get_feed_dict中定义的\r\n",
        "state": "closed",
        "user": "lonelydancer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-07T03:58:41+00:00",
        "updated_at": "2024-04-16T09:05:42+00:00",
        "closed_at": "2024-04-16T09:05:42+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "lonelydancer",
            "lonelydancer",
            "lonelydancer"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1220,
        "title": "docker 镜像内无法检测到GPU",
        "body": "提供的 registry.baidubce.com/paddlepaddle/serving:0.5.0-devel 这个镜像进去之后nvidia-smi ，bash: nvidia-smi: command not found，宿主机执行nvidia-smi是可以检测到的，另外试了其他的pytorch镜像也是没问题的",
        "state": "closed",
        "user": "Jeffrey98-AI",
        "closed_by": "Jeffrey98-AI",
        "created_at": "2021-05-13T06:54:44+00:00",
        "updated_at": "2021-07-20T01:41:16+00:00",
        "closed_at": "2021-07-20T01:41:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1226,
        "title": "负载均衡",
        "body": "您好：\r\n通过如下命令启动RPC serivce 时， 是应用brpc 还是grpc？\r\n```bash\r\npython -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292 \r\n```\r\n如果是brpc，请问可否 用nginx进行负载。\r\n刚接触brpc, 只知道可以在client端进行负载。\r\n业务需要，像nginx那样集中式负载，所以想问问paddle serving有哪些负载模式。\r\n\r\n大概场景是这样的：\r\n好多台机器，每个机器若干服务，\r\n对外只通过一个节点利用负载暴露服务",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-05-14T07:56:50+00:00",
        "updated_at": "2021-05-17T01:41:15+00:00",
        "closed_at": "2021-05-17T01:41:15+00:00",
        "comments_count": [
            "TeslaZhao",
            "zouxiaoshi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1251,
        "title": "官方的Docker 版本报错",
        "body": "直接上图\r\n![企业微信截图_76e6c33a-e32e-49f7-a56c-37df636540e6](https://user-images.githubusercontent.com/2086862/118741440-d55e8c00-b880-11eb-8a1c-b845e52016db.png)\r\n",
        "state": "closed",
        "user": "ouyangshixiong",
        "closed_by": "ouyangshixiong",
        "created_at": "2021-05-19T01:01:50+00:00",
        "updated_at": "2021-05-26T13:30:47+00:00",
        "closed_at": "2021-05-26T13:30:47+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1233,
        "title": "请教save_dygraph_model问题",
        "body": "## 环境：\r\n(pd) ➜  pip list | grep paddle\r\npaddle-serving-client 0.5.0\r\npaddle-serving-server 0.5.0\r\npaddlepaddle          2.0.2\r\n## 代码：\r\nresnet50 = paddle.vision.models.resnet50\r\nmodel = resnet50(num_classes=3)\r\nmodel = paddle.Model(model)\r\nmodel.prepare(optimizer=paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters()),\r\n                loss= nn.CrossEntropyLoss(),\r\n                metrics=paddle.metric.Accuracy())\r\ncallback = paddle.callbacks.VisualDL(log_dir='visualdl_log_dir')\r\nmodel.fit(train_dataset, epochs=100, batch_size=10, verbose=1, callbacks=callback)\r\n\r\nmodel.evaluate(test_dataset, batch_size=10, verbose=1)\r\n\r\nmodel.predict(test_dataset, batch_size=10)\r\n\r\nserving_io.save_dygraph_model(\"serving_model\", \"client_conf\", model)\r\n\r\n## 错误信息：\r\nPredict begin...\r\nstep 1/1 [==============================] - 129ms/step\r\nPredict samples: 2\r\nTraceback (most recent call last):\r\n  File \"/Users/ouyang/app/paddle2/ArKnight_Main_Screen_Classify/train.py\", line 31, in <module>\r\n    serving_io.save_dygraph_model(\"serving_model\", \"client_conf\", model)\r\n  File \"/opt/anaconda3/envs/pd/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py\", line 36, in save_dygraph_model\r\n    paddle.jit.save(model, \"serving_tmp\")\r\n  File \"<decorator-gen-60>\", line 2, in save\r\n  File \"/opt/anaconda3/envs/pd/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n    return wrapped_func(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/pd/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 39, in __impl__\r\n    return func(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/pd/lib/python3.7/site-packages/paddle/fluid/dygraph/jit.py\", line 623, in save\r\n    % type(layer))\r\nTypeError: The input layer of paddle.jit.save should be 'Layer', but received layer type is <class 'paddle.hapi.model.Model'>.\r\n\r\n\r\n![企业微信20210517-194925](https://user-images.githubusercontent.com/2086862/118483808-0dfb4a00-b749-11eb-815e-ad4074daf3e0.png)\r\n\r\n![企业微信20210517-195150](https://user-images.githubusercontent.com/2086862/118484069-63cff200-b749-11eb-8561-608c5881ad9a.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "ouyangshixiong",
        "closed_by": "ouyangshixiong",
        "created_at": "2021-05-17T11:50:04+00:00",
        "updated_at": "2021-05-20T03:05:10+00:00",
        "closed_at": "2021-05-20T03:05:10+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ouyangshixiong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1263,
        "title": "client_type 为 local_predictor 或 grpc时报错",
        "body": "如下配置文件中 client_type 为 local_predictor 或 grpc时报错\r\n```yml\r\n#build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\nbuild_dag_each_worker: false\r\n\r\ndag:\r\n    #op资源类型, True, 为线程模型；False，为进程模型\r\n    is_thread_op: False\r\n\r\n    #重试次数\r\n    retry: 1\r\n\r\n    #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n    use_profile: False\r\n    \r\n    tracer:\r\n        interval_s: 10\r\nop:\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 1\r\n\r\n        #超时时间, 单位ms\r\n        timeout: -1\r\n \r\n        #Serving交互重试次数，默认不重试\r\n        retry: 1\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            model_config: ../output/ppocr_rec_mobile_2.0_serving #ocr_rec_model\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准\r\n            fetch_list: [\"save_infer_model/scale_0.tmp_1\"] #[\"ctc_greedy_decoder_0.tmp_0\", \"softmax_0.tmp_0\"] \r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0\"\r\n            \r\n            ir_optim: True\r\n```\r\n\r\n\r\n环境：\r\npaddle-serving-app        0.2.0              \r\npaddle-serving-client     0.5.0              \r\npaddle-serving-server-gpu 0.5.0.post10       \r\npaddlepaddle-gpu          2.0.1.post100 \r\n\r\n模型：\r\n```bash\r\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar \r\npython3 -m paddle_serving_client.convert --dirname ./ch_ppocr_mobile_v2.0_rec_infer/ \\\r\n                                         --model_filename inference.pdmodel          \\\r\n                                         --params_filename inference.pdiparams       \\\r\n                                         --serving_server ../output/ppocr_rec_mobile_2.0_serving/  \\\r\n                                         --serving_client ../output/ppocr_rec_mobile_2.0_client/\r\n\r\n```\r\n\r\n报错：\r\n**client_type 为grpc时 报错信息如下**\r\n```text\r\nWARNING 2021-05-25 08:27:02,472 [pipeline_server.py:482] [CONF] client_type not set, use default: brpc\r\nWARNING 2021-05-25 08:27:02,472 [pipeline_server.py:482] [CONF] channel_size not set, use default: 0\r\nWARNING 2021-05-25 08:27:02,472 [pipeline_server.py:482] [CONF] batch_size not set, use default: 1\r\nWARNING 2021-05-25 08:27:02,473 [pipeline_server.py:482] [CONF] auto_batching_timeout not set, use default: -1\r\nWARNING 2021-05-25 08:27:02,473 [pipeline_server.py:482] [CONF] workdir not set, use default: \r\nWARNING 2021-05-25 08:27:02,473 [pipeline_server.py:482] [CONF] thread_num not set, use default: 2\r\nWARNING 2021-05-25 08:27:02,473 [pipeline_server.py:482] [CONF] device_type not set, use default: -1\r\nWARNING 2021-05-25 08:27:02,473 [pipeline_server.py:482] [CONF] mem_optim not set, use default: True\r\nWARNING 2021-05-25 08:27:02,474 [operator.py:126] rec Because auto_batching_timeout <= 0 or batch_size == 1, set auto_batching_timeout to None.\r\nINFO 2021-05-25 08:27:02,474 [operator.py:151] local_service_conf: {'client_type': 'grpc', 'model_config': '../output/ppocr_rec_mobile_2.0_serving', 'fetch_list': ['save_infer_model/scale_0.tmp_1'], 'devices': '0', 'ir_optim': True, 'workdir': '', 'thread_num': 2, 'device_type': -1, 'mem_optim': True}\r\nINFO 2021-05-25 08:27:02,474 [local_service_handler.py:126] Create ports for devices:[0]. Port:[12000]\r\nINFO 2021-05-25 08:27:02,474 [local_service_handler.py:141] Models(../output/ppocr_rec_mobile_2.0_serving) will be launched by device gpu. use_gpu:True, use_trt:False, use_lite:False, use_xpu:False, device_type:-1, devices:[0], mem_optim:True, ir_optim:True, use_profile:False, thread_num:2, client_type:grpc, fetch_names:None\r\nINFO 2021-05-25 08:27:02,506 [operator.py:221] rec \r\n\tinput_ops: @DAGExecutor,\r\n\tserver_endpoints: ['127.0.0.1:12000']\r\n\tfetch_list: ['save_infer_model/scale_0.tmp_1']\r\n\tclient_config: ../output/ppocr_rec_mobile_2.0_serving/serving_server_conf.prototxt\r\n\tconcurrency: 1,\r\n\ttimeout(s): -1,\r\n\tretry: 1,\r\n\tbatch_size: 1,\r\n\tauto_batching_timeout(s): None\r\nINFO 2021-05-25 08:27:02,506 [pipeline_server.py:205] ============= PIPELINE SERVER =============\r\nINFO 2021-05-25 08:27:02,507 [pipeline_server.py:206] \r\n{\r\n    \"rpc_port\":18090,\r\n    \"http_port\":9999,\r\n    \"worker_num\":20,\r\n    \"build_dag_each_worker\":false,\r\n    \"dag\":{\r\n        \"is_thread_op\":false,\r\n        \"retry\":1,\r\n        \"use_profile\":false,\r\n        \"tracer\":{\r\n            \"interval_s\":10\r\n        },\r\n        \"client_type\":\"brpc\",\r\n        \"channel_size\":0\r\n    },\r\n    \"op\":{\r\n        \"rec\":{\r\n            \"concurrency\":1,\r\n            \"timeout\":-1,\r\n            \"retry\":1,\r\n            \"local_service_conf\":{\r\n                \"client_type\":\"grpc\",\r\n                \"model_config\":\"../output/ppocr_rec_mobile_2.0_serving\",\r\n                \"fetch_list\":[\r\n                    \"save_infer_model/scale_0.tmp_1\"\r\n                ],\r\n                \"devices\":\"0\",\r\n                \"ir_optim\":true,\r\n                \"workdir\":\"\",\r\n                \"thread_num\":2,\r\n                \"device_type\":-1,\r\n                \"mem_optim\":true\r\n            },\r\n            \"batch_size\":1,\r\n            \"auto_batching_timeout\":-1\r\n        }\r\n    }\r\n}\r\nINFO 2021-05-25 08:27:02,507 [pipeline_server.py:213] -------------------------------------------\r\nINFO 2021-05-25 08:27:02,511 [operator.py:257] Op(rec) use local rpc service at port: [12000]\r\nINFO 2021-05-25 08:27:02,558 [dag.py:493] [DAG] Succ init\r\nINFO 2021-05-25 08:27:02,560 [dag.py:651] ================= USED OP =================\r\nINFO 2021-05-25 08:27:02,560 [dag.py:654] rec\r\nINFO 2021-05-25 08:27:02,560 [dag.py:655] -------------------------------------------\r\nINFO 2021-05-25 08:27:02,604 [dag.py:784] [DAG] Succ build DAG\r\nINFO 2021-05-25 08:27:02,609 [dag.py:816] [DAG] start\r\nINFO 2021-05-25 08:27:02,610 [dag.py:181] [DAG] set in channel succ, name [@DAGExecutor]\r\nINFO 2021-05-25 08:27:02,619 [pipeline_server.py:47] [PipelineServicer] succ init\r\nCRITICAL 2021-05-25 08:27:02,640 [operator.py:1082] [rec|0] failed to init op: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.UNAVAILABLE\r\n\tdetails = \"connections to all backends failing\"\r\n\tdebug_error_string = \"{\"created\":\"@1621931222.640109461\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":4165,\"referenced_errors\":[{\"created\":\"@1621931222.640092495\",\"description\":\"connections to all backends failing\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.cc\",\"file_line\":335,\"grpc_status\":14}]}\"\r\n>\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/pipeline/operator.py\", line 1079, in _run\r\n    profiler = self._initialize(is_thread_op, concurrency_idx)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/pipeline/operator.py\", line 1249, in _initialize\r\n    self.client = self.init_client(self._client_config,\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/pipeline/operator.py\", line 319, in init_client\r\n    client.connect(server_endpoints)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_client/__init__.py\", line 511, in connect\r\n    resp = self.stub_.GetClientConfig(get_client_config_req)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/grpc/_channel.py\", line 923, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/grpc/_channel.py\", line 826, in _end_unary_response_blocking\r\n    raise _InactiveRpcError(state)\r\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.UNAVAILABLE\r\n\tdetails = \"connections to all backends failing\"\r\n\tdebug_error_string = \"{\"created\":\"@1621931222.640109461\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":4165,\"referenced_errors\":[{\"created\":\"@1621931222.640092495\",\"description\":\"connections to all backends failing\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.cc\",\"file_line\":335,\"grpc_status\":14}]}\"\r\n```\r\n\r\n\r\n**local_predictor时报错信息**\r\n```text\r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] client_type not set, use default: brpc\r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] channel_size not set, use default: 0\r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] batch_size not set, use default: 1\r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] auto_batching_timeout not set, use default: -1\r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] workdir not set, use default: \r\nWARNING 2021-05-25 08:46:07,940 [pipeline_server.py:482] [CONF] thread_num not set, use default: 2\r\nWARNING 2021-05-25 08:46:07,941 [pipeline_server.py:482] [CONF] device_type not set, use default: -1\r\nWARNING 2021-05-25 08:46:07,941 [pipeline_server.py:482] [CONF] mem_optim not set, use default: True\r\nWARNING 2021-05-25 08:46:07,941 [operator.py:126] rec Because auto_batching_timeout <= 0 or batch_size == 1, set auto_batching_timeout to None.\r\nINFO 2021-05-25 08:46:07,941 [operator.py:151] local_service_conf: {'client_type': 'local_predictor', 'model_config': '../output/ppocr_rec_mobile_2.0_serving', 'fetch_list': ['save_infer_model/scale_0.tmp_1'], 'devices': '0', 'ir_optim': True, 'workdir': '', 'thread_num': 2, 'device_type': -1, 'mem_optim': True}\r\nINFO 2021-05-25 08:46:07,942 [local_service_handler.py:141] Models(../output/ppocr_rec_mobile_2.0_serving) will be launched by device gpu. use_gpu:True, use_trt:False, use_lite:False, use_xpu:False, device_type:-1, devices:[0], mem_optim:True, ir_optim:True, use_profile:False, thread_num:2, client_type:local_predictor, fetch_names:['save_infer_model/scale_0.tmp_1']\r\nINFO 2021-05-25 08:46:07,942 [operator.py:221] rec \r\n\tinput_ops: @DAGExecutor,\r\n\tserver_endpoints: None\r\n\tfetch_list: ['save_infer_model/scale_0.tmp_1']\r\n\tclient_config: ../output/ppocr_rec_mobile_2.0_serving/serving_server_conf.prototxt\r\n\tconcurrency: 1,\r\n\ttimeout(s): -1,\r\n\tretry: 1,\r\n\tbatch_size: 1,\r\n\tauto_batching_timeout(s): None\r\nINFO 2021-05-25 08:46:07,942 [pipeline_server.py:205] ============= PIPELINE SERVER =============\r\nINFO 2021-05-25 08:46:07,942 [pipeline_server.py:206] \r\n{\r\n    \"rpc_port\":18090,\r\n    \"http_port\":9999,\r\n    \"worker_num\":20,\r\n    \"build_dag_each_worker\":false,\r\n    \"dag\":{\r\n        \"is_thread_op\":false,\r\n        \"retry\":1,\r\n        \"use_profile\":false,\r\n        \"tracer\":{\r\n            \"interval_s\":10\r\n        },\r\n        \"client_type\":\"brpc\",\r\n        \"channel_size\":0\r\n    },\r\n    \"op\":{\r\n        \"rec\":{\r\n            \"concurrency\":1,\r\n            \"timeout\":-1,\r\n            \"retry\":1,\r\n            \"local_service_conf\":{\r\n                \"client_type\":\"local_predictor\",\r\n                \"model_config\":\"../output/ppocr_rec_mobile_2.0_serving\",\r\n                \"fetch_list\":[\r\n                    \"save_infer_model/scale_0.tmp_1\"\r\n                ],\r\n                \"devices\":\"0\",\r\n                \"ir_optim\":true,\r\n                \"workdir\":\"\",\r\n                \"thread_num\":2,\r\n                \"device_type\":-1,\r\n                \"mem_optim\":true\r\n            },\r\n            \"batch_size\":1,\r\n            \"auto_batching_timeout\":-1\r\n        }\r\n    }\r\n}\r\nINFO 2021-05-25 08:46:07,942 [pipeline_server.py:213] -------------------------------------------\r\nINFO 2021-05-25 08:46:07,942 [operator.py:257] Op(rec) use local rpc service at port: []\r\nINFO 2021-05-25 08:46:08,004 [dag.py:493] [DAG] Succ init\r\nINFO 2021-05-25 08:46:08,005 [dag.py:651] ================= USED OP =================\r\nINFO 2021-05-25 08:46:08,005 [dag.py:654] rec\r\nINFO 2021-05-25 08:46:08,005 [dag.py:655] -------------------------------------------\r\nINFO 2021-05-25 08:46:08,063 [dag.py:784] [DAG] Succ build DAG\r\nINFO 2021-05-25 08:46:08,072 [dag.py:816] [DAG] start\r\nINFO 2021-05-25 08:46:08,073 [dag.py:181] [DAG] set in channel succ, name [@DAGExecutor]\r\nINFO 2021-05-25 08:46:08,084 [pipeline_server.py:47] [PipelineServicer] succ init\r\nINFO 2021-05-25 08:46:08,120 [local_service_handler.py:141] Models(../output/ppocr_rec_mobile_2.0_serving) will be launched by device gpu. use_gpu:True, use_trt:False, use_lite:False, use_xpu:False, device_type:-1, devices:[0], mem_optim:True, ir_optim:True, use_profile:False, thread_num:2, client_type:local_predictor, fetch_names:None\r\nINFO 2021-05-25 08:46:08,120 [operator.py:1074] Init cuda env in process 0\r\nINFO 2021-05-25 08:46:08,120 [local_service_handler.py:184] GET_CLIENT : concurrency_idx=0, device_num=1\r\nCRITICAL 2021-05-25 08:46:08,126 [operator.py:1082] [rec|0] failed to init op: load_model_config() got an unexpected keyword argument 'use_lite'\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/pipeline/operator.py\", line 1076, in _run\r\n    self.local_predictor = self.service_handler.get_client(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/paddle_serving_server_gpu/pipeline/local_service_handler.py\", line 190, in get_client\r\n    self._local_predictor_client.load_model_config(\r\nTypeError: load_model_config() got an unexpected keyword argument 'use_lite'\r\n\r\n```\r\n\r\n\r\n**源码如下**\r\n```python\r\nfrom paddle_serving_server_gpu.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nimport cv2\r\nimport base64\r\nfrom ocr_reader import OCRReader\r\nfrom paddle_serving_app.reader import GetRotateCropImage, SortedBoxes\r\n\r\n_LOGGER = logging.getLogger()\r\n\r\n\r\nclass RecOp(Op):\r\n    def init_op(self):\r\n        self.ocr_reader = OCRReader(\r\n            char_dict_path=\"./ppocr_keys_v1.txt\")\r\n\r\n        self.get_rotate_crop_image = GetRotateCropImage()\r\n        self.sorted_boxes = SortedBoxes()\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        img_lst = []\r\n        max_wh_ratio = 0\r\n        for img_byte in input_dict[\"images\"]:\r\n            data = base64.b64decode(img_byte.encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            img_lst.append(im)\r\n        imgs = self.ocr_reader.preprocess(img_lst)\r\n        feed = {\"x\": imgs}\r\n        return feed, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, log_id):\r\n        rec_res = self.ocr_reader.postprocess(fetch_dict, with_score=True)\r\n        res_lst = []\r\n        for res in rec_res:\r\n            res_lst.append(res[0])\r\n        res = {\"res\": str(res_lst)}\r\n        return res, None, \"\"\r\n\r\n\r\nclass RecService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        rec_op = RecOp(name=\"rec\", input_ops=[read_op])\r\n        return rec_op\r\n\r\n\r\nservice = RecService(name=\"rec\")\r\nservice.prepare_pipeline_config(\"config_rec.yml\")\r\nservice.run_service()\r\n```\r\n\r\n**ppocr_keys_v1.txt**\r\nwget https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/ppocr/utils/ppocr_keys_v1.txt\r\n\r\n**ocr_reader.py**\r\nwget https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/deploy/pdserving/ocr_reader.py\r\n",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-25T09:00:54+00:00",
        "updated_at": "2024-03-05T06:49:37+00:00",
        "closed_at": "2024-03-05T06:49:37+00:00",
        "comments_count": [
            "TeslaZhao",
            "zouxiaoshi",
            "zouxiaoshi",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1265,
        "title": "uci_housing和lac例子跑不通",
        "body": "安装环境\r\n```dockerfile\r\nFROM registry.baidubce.com/paddlepaddle/serving:0.6.0-devel\r\n\r\nRUN pip install paddle-serving-server paddle-serving-app paddle-serving-client\r\n\r\nARG MODEL=uci_housing\r\n\r\nRUN wget --no-check-certificate https://paddle-serving.bj.bcebos.com/${MODEL}.tar.gz && tar -xzvf ${MODEL}.tar.gz\r\n\r\nENV MODEL=${MODEL}\r\n\r\nCMD python3 -m paddle_serving_server.serve --model ${MODEL}_model --port 8080 --name ${MODEL}\r\n```\r\n\r\n跑起镜像后container内curl\r\n```console\r\n$ curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://127.0.0.1:8080/uci_housing/prediction\r\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\r\n<title>500 Internal Server Error</title>\r\n<h1>Internal Server Error</h1>\r\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\r\n```\r\nserver 部分 log\r\n```console\r\n2021-05-27 09:20:37 (5.95 MB/s) - 'serving-cpu-avx-openblas-0.6.1.tar.gz' saved [90292300/90292300]\r\n\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralCopyOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralTextReaderOp\r\nI0100 00:00:00.000000    35 op_repository.h:68] RAW: Succ regist op: GeneralTextResponseOp\r\nI0100 00:00:00.000000    35 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000    35 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000    35 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000    35 general_model_service.pb.h:1507] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000    35 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000    35 paddle_engine.cpp:29] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0527 09:21:24.620241    68 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:12000\"): added 1\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0527 09:21:40.606832    84 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:12000\"): added 1\r\nIllegal instruction\r\nFrist time run, downloading PaddleServing components ...\r\nDecompressing files ..\r\nGoing to Run Comand\r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server/serving-cpu-avx-openblas-0.6.1/serving -enable_model_toolkit -inferservice_path workdir -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 12000 -precision fp32 -use_calib False -reload_interval_s 10 -resource_path workdir -resource_file resource.prototxt -workflow_path workdir -workflow_file workflow.prototxt -bthread_concurrency 2 -max_body_size 67108864\r\nW0527 09:21:40.621665    84 predictor.hpp:129] inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of fd=9 SocketId=1@127.0.0.1:12000@60890 [R1][E111]Fail to connect SocketId=8589934594@127.0.0.1:12000: Connection refused [R2][E112]Fail to select server from list://127.0.0.1:12000 lb=la\r\nE0527 09:21:40.621804    84 general_model.cpp:423] failed call predictor with req: insts { tensor_array { float_data: 0.0137 float_data: -0.1136 float_data: 0.2553 float_data: -0.0692 float_data: 0.0582 float_data: -0.0727 float_data: -0.1583 float_data: -0.0584 float_data: 0.6283 float_data: 0.4919 float_data: 0.1856 float_data: 0.0795 float_data: -0.0332 elem_type: 1 shape: 1 shape: 13 } } fetch_var_names: \"price\" log_id: 0\r\n[2021-05-27 09:21:40,624] ERROR in app: Exception on /uci_housing/prediction [POST]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/app.py\", line 1952, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/app.py\", line 1821, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/app.py\", line 1950, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/usr/local/lib/python3.6/site-packages/flask/app.py\", line 1936, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/serve.py\", line 426, in run\r\n    return web_service.get_prediction(request)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/web_service.py\", line 258, in get_prediction\r\n    feed=request.json[\"feed\"], fetch=fetch, fetch_map=fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/web_service.py\", line 360, in postprocess\r\n    for key in fetch_map:\r\nTypeError: 'NoneType' object is not iterable\r\n```",
        "state": "closed",
        "user": "Ruminateer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-27T09:32:31+00:00",
        "updated_at": "2024-03-05T06:49:38+00:00",
        "closed_at": "2024-03-05T06:49:38+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Ruminateer",
            "zhangjun",
            "Ruminateer",
            "zhangjun",
            "Jnoee",
            "TeslaZhao",
            "Jnoee",
            "TeslaZhao",
            "TeslaZhao",
            "Ruminateer",
            "Jnoee",
            "TeslaZhao",
            "Ruminateer",
            "TeslaZhao",
            "Jnoee",
            "bjjwwang",
            "Jnoee",
            "TeslaZhao",
            "Ruminateer",
            "TeslaZhao",
            "TeslaZhao",
            "Ruminateer",
            "Ruminateer",
            "Jnoee",
            "Jnoee",
            "TeslaZhao",
            "TeslaZhao",
            "Jnoee",
            "TeslaZhao",
            "Jnoee"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1273,
        "title": "pipeline server 重写 process 通过grpc调用其他模型",
        "body": "想通过重写process 函数，用grpc 调用其他模型，可否给个例子。",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-04T03:35:42+00:00",
        "updated_at": "2024-03-05T06:49:39+00:00",
        "closed_at": "2024-03-05T06:49:39+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1276,
        "title": "pipline 部署 cascade 模型 http 分析了多张图像后报 Segmentation fault",
        "body": "如题，使用 pipline 的方式部署了 cascade 服务，使用 http 接口进行图像预测，使用多线程方式调接口，分析了多张图像之后出现段错误。\r\n\r\n# 测试环境\r\n\r\n- CUDA 11.2\r\n- 显卡：RTX 3090\r\n- python 3.7.0\r\n- PaddlePaddle 2.1.0.post112\r\n- paddle-serving-server-gpu 0.6.0.post11\r\n- paddle_serving_app 0.6.0\r\n\r\n# web_service.py\r\n\r\n```python\r\nimport sys\r\nimport base64\r\nimport logging\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom paddle_serving_app.reader import *\r\nfrom paddle_serving_server.web_service import WebService, Op\r\n\r\nNAME = \"bank\"\r\n\r\nclass CascadeBankOp(Op):\r\n    def init_op(self):\r\n        self.img_preprocess = Sequential([\r\n            BGR2RGB(), Div(255.0),\r\n            Normalize([0.4966, 0.4876, 0.4861], [0.0270, 0.0245, 0.0238], False),\r\n            Resize((720, 1280)), Transpose((2, 0, 1)), PadStride(32)\r\n        ])\r\n        self.img_postprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        #print(\"keys\", input_dict.keys())\r\n        for key in input_dict.keys():\r\n            data = base64.b64decode(input_dict[key].encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            im = self.img_preprocess(im)\r\n            imgs.append({\r\n              \"image\": im[np.newaxis,:],\r\n              \"im_shape\": np.array(list(im.shape[1:])).reshape(-1)[np.newaxis,:],\r\n              \"scale_factor\": np.array([1.0, 1.0]).reshape(-1)[np.newaxis,:],\r\n            })\r\n        feed_dict = {\r\n            \"image\": np.concatenate([x[\"image\"] for x in imgs], axis=0),\r\n            \"im_shape\": np.concatenate([x[\"im_shape\"] for x in imgs], axis=0),\r\n            \"scale_factor\": np.concatenate([x[\"scale_factor\"] for x in imgs], axis=0)\r\n        }\r\n        #for key in feed_dict.keys():\r\n        #    print(key, feed_dict[key].shape)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, log_id):\r\n        # print(fetch_dict)\r\n        bbox_result = self.img_postprocess(fetch_dict, visualize=False)\r\n        bbox_result = list(filter(lambda x: x.get(\"score\") > 0.5, bbox_result))\r\n        res_dict = {\"bbox_result\": str(bbox_result)}\r\n        return res_dict, None, \"\"\r\n\r\n\r\nclass CascadeBankService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        cascade_bank_op = CascadeBankOp(name=NAME, input_ops=[read_op])\r\n        return cascade_bank_op\r\n\r\n\r\ncascade_bank_service = CascadeBankService(name=NAME)\r\ncascade_bank_service.prepare_pipeline_config(\"config.yml\")\r\ncascade_bank_service.run_service()\r\n```\r\n\r\n# config.yaml\r\n\r\n```yaml\r\ndag:\r\n  is_thread_op: false\r\n  tracer:\r\n    interval_s: 30\r\nhttp_port: 9292\r\nop:\r\n  bank:\r\n    concurrency: 4\r\n\r\n    local_service_conf:\r\n      client_type: local_predictor\r\n      device_type: 1\r\n      devices: '8'\r\n      fetch_list:\r\n      - save_infer_model/scale_0.tmp_1\r\n      model_config: serving_server/\r\nrpc_port: 9998\r\nworker_num: 32\r\n```\r\n\r\n# 错误信息\r\n\r\n```shell\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::AnalysisPredictor::ZeroCopyRun()\r\n1   paddle::framework::NaiveExecutor::Run()\r\n2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)\r\n3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const\r\n5   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CPUPlace, false, 2ul, paddle::operators::SliceKernel<paddle::platform::C\r\nPUDeviceContext, int>, paddle::operators::SliceKernel<paddle::platform::CPUDeviceContext, long>, paddle::operators::SliceKernel<paddle::platform::CPUDeviceContext, float>, paddle::operators::SliceKernel<paddle::p\r\nlatform::CPUDeviceContext, double>, paddle::operators::SliceKernel<paddle::platform::CPUDeviceContext, paddle::platform::complex64>, paddle::operators::SliceKernel<paddle::platform::CPUDeviceContext, paddle::plat\r\nform::complex128> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)\r\n6   void paddle::operators::SliceKernel<paddle::platform::CPUDeviceContext, float>::SliceCompute<2ul>(paddle::framework::ExecutionContext const&) const\r\n7   Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorSlicingOp<Eigen::DSizes<int, 2> const, Eigen::DSizes<int, 2> const\r\n, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const> const> const, Eigen::DefaultDevice, true, (Eigen::internal::TiledEvaluation)1>::run(Eigen::TensorAssignOp<Eigen::TensorMap<\r\nEigen::Tensor<float, 2, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorSlicingOp<Eigen::DSizes<int, 2> const, Eigen::DSizes<int, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePo\r\ninter> const> const> const&, Eigen::DefaultDevice const&)\r\n8   paddle::framework::SignalHandle(char const*, int)\r\n9   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1623228748 (unix time) try \"date -d @1623228748\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x7fd13d3f1600) received by PID 17800 (TID 0x7fd2b7261740) from PID 1027544576 ***]\r\n```",
        "state": "closed",
        "user": "okooo5km",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-09T09:07:29+00:00",
        "updated_at": "2024-10-29T06:44:01+00:00",
        "closed_at": "2024-10-29T06:44:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "okooo5km",
            "TeslaZhao",
            "okooo5km",
            "okooo5km",
            "TeslaZhao",
            "HuiHuiSun"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1285,
        "title": "多模型共用TaskExecutor的疑问",
        "body": "ReloadableInferEngine::proc_initialize中使用TaskExecutor的是个单例，那么多个模型会一次去设置、启动同一个TaskExecutor；\r\n在调用schedule()之后，TaskExecutor如何确定去调用哪个模型呢？\r\n",
        "state": "closed",
        "user": "BraveLii",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-22T13:15:21+00:00",
        "updated_at": "2024-03-05T06:49:40+00:00",
        "closed_at": "2024-03-05T06:49:40+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "HexToString"
        ],
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1288,
        "title": "部署ernie-tiny，server端报错维度不匹配",
        "body": "使用paddlehub训练好模型后，得到静态图参数和模型，使用paddle_serving_client.convert 转化成inference model 并且得到config。config长这样：\r\nfeed_var {\r\n  name: \"input_ids\"\r\n  alias_name: \"input_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 256\r\n  shape: 1\r\n}\r\n\r\nfeed_var {\r\n  name: \"position_ids\"\r\n  alias_name: \"position_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 256\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"segment_ids\"\r\n  alias_name: \"segment_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n  shape: 256\r\n  shape: 1\r\n}\r\nfeed_var {\r\n  name: \"input_mask\"\r\n  alias_name: \"input_mask\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 256\r\n  shape: 1\r\n}\r\nfetch_var {\r\n  name: \"fc_0.tmp_2\"\r\n  alias_name: \"fc_0.tmp_2\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n\r\n在client端使用reader预处理得到输入input_ids,segment_ids,positon_ids,input_mask\r\n，定义的max_len是256，所以每个输入的维度都是1X256X1，然后传到feed的dict里，server端会报这个错：\r\n\r\nInvalidArgumentError: The first dimension value of Input(Scale) must equal to be thesecond dimension value of the flattened 2D matrix of Input(X),But received the first dimension value of Input(Scale) is[1024], the second dimension value of the flattened 2D matrix of Input(Scale) is [262144].\r\n  [Hint: Expected ctx->GetInputDim(\"Scale\")[0] == right, but received ctx->GetInputDim(\"Scale\")[0]:1024 != right:262144.] (at /paddle/paddle/fluid/operators/layer_norm_op.cc:72)\r\n  [operator < layer_norm > error]\r\n",
        "state": "closed",
        "user": "Karenlyw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-30T15:55:51+00:00",
        "updated_at": "2024-03-05T06:49:41+00:00",
        "closed_at": "2024-03-05T06:49:41+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Karenlyw",
            "TeslaZhao",
            "Karenlyw",
            "Karenlyw",
            "TeslaZhao"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1287,
        "title": "windows下docker运行serving的imagenet示例，docker里面HTTP调用成功，宿主机windowsHTTP调用连不上",
        "body": "windows下docker运行serving的imagenet示例，docker里面HTTP调用成功，宿主机windowsHTTP调用连不上\r\n这种情况要怎么处理？",
        "state": "closed",
        "user": "pipishaw",
        "closed_by": "pipishaw",
        "created_at": "2021-06-27T12:59:41+00:00",
        "updated_at": "2021-07-02T13:29:51+00:00",
        "closed_at": "2021-07-02T13:29:51+00:00",
        "comments_count": [
            "bjjwwang",
            "pipishaw",
            "pipishaw"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1295,
        "title": "Pipeline Serving 的RPC服务启用GRPC多编程语言支持时，无法使用GPU",
        "body": "如题，使用 paddle serving的RPC多语言模式部署RPC服务，使用 java调用进行图像预测RPC接口，预测图像的时候不走GPU。\r\n\r\n### 测试环境\r\nCUDA 11.2\r\n显卡：RTX 3090\r\npython 3.6.0\r\npaddle-serving-server-gpu==0.6.0.post102\r\npaddle-serving-client==0.6.0\r\npaddle-serving-app==0.6.0\r\npaddlepaddle-gpu==2.1.0\r\n\r\n### 测试命令\r\n**服务端**\r\n`python3 -m paddle_serving_server.serve --model yolov4_model --use_multilang --port 9393 --gpu_ids 0`\r\n\r\n客户端代码使用\r\n`Serving-0.6.0/java/examples/src/main/java/PaddleServingClientExample.java`\r\n\r\n测试调用1次时间超过2s，而使用python直接预测时间小于0.1s\r\n<img width=\"1105\" alt=\"1\" src=\"https://user-images.githubusercontent.com/26927826/125217795-21afd080-e2f4-11eb-8b5a-e16e901948f3.png\">\r\n同时使用nvidia-smi监控gpu占用，未发现对应的python进程\r\n![2](https://user-images.githubusercontent.com/26927826/125217804-270d1b00-e2f4-11eb-8e31-71e814735ddb.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "ghost",
        "closed_by": null,
        "created_at": "2021-07-12T01:33:17+00:00",
        "updated_at": "2021-07-13T09:40:46+00:00",
        "closed_at": "2021-07-13T09:38:48+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "HexToString",
            "ghost",
            "ghost",
            "HexToString",
            "ghost",
            "HexToString",
            "ghost",
            "HexToString",
            "HexToString",
            "ghost",
            "HexToString"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1298,
        "title": "v0.6.2按照文档部署pipeline测试的时候卡住了【CPU版】",
        "body": "使用的版本是0.6.2分支，房价预测fit_a_line顺利通过，但是测试pipeline ocr的时候出现问题，启动web_service.py后停在了  start proxy service；启动pipeline_http_client.py 时候无任何输出，挂在了那儿不动了，无法完成测试\r\n![image](https://user-images.githubusercontent.com/1081377/125383797-96553e80-e3ca-11eb-8a92-08d6c620b479.png)\r\n\r\nhost系统是Mac OS 11.4， docker版本是 docker desktop 3.3.3(64133)，使用的官方镜像 registry.baidubce.com/paddlepaddle/serving:0.6.2-devel，dockerfile中安装了各种相关的包\r\n```\r\nRUN cd /home && git clone -b v0.6.2 https://github.com/PaddlePaddle/Serving\r\nRUN cd /home/Serving && pip3 install -r python/requirements.txt\r\nRUN pip3 install paddle-serving-client==0.6.2 paddle-serving-server==0.6.2 paddle-serving-app==0.6.2\r\n```",
        "state": "closed",
        "user": "learningpro",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-13T03:08:34+00:00",
        "updated_at": "2024-03-05T06:49:42+00:00",
        "closed_at": "2024-03-05T06:49:42+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1299,
        "title": "目标检测 如何实现http请求",
        "body": "给予 PaddleDetection 进行的目标检测训练，部署serving，但只找到了通过RPC形式进行测试，请问怎么实现http形式？\r\n\r\n尝试用curl:\r\n curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"image\":  \"beizi608.jpeg”}], \"fetch\":[\"save_infer_model/scale_0.tmp_1\"]}' http://127.0.0.1:9292/detection/prediction\r\n\r\n抛出错误：\r\n[172.17.0.2:9292][E1002]Fail to find method on `/detection/prediction'",
        "state": "closed",
        "user": "lizhangling1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-13T07:02:55+00:00",
        "updated_at": "2024-03-05T06:49:43+00:00",
        "closed_at": "2024-03-05T06:49:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1300,
        "title": "部署serving的error",
        "body": "调用paddle_serving_server.serve时候出现error\r\n可能是miss了哪个步骤吧，无从下手 :( \r\n\r\n![image](https://user-images.githubusercontent.com/56196194/125419110-a3505b26-8078-4a0e-9f46-c02b756b70aa.png)\r\n",
        "state": "closed",
        "user": "iwnl-chr1s",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-13T08:33:55+00:00",
        "updated_at": "2024-03-05T06:49:43+00:00",
        "closed_at": "2024-03-05T06:49:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "iwnl-chr1s",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1302,
        "title": "有运行时镜像吗？devel镜像太大了",
        "body": "带cuda的镜像14G，不带cuda的镜像5.8G。\r\n有没有精简的镜像？只需要支持加载模型运行的最简镜像。",
        "state": "closed",
        "user": "Jnoee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-13T09:42:57+00:00",
        "updated_at": "2024-04-12T08:20:47+00:00",
        "closed_at": "2024-04-12T08:20:47+00:00",
        "comments_count": [
            "bjjwwang",
            "Jnoee",
            "zouxiaoshi",
            "hbo-lambda"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1301,
        "title": "用docker的方式部署能支持加载多个模型吗？",
        "body": "用docker的方式部署能支持加载多个模型吗？\r\n怎么做？有没有例子？",
        "state": "closed",
        "user": "Jnoee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-13T09:40:56+00:00",
        "updated_at": "2024-03-05T06:49:44+00:00",
        "closed_at": "2024-03-05T06:49:44+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "HexToString",
            "Jnoee",
            "HexToString",
            "dwdcth",
            "yqp1453552101"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1306,
        "title": "pipeline客户端一直报错transport is closing",
        "body": null,
        "state": "closed",
        "user": "Karenlyw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-14T08:09:33+00:00",
        "updated_at": "2024-04-16T09:05:42+00:00",
        "closed_at": "2024-04-16T09:05:42+00:00",
        "comments_count": [
            "HexToString",
            "Karenlyw",
            "HexToString",
            "TeslaZhao",
            "Karenlyw",
            "TeslaZhao",
            "Karenlyw"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1307,
        "title": "如图，目标检测，模型直接预测没问题，部署成serving，通过rpc访问，返回结果，框偏了",
        "body": "![image](https://user-images.githubusercontent.com/55176464/125586930-14342a89-db54-4fe9-95d0-9c2470618a99.png)\r\n",
        "state": "closed",
        "user": "lizhangling1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-14T08:09:34+00:00",
        "updated_at": "2024-03-05T06:49:45+00:00",
        "closed_at": "2024-03-05T06:49:45+00:00",
        "comments_count": [
            "bjjwwang",
            "Nick198903",
            "TeslaZhao",
            "Nick198903"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1310,
        "title": "如何导出.proto文件？",
        "body": "使用https://github.com/PaddlePaddle/Serving/blob/v0.6.0/doc/SAVE_CN.md中的说明\r\n成功保存用于Paddle Serving的模型，已可以正常使用Python客户端与服务端的模型进行通信，得到识别结果。\r\n\r\n现希望用C++编写另外的客户端，要求高并发，高效的通信，选择方式使用bRPC进行通信。\r\nbRPC需要.prtot文件进行ProtoBuf编码，但是导出文件中只有.prototxt文件，没有.prtot文件。.prototxt文件中包含的信息少于.prtot文件，我无法从.prototxt文件导出.prtot文件。\r\n\r\n我现在的问题是缺少.prtot文件，无法使用bRPC进行通信。",
        "state": "closed",
        "user": "Hengmen",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-14T09:14:55+00:00",
        "updated_at": "2024-04-16T09:05:43+00:00",
        "closed_at": "2024-04-16T09:05:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "Hengmen"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1312,
        "title": "crnn inference模型转 server 模型没有ctc_greedy_decoder_0.tmp_0",
        "body": "inference模型转 server 模型没有client 文件夹里的prototxt 配置文件ctc_greedy_decoder_0.tmp_0\r\n![baidu1](https://user-images.githubusercontent.com/18739799/125783741-fefc256e-670a-4d4f-8160-e8b9b0a4f170.png)\r\n![baidu2](https://user-images.githubusercontent.com/18739799/125783783-3da3049b-16b7-4004-aaa5-7aa230e8143e.png)\r\n\r\n上面是百度提供的转换好的server模型，下面是训练的模型转换为server模型生成的prototxt 文件。\r\n在预测的时候会提示没有对应的key\r\n报错地址是 /usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/ocr_reader.py，下面这两行代码要根据key取数据。\r\nrec_idx_lod = outputs[\"save_infer_model/scale_0.tmp_1.lod\"]    \r\nrec_idx_batch = outputs[\"save_infer_model/scale_0.tmp_1\"]\r\n\r\n查看了recop 里面 postprocess 接收数据的字典key:\r\ndict_keys(['ctc_greedy_decoder_0.tmp_0', 'ctc_greedy_decoder_0.tmp_0.lod', 'softmax_0.tmp_0', 'softmax_0.tmp_0.lod'])\r\n\r\ndict_keys(['save_infer_model/scale_0.tmp_1'])\r\n下面是 python3 -m paddle_serving_client.convert 转换的，paddle_serving_client版本0.6\r\n",
        "state": "closed",
        "user": "Jeffrey98-AI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-15T11:53:35+00:00",
        "updated_at": "2024-04-16T09:05:44+00:00",
        "closed_at": "2024-04-16T09:05:44+00:00",
        "comments_count": [],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1311,
        "title": "serving部署faster rcnn模型出错",
        "body": "infer_cfg.yml文件如下：\r\nuse_python_inference: false\r\nmode: fluid\r\ndraw_threshold: 0.5\r\nmetric: COCO\r\narch: RCNN\r\nmin_subgraph_size: 40\r\nwith_background: true\r\nPreprocess:\r\n- is_channel_first: false\r\n  is_scale: true\r\n  mean:\r\n  - 0.485\r\n  - 0.456\r\n  - 0.406\r\n  std:\r\n  - 0.229\r\n  - 0.224\r\n  - 0.225\r\n  type: Normalize\r\n- interp: 1\r\n  max_size: 1333\r\n  target_size: 800\r\n  type: Resize\r\n  use_cv2: true\r\n- channel_first: true\r\n  to_bgr: false\r\n  type: Permute\r\n- stride: 32\r\n  type: PadStride\r\nlabel_list:\r\n- background\r\n- hat\r\n- person\r\n\r\nserving_client_conf.prototxt文件如下：\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_info\"\r\n  alias_name: \"im_info\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_size\"\r\n  alias_name: \"im_size\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms_0.tmp_0\"\r\n  alias_name: \"multiclass_nms_0.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n\r\n执行客户端预测报错：\r\nshi@shi-B360M-D2V:~/Desktop/20201102/tree/hat_code/inference_model/model$ python3 ../../deploy/serving/test_client.py ../../demo/3512.jpg\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0715 15:20:19.053577 14249 naming_service_thread.cpp:209] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nW0715 15:20:19.217628 14249 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW0715 15:20:19.217649 14249 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW0715 15:20:19.217653 14249 init.cc:231] The detail failure signal is:\r\n\r\nW0715 15:20:19.217655 14249 init.cc:234] *** Aborted at 1626333619 (unix time) try \"date -d @1626333619\" if you are using GNU date ***\r\nW0715 15:20:19.218354 14249 init.cc:234] PC: @                0x0 (unknown)\r\nW0715 15:20:19.218443 14249 init.cc:234] *** SIGSEGV (@0x60) received by PID 14249 (TID 0x7fc67df2f740) from PID 96; stack trace: ***\r\nW0715 15:20:19.219072 14249 init.cc:234]     @     0x7fc67d9aa040 (unknown)\r\nW0715 15:20:19.219252 14249 init.cc:234]     @     0x7fc5bd9499ab baidu::paddle_serving::general_model::PredictorClient::numpy_predict()\r\nW0715 15:20:19.219409 14249 init.cc:234]     @     0x7fc5bd9623bb _ZZN8pybind1112cpp_function10initializeIZN5baidu14paddle_serving13general_modelL28pybind11_init_serving_clientERNS_6moduleEEUlRNS4_15PredictorClientERKSt6vectorIS9_INS_7array_tIfLi16EEESaISB_EESaISD_EERKS9_ISsSaISsEERKS9_IS9_IiSaIiEESaISN_EERKS9_IS9_INSA_IlLi16EEESaISS_EESaISU_EESL_SR_SL_RNS4_12PredictorResERKiE12_iIS8_SH_SL_SR_SY_SL_SR_SL_S10_S12_EINS_4nameENS_9is_methodENS_7siblingENS_10call_guardIINS_18gil_scoped_releaseEEEEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE1_4_FUNES1N_.51054\r\nW0715 15:20:19.219579 14249 init.cc:234]     @     0x7fc5bd957db2 pybind11::cpp_function::dispatcher()\r\nW0715 15:20:19.219713 14249 init.cc:234]     @           0x566bec _PyCFunction_FastCallDict\r\nW0715 15:20:19.219835 14249 init.cc:234]     @           0x50a533 (unknown)\r\nW0715 15:20:19.219952 14249 init.cc:234]     @           0x50bf44 _PyEval_EvalFrameDefault\r\nW0715 15:20:19.220070 14249 init.cc:234]     @           0x507cd4 (unknown)\r\nW0715 15:20:19.220188 14249 init.cc:234]     @           0x509a00 (unknown)\r\nW0715 15:20:19.220304 14249 init.cc:234]     @           0x50a3fd (unknown)\r\nW0715 15:20:19.220412 14249 init.cc:234]     @           0x50cd15 _PyEval_EvalFrameDefault\r\nW0715 15:20:19.220527 14249 init.cc:234]     @           0x507cd4 (unknown)\r\nW0715 15:20:19.220600 14249 init.cc:234]     @           0x50ae13 PyEval_EvalCode\r\nW0715 15:20:19.220721 14249 init.cc:234]     @           0x635262 (unknown)\r\nW0715 15:20:19.220825 14249 init.cc:234]     @           0x635317 PyRun_FileExFlags\r\nW0715 15:20:19.220890 14249 init.cc:234]     @           0x638acf PyRun_SimpleFileExFlags\r\nW0715 15:20:19.220984 14249 init.cc:234]     @           0x639671 Py_Main\r\nW0715 15:20:19.221087 14249 init.cc:234]     @           0x4b0e40 main\r\nW0715 15:20:19.221774 14249 init.cc:234]     @     0x7fc67d98cbf7 __libc_start_main\r\nW0715 15:20:19.221845 14249 init.cc:234]     @           0x5b2f0a _start\r\nW0715 15:20:19.222424 14249 init.cc:234]     @                0x0 (unknown)\r\nSegmentation fault (core dumped)\r\n请问该问题如何解决？",
        "state": "closed",
        "user": "gss-bai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-15T08:06:03+00:00",
        "updated_at": "2024-03-05T06:49:46+00:00",
        "closed_at": "2024-03-05T06:49:46+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1313,
        "title": "pipeline serving 方式部署模型是否支持模型加解密",
        "body": "我查看了 Serving/python/examples/encryption/ 这个关于模型加解密的例子，请问采用pipeline serving方式部署模型是否支持加解密操作？需要做哪些配置？",
        "state": "closed",
        "user": "dianziMAN",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-16T09:10:00+00:00",
        "updated_at": "2024-04-16T09:05:45+00:00",
        "closed_at": "2024-04-16T09:05:45+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TeslaZhao",
            "dianziMAN"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1314,
        "title": "部署在alstudio上的服务端可以在本地机器上访问吗",
        "body": "",
        "state": "closed",
        "user": "zjqwer123",
        "closed_by": "TeslaZhao",
        "created_at": "2021-07-19T06:51:53+00:00",
        "updated_at": "2021-07-21T03:29:16+00:00",
        "closed_at": "2021-07-21T03:29:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1316,
        "title": "模型部署后占用的GPU显存大小与什么相关？",
        "body": "感觉显存不太够用，所以想有没有什么办法可以节约GPU显存。\r\n\r\n## 环境：\r\n显卡： Tesla P4   Tesla P100\r\npaddle-serving-server-gpu 0.6.1.post102\r\npaddlepaddle-gpu          2.1.0\r\npaddle-serving-client     0.6.1\r\npaddle-serving-app        0.6.1\r\n\r\n## 启动方式：\r\n```bash\r\npython -m paddle_serving_server.serve \\\r\n--model serving_server \\\r\n--thread 2 --port 9000 \\\r\n--gpu_ids 0 \\\r\n--use_trt \\\r\n--precision FP16\r\n```\r\n\r\n## 测试如下：  (测试1-3 用的是Tesla P4)\r\n```\r\n测试1： 同一模型架构，不同体积。 model_server, model_mobile(轻量backbone,量化压缩后的模型）\r\n模型                     模型大小                     模型占用GPU显存\r\nmodel_server            108MB                      2243MB\r\nmodel_mobile            2.6MB                       2243MB\r\n\r\n测试2：不同模型架构，相似体积。\r\n模型                     模型大小                  模型占用GPU显存\r\nmodel_A                  108MB                       2243MB\r\nmodel_B                  114MB                      2243MB\r\n\r\n测试3： 同一模型，是否启用tensorRT模式 （tensorRT模式下还尝试了不同precision FP32,FP16，GPU显存无变化）\r\nTensorRt                  模型大小              模型占用GPU显存\r\n启用                          108MB                      2243MB\r\n不启用                      108MB                     2243MB\r\n\r\n测试4： 不同GPU 同一模型 同一runtime镜像\r\nGPU型号             占用模型大小\r\nTesla P4                 2243MB\r\nTesla P100             2405MB\r\n```\r\n\r\nSO,\r\n模型占用GPU显存大小倒底与什么有关呢，\r\n如何在不影响性能情况下，减小GPU显存？",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-07-21T03:45:41+00:00",
        "updated_at": "2021-07-26T01:33:43+00:00",
        "closed_at": "2021-07-26T01:33:43+00:00",
        "comments_count": [
            "HexToString",
            "zouxiaoshi",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1315,
        "title": "RTX 6000显卡服务端PP YOLOv2 推演速度",
        "body": "RTX6000 单卡服务器上布置了Paddle-Serving服务，在开启服务的时候尝试了 --precision fp32、----precision fp16， --precision int8，--use_trt，测试PP yolov2推演（仅推演，不包含预处理+后处理）性能，推演时间都是130ms左右，这有点难以理解，毕竟精度减半，速度应该有较大提升才对，求指点",
        "state": "closed",
        "user": "Nick198903",
        "closed_by": "TeslaZhao",
        "created_at": "2021-07-20T07:54:26+00:00",
        "updated_at": "2021-07-21T03:11:33+00:00",
        "closed_at": "2021-07-21T03:11:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Nick198903",
            "TeslaZhao",
            "Nick198903",
            "TeslaZhao",
            "Nick198903",
            "Nick198903",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1317,
        "title": "OCR pipeline server 显存限制问题",
        "body": "paddle ocr pipeline server传入比较大的图像的时候会报下面的错误，测试小图像的base64传入却是正常的，大图分辨率1068*6118\r\n\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::framework::SignalHandle(char const*, int)\r\n1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1626861998 (unix time) try \"date -d @1626861998\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x0) received by PID 12224 (TID 0x7fdadb87b700) from PID 0 ***]\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n软件版本：\r\npaddle-serving-app        0.6.0\r\npaddle-serving-client     0.6.0\r\npaddle-serving-server-gpu 0.6.0.post102\r\npaddlepaddle-gpu          2.1.0\r\n\r\n调试了一下，发现与图像分辨率没什么关系，OCR 检测OP是没问题的，识别OP里面预处理也没问题，没有运行到后处理那一步，应该是在识别的预测阶段出错，与文本行数有关，33行以下就没问题，超过33行就会出现上面的错误\r\n\r\n",
        "state": "closed",
        "user": "Jeffrey98-AI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-21T10:10:13+00:00",
        "updated_at": "2024-03-05T06:49:47+00:00",
        "closed_at": "2024-03-05T06:49:47+00:00",
        "comments_count": [
            "HexToString",
            "HexToString",
            "TeslaZhao",
            "Jeffrey98-AI",
            "TeslaZhao",
            "Jeffrey98-AI",
            "Jeffrey98-AI",
            "HexToString",
            "Jeffrey98-AI",
            "HexToString",
            "Jeffrey98-AI",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1318,
        "title": "profile Tracer in 步骤指的是什么",
        "body": "用的是pipeline sever, 发现程序跑的慢，尝试profile\r\n日志如下：\r\n```\r\n2021-07-22 15:13:01,026 ==================== TRACER ======================\r\n2021-07-22 15:13:01,028 Op(det):\r\n2021-07-22 15:13:01,028 \tin[3127.71525 ms]\r\n2021-07-22 15:13:01,028 \tprep[95.324125 ms]\r\n2021-07-22 15:13:01,028 \tmidp[98.8565 ms]\r\n2021-07-22 15:13:01,028 \tpostp[0.424875 ms]\r\n2021-07-22 15:13:01,028 \t \r\n2021-07-22 15:13:01,028 \tidle[0.9418589912745391]\r\n2021-07-22 15:13:01,028 Op(rec):\r\n2021-07-22 15:13:01,029 \tin[285.29783333333336 ms]\r\n2021-07-22 15:13:01,029 \tprep[96.4275 ms]\r\n2021-07-22 15:13:01,029 \tmidp[3413.7105 ms]\r\n2021-07-22 15:13:01,029 \tpostp[55.8155 ms]\r\n2021-07-22 15:13:01,029 \tout[4.778333333333333 ms]\r\n2021-07-22 15:13:01,029 \tidle[0.07522664287939007]\r\n2021-07-22 15:13:01,029 DAGExecutor:\r\n2021-07-22 15:13:01,029 \tQuery count[6]\r\n2021-07-22 15:13:01,029 \tQPS[1.2 q/s]\r\n2021-07-22 15:13:01,029 \tSucc[1.0]\r\n2021-07-22 15:13:01,029 \tError req[]\r\n2021-07-22 15:13:01,029 \tLatency:\r\n2021-07-22 15:13:01,029 \t\tave[3866.2870000000003 ms]\r\n2021-07-22 15:13:01,030 \t\t.50[3970.155 ms]\r\n2021-07-22 15:13:01,030 \t\t.60[3970.155 ms]\r\n2021-07-22 15:13:01,030 \t\t.70[4143.798 ms]\r\n2021-07-22 15:13:01,030 \t\t.80[4143.798 ms]\r\n2021-07-22 15:13:01,030 \t\t.90[4208.146 ms]\r\n2021-07-22 15:13:01,030 \t\t.95[4208.146 ms]\r\n2021-07-22 15:13:01,030 \t\t.99[4208.146 ms]\r\n2021-07-22 15:13:01,030 Channel (server worker num[4]):\r\n2021-07-22 15:13:01,030 \tchl0(In: ['@DAGExecutor'], Out: ['det']) size[0/0]\r\n2021-07-22 15:13:01,031 \tchl1(In: ['det'], Out: ['rec']) size[0/0]\r\n2021-07-22 15:13:01,031 \tchl2(In: ['rec'], Out: ['@DAGExecutor']) size[0/0]\r\n```\r\n其中\r\n```\r\n2021-07-22 15:13:01,028 Op(det):\r\n2021-07-22 15:13:01,028 \tin[3127.71525 ms]\r\n```\r\n像 prep midp postp 好理解，但是这里的 **_in[3127.71525 ms]_** 和  **_out[24.809 ms]_** 指的是什么步骤，\r\n是 RequestOP 和 ResponseOp吗?\r\n为什么   **_in[3127.71525 ms]_** 会如此慢？ 如何才能优化？\r\n",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "zouxiaoshi",
        "created_at": "2021-07-22T07:34:05+00:00",
        "updated_at": "2021-09-26T14:27:05+00:00",
        "closed_at": "2021-09-26T14:27:05+00:00",
        "comments_count": [
            "TeslaZhao",
            "zouxiaoshi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1319,
        "title": "serving0.6的版本是否支持使用paddlepaddle1.8.5训练的模型",
        "body": "如题\r\n 我使用0.4版本的serving部署，同样监控的图片部署，有的会奔溃，错误信息\r\nError Message Summary:\r\n----------------------\r\nError: Tensor not initialized yet when Tensor::type() is called.\r\n  [Hint: holder_ should not be null.] at (/paddle/paddle/fluid/framework/tensor.h:\r\n使用0.5的版本serving部署，也会有同样的情况 \r\n\r\n0.6的版本是否支持使用paddlepaddle1.8.5训练的模型？",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-23T14:05:06+00:00",
        "updated_at": "2024-04-16T09:05:46+00:00",
        "closed_at": "2024-04-16T09:05:46+00:00",
        "comments_count": [
            "TeslaZhao",
            "ponycloud235"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1320,
        "title": "serving0.6模型在特定的图片下报错",
        "body": "使用paddlepaddle1.8.5 训练的模型在serving0.6 特定图片会出差。 尝试了0.4、0.5的版本均有此问题。如果有需要可以提供模型",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-23T15:28:24+00:00",
        "updated_at": "2024-04-16T09:05:47+00:00",
        "closed_at": "2024-04-16T09:05:47+00:00",
        "comments_count": [
            "TeslaZhao",
            "HexToString",
            "dylanchiang",
            "ponycloud235"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1322,
        "title": "Jetson",
        "body": "请问Serving0.6.0在jetson平台使用，直接pip或者docket pull就可以，还是需要进行其他环境方面的编译。",
        "state": "closed",
        "user": "Sqhttwl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-26T11:46:21+00:00",
        "updated_at": "2024-03-05T06:49:48+00:00",
        "closed_at": "2024-03-05T06:49:48+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zhangjun"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1323,
        "title": "加载优化器参数文件后报错conv2d_2.w_0_moment1_0 should in state dict",
        "body": "如题，在一个.py文件中训练模型并保存模型和优化器参数文件后，执行另一个训练方法（使用相同的模型类创建的对象，保证模型结构没有更改，也保证不存在中文路径加载错误问题），使用opt.set_state_dict(opt_dict)加载刚才保存的优化器参数文件会报错AssertionError: Optimizer set error, conv2d_2.w_0_moment1_0 should in state dict。\r\n\r\n但是奇怪的是，如果先执行训练保存优化器参数文件，然后停止运行，然后把前面的训练部分注释掉，而直接执行加载优化器参数文件后进行训练，则一切正常。不知道是什么情况。无论是在aistudio还是本地pycharm运行都存在这个问题。\r\n\r\n另外，这个教程页面的代码就存在这个问题：https://www.paddlepaddle.org.cn/tutorials/projectdetail/1516118\r\n\r\n在上面的链接【10 【手写数字识别】之动转静部署】这个教程中一共有三个cell，若从头到尾执行所有cell，就会以100%的概率复现该问题。然后接着点击【代码执行器 -> 重启执行器】，重启执行器后，先执行定义网络结构的那个cell（即第一个cell），再执行加载参数文件并训练的cell（即最后一个cell），就一切正常不会报错。不知是什么原因.\r\n\r\n另外，如果从头到尾连续执行三个cell后，再多次点击执行最后的一个cell，每次点击后观察报错信息，会发现每次报的错误不一样，具体是AssertionError: Optimizer set error, conv2d_【X】.w_0_moment1_0 should in state dict这行报错信息中【X】的位置第一次是2，接下里每次执行都会加2，分别变成4、6、8、10、12....",
        "state": "closed",
        "user": "Tears1997",
        "closed_by": "TeslaZhao",
        "created_at": "2021-07-27T08:53:46+00:00",
        "updated_at": "2021-07-30T08:33:04+00:00",
        "closed_at": "2021-07-30T08:33:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1327,
        "title": "Error response from daemon: Get https://registry.baidubce.com/v2/: x509: certificate signed by unknown authority",
        "body": "[黄埔学院]\r\n系统：Centos 7\r\nPaddlePaddle-gpu: 2.1.0\r\n问题1. 在拉取Paddle Serving镜像时一直失败\r\n使用命令：nvidia-docker pull registry.baidubce.com/paddlepaddle/serving:0.6.0-cuda10.2-cudnn8-devel\r\n报错如下：\r\n<img width=\"1163\" alt=\"截屏2021-07-29 09 49 55\" src=\"https://user-images.githubusercontent.com/10570127/127420851-5a62ae30-cf56-4161-89bf-ec8894f26e36.png\">\r\n问题2. cudnn我使用的是7，但是镜像里没有cuda10.2与cudnn7的组合，这样会有问题吗",
        "state": "closed",
        "user": "Andy546",
        "closed_by": "Andy546",
        "created_at": "2021-07-29T02:14:01+00:00",
        "updated_at": "2021-07-30T02:53:15+00:00",
        "closed_at": "2021-07-30T02:53:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Andy546",
            "TeslaZhao",
            "Andy546"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1326,
        "title": "训练好的transformer翻译模型部署到服务器不能预测",
        "body": "1. 按照https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer操作训练，可以在本地电脑python环境预测模型，结果可以。\r\n2. 但按照https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/deploy/serving说明部署后，不能预测，客户端返回提示failed call predictor with req: insts { tensor_array { int64_data: 36 int64_data: 19 int64_data: 37 int64_data:。。。。}\r\n3. 部署环境：paddlepaddle-gpu==2.1.1.post102, paddle-server-gpu==0.6.2,  paddle-server-app==0.6.2,\r\n   没有安装tensorRT.",
        "state": "closed",
        "user": "dlkht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-28T08:03:23+00:00",
        "updated_at": "2024-03-05T06:49:49+00:00",
        "closed_at": "2024-03-05T06:49:49+00:00",
        "comments_count": [
            "TeslaZhao",
            "dlkht",
            "TeslaZhao",
            "dlkht",
            "TeslaZhao",
            "dlkht",
            "dlkht",
            "TeslaZhao",
            "dlkht",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1328,
        "title": "使用yolov4例子报错",
        "body": "环境：\r\ndocker PaddleServing serving:0.6.0-cuda10.2-cudnn8-devel\r\n在镜像内安装了\r\npaddle-serving-app        0.6.0\r\npaddle-serving-client     0.6.0\r\npaddle-serving-server-gpu 0.6.0.post102\r\npaddlepaddle-gpu          2.1.0\r\n运行examples/yolov4例子时一直报错TypeError: 'NoneType' object does not support item assignment\r\n<img width=\"602\" alt=\"截屏2021-07-30 09 09 47\" src=\"https://user-images.githubusercontent.com/10570127/127585329-72be2708-28a5-45b9-af77-693af0a31f0b.png\">\r\n",
        "state": "closed",
        "user": "Andy546",
        "closed_by": "Andy546",
        "created_at": "2021-07-30T01:17:44+00:00",
        "updated_at": "2021-08-04T01:06:32+00:00",
        "closed_at": "2021-08-04T01:06:32+00:00",
        "comments_count": [
            "TeslaZhao",
            "Andy546",
            "TeslaZhao",
            "Andy546",
            "TeslaZhao",
            "TeslaZhao",
            "Andy546",
            "TeslaZhao",
            "Andy546"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1331,
        "title": "关于热更新的问题",
        "body": "您好，我想请问如果不用monitor服务，直接替换serving_server中的模型文件，不重启server服务的话可以实现模型热更新吗？",
        "state": "closed",
        "user": "Karenlyw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-03T06:50:12+00:00",
        "updated_at": "2024-04-16T09:05:48+00:00",
        "closed_at": "2024-04-16T09:05:48+00:00",
        "comments_count": [
            "TeslaZhao",
            "Karenlyw"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1332,
        "title": "Serving 是否支持TTS流式合成这种任务？",
        "body": "Serving 是否支持TTS流式合成这种任务？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-03T14:25:10+00:00",
        "updated_at": "2024-03-05T06:49:50+00:00",
        "closed_at": "2024-03-05T06:49:50+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "youngstu",
            "TeslaZhao",
            "yt605155624",
            "yt605155624",
            "youngstu",
            "TeslaZhao",
            "youngstu",
            "TeslaZhao"
        ],
        "labels": [
            "feature-request"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1333,
        "title": "docker 部署两个服务报错",
        "body": "[黄埔学院]\r\ndocker部署了两个服务（文字检测和文字识别），但是有冲突，启动一个后另一个会无法启动或者直接被kill。\r\n环境\r\npython 3.7.11\r\npaddle-serving-app                0.6.0\r\npaddle-serving-client             0.6.0\r\npaddle-serving-server             0.6.0\r\npaddle2onnx                       0.7\r\npaddlehub                         2.1.0\r\npaddlenlp                         2.0.7\r\npaddlepaddle                      2.1.0\r\n\r\n文字检测参考：[https://github.com/PaddlePaddle/Serving/tree/v0.6.0/python/examples/pipeline/PaddleDetection/yolov3](url)\r\n文字识别使用的是PaddleHub启动，参考 [https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/deploy/hubserving/readme.md](url)\r\n问题1. 先启动文字检测服务(Paddle Serving)后，PaddleHub无法启动，启动直接kill\r\npaddlehub被kill\r\n![image](https://user-images.githubusercontent.com/10570127/128106236-6f5693ae-b38e-4ef4-8fb9-3dd52d298bc2.png)\r\n启动PaddleHub时Paddle Serving 如下输出\r\n<img width=\"1186\" alt=\"截屏2021-08-04 09 16 51\" src=\"https://user-images.githubusercontent.com/10570127/128106841-30f92ced-ecdf-4361-bf18-074efbee22d8.png\">\r\n问题2. 先启动文字识别服务(Paddle hub)，再启动文字识别服务，PaddleHub也直接被kill\r\n<img width=\"1751\" alt=\"截屏2021-08-04 09 22 14\" src=\"https://user-images.githubusercontent.com/10570127/128106943-94d8016b-e894-40ef-b2ca-c02d1935db7a.png\">",
        "state": "closed",
        "user": "Andy546",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-04T01:25:02+00:00",
        "updated_at": "2024-04-16T09:05:49+00:00",
        "closed_at": "2024-04-16T09:05:49+00:00",
        "comments_count": [
            "bjjwwang",
            "Andy546",
            "Andy546",
            "bjjwwang",
            "Andy546",
            "Andy546",
            "bjjwwang",
            "Andy546",
            "Andy546",
            "bjjwwang",
            "bjjwwang",
            "Andy546",
            "bjjwwang",
            "Andy546",
            "bjjwwang",
            "Andy546",
            "Andy546",
            "TeslaZhao",
            "Andy546"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1334,
        "title": "自定义sdk借用paddle-serving部署",
        "body": "假设我现在有一个自己的sdk,想借助paddle-serving 部署简单的服务，需要怎么操作？",
        "state": "closed",
        "user": "77h2l",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-04T13:36:57+00:00",
        "updated_at": "2024-03-05T06:49:51+00:00",
        "closed_at": "2024-03-05T06:49:51+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "77h2l",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1342,
        "title": "About 'python -m paddle_serving_client.convert', ModuleNotFoundError",
        "body": "Dear, \r\nWhen I want to convert inference model to paddle serving model, I use python -m paddle_serving_client.convert --dirname..., but it has ModuleNotFoundError, No module named 'paddle_serving_client.proto', so I turned back to the dir python/paddle_serving_client, failed to find any file named proto, so what can i do, when i had the inference model, and i want to convert it to a serving model, by using paddle_serving_client.convert??",
        "state": "closed",
        "user": "question7603",
        "closed_by": "question7603",
        "created_at": "2021-08-11T04:10:09+00:00",
        "updated_at": "2021-08-11T07:08:59+00:00",
        "closed_at": "2021-08-11T07:08:59+00:00",
        "comments_count": [
            "github-actions[bot]",
            "question7603",
            "question7603"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1343,
        "title": "pipeline 模式启动server 无法添加类似命令行启动的参数",
        "body": "命令行启动 server 实际是运行如下的命令：\r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server/serving-gpu-102-0.6.0/serving -enable_model_toolkit -inferservice_path workdir_0 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 2 -port 12000 -precision fp32 -use_calib False -reload_interval_s 10 -resource_path workdir_0 -resource_file resource.prototxt -workflow_path workdir_0 -workflow_file workflow.prototxt -bthread_concurrency 2 -gpuid 0 -max_body_size 1073741824 \r\n\r\npipeline模式启动 server 并非如此，pipeline 模式如何添加 --max_body_size 参数",
        "state": "closed",
        "user": "Jeffrey98-AI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-12T07:01:59+00:00",
        "updated_at": "2024-03-05T06:49:52+00:00",
        "closed_at": "2024-03-05T06:49:52+00:00",
        "comments_count": [
            "HexToString",
            "Jeffrey98-AI",
            "TeslaZhao",
            "Jeffrey98-AI",
            "HexToString"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1346,
        "title": "pipeline 方式 0.4.0 报错",
        "body": "是不是pipeline 方式需要安装 paddle 环境？\r\n但是我安装paddle 1.8 后，依然报错，应该是pipeline的 proxy.so 跟 paddle 环境冲突导致？这种怎么解？\r\n```\r\nImportError: dlopen: cannot load any more object with static TLS\r\nCRITICAL 2021-08-12 21:52:10,512 [operator.py:1045] [det|1] failed to init op: dlopen: cannot load any more object with static TLS\r\nTraceback (most recent call last):\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle_serving_server/pipeline/operator.py\", line 1038, in _run\r\n    concurrency_idx)\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle_serving_server/pipeline/local_service_handler.py\", line 137, in get_client\r\n    from paddle_serving_app.local_predict import LocalPredictor\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle_serving_app/local_predict.py\", line 22, in <module>\r\n    import paddle.fluid as fluid\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/__init__.py\", line 37, in <module>\r\n    import paddle.complex\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/complex/__init__.py\", line 15, in <module>\r\n    from . import tensor\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/complex/tensor/__init__.py\", line 15, in <module>\r\n    from . import math\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/complex/tensor/math.py\", line 15, in <module>\r\n    from paddle.common_ops_import import *\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/common_ops_import.py\", line 15, in <module>\r\n    from paddle.fluid.layer_helper import LayerHelper\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/fluid/__init__.py\", line 35, in <module>\r\n    from . import framework\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 35, in <module>\r\n    from . import core\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/fluid/core.py\", line 273, in <module>\r\n    raise e\r\n  File \"/home/ssd1/work/python3.6/lib/python3.6/site-packages/paddle/fluid/core.py\", line 243, in <module>\r\n    from .core_avx import *\r\nImportError: dlopen: cannot load any more object with static TLS\r\n```",
        "state": "closed",
        "user": "linghaolu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-12T13:53:20+00:00",
        "updated_at": "2024-04-16T09:05:50+00:00",
        "closed_at": "2024-04-16T09:05:50+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "linghaolu"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1356,
        "title": "pipeline没有使用gpu",
        "body": "config.yml里设置了device_type=2\r\n`op:\r\n   ivane_op:\r\n        timeout: -1\r\n        retry: 1 \r\n        #concurrency: 3\r\n        batch_size: 1 \r\n        auto_batching_timeout: -1 \r\n        local_service_conf:\r\n            device_type: 2\r\n            client_type: 'local_predictor'\r\n            model_config: '/home/Serving/serving_server/'\r\n            fetch_list: [\"fc_0.tmp_2\"]\r\n            devices: \"0\" \r\n`            \r\n用python3.6 pipeline.py启动pipeline服务，但是gpu监控里查不到对应的进程id，而且推理速度非常慢，感觉是没用到gpu。请问这个是啥情况呢？",
        "state": "closed",
        "user": "Karenlyw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-18T15:16:14+00:00",
        "updated_at": "2024-03-05T06:49:53+00:00",
        "closed_at": "2024-03-05T06:49:53+00:00",
        "comments_count": [
            "TeslaZhao",
            "Karenlyw",
            "knwng"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1360,
        "title": "anaconda下的python 运行 serving_server报错No module named paddle_serving_server_gpu.serve",
        "body": "anaconda下的python 运行paddle_serving_server\r\n/u01/anaconda3/bin/python: No module named paddle_serving_server_gpu.serve\r\n![image](https://user-images.githubusercontent.com/79828574/130212750-3241723b-d12d-4131-9c07-17575cbdefb7.png)\r\n\r\n源网页是\r\nhttps://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_classification/pretrained_models\r\n![image](https://user-images.githubusercontent.com/79828574/130212363-e52ed973-ff35-4fdc-b5c6-9c15bf44ce37.png)\r\n环境信息：\r\n![image](https://user-images.githubusercontent.com/79828574/130212582-cdda2da1-179f-476a-aca9-1c92a25287cc.png)\r\nannaconda python 3.7",
        "state": "closed",
        "user": "windy-zhr",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-20T09:30:31+00:00",
        "updated_at": "2024-03-05T06:49:54+00:00",
        "closed_at": "2024-03-05T06:49:54+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1361,
        "title": "paddle serving的lac只有分词，没有词性标注么？",
        "body": "hub serving 的lac模块既支持分词也支持词性标注，为啥paddle serving lac的例子只有分词结果呢",
        "state": "closed",
        "user": "weixiu00",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-23T07:16:15+00:00",
        "updated_at": "2024-03-05T06:49:54+00:00",
        "closed_at": "2024-03-05T06:49:54+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1363,
        "title": "除了支持cpu, gpu paddle serving还能支持其他AI加速器吗？",
        "body": "比如说TPU，或者说自己研制的AI加速器，如果想用paddle serving来做部署，需要定制化开发哪些地方呢？",
        "state": "closed",
        "user": "dulvqingyunLT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-26T03:27:59+00:00",
        "updated_at": "2024-03-05T06:49:55+00:00",
        "closed_at": "2024-03-05T06:49:55+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1364,
        "title": "关于 OcrService pipeline 部署相关请教",
        "body": "serving 版本 0.6.0  paddle版本 2.0 以上  \r\n![image](https://user-images.githubusercontent.com/53547082/130895955-0460c91e-8790-41dd-a081-fe96b86a98d5.png)\r\n\r\n我这边想在检测模型之前加一个方向分类器模型  但是我想通过客户端传递过来的参数 来判断是否需要执行方向分类模型  本地我尝试过3个模型串联可以成功   选择性执行模型不知道有没有参考的实例，求指导！",
        "state": "closed",
        "user": "wa3926",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-26T03:33:57+00:00",
        "updated_at": "2024-04-16T09:05:51+00:00",
        "closed_at": "2024-04-16T09:05:51+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "wa3926",
            "TeslaZhao",
            "wa3926"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1365,
        "title": "solo模型  解析fetch var 报错",
        "body": "您好：\r\n\r\n模型是用的实例分割的 [solo模型](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/configs/solov2/README.md)\r\n代码如下：\r\n```python\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\n\r\npreprocess = Sequential([\r\n    File2Image(), \r\n    BGR2RGB(),\r\n    Div(255),\r\n    Resize((512,512)),\r\n    Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\r\n    Transpose((2, 0, 1)),\r\n    PadStride(32),\r\n])\r\n\r\nimg_path = './imgs/000363.jpg'\r\nim = preprocess(img_path)\r\n\r\nclient = Client()\r\nclient.load_client_config(\"client_config/serving_client_conf.prototxt\")\r\nclient.connect(['10.25.131.11:9007'])\r\n\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": np.array(im).astype(np.float32),\r\n        \"im_shape\": np.array([512,512]),\r\n        \"scale_factor\":np.array([1, 1]),\r\n    },\r\n    fetch=[\r\n             \"save_infer_model/scale_0.tmp_1\", # segm\r\n            \"save_infer_model/scale_1.tmp_1\", # category id\r\n            \"save_infer_model/scale_2.tmp_1\", # score\r\n             \"save_infer_model/scale_3.tmp_1\" # box num\r\n            ],\r\n    batch=False\r\n)\r\nprint(fetch_map)\r\n```\r\n\r\n#### fetch 中包含 “save_infer_model/scale_0.tmp_1” 时\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test_model.py\", line 25, in <module>\r\n    fetch_map = client.predict(\r\n  File \"/usr/local/lib/python3.8/site-packages/paddle_serving_client/client.py\", line 482, in predict\r\n    result_map[name] = result_batch_handle.get_int32_by_name(\r\nAttributeError: 'paddle_serving_client.serving_client.PredictorRes' object has no attribute 'get_int32_by_name'\r\n\r\n```\r\n#### fetch 中包含 “save_infer_model/scale_3.tmp_1” 时\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test_model.py\", line 25, in <module>\r\n    fetch_map = client.predict(\r\n  File \"/usr/local/lib/python3.8/site-packages/paddle_serving_client/client.py\", line 457, in predict\r\n    raise ValueError(\r\nValueError: Failed to fetch, maybe the type of [save_infer_model/scale_3.tmp_1] is wrong, please check the model file\r\n\r\n```\r\n\r\n#### serving_server_conf.prototxt 文件如下：\r\n```text\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"scale_factor\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 2\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_1.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_1.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 0\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_2.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_2.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_3.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_3.tmp_1\"\r\n  is_lod_tensor: true\r\n  shape: -1\r\n}\r\n```\r\n\r\n#### 环境：\r\n```\r\npaddle-serving-app 0.6.1\r\npaddle-serving-client 0.6.1\r\npaddle-serving-server-gpu 0.6.1.post102\r\npaddlepaddle-gpu 2.1.2\r\n```",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-27T07:38:16+00:00",
        "updated_at": "2024-04-16T09:05:52+00:00",
        "closed_at": "2024-04-16T09:05:51+00:00",
        "comments_count": [
            "TeslaZhao",
            "ShiningZhang",
            "zouxiaoshi",
            "TeslaZhao",
            "ShiningZhang",
            "ShiningZhang",
            "zouxiaoshi",
            "zouxiaoshi",
            "ShiningZhang",
            "zouxiaoshi",
            "ShiningZhang",
            "TeslaZhao",
            "zouxiaoshi",
            "zouxiaoshi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1367,
        "title": "在aistudio notebook上部署出错",
        "body": "运行代码：\r\n\r\nfrom paddle_serving_client import Client\r\nclient = Client()\r\nclient.load_client_config(\"uci_housing_client/serving_client_conf.prototxt\")\r\n\r\n报错：\r\nImportError: libcrypto.so.10: cannot open shared object file: No such file or directory\r\n\r\n\r\nnotebook环境：\r\npaddlepaddle2.1.2\r\npython3.7\r\n使用pip命令安装了paddle-serving-server 、paddle-serving-client 、paddle-serving-app\r\n\r\n\r\n",
        "state": "closed",
        "user": "huilin16",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-27T09:29:12+00:00",
        "updated_at": "2024-04-16T09:05:52+00:00",
        "closed_at": "2024-04-16T09:05:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "huilin16",
            "TeslaZhao",
            "huilin16",
            "bjjwwang",
            "huilin16",
            "skywalk163",
            "huilin16"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1368,
        "title": "部署自己训练模型问题",
        "body": "![Selection_001](https://user-images.githubusercontent.com/45814273/131302830-b6e3ee7d-4b23-406e-b373-e12a6544a9fa.png)\r\n\r\n我试着以pdserving方式部署ocr pipeline，但是预测的时候报错说:\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/envs/paddle_py38/lib/python3.8/site-packages/paddle_serving_server/pipeline/operator.py\", line 790, in _run_process\r\n    midped_batch = self.process(feed_batch, typical_logid)\r\n  File \"/opt/anaconda3/envs/paddle_py38/lib/python3.8/site-packages/paddle_serving_server/pipeline/operator.py\", line 443, in process\r\n    call_result = self.client.predict(\r\n  File \"/opt/anaconda3/envs/paddle_py38/lib/python3.8/site-packages/paddle_serving_app/local_predict.py\", line 219, in predict\r\n    if isinstance(feed[name], list):\r\nKeyError: 'x'\r\n\r\n\r\n请问我转换的模型正确吗，还是哪里出的问题",
        "state": "closed",
        "user": "Ray8716397",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-30T07:35:21+00:00",
        "updated_at": "2024-03-05T06:49:56+00:00",
        "closed_at": "2024-03-05T06:49:56+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Ray8716397",
            "TeslaZhao",
            "Ray8716397",
            "TeslaZhao",
            "Ray8716397",
            "TeslaZhao",
            "Ray8716397",
            "guijuzhejiang",
            "Ray8716397",
            "Ray8716397",
            "zoe531",
            "Ray8716397",
            "zoe531",
            "zoe531",
            "Ray8716397",
            "Ray8716397",
            "zoe531"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1374,
        "title": "vocab.txt打开显示都是unused",
        "body": "![image](https://user-images.githubusercontent.com/17264083/131976563-4a70cec0-2405-480f-9294-3bbd42b6fb7e.png)\r\n\r\nicode显示如上, Spaces=4 UTF-8. \r\n",
        "state": "closed",
        "user": "Intsigstephon",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-03T08:39:13+00:00",
        "updated_at": "2024-03-05T06:49:57+00:00",
        "closed_at": "2024-03-05T06:49:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1382,
        "title": "部署ocr pipline服务出现问题",
        "body": "环境：\r\npython3.7 cuda11\r\npaddle-serving-app                0.6.2\r\npaddle-serving-client             0.6.2\r\npaddle-serving-server-gpu         0.6.2.post11\r\npaddlepaddle-gpu                  2.1.2.post110\r\n训练是没有问题的，\r\n\r\n运行 python web_service.py >log.txt 后出现错误代码如下：\r\n2021/09/10 15:13:23 start proxy service\r\nW0910 15:13:25.915069 15963 analysis_predictor.cc:715] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nW0910 15:13:25.958106 15973 analysis_predictor.cc:715] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nW0910 15:13:25.961266 15956 analysis_predictor.cc:715] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nW0910 15:13:26.001137 15984 analysis_predictor.cc:715] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\n\r\n",
        "state": "closed",
        "user": "jtcjump",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-10T07:14:24+00:00",
        "updated_at": "2024-03-05T06:49:59+00:00",
        "closed_at": "2024-03-05T06:49:59+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "jtcjump",
            "jtcjump",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1381,
        "title": "MAC 部署快速案例失败",
        "body": "macOS 11.4 M1\r\ndocker desktop  m1版本dmg\r\nDocker Engine   v20.10.8\r\n\r\ndocker pull registry.baidubce.com/paddlepaddle/serving:0.6.2-devel\r\ndocker run -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:0.6.2-devel bash\r\ndocker exec -it test bash\r\ngit clone https://github.com/PaddlePaddle/Serving0.6.2\r\n\r\ncd Serving\r\npip3 install -r python/requirements.txt\r\n\r\npip3 install paddle-serving-client==0.6.2 -i 阿里源\r\npip3 install paddle-serving-server==0.6.2 # CPU  -i 阿里源\r\npip3 install paddle-serving-app==0.6.2  -i 阿里源\r\n\r\npython3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292 --name uci \r\n![image](https://user-images.githubusercontent.com/15975613/132693800-f4185a4f-0958-40d0-b719-ceee8fcb2e04.png)\r\n\r\n![image](https://user-images.githubusercontent.com/15975613/132693882-81722a8e-a6ba-47bb-af38-ac82577311db.png)\r\n\r\n\r\nclient:\r\ncurl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://127.0.0.1:9292/uci/prediction\r\n\r\n![image](https://user-images.githubusercontent.com/15975613/132693984-6279eb2a-86b1-4e0a-8cfd-f836457969c0.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "yqsoooo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-09T13:24:51+00:00",
        "updated_at": "2024-03-05T06:49:58+00:00",
        "closed_at": "2024-03-05T06:49:58+00:00",
        "comments_count": [
            "github-actions[bot]",
            "yqsoooo",
            "yqsoooo",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "TeslaZhao",
            "yqsoooo",
            "yqsoooo",
            "TeslaZhao",
            "bingo789",
            "yandongxu",
            "yqsoooo",
            "TeslaZhao",
            "felixhjh"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1387,
        "title": "如何关闭占用端口的部署？",
        "body": "使用PaddleServing，某次启动后发现config信息不对，再次启动服务会占用端口，如何关闭该服务，或者重新启动服务？",
        "state": "closed",
        "user": "BothCats",
        "closed_by": "BothCats",
        "created_at": "2021-09-15T12:27:44+00:00",
        "updated_at": "2021-09-16T07:01:31+00:00",
        "closed_at": "2021-09-16T07:01:31+00:00",
        "comments_count": [
            "github-actions[bot]",
            "BothCats",
            "TeslaZhao",
            "BothCats"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1388,
        "title": "怎么实现三个模型串联，已经实现两个模型串联，但只能预测一张图片",
        "body": "![skd](https://user-images.githubusercontent.com/51903595/133438257-7cd694b5-ddc3-4097-9a7e-2813c739bb23.png)\r\n图片路径没问题，但只能预测一张图片，其余会报错\r\n而且pdserving的config只提供了det与rec，没有提供cls的接口，虽然不用也能识别，但是还是想加上cls模型",
        "state": "closed",
        "user": "BothCats",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-15T13:05:06+00:00",
        "updated_at": "2024-03-05T06:50:00+00:00",
        "closed_at": "2024-03-05T06:50:00+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1389,
        "title": "缺少相应的配图",
        "body": "PaddleServing设计文档，2.2节关于分布式稀疏参数索引一节，最后一个段落，缺少相应的配图",
        "state": "closed",
        "user": "Intsigstephon",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-16T04:00:35+00:00",
        "updated_at": "2024-03-05T06:50:01+00:00",
        "closed_at": "2024-03-05T06:50:01+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "documents"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1393,
        "title": "OCR pipeline 检测和识别的参数设置？",
        "body": "检测的参数设置和识别的参数设置在哪里修改呢？在看到了 DBPostProcess 的后处理的参数设置，但是前处理，比如检测算法的前处理 det_limit_side_len 的参数设置在哪里修改呢",
        "state": "closed",
        "user": "CharlesWu123",
        "closed_by": "CharlesWu123",
        "created_at": "2021-09-22T07:40:53+00:00",
        "updated_at": "2021-09-22T08:25:02+00:00",
        "closed_at": "2021-09-22T08:25:02+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1391,
        "title": "请问有没有cuda11.2的serving-gpu安装包",
        "body": "RTX3090得用cuda11.2以上才行，自己编译貌似有的麻烦",
        "state": "closed",
        "user": "hymanzhu1983",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-18T07:17:25+00:00",
        "updated_at": "2024-03-05T06:50:02+00:00",
        "closed_at": "2024-03-05T06:50:02+00:00",
        "comments_count": [
            "github-actions[bot]",
            "hymanzhu1983",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1392,
        "title": "OCR Pipeline 增加坐标的返回",
        "body": "你好，我想在 OCR Pipeline 最后返回的时候带上每个文本的坐标值，这个应该怎么修改呢？\r\n我在 RecOp 的 preprocess 里把排序后的坐标赋值给 self.det_boxes，然后在 postprocess 里边把 self.det_boxes 处理后作为返回值作为最终的坐标值返回，但是这样在 preprocess 和 postprocess 里边的 self.det_boxes 的值不一样，有改变，这又是为什么呢？",
        "state": "closed",
        "user": "CharlesWu123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-22T02:52:04+00:00",
        "updated_at": "2024-03-05T06:50:03+00:00",
        "closed_at": "2024-03-05T06:50:03+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "CharlesWu123",
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1395,
        "title": "serving部署",
        "body": "请问一下，有没有提供类似与tensorflow-serving的类似与model_config_file的部署方式去部署我们的模型呢，需求就是一个client能够包含多个模型",
        "state": "closed",
        "user": "wozaimalubian",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-23T02:16:48+00:00",
        "updated_at": "2024-03-05T06:50:04+00:00",
        "closed_at": "2024-03-05T06:50:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1396,
        "title": "Warning: PaddlePaddle catches a failure signal, it may not work properly",
        "body": "第一次运行程序出现问题，重启后再次运行问题还是一样。\r\n版本、环境信息：\r\n1）\tPaddlePaddle版本：1.7.2 （cpu）\r\n2）\tCPU：Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz   3.19 GHz\r\n3）\t系统环境：Ubuntu18.04（虚拟机）\r\n4）\tPython版本号 3.6.13\r\n5）\t问题描述：\r\n W0923 16:54:47.450151  3006 init.cc:209] Warning: PaddlePaddle catches a failure signal, it may not work properly\r\nW0923 16:54:47.452323  3006 init.cc:211] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW0923 16:54:47.452795  3006 init.cc:214] The detail failure signal is:\r\n\r\nW0923 16:54:47.455102  3006 init.cc:217] *** Aborted at 1632387287 (unix time) try \"date -d @1632387287\" if you are using GNU date ***\r\nW0923 16:54:47.456691  3006 init.cc:217] PC: @                0x0 (unknown)\r\nW0923 16:54:47.459455  3006 init.cc:217] *** SIGSEGV (@0x55c64a38f320) received by PID 3006 (TID 0x7fb8c3c090c0) from PID 1245246240; stack trace: ***\r\nW0923 16:54:47.460755  3006 init.cc:217]     @     0x7fb8c3048980 (unknown)\r\nW0923 16:54:47.461793  3006 init.cc:217]     @     0x55c64a38f320 (unknown)\r\nSegmentation fault (core dumped)\r\n\r\n",
        "state": "closed",
        "user": "NSL-Wx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-23T09:15:35+00:00",
        "updated_at": "2024-03-05T06:50:05+00:00",
        "closed_at": "2024-03-05T06:50:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "NSL-Wx",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1397,
        "title": "Pipeline Serving 的 config.yml 能否支持环境变量？",
        "body": "有些配置参数希望能够动态传入，例如我希望根据机器的配置来确定GPU、进程数等，配置文件能否支持环境变量？\r\n例如：\r\nworker_num: $WORKER_NUM\r\n表示从系统环境变量获取该值。",
        "state": "closed",
        "user": "Jnoee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-23T09:25:39+00:00",
        "updated_at": "2024-03-05T06:50:06+00:00",
        "closed_at": "2024-03-05T06:50:06+00:00",
        "comments_count": [
            "TeslaZhao",
            "Jnoee",
            "TeslaZhao",
            "Jnoee",
            "ponycloud235"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1398,
        "title": "Serve 服务启动加  use_multilang 参数后，python 客户端预测报错",
        "body": "\r\n用以下命令服务端启动正常，python 客户端预测也正常。\r\npython -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_id 0  \r\n\r\n\r\n用以下命令服务端启动正常，python 客户端执行 fetch_map = client.predict(***）预测报错\r\npython -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_id 0   --use_multilang\r\n\r\n错误内容如下：\r\n\r\nW0924 09:27:40.969290 13202 socket.cpp:1739] Fail to keep-write into fd=3 SocketId=1@127.0.0.1:9393@51798: Broken pipe [32]\r\nW0924 09:27:40.969334 13208 redis_protocol.cpp:69] No corresponding PipelinedInfo in socket\r\nW0924 09:27:40.970535 13201 redis_protocol.cpp:69] No corresponding PipelinedInfo in socket\r\nW0924 09:27:40.970551 13202 socket.cpp:1739] Fail to keep-write into fd=7 SocketId=102@127.0.0.1:9393@51800: Broken pipe [32]\r\nW0924 09:27:40.971127 13208 redis_protocol.cpp:69] No corresponding PipelinedInfo in socket\r\nW0924 09:27:40.971158 13202 socket.cpp:1739] Fail to keep-write into fd=3 SocketId=104@127.0.0.1:9393@51802: Broken pipe [32]\r\nW0924 09:27:40.971328 13187 predictor.hpp:129] inference call failed, message: [E1014]1/1 channels failed, fail_limit=1 [C0][E32]Fail to keep-write into fd=3 SocketId=1@127.0.0.1:9393@51798: Broken pipe [R1][E32]Fail to keep-write into fd=7 SocketId=102@127.0.0.1:9393@51800: Broken pipe [R2][E1014]Got EOF of fd=3 SocketId=104@127.0.0.1:9393@51802\r\nE0924 09:27:40.971411 13187 general_model.cpp:423] failed call predictor with req: insts { tensor_array { float_data: 0.56078434 floa\r\n",
        "state": "closed",
        "user": "lguowang",
        "closed_by": "lguowang",
        "created_at": "2021-09-24T09:41:17+00:00",
        "updated_at": "2021-09-26T07:56:53+00:00",
        "closed_at": "2021-09-24T10:08:11+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "lguowang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1401,
        "title": "波士顿房价预测报错",
        "body": "运行测试命令：\r\ncd Serving/python/examples/fit_a_line\r\npython test_client.py uci_housing_client/serving_client_conf.prototxt\r\n\r\n报错：\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 20, in <module>\r\n    client = Client()\r\n  File \"/home/bingo/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle_serving_client/client.py\", line 146, in __init__\r\n    from .serving_client import PredictorRes\r\nImportError: libcrypto.so.10: cannot open shared object file: No such file or directory\r\n\r\nide查看源码发现 from .serving_client import PredictorRes 这句话无法导航到PredictorRes，也没发现serving_client目录和serving_client.py文件，请问是哪里的原因？\r\n\r\n附版本信息：\r\npaddle-serving-client             0.6.0\r\npaddle-serving-server             0.6.0",
        "state": "closed",
        "user": "bingo789",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-25T14:23:54+00:00",
        "updated_at": "2024-03-05T06:50:07+00:00",
        "closed_at": "2024-03-05T06:50:07+00:00",
        "comments_count": [
            "github-actions[bot]",
            "sdl415",
            "bingo789",
            "TeslaZhao",
            "bingo789",
            "bingo789",
            "ShiningZhang"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1404,
        "title": "从动态图模型导出",
        "body": "https://github.com/PaddlePaddle/Serving/blob/v0.6.0/doc/SAVE_CN.md 中提到从动态图模型导出，但是从 --model_filename dygraph_model.pdmodel --params_filename dygraph_model.pdiparams 看来，似乎还是需要走一下 paddle 动转静的流程把模型保存成静态图才可以（静态图：*.pdmodel, *.pdiparams 和 *.pdiparams.info）, 然而目前如果模型代码里面有非 paddle 原生算子（如 有些 算子是 paddle 不支持的，需要自己写一下，在此过程中可能会引入 numpy 等），动转静是无法走通的，请问 serving 有木有提供直接从模型代码保存 serving 所需模型文件的接口呢？（比如你们提供了一个类似于 `paddle.jit.save` 的函数可以直接把模型转成 *.prototxt 但是比它做的更好甚至可以支持非 paddle 算子..）",
        "state": "closed",
        "user": "yt605155624",
        "closed_by": "yt605155624",
        "created_at": "2021-09-28T06:54:36+00:00",
        "updated_at": "2021-10-01T10:36:04+00:00",
        "closed_at": "2021-10-01T10:36:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "yt605155624",
            "bjjwwang",
            "yt605155624",
            "bjjwwang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1402,
        "title": "serving下的两个example执行结果后，识别框位置全都偏了，这是什么原因？",
        "body": "环境：\r\nVMware虚拟机\r\nubuntu 18.4 \r\npaddlepaddle 2.1.3\r\nPython 3.8.8\r\npaddle serving server 0.6.3\r\npaddle serving client 0.6.3\r\npaddle serving app 0.6.3\r\npaddle刚入门，自己试着训练目标检测模型的同时，把serving提供的例子跑了一下，采用CPU方式，结果发现识别框结果都跑偏了。\r\n执行的两个example是：\r\n/Serving-0.6.0/python/examples/yolov4 完全跑偏（标准框位置完全超出图片左边轴）\r\n/Serving-0.6.0/python/examples/cascade_rcnn 完全跑偏（貌似整体向右下偏移了很多很多）\r\n/Serving-0.6.0/python/examples/detection/yolov3_darknet53_270e_coco 向下跑偏\r\n/Serving-0.6.0/python/examples/detection/ppyolo_r50vd_dcn_1x_coco 与yolov3_darknet53_270e_coco 一样的跑偏效果\r\n/Serving-0.6.0/python/examples/detection/faster_rcnn_r50_fpn_1x_coco 结果正常，不跑偏\r\n\r\n\r\n接下来，自己用paddleX训练了一个吸烟的目标检测模型，单模型测试，识别位置正常，且预测到一个结果。\r\n将模型转化成serving可用的模型，继承Op，实现webservice\r\n可问题来了：\r\n1.同一张图片，与单模型测试时不同，这次回来三个识别框位置（但位置和大小相差不多）\r\n2.识别框位置完全偏移了，与上述example里的问题一样，且程序不报错，完全不知道原因\r\n我用\bpaddlex 训练时，主要参数如下：\r\n模型：FastRcnn\r\nbackbone选择：ResNet50_vd\r\n其它参数都是paddleX默认参数\r\n\r\n转换后得到的 serving_server_conf.prototxt 如下：\r\n\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_info\"\r\n  alias_name: \"im_info\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_0\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n\r\nwebservice 代码如下：\r\n’‘’\r\ninit_op 中 Resize参数是 ((800, 1333))，位置不对，换成（640,640）位置也不对,为啥？\r\n‘’‘\r\nfrom paddle_serving_server.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nimport sys\r\nimport cv2\r\nfrom paddle_serving_app.reader import *\r\nimport base64\r\n\r\nclass FasterRCNNOp(Op):\r\n    def init_op(self):\r\n        self.img_preprocess = Sequential([\r\n            BGR2RGB(), Div(255.0),\r\n            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n            Resize((640, 640)), Transpose((2, 0, 1)), PadStride(32)\r\n        ])\r\n        self.img_postprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        #print(\"keys\", input_dict.keys())\r\n        for key in input_dict.keys():\r\n            data = base64.b64decode(input_dict[key].encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            im = self.img_preprocess(im)\r\n            imgs.append({\r\n              \"image\": im[np.newaxis,:],\r\n              \"im_shape\": np.array(list(im.shape[1:]) + [1.0])[np.newaxis,:],\r\n              \"im_info\": np.array(list(im.shape[1:]) + [1.0])[np.newaxis,:]\r\n            })\r\n\r\n\r\n        feed_dict = {\r\n            \"image\": np.concatenate([x[\"image\"] for x in imgs], axis=0),\r\n            \"im_shape\": np.concatenate([x[\"im_shape\"] + [1.0] for x in imgs], axis=0),\r\n            \"im_info\": np.concatenate([x[\"im_shape\"] + [1.0] for x in imgs], axis=0)\r\n        }\r\n        #for key in feed_dict.keys():\r\n        #    print(key, feed_dict[key].shape)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, log_id):\r\n\r\n        print(fetch_dict)\r\n        res_dict = {\"bbox_result\": str(self.img_postprocess(fetch_dict, visualize=False))}\r\n        return res_dict, None, \"\"\r\n\r\n\r\nclass FasterRCNNService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        faster_rcnn_op = FasterRCNNOp(name=\"faster_rcnn\", input_ops=[read_op])\r\n        return faster_rcnn_op\r\n\r\n\r\nfasterrcnn_service = FasterRCNNService(name=\"faster_rcnn\")\r\nfasterrcnn_service.prepare_pipeline_config(\"config.yml\")\r\nfasterrcnn_service.run_service()",
        "state": "closed",
        "user": "sdl415",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-26T05:03:08+00:00",
        "updated_at": "2024-03-05T06:50:07+00:00",
        "closed_at": "2024-03-05T06:50:07+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1405,
        "title": "seving 版本0.6.2 ，HTTP服务 异常",
        "body": "seving 版本0.6.2 ，启用RPC服务正常\r\npython3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292\r\n\r\n但测试 HTTP服务 异常\r\n请问用什么版本可以正常测试HTTP服务，有图片相关的案例吗？需要测试可用的；\r\n\r\n另外，能更新一下说明文档吗？",
        "state": "closed",
        "user": "lguowang",
        "closed_by": "lguowang",
        "created_at": "2021-09-28T10:06:38+00:00",
        "updated_at": "2021-09-30T02:18:25+00:00",
        "closed_at": "2021-09-30T02:18:25+00:00",
        "comments_count": [
            "HexToString"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1406,
        "title": "docker启动serving失败",
        "body": "![docker_serving](https://user-images.githubusercontent.com/40812718/135206336-81d9ac8a-397c-4069-938a-788da2dc0090.png)\r\n请问这是什么原因？",
        "state": "closed",
        "user": "bingo789",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-29T05:05:29+00:00",
        "updated_at": "2024-03-05T06:50:08+00:00",
        "closed_at": "2024-03-05T06:50:08+00:00",
        "comments_count": [
            "bjjwwang"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1407,
        "title": "serving部署后为什么预测结果里有非常多置信度低的bbox？",
        "body": "Serving部署运行后，预测结果中很有多置信度低的bbox，这些无效的结果是否可以在server端去除掉？通过什么方式呢？config.yml 中是否有这种参数可以 过滤掉置信度的bbox？\r\n目前我的思路是自己在 server端的postprocess 方法中设置一个阈值（如0.3）大于这个阈值的才返回。不知道这个功能是不是paddle serving 已经实现了，请高人指点！！\r\n另外，有一点十分不解，为啥用用\r\n\r\n\r\npython -m paddle_serving_server.serve --model serving_server --port 9494 --gpu_ids 1\r\n\r\n\r\n部署 output 就一个bbox，难道paddle_serving_server.serve 自动过滤掉低置信度的bbox了？\r\n自己写的web_server就这么多置信度低的bbox呢，实在不解，请百度的大神们给解惑！\r\n\r\nServing 目标监测 预测结果如下：\r\n{'err_no': 0, 'err_msg': '', 'key': ['bbox_result'], 'value': [\"[{'category_id': 0, 'bbox': [187.03741455078125, 255.63929748535156, 205.98016357421875, 88.82615661621094], 'score': 0.9474021196365356}, {'category_id': 0, 'bbox': [181.50440979003906, 254.72242736816406, 208.88352966308594, 90.91026306152344], 'score': 0.05671010911464691}, {'category_id': 0, 'bbox': [401.2098693847656, 517.982666015625, 72.83221435546875, 43.914306640625], 'score': 0.02684014104306698}, {'category_id': 0, 'bbox': [412.7727355957031, 263.4219970703125, 93.93243408203125, 84.128662109375], 'score': 0.022902952507138252}, {'category_id': 0, 'bbox': [328.75335693359375, 502.7386779785156, 201.367919921875, 137.25302124023438], 'score': 0.021608827635645866}, {'category_id': 0, 'bbox': [97.00177001953125, 175.4281463623047, 400.4940185546875, 257.75282287597656], 'score': 0.017718592658638954}, {'category_id': 0, 'bbox': [361.63916015625, 564.093994140625, 31.89556884765625, 75.906005859375], 'score': 0.01378613617271185}, {'category_id': 0, 'bbox': [468.4181213378906, 15.370582580566406, 41.89312744140625, 180.01937103271484], 'score': 0.012252263724803925}, {'category_id': 0, 'bbox': [509.7027282714844, 498.7590026855469, 34.746673583984375, 32.500396728515625], 'score': 0.01143593993037939}]\"]}\r\n\r\n",
        "state": "closed",
        "user": "sdl415",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-29T07:26:43+00:00",
        "updated_at": "2024-03-05T06:50:09+00:00",
        "closed_at": "2024-03-05T06:50:09+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1408,
        "title": "OSError: /usr/local/lib/python3.6/dist-packages/paddle_serving_server/pipeline/gateway/libproxy_server.so: cannot open shared object file: No such file or directory",
        "body": "NVIDIA Jetson AGX Xavier Jetpack 4.5.1 \r\n![image](https://user-images.githubusercontent.com/39875080/135713420-40d8b5ac-6939-415f-b70e-c797bdbb0926.png)\r\n",
        "state": "closed",
        "user": "jiafuzeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-02T11:02:32+00:00",
        "updated_at": "2024-03-05T06:50:10+00:00",
        "closed_at": "2024-03-05T06:50:10+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "WuXinZhiMing"
        ],
        "labels": [
            "install",
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1409,
        "title": "paddle serving rpc: GPU预测时，结果有问题",
        "body": "paddle serving rpc使用GPU（3080 Ti）时做obj detect 预测出来，数据都是负数，这个肯定时错的，不知道什么原因？\r\n如果serving时使用cpu，配置device cpu, 一切都是正常没有问题\r\n\r\n\r\n此外，我在另外一台机器上使用同样的配置方式，cpu，gpu（rtx 6000） serving都是好的。\r\n\r\n请问这个环境的bug可能在哪里？在线等，谢谢！",
        "state": "closed",
        "user": "jeffzhengye",
        "closed_by": "jeffzhengye",
        "created_at": "2021-10-09T03:33:21+00:00",
        "updated_at": "2021-10-28T12:32:08+00:00",
        "closed_at": "2021-10-25T08:06:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang",
            "jeffzhengye",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1423,
        "title": "请教一个问题，aistudio 部署时，传入视频做目标跟踪，其reader应该怎么写",
        "body": "\r\n![image](https://user-images.githubusercontent.com/22255835/137694137-c2c9ae0a-d9dd-49ef-963e-1c4bbbc71f4d.png)\r\n\r\n请教一个问题，aistudio 部署时，传入视频做目标跟踪，其reader应该怎么写",
        "state": "closed",
        "user": "aFtBfF",
        "closed_by": "aFtBfF",
        "created_at": "2021-10-18T08:18:19+00:00",
        "updated_at": "2021-10-19T08:57:52+00:00",
        "closed_at": "2021-10-19T08:57:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "aFtBfF",
            "aFtBfF",
            "TeslaZhao",
            "aFtBfF"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1411,
        "title": "pipeline显存占用问题",
        "body": "使用pipeline部署多个模型时，无论模型多大，总是会把显存都占满，请问可以调整哪个参数设置初始化显存大小呢",
        "state": "closed",
        "user": "melon12321",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-11T03:44:53+00:00",
        "updated_at": "2024-03-05T06:50:11+00:00",
        "closed_at": "2024-03-05T06:50:11+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "melon12321",
            "TeslaZhao",
            "Juruobudong"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1412,
        "title": "部署后执行案例python$ python -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292报错",
        "body": "完整错误如下：\r\n版本0.6.2\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu01/anaconda3/envs/paddleserving/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/ubuntu01/anaconda3/envs/paddleserving/lib/python3.8/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/code/paddle/Serving-0.6.2/python/paddle_serving_server/__init__.py\", line 17, in <module>\r\n    from . import rpc_service\r\n  File \"/home/code/paddle/Serving-0.6.2/python/paddle_serving_server/rpc_service.py\", line 20, in <module>\r\n    from .proto import general_model_config_pb2 as m_config\r\nModuleNotFoundError: No module named 'paddle_serving_server.proto'\r\n\r\npaddle版本2.1.0\r\npython3.8\r\nCUDA 10.1 \r\npaddle-serving-server-gpu==0.6.2.post101",
        "state": "closed",
        "user": "wxf764571829",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-11T09:00:11+00:00",
        "updated_at": "2024-03-05T06:50:12+00:00",
        "closed_at": "2024-03-05T06:50:12+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zhaobinchen"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1425,
        "title": "tensorrt dynamic input shape",
        "body": "# 问题描述\r\n用 develop分支编译的 paddle serving 部署 OCR识别模型(https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) \r\n 识别速度特别慢。（用6.1版本的paddle serving 同一张照片 0.03s, develop分支编译版本则是0.2s，差了6倍）后来，用了trt后，速度有了明显提高，达到了0.07s, 但是 在使用时有如下报错： 【报错显示paddle serving使用的tensorrt版本太低，无法使用dynamic input shape 】\r\n这里必须使用dynamic input shape， 固定input shape，识别效果不佳。\r\n\r\n# 报错\r\n```bash\r\nInvalidArgumentError: Input shapes are inconsistent with the model. Expect [3, 32, 100] in model description, but got [3, 32, 318] in runtime. TRT 5 or lower version does not support dynamic input shapes. Please check and modify your input shapes.\r\n  [Hint: Expected model_input_shape == runtime_input_shape == true, but received model_input_shape == runtime_input_shape:0 != true:1.] (at /paddle/paddle/fluid/operators/tensorrt/tensorrt_engine_op.h:76)\r\n```\r\n\r\n# 环境：【最新 develop分支 编译】\r\n```bash\r\npaddle-serving-app        0.0.0\r\npaddle-serving-client     0.0.0\r\npaddle-serving-server-gpu 0.0.0.post102\r\npaddlepaddle-gpu          2.1.0\r\n\r\nGPU: P100 \r\nCUDA: 10.2\r\npython 3.8\r\n```\r\n\r\n# 补充信息\r\n在镜像中能找到 7.x 版本的 TensorRt, 位于 /usr/local/TensorRT-7.1.3.4\r\n\r\n# TODO\r\n请问，如何解决 tensor rt dynamic input shape问题，是需要在镜像中，重新安装 tesnorrt吗？ \r\n还是说，需要重新编译 paddle-serving-server-gpu?\r\n或者说，需要通过其他方式解决。\r\n\r\n谢谢。\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-19T08:55:15+00:00",
        "updated_at": "2024-04-16T09:05:53+00:00",
        "closed_at": "2024-04-16T09:05:53+00:00",
        "comments_count": [
            "ShiningZhang",
            "zouxiaoshi"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1426,
        "title": "PaddleServing在做图片处理的时候疯狂的读取磁盘是什么原因？",
        "body": "![image](https://user-images.githubusercontent.com/6296864/137890127-e31d8ea8-864c-4441-90ef-a918db4850b9.png)\r\n",
        "state": "closed",
        "user": "zero20121222",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-19T10:14:33+00:00",
        "updated_at": "2024-03-05T06:50:13+00:00",
        "closed_at": "2024-03-05T06:50:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "felixhjh"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1427,
        "title": "PaddleServing对应的服务都有哪些，相关接口说明有吗？",
        "body": null,
        "state": "closed",
        "user": "zero20121222",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-19T11:57:47+00:00",
        "updated_at": "2024-03-05T06:50:14+00:00",
        "closed_at": "2024-03-05T06:50:13+00:00",
        "comments_count": [
            "felixhjh"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1429,
        "title": "请问PipelineClient有没有超时的设置",
        "body": "目前PipelineClient的predict方法发出请求后，如果服务端没有开启或发生了错误，这个predict方法会一直阻塞住。有没有设置一个超时的方法？",
        "state": "closed",
        "user": "zrct0",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-21T02:07:50+00:00",
        "updated_at": "2024-03-05T06:50:14+00:00",
        "closed_at": "2024-03-05T06:50:14+00:00",
        "comments_count": [
            "github-actions[bot]",
            "bjjwwang"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1430,
        "title": "Serving 支持python的前处理和后处理但由于python GIL全局锁存在怎样支持并发呢？",
        "body": "Serving 支持python的前处理和后处理但由于python GIL全局锁存在怎样支持并发呢？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-21T02:57:20+00:00",
        "updated_at": "2024-03-05T06:50:15+00:00",
        "closed_at": "2024-03-05T06:50:15+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1433,
        "title": "Pytorch训练的模型有什么方法在PaddleServing部署吗？",
        "body": "Pytorch训练的模型有什么方法在PaddleServing部署吗？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-21T06:28:37+00:00",
        "updated_at": "2024-03-05T06:50:17+00:00",
        "closed_at": "2024-03-05T06:50:17+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao",
            "youngstu",
            "1037419569"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1432,
        "title": "PaddleServing相较与TorchServe和TensorflowServing 有什么优势呢？",
        "body": "PaddleServing相较与TorchServe和TensorflowServing 有什么优势呢？包括性能和使用方面.",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-21T06:27:48+00:00",
        "updated_at": "2024-03-05T06:50:16+00:00",
        "closed_at": "2024-03-05T06:50:16+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1435,
        "title": "error while loading shared libraries: libnvinfer.so.7: cannot open shared object file: No such file or directory",
        "body": "python3.6/site-packages/paddle_serving_server/serving-gpu-cuda11-0.0.0/serving: error while loading shared libraries: libnvinfer.so.7: cannot open shared object file: No such file or directory",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-22T02:59:13+00:00",
        "updated_at": "2024-03-05T06:50:18+00:00",
        "closed_at": "2024-03-05T06:50:18+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "wstchhwp"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1443,
        "title": "PaddleServing和Triton Inference Server 相比较有什么优势？",
        "body": "PaddleServing和Triton Inference Server 相比较有什么优势？性能和易用性方面有对比和评估吗？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T05:56:03+00:00",
        "updated_at": "2024-03-05T06:50:19+00:00",
        "closed_at": "2024-03-05T06:50:19+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1444,
        "title": "TypeError: load() missing 1 required positional argument: 'Loader'",
        "body": "https://github.com/PaddlePaddle/Serving/tree/v0.6.0/python/examples/pipeline/ocr \r\n\r\npython web_service.py \r\n执行报错如下：\r\n![Screenshot from 2021-10-27 14-45-31](https://user-images.githubusercontent.com/26452514/139013894-1ac6f525-99b9-46f6-8149-600202d77d94.png)\r\n",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T06:46:12+00:00",
        "updated_at": "2024-03-05T06:50:20+00:00",
        "closed_at": "2024-03-05T06:50:19+00:00",
        "comments_count": [
            "youngstu",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1445,
        "title": "PaddleServing Op不需要模型推理，只需要前后处理支持？",
        "body": "PaddleServing Op不需要模型推理，只需要前后处理支持？\r\n\r\n如：只需要preprocess和postprocess但不需要process该如何设置呢？ 如下图OP4\r\n\r\n![image](https://user-images.githubusercontent.com/26452514/139026124-44287782-c232-4a66-b863-c63035587622.png)\r\n",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T07:48:39+00:00",
        "updated_at": "2024-03-05T06:50:20+00:00",
        "closed_at": "2024-03-05T06:50:20+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1446,
        "title": "如果是本机调用，OP的预测类型local_predictor类型是否是更合适些？",
        "body": "![Screenshot from 2021-10-27 15-53-36](https://user-images.githubusercontent.com/26452514/139023700-b7e40940-e42b-47b9-94ca-11b06787762f.png)\r\n\r\n如果是本机调用，OP的预测类型local_predictor类型是否是更合适些？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T07:56:03+00:00",
        "updated_at": "2024-03-05T06:50:21+00:00",
        "closed_at": "2024-03-05T06:50:21+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1447,
        "title": "Serving是有消息队列机制来缓存消息？",
        "body": "比如请求量激增情况下，是否有消息队列机制来缓存请求消息，来避免海量请求同时打满后端服务，造成服务不可用？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T07:58:32+00:00",
        "updated_at": "2024-03-05T06:50:22+00:00",
        "closed_at": "2024-03-05T06:50:22+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1449,
        "title": "build_dag_each_worker和is_thread_op 各自作用是什么？",
        "body": "![image](https://user-images.githubusercontent.com/26452514/139027227-9dbaa965-cb76-4046-a0c9-f4731ad8e049.png)\r\n\r\nbuild_dag_each_worker和is_thread_op 各自作用是什么呢？如果是线程方式如何解决GIL锁问题？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T08:18:56+00:00",
        "updated_at": "2024-03-05T06:50:24+00:00",
        "closed_at": "2024-03-05T06:50:24+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1448,
        "title": "worker_num和concurrency区别是？",
        "body": "![Screenshot from 2021-10-27 16-05-21](https://user-images.githubusercontent.com/26452514/139025865-e14fcf6a-ebe5-4c32-b502-d520658b17c2.png)\r\n\r\n如上图worker_num和concurrency区别是？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T08:07:53+00:00",
        "updated_at": "2024-03-05T06:50:23+00:00",
        "closed_at": "2024-03-05T06:50:23+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1450,
        "title": "在前后处理完成paddle模型推理方案中，在op init函数初始化paddle模型报错",
        "body": "由于paddle动态图转静态图失败，计划省去process环节，直接在post_process完成paddle模型推理。因此需要在init函数里面完成模型加载和初始化，但是cuda报错（查资料好像是由于多进程原因）。\r\n\r\n![image](https://user-images.githubusercontent.com/26452514/139039684-f5af40c0-e047-49c7-8906-1fecef054bbd.png)\r\n",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T09:32:41+00:00",
        "updated_at": "2024-03-05T06:50:25+00:00",
        "closed_at": "2024-03-05T06:50:25+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1451,
        "title": "PaddleServing 是否可以像Triton支持python backend？",
        "body": "PaddleServing 是否可以像Triton支持python backend？如果支持基本可以直接把paddle训练代码迁移过来就能直接部署服务，也不需要其他模型动态图转静态图转Serving等流程。",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T09:38:59+00:00",
        "updated_at": "2024-03-05T06:50:25+00:00",
        "closed_at": "2024-03-05T06:50:25+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1452,
        "title": "How to Add Https using openssl?",
        "body": "i wanted to create a simple way to start my https://127.0.0.1:5000/ocr/prediction like that i wanted to deploy the ocr pipeline. i have done this thing using openssl in flask like ssl_context=(cert.pem, key.pem) like this way i have achieved a https in my domain mai i know is there any way to do same thing in pdserving? ",
        "state": "closed",
        "user": "YC7225",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-27T12:27:00+00:00",
        "updated_at": "2024-03-05T06:50:26+00:00",
        "closed_at": "2024-03-05T06:50:26+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1453,
        "title": "rpc client和server之间通讯不支持numpy/list/tuple/dict等相对复杂一些数据类型",
        "body": "rpc client和server之间通讯不支持numpy/list/tuple/dict等相对复杂一些数据类型。仅支持字符串类型。\r\n后续是否有支持numpy/list/tuple/dict 自动序列化和反序列化计划呢？每次都要base64编解码比较繁琐。",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-28T07:33:35+00:00",
        "updated_at": "2024-03-05T06:50:27+00:00",
        "closed_at": "2024-03-05T06:50:27+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1454,
        "title": "connections to all backends failing",
        "body": "![image](https://user-images.githubusercontent.com/26452514/139218303-79c72081-e541-40de-b541-07cfcaea34eb.png)\r\n\r\n\r\n在一个服务中使用rpc请求另一个服务报错如上，帮忙分析一下，谢谢～",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "HexToString",
        "created_at": "2021-10-28T08:33:30+00:00",
        "updated_at": "2021-10-29T07:37:33+00:00",
        "closed_at": "2021-10-29T07:37:33+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1456,
        "title": "Serving 相较于Flask或Django Web服务器有何优势呢？",
        "body": "您好，Serving 相较于Flask或Django Web服务器有什么优势呢？简单测试了一下，性能层面Serving比Flask要差一些，而且经常会有波动，波动情况性能就差得比较多。帮忙研究一下，谢谢~",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-28T13:49:43+00:00",
        "updated_at": "2024-03-05T06:50:28+00:00",
        "closed_at": "2024-03-05T06:50:28+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1457,
        "title": "多线程rpc函数需要每次都创建rpc_client和connect？",
        "body": "您好，请问多线程rpc函数需要每次都创建rpc_client和connect？还是创建一个rpc_client池子，如果rpc_client池子怎么判断rpc请求状态（比如上一个rpc请求没有完成，又发起新的请求如何处理）？",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-28T14:03:08+00:00",
        "updated_at": "2024-04-16T09:05:54+00:00",
        "closed_at": "2024-04-16T09:05:54+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1458,
        "title": "TTS合成一个段落，单句调用rpc并行合成比一个段落单次合成慢很多",
        "body": "您好，TTS合成一个段落，单句调用rpc并行合成比一个段落单次合成慢很多，且多线程rpc请求并行合成时间逐次变长。帮忙分析一下，谢谢~",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-28T14:22:22+00:00",
        "updated_at": "2024-03-05T06:50:29+00:00",
        "closed_at": "2024-03-05T06:50:29+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1459,
        "title": "rpc connect连接不请求会持续多长时间才会断掉连接？",
        "body": "您好，请问rpc connect连接不请求会持续多长时间才会断掉连接？帮忙回答一下，谢谢~",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-28T14:41:18+00:00",
        "updated_at": "2024-03-05T06:50:30+00:00",
        "closed_at": "2024-03-05T06:50:30+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1460,
        "title": "rpc性能不是很好并且不是很稳定，经常性通讯时长1s多",
        "body": "您好，rpc性能不是很好并且不是很稳定，经常性通讯时长1s多，帮忙看下原因，谢谢。",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "youngstu",
        "created_at": "2021-10-28T14:56:05+00:00",
        "updated_at": "2021-10-29T10:44:11+00:00",
        "closed_at": "2021-10-29T09:34:46+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1462,
        "title": "Kong UI is not working.",
        "body": "I have tried docker auth tutorial kong ui for me https://127.0.0.1:8001 not working me. its showing nothing to me any idea how can i check and get the tokens key for api call?",
        "state": "closed",
        "user": "YC7225",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-29T10:30:44+00:00",
        "updated_at": "2024-04-16T09:05:55+00:00",
        "closed_at": "2024-04-16T09:05:55+00:00",
        "comments_count": [
            "bjjwwang",
            "YC7225",
            "bjjwwang",
            "YC7225"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1463,
        "title": "在serving中部署了OCR，最后只能得到识别的文字，怎样才能获得对于文字的位置，另外多个预测出来的结果的顺序会不会变",
        "body": "1、在serving中部署了OCR，最后只能得到识别的文字，怎样才能获得对于文字的位置，\r\n2、检测模型检测多个结果，识别模型预测出来的结果的顺序和检测模型给出的顺序是不是一样，（我试的结果是有时候一样，有时候不一样）",
        "state": "closed",
        "user": "pipishaw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-30T03:30:20+00:00",
        "updated_at": "2024-03-05T06:50:31+00:00",
        "closed_at": "2024-03-05T06:50:30+00:00",
        "comments_count": [
            "ShiningZhang",
            "bikerr",
            "LiangDazhu",
            "Jaccica"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1464,
        "title": "RuntimeError: can't start new thread",
        "body": "\r\n您好，多并发测试时GRPC 内部报错，无法启动新线程。帮忙看一下，谢谢～\r\n\r\n```\r\n\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/home/yangshaoxiong/.conda/envs/py36_paddle2.1/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/yangshaoxiong/.conda/envs/py36_paddle2.1/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/yangshaoxiong/.local/lib/python3.6/site-packages/grpc/_server.py\", line 875, in _serve\r\n    if not _process_event_and_continue(state, event):\r\n  File \"/home/yangshaoxiong/.local/lib/python3.6/site-packages/grpc/_server.py\", line 842, in _process_event_and_continue\r\n    concurrency_exceeded)\r\n  File \"/home/yangshaoxiong/.local/lib/python3.6/site-packages/grpc/_server.py\", line 750, in _handle_call\r\n    thread_pool)\r\n  File \"/home/yangshaoxiong/.local/lib/python3.6/site-packages/grpc/_server.py\", line 726, in _handle_with_method_handler\r\n    method_handler, thread_pool)\r\n  File \"/home/yangshaoxiong/.local/lib/python3.6/site-packages/grpc/_server.py\", line 632, in _handle_unary_unary\r\n    method_handler.response_serializer)\r\n  File \"/home/yangshaoxiong/.conda/envs/py36_paddle2.1/lib/python3.6/concurrent/futures/thread.py\", line 123, in submit\r\n    self._adjust_thread_count()\r\n  File \"/home/yangshaoxiong/.conda/envs/py36_paddle2.1/lib/python3.6/concurrent/futures/thread.py\", line 142, in _adjust_thread_count\r\n    t.start()\r\n  File \"/home/yangshaoxiong/.conda/envs/py36_paddle2.1/lib/python3.6/threading.py\", line 846, in start\r\n    _start_new_thread(self._bootstrap, ())\r\nRuntimeError: can't start new thread\r\n```",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-03T05:55:35+00:00",
        "updated_at": "2024-04-16T09:05:56+00:00",
        "closed_at": "2024-04-16T09:05:56+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao",
            "youngstu"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1468,
        "title": "并发超过临界点，Error req数量陡然增加",
        "body": "\r\n您好，并发超过临界点，Error req数量陡然增加，什么情况下会导致Error req呢？帮忙解答一下，谢谢～\r\n\r\n如下是Tracer日志：\r\n\r\n```\r\nDAGExecutor:\r\n2021-11-04 16:02:40,743 \tQuery count[1000]\r\n2021-11-04 16:02:40,743 \tQPS[100.0 q/s]\r\n2021-11-04 16:02:40,743 \tSucc[0.33699999999999997]\r\n2021-11-04 16:02:40,743 \tError req[4603, 4605, 4607, 4608, 4609, 4610, 4611, 4613, 4614, 4616, 4615, 4617, 4619, 4618, 4620, 4621, 4623, 4624, 4625, 4626, 4628, 4629, 4627, 4630, 4632, 4631, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4642, 4643, 4644, 4648, 4649, 4651, 4652, 4653, 4655, 4656, 4657, 4658, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4679, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4697, 4698, 4699, 4701, 4702, 4703, 4704, 4705, 4706, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4718, 4719, 4720, 4721, 4723, 4725, 4726, 4727, 4730, 4731, 4732, 4733, 4734, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4749, 4750, 4751, 4752, 4753, 4755, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4776, 4777, 4778, 4780, 4781, 4782, 4783, 4784, 4785, 4787, 4788, 4789, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4810, 4811, 4812, 4813, 4814, 4815, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4828, 4829, 4831, 4832, 4833, 4834, 4835, 4837, 4838, 4839, 4840, 4841, 4842, 4844, 4845, 4846, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4879, 4881, 4882, 4883, 4885, 4884, 4886, 4887, 4889, 4890, 4891, 4892, 4893, 4895, 4896, 4899, 4900, 4902, 4903, 4904, 4905, 4906, 4907, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4919, 4921, 4922, 4923, 4924, 4926, 4927, 4928, 4930, 4931, 4932, 4933, 4934, 4935, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4948, 4949, 4950, 4951, 4952, 4955, 4956, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4981, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4994, 4995, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5011, 5012, 5013, 5014, 5015, 5016, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5037, 5038, 5039, 5040, 5041, 5042, 5046, 5047, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5059, 5060, 5061, 5062, 5064, 5065, 5066, 5067, 5069, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5096, 5097, 5099, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5112, 5113, 5114, 5116, 5117, 5119, 5120, 5121, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5135, 5136, 5137, 5139, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5150, 5151, 5153, 5154, 5155, 5156, 5157, 5159, 5160, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5177, 5178, 5179, 5180, 5181, 5183, 5184, 5185, 5186, 5188, 5189, 5190, 5191, 5192, 5194, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5206, 5207, 5208, 5210, 5211, 5214, 5215, 5216, 5217, 5218, 5220, 5221, 5222, 5223, 5225, 5226, 5228, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5245, 5246, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5258, 5259, 5260, 5261, 5262, 5263, 5265, 5266, 5268, 5269, 5271, 5272, 5273, 5275, 5276, 5277, 5278, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5312, 5313, 5314, 5315, 5322, 5323, 5325, 5326, 5327, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5356, 5357, 5358, 5360, 5362, 5363, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5377, 5378, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5399, 5400, 5401, 5403, 5404]\r\n```",
        "state": "closed",
        "user": "youngstu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-04T08:10:09+00:00",
        "updated_at": "2024-04-16T09:05:57+00:00",
        "closed_at": "2024-04-16T09:05:57+00:00",
        "comments_count": [
            "TeslaZhao",
            "youngstu",
            "TeslaZhao",
            "youngstu"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1469,
        "title": "能否提供PaddleDetection 在win10下运行的webserver和Client的例子",
        "body": "试了各种写法，参考ocr 的代码也不行，快崩溃了\r\n\r\n# filename:your_webservice.py\r\nfrom paddle_serving_server.web_service import WebService\r\nimport base64\r\nimport numpy as np\r\nimport cv2\r\n# 如果是GPU版本，请使用 from paddle_serving_server.web_service import WebService\r\nclass MyWebService(WebService):\r\n    def preprocess(self, feed=[], fetch=[], is_batch=[]):\r\n        #在这里实现前处理\r\n        #feed_dict是 key: var names, value: numpy array input\r\n        #fetch_names 是fetch变量名列表\r\n        #is_batch的含义是feed_dict的value里的numpy array是否包含了batch维度\r\n\r\n        data = base64.b64decode(feed[\"image\"].encode('utf8'))\r\n        data = np.frombuffer(data, np.uint8)\r\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n\r\n        feed[\"image\"] = data\r\n        feed[\"im_shape\"] = np.array(list(im.shape[1:])).reshape(-1)\r\n        feed[\"scale_factor\"] = np.array([1.0, 1.0]).reshape(-1)\r\n\r\n        return feed, fetch, is_batch\r\n    def postprocess(self, feed={}, fetch=[], fetch_map=None):\r\n        #fetch map是经过预测之后的返回字典，key是process返回时给定的fetch names，value是对应fetch names的var具体值\r\n        #在这里做处理之后，结果需重新转换成字典，并且values的类型应是列表list，这样可以JSON序列化方便web返回\r\n        return response\r\n\r\nmy_service = MyWebService(name=\"fasterRCNN\")\r\nmy_service.load_model_config(\"serving_server\")\r\nmy_service.prepare_server(workdir=\"workdir\", port=9292)\r\n# 如果是GPU用户，可以参照python/examples/ocr下的python示例\r\nmy_service.run_debugger_service()\r\n# Windows平台不可以使用 run_rpc_service()接口\r\nmy_service.run_web_service()\r\n\r\n\r\n\r\nimport requests\r\nimport json\r\nimport cv2\r\nimport base64\r\nimport os, sys\r\nimport time\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimport sys\r\nfrom paddle_serving_app.reader import *\r\n\r\n\r\n# 二进制转numpy\r\n#with open(image_path, \"rb\") as file:\r\n#    jpg_bin = file.read()\r\n#    image = cv2.imdecode(np.asarray(bytearray(jpg_bin), dtype='uint8'), cv2.IMREAD_COLOR)\r\n\r\n# numpy转二进制\r\n#with open(tmp_image_path, 'wb') as tmp_file:\r\n#    tmp_jpg_bin = np.array(cv2.imencode('.jpg', image)[1]).tobytes()\r\n#    tmp_file.write(tmp_jpg_bin)\r\n\r\npreprocess = Sequential([\r\n    File2Image(), BGR2RGB(), Resize(\r\n        (608, 608), interpolation=cv2.INTER_LINEAR), Div(255.0), Transpose(\r\n            (2, 0, 1))\r\n])\r\n\r\n\r\ndef cv2_to_base64(image):\r\n    #data = cv2.imdecode(np.asarray(bytearray(image), dtype='uint8'), cv2.IMREAD_COLOR)\r\n    #image = cv2.cvtColor(data, cv2.COLOR_RGB2GRAY)\r\n    #tmp_jpg_bin = np.array(cv2.imencode('.jpg', image)[1]).tobytes()\r\n    tmp_jpg_bin = image\r\n    return base64.b64encode(tmp_jpg_bin).decode(\r\n        'utf8')  #data.tostring()).decode('utf8')\r\n\r\n\r\n\r\n\r\nheaders = {\"Content-type\": \"application/json\"}\r\nurl = \"http://127.0.0.1:9292/fasterRCNN/prediction\"\r\n\r\ntest_img_dir = \"imgs/\"\r\nfor idx, img_file in enumerate(os.listdir(test_img_dir)):\r\n    with open(os.path.join(test_img_dir, img_file), 'rb') as file:\r\n        image_data1 = file.read()\r\n        im = preprocess(os.path.join(test_img_dir, img_file))\r\n    image = cv2_to_base64(image_data1)\r\n    for i in range(1):\r\n        #data = {\"feed\": [{\"im_shape\": []},{\"image\": image},{\"scale_factor\": \"6654\"}], \"fetch\": [{\"save_infer_model/scale_0.tmp_1\": \"555\"},{\"save_infer_model/scale_1.tmp_1\": \"666\"}], \"is_batch\": False}\r\n        #data = new map\r\n        #data[\"feed\"][\"image\"] = image;\r\n        #data[\"feed\"][\"im_shape\"] = np.array(list(im.shape[1:])).reshape(-1);\r\n        #data[\"feed\"][\"scale_factor\"] = np.array([1.0, 1.0]).reshape(-1);\r\n        #data[\"fetch\"] = [\"save_infer_model/scale_0.tmp_1\"];\r\n        #data[\"batch\"] = False;\r\n\r\n\r\n        data = {\"feed\": {\r\n            \"image\": image,\r\n            \"im_shape\": [608,608],\r\n            \"scale_factor\": [1.0,1.0],\r\n        },\r\n            \"fetch\": [\"save_infer_model/scale_0.tmp_1\"],\r\n            \"is_batch\": False}\r\n        r = requests.post(url=url, headers=headers, data=json.dumps(data))\r\n        print(r.json())\r\n\r\ntest_img_dir = \"imgs/\"\r\nprint(\"==> total number of test imgs: \", len(os.listdir(test_img_dir)))\r\n\r\n",
        "state": "closed",
        "user": "zzkzzk1984",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-05T01:29:58+00:00",
        "updated_at": "2024-03-05T06:50:32+00:00",
        "closed_at": "2024-03-05T06:50:32+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zzkzzk1984",
            "zzkzzk1984",
            "zzkzzk1984",
            "TeslaZhao",
            "zzkzzk1984",
            "TeslaZhao",
            "zzkzzk1984",
            "zzkzzk1984",
            "TeslaZhao",
            "zzkzzk1984",
            "TeslaZhao",
            "zzkzzk1984",
            "TeslaZhao",
            "zzkzzk1984",
            "TeslaZhao",
            "simonchf"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1471,
        "title": "性能指示不明确，待改进",
        "body": "你好，我在查看关于[ocr服务](https://github.com/PaddlePaddle/Serving/blob/v0.7.0/python/examples/ocr/README_CN.md#:~:text=python3%20ocr_web_client.py-,%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87,-CPU%3A%20Intel(R))的启动部署案例的时候，关于性能指标是CPU环境下还是GPU环境下的呢，这方面不清楚，望解答",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2021-11-08T02:57:55+00:00",
        "updated_at": "2021-11-08T05:45:33+00:00",
        "closed_at": "2021-11-08T05:45:33+00:00",
        "comments_count": [
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1477,
        "title": "(NotFound) Cannot open file ../../../output/pp_rec_infer/__model__, please confirm whether the file is normal.",
        "body": "在windows上进行服务部署测试的时候出现下面的异常\r\n`(NotFound) Cannot open file ../../../output/pp_rec_infer/__model__, please confirm whether the file is normal.`\r\n\r\n其中paddle的版本为2.1，训练的模型也为新框架训练的模型\r\nserving的版本为0.6.3\r\n__model__应该是1.X预测保存的模型格式吧，2.X没有这种保存方法了吧",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-09T02:31:22+00:00",
        "updated_at": "2024-03-05T06:50:33+00:00",
        "closed_at": "2024-03-05T06:50:33+00:00",
        "comments_count": [
            "BeyondYourself",
            "BeyondYourself",
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1480,
        "title": "部署单个模型，不用Pipeline是否会更好",
        "body": "在实际业务中会有多个模型的部署，有个疑问，如果只用单个模型部署，是否需要采用pipeline。这种有对比过效率吗",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "BeyondYourself",
        "created_at": "2021-11-09T07:57:24+00:00",
        "updated_at": "2021-11-11T02:30:13+00:00",
        "closed_at": "2021-11-11T02:30:13+00:00",
        "comments_count": [
            "TeslaZhao",
            "BeyondYourself",
            "BeyondYourself",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1530,
        "title": "基于PaddleServing的服务化部署识别模型，精度下降",
        "body": "基于[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)项目，使用自定义数据集和字典训练的识别模型，inference模型识别效果很好，参考[文档](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/deploy/pdserving/README_CN.md),，安装的环境：\r\n```bash\r\npip3 install paddle-serving-server==0.6.1 # for CPU\r\npip3 install paddle-serving-client==0.6.3\r\npip3 install paddle-serving-app==0.6.3\r\n```\r\n模型转换和启动服务都是成功的。\r\n发送服务请求之后\r\n```\r\npython3 pipeline_http_client.py\r\n```\r\n得到的结果如下：(10张图片有4张识别错误（多识别了一个字符或少识别了一个字符））\r\n```\r\n(paddle_env) watermeter@iZ2vc1hkgmux9pl3e4tngrZ:~/projects/PaddleOCR/deploy/pdserving$ python3 pipeline_http_client.py\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['0011']\"]} # 识别错误\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['00011']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['80178']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['00118']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['001122']\"]} # 识别错误\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['00119']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['00113']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['0011#']\"]}\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['000034']\"]} # 识别错误\r\n{'err_no': 0, 'err_msg': '', 'key': ['res'], 'value': [\"['001122']\"]} # 识别错误\r\n==> total number of test imgs:  10\r\n```\r\n补充：\r\n检测模型和识别模型串联推理的效果是很好的，10张图片都是正确的。\r\n串联推理：\r\n```\r\npython3 tools/infer/predict_system.py --image_dir=\"./doc/watermeter_test/det/\" --det_model_dir=\"./inference/det_db_watermeter/\" --rec_model_dir=\"./inference/rec_number_watermeter_ch/\" --use_angle_cls=false --use_gpu=false\r\n```\r\n串联推理部分输出：\r\n```\r\n[2021/11/16 16:34:05] root INFO: 0011#, 0.952\r\n[2021/11/16 16:34:08] root INFO: 00011, 0.906\r\n[2021/11/16 16:34:09] root INFO: 00003, 0.881\r\n[2021/11/16 16:34:10] root INFO: 80178, 0.937\r\n[2021/11/16 16:34:12] root INFO: 00112, 0.908\r\n[2021/11/16 16:34:15] root INFO: 00112, 0.941\r\n[2021/11/16 16:34:17] root INFO: 0011#, 0.991\r\n[2021/11/16 16:34:19] root INFO: 0011#, 0.999\r\n[2021/11/16 16:34:21] root INFO: 00118, 0.883\r\n[2021/11/16 16:34:23] root INFO: 00119, 0.969\r\n[2021/11/16 16:34:25] root INFO: The predict total time is 21.36901330947876\r\n[2021/11/16 16:34:25] root INFO:\r\nThe predict total time is 8.96869683265686\r\n```\r\n",
        "state": "closed",
        "user": "lexiaoyuan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-18T12:39:08+00:00",
        "updated_at": "2024-03-05T06:50:34+00:00",
        "closed_at": "2024-03-05T06:50:34+00:00",
        "comments_count": [
            "github-actions[bot]",
            "lexiaoyuan",
            "Yinyihang857",
            "Jaccica"
        ],
        "labels": [
            "精度"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1491,
        "title": "Paddle的Serving支持在paddlecloud上部署吗？",
        "body": "想在paddlecloud上部署一个NER服务，想问一下如何支持",
        "state": "closed",
        "user": "HuimengZhang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-12T09:33:50+00:00",
        "updated_at": "2024-03-05T06:50:33+00:00",
        "closed_at": "2024-03-05T06:50:33+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "HuimengZhang",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1531,
        "title": "GetClientConfig好像没有了？",
        "body": "GetClientConfig接口在0.7.0版本里面找不到了？是不提供了么？那只能读本地模型配置文件么？",
        "state": "closed",
        "user": "OptimusV5",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-18T12:59:51+00:00",
        "updated_at": "2024-03-05T06:50:35+00:00",
        "closed_at": "2024-03-05T06:50:35+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ShiningZhang",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1534,
        "title": "异构环境适配需求",
        "body": "我们这样有十几台的服务器是x86+华为Ascend 310的异构架构运行paddle框架需求，考虑适配下？ \r\n目前是采用paddlite方式进行适配",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "ponycloud235",
        "created_at": "2021-11-22T02:42:44+00:00",
        "updated_at": "2021-11-23T06:47:52+00:00",
        "closed_at": "2021-11-23T06:47:52+00:00",
        "comments_count": [
            "ShiningZhang",
            "ponycloud235"
        ],
        "labels": [
            "硬件适配"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1533,
        "title": "How to get serving-gateway/proto? in proxy_server.go",
        "body": "How to get serving-gateway/proto? in proxy_server.go\r\n\r\n\r\nusing go get for package initialization?",
        "state": "closed",
        "user": "YC7225",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-18T21:57:14+00:00",
        "updated_at": "2024-03-05T06:50:37+00:00",
        "closed_at": "2024-03-05T06:50:37+00:00",
        "comments_count": [
            "felixhjh",
            "YC7225",
            "felixhjh",
            "YC7225",
            "felixhjh"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1532,
        "title": "Pipeline running question?",
        "body": "'from paddle_serving_server.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nimport cv2\r\nimport base64\r\nfrom paddle_serving_app.reader import OCRReader\r\nfrom paddle_serving_app.reader import Sequential, ResizeByFactor\r\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\r\nfrom paddle_serving_app.reader import DBPostProcess, FilterBoxes, GetRotateCropImage, SortedBoxes\r\n\r\n_LOGGER = logging.getLogger()\r\n\r\n\r\nclass DetOp(Op):\r\n    def init_op(self):\r\n        self.det_preprocess = Sequential([\r\n            ResizeByFactor(32, 960), Div(255),\r\n            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), Transpose(\r\n                (2, 0, 1))\r\n        ])\r\n        self.filter_func = FilterBoxes(10, 10)\r\n        self.post_func = DBPostProcess({\r\n            \"thresh\": 0.3,\r\n            \"box_thresh\": 0.5,\r\n            \"max_candidates\": 1000,\r\n            \"unclip_ratio\": 1.5,\r\n            \"min_size\": 3\r\n        })\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        for key in input_dict.keys():\r\n            data = base64.b64decode(input_dict[key].encode('utf8'))\r\n            self.raw_im = data\r\n            data = np.frombuffer(data, np.uint8)\r\n            self.im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            self.ori_h, self.ori_w, _ = self.im.shape\r\n            det_img = self.det_preprocess(self.im)\r\n            _, self.new_h, self.new_w = det_img.shape\r\n            imgs.append(det_img[np.newaxis, :].copy())\r\n        return {\"image\": np.concatenate(imgs, axis=0)}, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, log_id):\r\n        #        print(fetch_dict)\r\n        det_out = fetch_dict[\"concat_1.tmp_0\"]\r\n        ratio_list = [\r\n            float(self.new_h) / self.ori_h, float(self.new_w) / self.ori_w\r\n        ]\r\n        dt_boxes_list = self.post_func(det_out, [ratio_list])\r\n        dt_boxes = self.filter_func(dt_boxes_list[0], [self.ori_h, self.ori_w])\r\n        out_dict = {\"dt_boxes\": dt_boxes, \"image\": self.raw_im}\r\n        return out_dict, None, \"\"\r\n\r\n\r\nclass RecOp(Op):\r\n    def init_op(self):\r\n        self.ocr_reader = OCRReader()\r\n        self.get_rotate_crop_image = GetRotateCropImage()\r\n        self.sorted_boxes = SortedBoxes()\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        raw_im = input_dict[\"image\"]\r\n        data = np.frombuffer(raw_im, np.uint8)\r\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n        dt_boxes = input_dict[\"dt_boxes\"]\r\n        dt_boxes = self.sorted_boxes(dt_boxes)\r\n        feed_list = []\r\n        img_list = []\r\n        max_wh_ratio = 0\r\n\r\n        ## One batch, the type of feed_data is dict.\r\n        \"\"\" \r\n        for i, dtbox in enumerate(dt_boxes):\r\n            boximg = self.get_rotate_crop_image(im, dt_boxes[i])\r\n            img_list.append(boximg)\r\n            h, w = boximg.shape[0:2]\r\n            wh_ratio = w * 1.0 / h\r\n            max_wh_ratio = max(max_wh_ratio, wh_ratio)\r\n        _, w, h = self.ocr_reader.resize_norm_img(img_list[0],\r\n                                                  max_wh_ratio).shape\r\n        imgs = np.zeros((len(img_list), 3, w, h)).astype('float32')\r\n        for id, img in enumerate(img_list):\r\n            norm_img = self.ocr_reader.resize_norm_img(img, max_wh_ratio)\r\n            imgs[id] = norm_img\r\n        feed = {\"image\": imgs.copy()}\r\n        \"\"\"\r\n\r\n        ## Many mini-batchs, the type of feed_data is list.\r\n        max_batch_size = len(dt_boxes)\r\n\r\n        # If max_batch_size is 0, skipping predict stage\r\n        if max_batch_size == 0:\r\n            return {}, True, None, \"\"\r\n        boxes_size = len(dt_boxes)\r\n        batch_size = boxes_size // max_batch_size\r\n        rem = boxes_size % max_batch_size\r\n        #_LOGGER.info(\"max_batch_len:{}, batch_size:{}, rem:{}, boxes_size:{}\".format(max_batch_size, batch_size, rem, boxes_size))\r\n        for bt_idx in range(0, batch_size + 1):\r\n            imgs = None\r\n            boxes_num_in_one_batch = 0\r\n            if bt_idx == batch_size:\r\n                if rem == 0:\r\n                    continue\r\n                else:\r\n                    boxes_num_in_one_batch = rem\r\n            elif bt_idx < batch_size:\r\n                boxes_num_in_one_batch = max_batch_size\r\n            else:\r\n                _LOGGER.error(\"batch_size error, bt_idx={}, batch_size={}\".\r\n                              format(bt_idx, batch_size))\r\n                break\r\n\r\n            start = bt_idx * max_batch_size\r\n            end = start + boxes_num_in_one_batch\r\n            img_list = []\r\n            for box_idx in range(start, end):\r\n                boximg = self.get_rotate_crop_image(im, dt_boxes[box_idx])\r\n                img_list.append(boximg)\r\n                h, w = boximg.shape[0:2]\r\n                wh_ratio = w * 1.0 / h\r\n                max_wh_ratio = max(max_wh_ratio, wh_ratio)\r\n            _, w, h = self.ocr_reader.resize_norm_img(img_list[0],\r\n                                                      max_wh_ratio).shape\r\n            #_LOGGER.info(\"---- idx:{}, w:{}, h:{}\".format(bt_idx, w, h))\r\n\r\n            imgs = np.zeros((boxes_num_in_one_batch, 3, w, h)).astype('float32')\r\n            for id, img in enumerate(img_list):\r\n                norm_img = self.ocr_reader.resize_norm_img(img, max_wh_ratio)\r\n                imgs[id] = norm_img\r\n            feed = {\"image\": imgs.copy()}\r\n            feed_list.append(feed)\r\n        #_LOGGER.info(\"feed_list : {}\".format(feed_list))\r\n\r\n        return feed_list, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_data, log_id):\r\n        res_list = []\r\n        if isinstance(fetch_data, dict):\r\n            if len(fetch_data) > 0:\r\n                rec_batch_res = self.ocr_reader.postprocess(\r\n                    fetch_data, with_score=True)\r\n                for res in rec_batch_res:\r\n                    res_list.append(res[0])\r\n        elif isinstance(fetch_data, list):\r\n            for one_batch in fetch_data:\r\n                one_batch_res = self.ocr_reader.postprocess(\r\n                    one_batch, with_score=True)\r\n                for res in one_batch_res:\r\n                    res_list.append(res[0])\r\n\r\n        res = {\"res\": str(res_list)}\r\n        return res, None, \"\"\r\n\r\n\r\nclass OcrService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        det_op = DetOp(name=\"det\", input_ops=[read_op])\r\n        rec_op = RecOp(name=\"rec\", input_ops=[det_op])\r\n        return rec_op\r\n\r\n\r\nuci_service = OcrService(name=\"ocr\")\r\nuci_service.prepare_pipeline_config(\"config.yml\")\r\nuci_service.run_service()`\r\n\r\n\r\nCould i use  run_server() to run_web_server() ? ",
        "state": "closed",
        "user": "YC7225",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-18T14:19:04+00:00",
        "updated_at": "2024-03-05T06:50:36+00:00",
        "closed_at": "2024-03-05T06:50:36+00:00",
        "comments_count": [
            "TeslaZhao",
            "YC7225",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1538,
        "title": "ModuleNotFoundError: No module named 'paddle_serving_app'",
        "body": "```\r\n python senta_web_service.py\r\nTraceback (most recent call last):\r\n  File \"senta_web_service.py\", line 21, in <module>\r\n    from paddle_serving_app.reader import LACReader, SentaReader\r\nModuleNotFoundError: No module named 'paddle_serving_app'\r\n```\r\n运行样例报错，装的是cpu版的，在Ubuntu16.04里面的",
        "state": "closed",
        "user": "w5688414",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-24T09:09:40+00:00",
        "updated_at": "2024-03-05T06:50:38+00:00",
        "closed_at": "2024-03-05T06:50:38+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1539,
        "title": "serving 预测报错",
        "body": "`astor                     0.8.1\r\nbrotlipy                  0.7.0\r\ncertifi                   2020.6.20\r\ncffi                      1.14.0\r\nchardet                   4.0.0\r\nclick                     8.0.1\r\ncryptography              3.1\r\ncycler                    0.10.0\r\ndecorator                 5.0.9\r\nFlask                     1.1.2\r\nfunc-timeout              4.3.5\r\nfuncsigs                  1.0.2\r\ngast                      0.5.1\r\ngraphviz                  0.17\r\ngrpcio                    1.31.0\r\ngrpcio-tools              1.31.0\r\nidna                      2.10\r\nimportlib-metadata        4.6.3\r\ninstall                   1.3.4\r\nitsdangerous              1.1.0\r\nJinja2                    2.11.3\r\njoblib                    1.0.1\r\nkiwisolver                1.3.1\r\nMarkupSafe                1.1.1\r\nmatplotlib                3.3.4\r\nmkl-fft                   1.3.0\r\nmkl-random                1.1.1\r\nmkl-service               2.3.0\r\nnltk                      3.6.2\r\nnumpy                     1.19.2\r\nnvidia-pyindex            1.0.9\r\nobjgraph                  3.5.0\r\nolefile                   0.46\r\nopencv-python             4.2.0.32\r\npaddle-serving-app        0.6.0\r\npaddle-serving-client     0.6.0\r\npaddle-serving-server     0.6.0\r\npaddle-serving-server-gpu 0.6.0.post101\r\npaddlepaddle-gpu          2.1.2\r\npathlib                   1.0.1\r\nPillow                    8.3.1\r\npip                       20.2.2\r\nprettytable               2.1.0\r\nprotobuf                  3.17.2\r\npy-cpuinfo                5.0.0\r\npyclipper                 1.2.0\r\npycparser                 2.20\r\npyOpenSSL                 20.0.1\r\npyparsing                 2.4.7\r\nPySocks                   1.7.1\r\npython-dateutil           2.8.2\r\nPyYAML                    5.4.1\r\nrarfile                   4.0\r\nregex                     2021.8.3\r\nrequests                  2.25.1\r\nscipy                     1.5.4\r\nsentencepiece             0.1.83\r\nsetuptools                49.6.0.post20200814\r\nShapely                   1.7.1\r\nsix                       1.16.0\r\ntornado                   6.1\r\ntqdm                      4.62.0\r\ntyping-extensions         3.10.0.0\r\nurllib3                   1.26.6\r\nwcwidth                   0.2.5\r\nWerkzeug                  1.0.1\r\nwheel                     0.34.2\r\nzipp                      3.5.0`\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/9928596/143417793-faf1da84-8b8a-4f9f-9399-5cc87ada8d66.png)\r\n\r\n\r\n系统 openssl\r\n\r\n![image](https://user-images.githubusercontent.com/9928596/143418568-a50bb1cf-8009-44d2-ab0b-1be92311d3f4.png)\r\n\r\n\r\n\r\nconda openssl\r\n\r\n![image](https://user-images.githubusercontent.com/9928596/143418036-55cd6dec-eecf-424f-86f6-9c2b3ea019ad.png)\r\n\r\n\r\npaddleserver 试了还几个版本，都这样，该怎么解决呢，头疼，",
        "state": "closed",
        "user": "Bobo-y",
        "closed_by": "Bobo-y",
        "created_at": "2021-11-25T09:47:20+00:00",
        "updated_at": "2021-11-29T07:09:45+00:00",
        "closed_at": "2021-11-29T07:09:28+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1535,
        "title": "执行 paddle serving 报错",
        "body": "## 问题：\r\n\r\n### Q1: \r\n执行如下代码时报错：\r\n```bash\r\nexport SERVING_BIN=/usr/local/serving_bin/serving\r\npython -m paddle_serving_server.serve \\\r\n--model ./serving_server \\\r\n--thread 8 --port 10010 \\\r\n--gpu_ids 0 \r\n``` \r\n\r\n错误信息：\r\n``` bash\r\nError Message Summary:\r\n----------------------\r\nNotFoundError: Cannot open file ./serving_server/__model__, please confirm whether the file is normal.\r\n  [Hint: Expected static_cast<bool>(fin.is_open()) == true, but received static_cast<bool>(fin.is_open()):0 != true:1.] (at /paddle/paddle/fluid/inference/api/analysis_predictor.cc:1119)\r\n```\r\n后用通过如下代码 进行转换：\r\n\r\n```bash\r\npython -m paddle_serving_client.convert --dirname . \\\r\n                                         --model_filename model.pdmodel          \\\r\n                                         --params_filename model.pdiparams       \\\r\n                                         --serving_server ./serving_server/ \\\r\n                                         --serving_client ./serving_client/\r\n```\r\n得到 如下文件：\r\n```bash\r\n.\r\n├── model.pdiparams\r\n├── model.pdmodel\r\n├── serving_server_conf.prototxt\r\n└── serving_server_conf.stream.prototxt\r\n```\r\n\r\n### Q2:\r\n强制 对 model.pdmodel 重命名， `mv model.pdmodel __model__`\r\n然后启动 paddle serving 服务，得到如下错误：\r\n#### SOLOv2 模型\r\n```bash\r\nError Message Summary:\r\n----------------------\r\nUnavailableError: Load operator fail to open file ./serving_server/sync_batch_norm_48.w_1, please check whether the model file is complete or damaged.\r\n  [Hint: Expected static_cast<bool>(fin) == true, but received static_cast<bool>(fin):0 != true:1.] (at /paddle/paddle/fluid/operators/load_op.h:41)\r\n  [operator < load > error]\r\n\r\n```\r\n#### Yolov3 模型\r\n```bash\r\nError Message Summary:\r\n----------------------\r\nUnavailableError: Load operator fail to open file ./serving_server/batch_norm_41.b_0, please check whether the model file is complete or damaged.\r\n  [Hint: Expected static_cast<bool>(fin) == true, but received static_cast<bool>(fin):0 != true:1.] (at /paddle/paddle/fluid/operators/load_op.h:41)\r\n  [operator < load > error]\r\n```\r\n\r\nYOLO V3 模型在 以下环境下运行是可以的：\r\n```bash\r\npaddle-serving-app        0.6.1\r\npaddle-serving-client     0.6.1\r\npaddle-serving-server-gpu 0.6.1.post102\r\npaddlepaddle-gpu          2.1.0\r\n```\r\n\r\n\r\n## 环境\r\n```bash\r\npaddle-serving-app        0.7.0\r\npaddle-serving-client     0.7.0\r\npaddle-serving-server-gpu 0.7.0.post102\r\npaddlepaddle-gpu          2.2.0\r\n\r\ncuda 10.2\r\nTesla V100\r\npython 3.8\r\n```",
        "state": "closed",
        "user": "zouxiaoshi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-23T08:03:32+00:00",
        "updated_at": "2024-12-03T06:43:09+00:00",
        "closed_at": "2024-12-03T06:43:09+00:00",
        "comments_count": [
            "bjjwwang",
            "TeslaZhao",
            "zouxiaoshi",
            "zouxiaoshi",
            "bjjwwang",
            "bjjwwang",
            "wenjia322"
        ],
        "labels": [
            "模型保存与转换"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1541,
        "title": "模型转换报错",
        "body": "请问各位前辈，我按照文档做图像识别的paddleserving部署时出现了这个错误该如何解决呢？serving/client/serving-app均已安装。",
        "state": "closed",
        "user": "lululu86",
        "closed_by": "lululu86",
        "created_at": "2021-12-01T02:37:43+00:00",
        "updated_at": "2022-03-16T08:37:12+00:00",
        "closed_at": "2021-12-02T06:12:07+00:00",
        "comments_count": [
            "lululu86",
            "github-actions[bot]",
            "lululu86",
            "lululu86",
            "kaierlong",
            "lululu86",
            "kaierlong",
            "lxhmark7"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1548,
        "title": "启动服务端，加载模型异常",
        "body": "在远程启动服务的时候，在加载模型的时候出现以下异常，在其他服务器上用同样的模型配置，可以正常启动。\r\n求教各位解答，谢谢\r\n\r\n![image](https://user-images.githubusercontent.com/12640462/144784628-0c6096b7-a6de-433c-a728-682d5657da85.png)\r\n",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-06T04:02:07+00:00",
        "updated_at": "2024-03-05T06:50:39+00:00",
        "closed_at": "2024-03-05T06:50:39+00:00",
        "comments_count": [
            "BeyondYourself",
            "BeyondYourself",
            "TeslaZhao",
            "TeslaZhao",
            "BeyondYourself",
            "TeslaZhao",
            "1019058432",
            "BeyondYourself",
            "1019058432",
            "BeyondYourself",
            "1019058432",
            "BeyondYourself",
            "1019058432"
        ],
        "labels": [
            "模型保存与转换"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1553,
        "title": "房价预测例子使用HTTP不能走通",
        "body": "使用docker，paddle Serving 0.7.0，按照文档一步一步走下来，在使用HTTP方法预测房价时在服务端输入python3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292 --name uci\r\n出现\r\n![image](https://user-images.githubusercontent.com/94336500/145175358-5dcb91d3-b554-49a6-a28e-ad55472c7bd3.png)\r\n![image](https://user-images.githubusercontent.com/94336500/145175756-28205cd8-b2c4-49b0-a299-448b32f98fcf.png)\r\n\r\n",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-08T08:36:55+00:00",
        "updated_at": "2024-04-16T09:05:58+00:00",
        "closed_at": "2024-04-16T09:05:58+00:00",
        "comments_count": [
            "snoopy1316",
            "github-actions[bot]",
            "TeslaZhao",
            "snoopy1316"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1555,
        "title": "房价预测模型使用HTTP方式走不通",
        "body": "使用的是paddle serving v0.7.0，修改服务端没有--name，即python3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292，在客户端输入curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://127.0.0.1:9292/uci/prediction\r\n结果出现\r\n![image](https://user-images.githubusercontent.com/94336500/145320567-ac6eba43-42b8-4a8e-adf1-e841f0ae4edd.png)\r\n参考文档：https://github.com/PaddlePaddle/Serving/blob/v0.7.0/doc/Quick_Start_CN.md",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-09T02:01:15+00:00",
        "updated_at": "2024-05-14T06:42:28+00:00",
        "closed_at": "2024-05-14T06:42:28+00:00",
        "comments_count": [
            "TeslaZhao",
            "snoopy1316",
            "EricMo",
            "zhiqiangohuo",
            "genius0182",
            "riyun1989"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1559,
        "title": "serving部署自己的模型预测发生错误",
        "body": "client端：\r\nI1210 16:46:38.004170 23575 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\nI1210 16:46:39.292992 23575 general_model.cpp:490] [client]logid=0,client_cost=1244.69ms,\r\n{}\r\nTraceback (most recent call last):\r\n  File \"../../deploy/serving/test_client.py\", line 75, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/home/ubuntu01/anaconda3/envs/paddleserving/lib/python3.8/site-packages/paddle_serving_app/reader/image_reader.py\", line 426, in __call__\r\n    bbox_result = self._get_bbox_result(image_with_bbox, fetch_name,\r\n  File \"/home/ubuntu01/anaconda3/envs/paddleserving/lib/python3.8/site-packages/paddle_serving_app/reader/image_reader.py\", line 343, in _get_bbox_result\r\n    output = fetch_map[fetch_name]\r\nKeyError: ''\r\n\r\n\r\n服务端：\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: The 'shape' attribute in ReshapeOp is invalid. The input tensor X'size must be divisible by known capacity of 'shape'. But received X's shape = [1, 33, 19, 19], X's size = 11913, 'shape' is [-1, 3, 11, 400], known capacity of 'shape' is -13200.\r\n  [Hint: Expected output_shape[unk_dim_idx] * capacity == -in_size, but received output_shape[unk_dim_idx] * capacity:0 != -in_size:-11913.] (at /paddle/paddle/fluid/operators/reshape_op.cc:210)\r\n  [operator < reshape2 > error]\r\nAborted (core dumped)\r\n\r\n请求大佬们帮忙看下，万分感谢",
        "state": "closed",
        "user": "wxf764571829",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-10T08:50:11+00:00",
        "updated_at": "2024-03-05T06:50:40+00:00",
        "closed_at": "2024-03-05T06:50:40+00:00",
        "comments_count": [
            "HexToString",
            "wxf764571829",
            "wxf764571829",
            "wxf764571829",
            "TeslaZhao",
            "wxf764571829",
            "fanruifeng"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1556,
        "title": "0.7.0版本client端：No module named 'paddle_serving_client.serving_client'",
        "body": "win10客户端使用test_client 报 F:\\ruanjian\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle_serving_client\\client.py:149: in __init__\r\n    from .serving_client import PredictorRes\r\nE   ModuleNotFoundError: No module named 'paddle_serving_client.serving_client'\r\n求解，版本是0.7.0\r\nclient端的采用pip安装的paddle_serving_client",
        "state": "closed",
        "user": "wxf764571829",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-09T05:57:47+00:00",
        "updated_at": "2024-03-05T06:50:40+00:00",
        "closed_at": "2024-03-05T06:50:40+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao",
            "wxf764571829",
            "wxf764571829",
            "wxf764571829",
            "wxf764571829",
            "HexToString",
            "TeslaZhao",
            "wxf764571829",
            "regainOWO",
            "whtwhtw"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1560,
        "title": "paddleServing v0.7.0，在paddleOCR例子中，获取模型之后，开启服务异常",
        "body": "在linux中docker内部署安装paddleServing v0.7.0，在paddleOCR例子中，获取模型之后，开启服务，输入python3 web_service.py，显示\r\n![image](https://user-images.githubusercontent.com/94336500/145768996-44ca3d27-5391-4de1-bbb9-32c61817d20e.png)\r\n在客户端输入python3 pipeline_http_client.py，\r\n显示：\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py\", line 532, in increment\r\n    raise six.reraise(type(error), error, _stacktrace)\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py\", line 769, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"/usr/local/lib/python3.6/http/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"pipeline_http_client.py\", line 36, in <module>\r\n    r = requests.post(url=url, data=json.dumps(data))\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/api.py\", line 117, in post\r\n    return request('post', url, data=data, json=json, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/requests/adapters.py\", line 498, in send\r\n    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n\r\n\r\n",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "snoopy1316",
        "created_at": "2021-12-13T07:29:01+00:00",
        "updated_at": "2021-12-13T07:36:09+00:00",
        "closed_at": "2021-12-13T07:36:09+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1567,
        "title": "generate_runtime_docker.sh构建docker镜像报错",
        "body": "文档命令：\r\n`bash tools/generate_runtime_docker.sh --env cuda10.1 --python 3.6 --name serving_runtime:cuda10.1-py36`\r\n输出：\r\n```\r\nInvalid arguments. paddle or env or python or serving is missing.\r\nusage: sh tools/generate_runtime_docker.sh --SOME_ARG ARG_VALUE\r\n\r\n   --env                 : running env, cpu/cuda10.1/cuda10.2/cuda11.2\r\n   --python              : python version, 3.6/3.7/3.8\r\n   --image_name          : image name(default serving_runtime:env-python)\r\n  -h | --help            : helper\r\n```\r\n看提示发现，命令对不上了，一个image_name，一个name\r\n\r\n修改完再测试，发现还是同样的输出，看内容发现脚本本身就有问题。。。\r\n\r\n/tools/generate_runtime_docker.sh文件\r\n28-29行，把两个参数注释掉了\r\n44行，又去验证被注释的参数\r\n72行，又把python重新定义，这本应该是paddle变量\r\n\r\n",
        "state": "closed",
        "user": "erma0",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-17T10:55:03+00:00",
        "updated_at": "2024-04-16T09:05:59+00:00",
        "closed_at": "2024-04-16T09:05:59+00:00",
        "comments_count": [],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1564,
        "title": "paddle detection 进行serving部署时产生的问题 ",
        "body": "使用paddle servingv0.7.0,在docker中部署，在启动服务端时，显示没有paddle_serving_server模块，但是在使用serving时已经下载了，在路径中可查，具体情况如下：\r\n![image](https://user-images.githubusercontent.com/94336500/146290983-4214a0c1-028d-42c6-9a2c-c5690e811eda.png)\r\n![image](https://user-images.githubusercontent.com/94336500/146291019-0409a416-667c-4344-8d92-a5d70b89278d.png)\r\n请教该如何解决\r\n参考文档:https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.3/deploy/serving\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.7.0/doc/Quick_Start_CN.md",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-16T01:27:58+00:00",
        "updated_at": "2024-04-16T09:05:59+00:00",
        "closed_at": "2024-04-16T09:05:59+00:00",
        "comments_count": [
            "bjjwwang",
            "snoopy1316"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1561,
        "title": "部署安装paddleServing v0.7.0，在paddleOCR例子中，获取模型之后，开启服务异常",
        "body": "在linux中docker内部署安装paddleServing v0.7.0，在paddleOCR例子中，获取模型之后，开启服务，输入python3 web_service.py，显示\r\n![image](https://user-images.githubusercontent.com/94336500/145768996-44ca3d27-5391-4de1-bbb9-32c61817d20e.png)\r\n在客户端输入python3 pipeline_http_client.py，无任何响应\r\n\r\n\r\n",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-13T07:34:24+00:00",
        "updated_at": "2024-03-05T06:50:41+00:00",
        "closed_at": "2024-03-05T06:50:41+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1587,
        "title": "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:",
        "body": "```\r\n File \"rpc_client.py\", line 27, in <module>\r\n    ret = client.predict(feed_dict=feed, fetch=[\"res\"])\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/paddle_serving_server/pipeline/pipeline_client.py\", line 202, in predict\r\n    resp = self._stub.inference(req)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\", line 946, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\r\n    raise _InactiveRpcError(state)\r\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n        status = StatusCode.UNAVAILABLE\r\n        details = \"connections to all backends failing\"\r\n        debug_error_string = \"{\"created\":\"@1641475038.124005663\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":5420,\"referenced_errors\":[{\"created\":\"@1641475038.123999296\",\"description\":\"connections to all backends failing\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.cc\",\"file_line\":336,\"grpc_status\":14}]}\"\r\n```\r\n\r\n请问这是啥原因？",
        "state": "closed",
        "user": "w5688414",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-06T13:19:10+00:00",
        "updated_at": "2024-03-05T06:50:43+00:00",
        "closed_at": "2024-03-05T06:50:43+00:00",
        "comments_count": [
            "w5688414",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1574,
        "title": "使用PaddleServing v0.7.0,dui paddledetection部署serving 服务，并使用Java的客户端",
        "body": "在serving端使用python3 -m paddle_serving_server.serve --model serving_server --port 9494 --gpu_ids 0开启服务，另一台电脑在客户端具体应该怎么做\r\n\r\n在第二个链接使用的预测房价中客户端语句 ：\r\ncd ../../../java/examples/target\r\njava -cp paddle-serving-sdk-java-examples-0.0.1-jar-with-dependencies.jar PipelineClientExample indarray_predict\r\n现在更换PaddleDetection，用Java客户端端口号以及IP怎样设置\r\n\r\n参考链接：https://github.com/PaddlePaddle/Serving/tree/v0.7.0/examples/C%2B%2B/PaddleDetection/yolov3_darknet53_270e_coco\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.7.0/java/README_CN.md\r\n\r\n（Paddle Detection 服务端和客户端用两台电脑进行）",
        "state": "closed",
        "user": "snoopy1316",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-12-22T02:38:32+00:00",
        "updated_at": "2024-03-05T06:50:42+00:00",
        "closed_at": "2024-03-05T06:50:42+00:00",
        "comments_count": [
            "HexToString"
        ],
        "labels": [
            "多语言client"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1593,
        "title": "编译 paddle-serving-server 出错",
        "body": "我没有修改任何文件，使用的是 paddlepaddle/serving:0.7.0-cuda11.2-cudnn8-devel 镜像，在编译 paddle-serving-server 的时候出错\r\n<img width=\"942\" alt=\"wecom-temp-d8d5e9fec10c760cd028edc4062264ca\" src=\"https://user-images.githubusercontent.com/38948322/149897976-c4996e11-1eb6-4320-bff3-562772b20547.png\">\r\n\r\n按照 https://github.com/PaddlePaddle/Serving/blob/v0.7.0/doc/Compile_CN.md 这个文档进行编译的\r\n\r\n\r\n",
        "state": "closed",
        "user": "CharlesWu123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-18T08:20:54+00:00",
        "updated_at": "2024-04-30T06:43:04+00:00",
        "closed_at": "2024-04-30T06:43:04+00:00",
        "comments_count": [
            "TeslaZhao",
            "Yinyihang857",
            "pupubushilulu"
        ],
        "labels": [
            "编译问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1591,
        "title": "串联服务",
        "body": "搭建pipline串联服务，是如何加载模型的?没有找到加载模型的方法",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-14T08:39:30+00:00",
        "updated_at": "2024-03-05T06:50:44+00:00",
        "closed_at": "2024-03-05T06:50:44+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1599,
        "title": "如何使用http服务远程访问",
        "body": "针对yolov3模型，执行命令`python -m paddle_serving_server_gpu.serve --model serving_server/ --port 9393 --gpu_id 0`后，启动了一个服务端，如何远程访问服务进行推理",
        "state": "closed",
        "user": "Huihuihh",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-24T03:15:00+00:00",
        "updated_at": "2024-03-05T06:50:45+00:00",
        "closed_at": "2024-03-05T06:50:45+00:00",
        "comments_count": [
            "TeslaZhao",
            "Huihuihh",
            "HexToString",
            "Yan-Ke"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1605,
        "title": "paddle serving对ernie部署",
        "body": "请问有paddle serving对ernie部署的试例吗",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-24T13:11:47+00:00",
        "updated_at": "2024-03-05T06:50:46+00:00",
        "closed_at": "2024-03-05T06:50:46+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1606,
        "title": "篇章级nlp任务",
        "body": "对于篇章级nlp任务，比如对篇章进行实体识别，把篇章分成句子集，每次请求是句子集，pipline应该怎么做",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-24T14:12:36+00:00",
        "updated_at": "2024-04-16T09:06:00+00:00",
        "closed_at": "2024-04-16T09:06:00+00:00",
        "comments_count": [
            "TeslaZhao",
            "ZTurboX",
            "ZTurboX"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1607,
        "title": "OSError:  '/usr/lib/libproxy_server.so' (no such file)",
        "body": "OS: macOS Monterey 12.1\r\n\r\n该目录下存在libproxy_server.so，gcc version 8.5\r\n\r\n用pipline方法报错\r\nOSError: dlopen(/Users/zhentao/opt/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/gateway/libproxy_server.so, 0x0006): tried: '/Users/zhentao/opt/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/gateway/libproxy_server.so' (not a mach-o file), '/usr/local/lib/libproxy_server.so' (no such file), '/usr/lib/libproxy_server.so' (no such file)",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-25T05:39:23+00:00",
        "updated_at": "2024-03-05T06:50:47+00:00",
        "closed_at": "2024-03-05T06:50:47+00:00",
        "comments_count": [
            "TeslaZhao",
            "mjwSilence"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1612,
        "title": "启动 CPU Docker出错，求老师指点",
        "body": "启动 CPU Docker操作中\r\n执行命令docker pull paddlepaddle/serving:0.7.0-devel出现下方错误\r\nError response from daemon: Get https://registry-1.docker.io/v2/: x509: certificate is valid for *.jgj25ec6i17v8.us-east-1.cs.amazonlightsail.com, not registry-1.docker.i\r\n使用的是虚拟机中的centos7系统，在其上安装了docker",
        "state": "closed",
        "user": "lifloveyou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-26T10:20:40+00:00",
        "updated_at": "2024-03-05T06:50:47+00:00",
        "closed_at": "2024-03-05T06:50:47+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1613,
        "title": "pipline client",
        "body": "用pipline时，在client端调用server服务输出的结果用于业务逻辑，可否再包一层flask接口",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-26T10:45:17+00:00",
        "updated_at": "2024-04-16T09:06:01+00:00",
        "closed_at": "2024-04-16T09:06:01+00:00",
        "comments_count": [
            "TeslaZhao",
            "ZTurboX"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1614,
        "title": "pip3 install -r python/requirements.txt报错",
        "body": "执行pip3 install -r python/requirements.txt报如下错误\r\nCould not find a version that satisfies the requirement grpcio-tools>=1.28.1 (from -r python/requirements.txt (line 7)) (from versions: )\r\nNo matching distribution found for grpcio-tools>=1.28.1 (from -r python/requirements.txt (line 7))\r\n",
        "state": "closed",
        "user": "lifloveyou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-26T11:08:01+00:00",
        "updated_at": "2024-03-05T06:50:48+00:00",
        "closed_at": "2024-03-05T06:50:48+00:00",
        "comments_count": [
            "TeslaZhao",
            "lifloveyou",
            "BeyondYourself"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1615,
        "title": "啥意思Message that will be displayed on users' first issue",
        "body": "Message that will be displayed on users' first issue\r\n\r\n_Originally posted by @github-actions[bot] in https://github.com/PaddlePaddle/Serving/issues/1612#issuecomment-1022060670_",
        "state": "closed",
        "user": "lifloveyou",
        "closed_by": "lifloveyou",
        "created_at": "2022-01-26T11:12:37+00:00",
        "updated_at": "2022-01-27T08:11:54+00:00",
        "closed_at": "2022-01-27T08:11:31+00:00",
        "comments_count": [
            "TeslaZhao",
            "lifloveyou"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1620,
        "title": "希望 paddle框架胜过tensorflow,希望官方和我们跟进未解决问题，不是有回复了就解决了",
        "body": "请问，linux中docker执行如下语句时候提示grpcio-tools>=1.28.1没有匹配\r\ncd Serving\r\npip3 install -r python/requirements.txt",
        "state": "closed",
        "user": "lifloveyou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-27T09:01:48+00:00",
        "updated_at": "2024-03-05T06:50:49+00:00",
        "closed_at": "2024-03-05T06:50:49+00:00",
        "comments_count": [
            "HexToString",
            "TeslaZhao",
            "lifloveyou",
            "lifloveyou",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1624,
        "title": "pipline性能调优",
        "body": "pipline如何根据机器性能调优，比如配置文件中 的worker_num和concurrency",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-28T04:31:35+00:00",
        "updated_at": "2024-03-05T06:50:50+00:00",
        "closed_at": "2024-03-05T06:50:50+00:00",
        "comments_count": [
            "HexToString",
            "ZTurboX",
            "HexToString",
            "BeyondYourself"
        ],
        "labels": [
            "性能"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1627,
        "title": "cuda error",
        "body": "用paddle inference 不会报错， 用pipline serving会报错\r\n\r\nCRITICAL 2022-02-09 10:46:11,025 [operator.py:1315] [ner|1] failed to init op: (External) CUDA error(3), initialization error. \r\n  [Hint: 'cudaErrorInitializationError'. The API call failed because the CUDA driver and runtime could not be initialized. ] (at /paddle/paddle/fluid/platform/gpu_info.cc:355)\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/operator.py\", line 1308, in _run\r\n    concurrency_idx)\r\n  File \"/root/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/local_service_handler.py\", line 228, in get_client\r\n    mkldnn_bf16_op_list=self._mkldnn_bf16_op_list)\r\n  File \"/root/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_app/local_predict.py\", line 235, in load_model_config\r\n    self.predictor = paddle_infer.create_predictor(config)\r\nOSError: (External) CUDA error(3), initialization error. \r\n  [Hint: 'cudaErrorInitializationError'. The API call failed because the CUDA driver and runtime could not be initialized. ] (at /paddle/paddle/fluid/platform/gpu_info.cc:355)\r\n",
        "state": "closed",
        "user": "ZTurboX",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-02-09T02:49:40+00:00",
        "updated_at": "2024-03-05T06:50:51+00:00",
        "closed_at": "2024-03-05T06:50:51+00:00",
        "comments_count": [
            "TeslaZhao",
            "ZTurboX",
            "KRuok",
            "WenmuZhou"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1630,
        "title": "使用pipeline启动yolov3的服务报错",
        "body": "针对自己的模型修改对应的`web_service.py`，如下所示：\r\n```\r\nfrom paddle_serving_server_gpu.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nimport sys\r\nimport cv2\r\nfrom paddle_serving_app.reader import *\r\nimport base64\r\n\r\n\r\nclass Yolov3Op(Op):\r\n    def init_op(self):\r\n        self.img_preprocess = Sequential([\r\n            File2Image(),BGR2RGB(), Div(255.0),\r\n            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], False),\r\n            Resize((608, 608)), Transpose((2, 0, 1))\r\n        ])\r\n        self.img_postprocess = RCNNPostprocess(\"label.txt\", \"output\")\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        #print(\"keys\", input_dict.keys())\r\n        for key in input_dict.keys():\r\n            data = base64.b64decode(input_dict[key].encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            im = self.img_preprocess(im)\r\n            imgs.append({\r\n                \"image\": im,\r\n                \"im_size\": np.array(list(im.shape[1:]))\r\n            })\r\n        feed_dict = {\r\n            \"image\": np.concatenate(\r\n                [x[\"image\"] for x in imgs], axis=0),\r\n            \"im_size\": np.concatenate(\r\n                [x[\"im_size\"] for x in imgs], axis=0)\r\n        }\r\n        #for key in feed_dict.keys():\r\n        #    print(key, feed_dict[key].shape)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\r\n        #print(fetch_dict)\r\n        res_dict = {\r\n            \"bbox_result\":\r\n            str(self.img_postprocess(\r\n                fetch_dict, visualize=False))\r\n        }\r\n        return res_dict, None, \"\"\r\n\r\n\r\nclass Yolov3Service(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        yolov3_op = Yolov3Op(name=\"yolov3\", input_ops=[read_op])\r\n        return yolov3_op\r\n\r\n\r\nyolov3_service = Yolov3Service(name=\"yolov3\")\r\nyolov3_service.prepare_pipeline_config(\"config.yml\")\r\nyolov3_service.run_service()\r\n```\r\n执行python web_service.py报错\r\n```\r\n/root/miniconda3/lib/python3.7/site-packages/paddle_serving_server_gpu/pipeline/pipeline_server.py:339: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\r\n  conf = yaml.load(f.read())\r\n2022/02/09 22:01:14 start proxy service\r\nI0209 22:01:16.640770 30138 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.641319 30138 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.641376 30138 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.641431 30138 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.641429 30116 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.641460 30138 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.642005 30116 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.642060 30116 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.642117 30116 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.642143 30116 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.651283 30287 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.651662 30287 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.651701 30287 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.651764 30287 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.651789 30287 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.681789 30257 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.682286 30257 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.682329 30257 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.682363 30257 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.682384 30257 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.683425 30097 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.683840 30097 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.683887 30097 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.683945 30097 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.683975 30097 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.684293 30194 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.684671 30194 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.684710 30194 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.684754 30194 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.684774 30194 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.694684 30164 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.695300 30164 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.695369 30164 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.695427 30164 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.695463 30164 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.708683 30239 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.709261 30239 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.709304 30239 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.709345 30239 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.709365 30239 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.709383 30217 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.710014 30217 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.710062 30217 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.710101 30217 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.710125 30217 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nI0209 22:01:16.710536 30081 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.711048 30081 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.711087 30081 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:16.711127 30081 analysis_config.cc:424] use_dlnne_:0\r\nW0209 22:01:16.711158 30081 analysis_predictor.cc:1142] Deprecated. Please use CreatePredictor instead.\r\nW0209 22:01:18.951959 30257 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:18.952088 30257 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:18.952114 30257 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nW0209 22:01:18.965909 30194 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:18.966055 30194 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:18.966080 30194 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nW0209 22:01:18.976042 30287 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:18.976346 30287 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:18.976382 30287 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:18.993237 30257 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nW0209 22:01:19.004395 30081 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.004616 30081 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.004642 30081 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:19.008275 30194 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nI0209 22:01:19.020421 30287 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nW0209 22:01:19.028826 30138 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.028990 30138 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.029013 30138 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:19.044615 30081 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nW0209 22:01:19.056638 30097 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.056921 30097 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.056951 30097 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:19.071693 30138 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nW0209 22:01:19.076261 30239 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.076536 30239 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.076570 30239 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nW0209 22:01:19.089545 30217 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.089638 30217 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.089660 30217 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:19.108762 30097 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nI0209 22:01:19.119357 30239 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nI0209 22:01:19.129479 30217 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nW0209 22:01:19.177314 30116 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.177659 30116 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.177695 30116 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nW0209 22:01:19.202306 30164 analysis_predictor.cc:674] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0209 22:01:19.202389 30164 analysis_config.cc:424] use_dlnne_:0\r\nI0209 22:01:19.202409 30164 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\nI0209 22:01:19.218194 30116 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n--- Running analysis [ir_graph_build_pass]\r\nI0209 22:01:19.235167 30164 analysis_predictor.cc:571] ir_optim is turned off, no IR pass will be executed\r\n```",
        "state": "closed",
        "user": "Huihuihh",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-02-10T03:08:15+00:00",
        "updated_at": "2024-03-05T06:50:52+00:00",
        "closed_at": "2024-03-05T06:50:52+00:00",
        "comments_count": [
            "TeslaZhao",
            "rubyangxg"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1634,
        "title": "请问 TRACER中的in是代表什么",
        "body": "为什么只有一次请求的时候in反而变大，还大于10秒了\r\n\r\n![image](https://user-images.githubusercontent.com/53896985/153702048-8faffc35-75bf-4ce7-a928-c987e7415d80.png)\r\n![image](https://user-images.githubusercontent.com/53896985/153702071-ceab988a-9dbf-432a-aa70-5cc948ae30fa.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "klw2020",
        "closed_by": "klw2020",
        "created_at": "2022-02-12T07:54:26+00:00",
        "updated_at": "2022-02-16T07:40:43+00:00",
        "closed_at": "2022-02-16T07:40:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1635,
        "title": "KeyError: 'multiclass_nms3_0.tmp_2.lod'",
        "body": "部署fasterrcnn模型时，预测出错\r\n预测的代码\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport cv2\r\n\r\npreprocess = DetectionSequential([\r\n        DetectionFile2Image(),\r\n        DetectionNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], True),\r\n        DetectionResize(\r\n        (800, 1333), True, interpolation=cv2.INTER_LINEAR), \r\n        DetectionTranspose((2,0,1)),\r\n        DetectionPadStride(128)\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\n\r\nclient.load_client_config(\"serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\n\r\nim, im_info = preprocess(sys.argv[1])\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n        \"im_shape\": np.array(list(im.shape[1:])).reshape(-1),\r\n        \"scale_factor\": im_info['scale_factor'],\r\n    },\r\n    #fetch=[\"save_infer_model/scale_0.tmp_1\"],#multiclass_nms3_0.tmp_2\r\n    fetch=[\"multiclass_nms3_0.tmp_2\"],\r\n    batch=False)\r\nfetch_map[\"image\"] = sys.argv[1]\r\npostprocess(fetch_map)\r\n\r\nserving_client_conf.prototxt:\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"scale_factor\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"concat_12.tmp_0\"\r\n  alias_name: \"concat_12.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 6\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_2\"\r\n  alias_name: \"multiclass_nms3_0.tmp_2\"\r\n  is_lod_tensor: false\r\n  fetch_type: 2\r\n}\r\n\r\nclient报错信息\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 47, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'multiclass_nms3_0.tmp_2.lod'\r\n\r\nserver log:\r\nW0216 10:07:18.747469  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:07:18.747480  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:07:28.747581  2563 server.cpp:151] Begin reload framework...\r\nW0216 10:07:28.747633  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:07:28.747643  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:07:38.747742  2563 server.cpp:151] Begin reload framework...\r\nW0216 10:07:38.747807  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:07:38.747819  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:07:44.068684  2572 general_model_service.pb.cc:3319] (logid=0) remote_side=[127.0.0.1:46034]\r\nI0216 10:07:44.068742  2572 general_model_service.pb.cc:3320] (logid=0) local_side=[127.0.0.1:9292]\r\nI0216 10:07:44.068750  2572 general_model_service.pb.cc:3321] (logid=0) service_name=[GeneralModelService]\r\nI0216 10:07:44.068750  2572 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0216 10:07:44.068750  2572 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0216 10:07:44.068750  2572 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0216 10:07:44.079159  2572 op.cpp:164] (logid=0) general_reader_0_time=[10296]\r\nI0216 10:07:44.326128  2572 op.cpp:164] (logid=0) general_infer_0_time=[246925]\r\nI0216 10:07:44.326174  2572 op.cpp:164] (logid=0) general_response_0_time=[8]\r\nI0216 10:07:44.326189  2572 service.cpp:263] (logid=0) workflow total time: 257421\r\nI0216 10:07:44.326215  2572 general_model_service.pb.cc:3343] [serving]logid=0,cost=257.534ms.\r\nI0216 10:07:48.747907  2563 server.cpp:151] Begin reload framework...\r\nW0216 10:07:48.747962  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:07:48.747973  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:07:58.748062  2563 server.cpp:151] Begin reload framework...\r\nW0216 10:07:58.748112  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:07:58.748121  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:08:08.748234  2563 server.cpp:151] Begin reload framework...\r\nW0216 10:08:08.748302  2563 infer.cpp:287] Succ reload version engine: 18446744073709551615\r\nI0216 10:08:08.748315  2563 manager.h:131] Finish reload 1 workflow(s)\r\nI0216 10:08:18.748411  2563 server.cpp:151] Begin reload framework...\r\n处理\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_2\"\r\n  alias_name: \"multiclass_nms3_0.tmp_2\"\r\n  is_lod_tensor: false\r\n  fetch_type: 2\r\n}\r\n修改为\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_2\"\r\n  alias_name: \"multiclass_nms3_0.tmp_2\"\r\n  is_lod_tensor: true\r\n  fetch_type: 2\r\n}\r\n也不行\r\n",
        "state": "closed",
        "user": "wjplove8",
        "closed_by": "wjplove8",
        "created_at": "2022-02-16T09:58:14+00:00",
        "updated_at": "2023-02-22T07:16:38+00:00",
        "closed_at": "2022-02-18T09:40:08+00:00",
        "comments_count": [
            "github-actions[bot]",
            "felixhjh",
            "wjplove8",
            "ChiTaoGIS",
            "zhiqiangohuo",
            "wjplove8",
            "wjplove8"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1637,
        "title": "java调用远端预测模型，返回的一堆字符串我该如何进一步处理，如何转成图片",
        "body": "PaddleServingClientExample.java\r\n我模仿了例子中的yolov4方法，预测一张图片，\r\nClient client = new Client();\r\n        client.setIP(\"127.0.0.1\");\r\n        client.setPort(\"9292\");\r\n        client.loadClientConfig(model_config_path);\r\n        String result = client.predict(feed_data, fetch, true, 0);\r\n        System.out.println(result);\r\n我如何处理这里的result，目前我得到的是这样的数据，我如何转换成图片？\r\noutputs {\r\n  tensor {\r\n    int64_data: 0\r\n    int64_data: 0\r\n    int64_data: 0\r\n    int64_data: 0\r\n    int64_data: 0\r\n    ......\r\n    shape: 1\r\n    shape: 275\r\n    shape: 170\r\n    name: \"depthwise_conv2d_11.tmp_0\"\r\n    alias_name: \"argmax_0.tmp_0\"\r\n  }\r\n  engine_name: \"general_infer_0\"\r\n}\r\nprofile_time: 1645339861250398\r\nprofile_time: 1645339861346107\r\n我看到有python的demo可以实现\r\nimg = seq(args.image_path)\r\n    fetch_map = client.predict(\r\n        feed={\"x\": img}, fetch=[\"argmax_0.tmp_0\"])\r\n\r\n    result = fetch_map[\"argmax_0.tmp_0\"]\r\n    color_img = get_pseudo_color_map(result[0])\r\n    color_img.save(\"./result.png\")\r\n    print(\"The segmentation image is saved in ./result.png\")\r\n如果换成java，如何保存图片，能否把example代码补全",
        "state": "closed",
        "user": "rubyangxg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-02-20T07:29:09+00:00",
        "updated_at": "2024-03-05T06:50:53+00:00",
        "closed_at": "2024-03-05T06:50:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "lichengzhang19",
            "shiyun123456789"
        ],
        "labels": [
            "多语言client"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1638,
        "title": "自训练模型在serving部署后预测错误",
        "body": "我训练用的是obj365里的cascade_rcnn_dcnv2_se154_vd_fpn_gn_cas.yml，报错\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 38, in <module>\r\n    fetch_map[\"image\"] = sys.argv[1]\r\nTypeError: 'NoneType' object does not support item assignment\r\n\r\n这是client代码\r\nimport sys\r\nimport numpy as np\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import *\r\nimport cv2\r\n\r\npreprocess = DetectionSequential([\r\n    DetectionFile2Image(),\r\n    DetectionNormalize([102.9801,115.9465,122.7717],[1.0,1.0,1.0],False),\r\n\r\n#    DetectionTranspose((2, 0, 1)), DetectionPadStride(32)\r\n])\r\n\r\npostprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\nclient = Client()\r\n\r\nclient.load_client_config(\"../serving_client/serving_client_conf.prototxt\")\r\nclient.connect(['127.0.0.1:9292'])\r\nprint(client)\r\nprint(\"----------------------------------------------------\")\r\nim, im_info = preprocess(sys.argv[1])\r\nfetch_map = client.predict(\r\n    feed={\r\n        \"image\": im,\r\n\r\n        \"im_shape\": np.array([320,480,1]),\r\n\r\n        \"im_info\": np.array([480,640,2]),\r\n    },\r\n    fetch=[\"multiclass_nms_0.tmp_0\"],\r\n    batch=False)\r\n\r\nprint(fetch_map)\r\nprint(sys.argv[1])\r\nfetch_map[\"image\"] = sys.argv[1]\r\npostprocess(fetch_map)\r\n\r\n这是输入输出\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_info\"\r\n  alias_name: \"im_info\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms_0.tmp_0\"\r\n  alias_name: \"multiclass_nms_0.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}",
        "state": "closed",
        "user": "zhaoyids",
        "closed_by": "zhaoyids",
        "created_at": "2022-02-21T02:00:21+00:00",
        "updated_at": "2022-12-20T14:43:24+00:00",
        "closed_at": "2022-03-05T13:37:23+00:00",
        "comments_count": [
            "github-actions[bot]",
            "felixhjh",
            "zhaoyids",
            "ddgetget"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1659,
        "title": "paddleocr deploy 无法使用gpu",
        "body": "使用docker部署\r\n环境：\r\npaddlepaddle/serving:0.7.0-cuda11.2-cudnn8-devel\r\npip3 install paddle-serving-server-gpu==0.7.0.post112\r\n\r\nRUN pip3 install https://paddle-inference-lib.bj.bcebos.com/2.2.2/python/Linux/GPU/x86-64_gcc8.2_avx_mkl_cuda11.2_cudnn8.2.1_trt8.0.3.4/paddlepaddle_gpu-2.2.2.post112-cp36-cp36m-linux_x86_64.whl\r\n\r\n\r\ncd Serving\r\npip3 install -r python/requirements.txt\r\n\r\n下载paddleocr的代码，启动服务 python3 web_service.py\r\n\r\n```\r\nW0222 07:39:08.607676 15964 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607674 16020 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607687 15948 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607676 15955 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607683 16035 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607681 16007 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.607686 15993 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\nW0222 07:39:08.694010 15979 init.cc:143] Compiled with WITH_GPU, but no GPU found in runtime.\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n  \"You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\"\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n  \"You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\"\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n  \"You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\"\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n  \"You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\"\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n  \"You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\"\r\n/usr/local/lib/python3.6/site-packages/paddle/fluid/framework.py:312: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\r\n\r\n```\r\n\r\n版本应该文档说明配置的，应该都是match的。请问是还有什么依赖没有安装吗？\r\n",
        "state": "closed",
        "user": "thomaszheng",
        "closed_by": "thomaszheng",
        "created_at": "2022-02-22T07:40:45+00:00",
        "updated_at": "2022-06-21T10:03:50+00:00",
        "closed_at": "2022-06-21T10:03:50+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "thomaszheng",
            "thomaszheng"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1660,
        "title": "uci_housing_model无法正常启动",
        "body": "CPU：AMD R7 5800H\r\n显卡：无\r\n系统：Win10\r\n虚拟机：Centos 7\r\n部署方案：Docker （Server Version: 20.10.12）\r\nServing镜像版本：paddlepaddle/serving:0.7.0-devel\r\n部署参考文档（CPU）：\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.7.0/doc/Install_CN.md\r\n快速开始文档：\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.7.0/doc/Quick_Start_CN.md\r\n问题描述：\r\n无论是快速开始的波士顿房价还是PaddleOCR中的例子都无法正常启动相关模型服务。会一直卡在下面输出位置：\r\n```\r\nλ 32a88fba94d8 /home/Serving/examples/C++/fit_a_line {v0.7.0} python3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292\r\n/usr/local/lib/python3.6/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nFrist time run, downloading PaddleServing components ...\r\n--2022-02-23 03:09:01--  https://paddle-serving.bj.bcebos.com/test-dev/bin/serving-cpu-avx-mkl-0.7.0.tar.gz\r\nResolving paddle-serving.bj.bcebos.com (paddle-serving.bj.bcebos.com)... 220.181.33.43, 220.181.33.44, 2409:8c04:1001:1002:0:ff:b001:368a\r\nConnecting to paddle-serving.bj.bcebos.com (paddle-serving.bj.bcebos.com)|220.181.33.43|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 146905208 (140M) [application/octet-stream]\r\nSaving to: ‘serving-cpu-avx-mkl-0.7.0.tar.gz’\r\n\r\nserving-cpu-avx-mkl-0.7.0.tar.gz                         100%[===============================================================================================================================>] 140.10M  11.2MB/s    in 19s     \r\n\r\n2022-02-23 03:09:23 (7.23 MB/s) - ‘serving-cpu-avx-mkl-0.7.0.tar.gz’ saved [146905208/146905208]\r\n\r\nDecompressing files ..\r\nGoing to Run Comand\r\n/usr/local/lib/python3.6/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.7.0/serving -enable_model_toolkit -inferservice_path workdir_9292 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 10 -port 9292 -precision fp32 -use_calib=False -reload_interval_s 10 -resource_path workdir_9292 -resource_file resource.prototxt -workflow_path workdir_9292 -workflow_file workflow.prototxt -bthread_concurrency 10 -max_body_size 536870912 \r\nI0100 00:00:00.000000   226 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000   226 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000   226 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000   226 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000   226 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000   226 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   226 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000   226 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000   226 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000   226 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000   226 paddle_engine.cpp:29] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\n```\r\n不确定是否这个位置就是启动成功，如果是启动成功，那么运行文档中的HTTP测试curl会出现如下错误：\r\n```\r\n[root@localhost ~]# curl -H \"Content-Type:application/json\" -X POST -d '{\"feed\":[{\"x\": [0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332]}], \"fetch\":[\"price\"]}' http://127.0.0.1:9292/uci/prediction\r\n[172.17.0.2:9292][E1002]Fail to find method on `/uci/prediction'[root@localhost ~]#\r\n```\r\n\r\n",
        "state": "closed",
        "user": "EricMo",
        "closed_by": "HexToString",
        "created_at": "2022-02-23T07:17:32+00:00",
        "updated_at": "2022-02-24T05:30:24+00:00",
        "closed_at": "2022-02-24T05:30:24+00:00",
        "comments_count": [
            "github-actions[bot]",
            "EricMo",
            "HexToString"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1679,
        "title": "key error during serving the default transformer model",
        "body": "I trained the default en-de translation transfer model and served it following the schedule you guys recommend but it report the key error as follow:\"[2022-02-28 18:02:39,622] ERROR in app: Exception on /transformer/prediction [POST]\r\nTraceback (most recent call last):\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/app.py\", line 1952, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/app.py\", line 1821, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/app.py\", line 1950, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/flask/app.py\", line 1936, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/paddle_serving_server/web_service.py\", line 334, in run\r\n    return self.get_prediction(request)\r\n  File \"/data/venv/paddle/lib/python3.6/site-packages/paddle_serving_server/web_service.py\", line 303, in get_prediction\r\n    feed=request.json[\"feed\"], fetch=fetch, fetch_map=fetch_map)\r\n  File \"transformer_web_server.py\", line 79, in postprocess\r\n    \"save_infer_model/scale_0.tmp_1\"]).transpose([0, 2, 1])\r\nKeyError: 'save_infer_model/scale_0.tmp_1'\"\r\n The error refers to the paddle serving module, and I cannot figure out whats going on.",
        "state": "closed",
        "user": "ghtwht",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-02-28T10:17:37+00:00",
        "updated_at": "2024-03-05T06:50:54+00:00",
        "closed_at": "2024-03-05T06:50:54+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1698,
        "title": "web_service.py 运行问题",
        "body": "你好，我用paddleDetection 训练pp-picodet模型，然后用paddleDetection自带的工具导出模型，然后用参考Serving/examples/Pipeline/PaddleDetection/ppyolo_mbv3，修改config.yml，然后运行web_service.py  pipeline_http_client.py，提示以下错误：\r\n{'err_no': 8, 'err_msg': \"(data_id=0 log_id=0) [ppyolo|0] Failed to postprocess: 'transpose_17.tmp_0.lod'\", 'key': [], 'value': [], 'tensors': []}\r\n\r\n请问我如何修改？？？\r\n\r\nserving_server/serving_server_conf.prototxt ，内容如下：\r\n\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 1\r\n  shape: 3\r\n  shape: 416\r\n  shape: 416\r\n}\r\nfetch_var {\r\n  name: \"transpose_10.tmp_0\"\r\n  alias_name: \"transpose_10.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 2704\r\n  shape: 4\r\n}\r\nfetch_var {\r\n  name: \"transpose_11.tmp_0\"\r\n  alias_name: \"transpose_11.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 2704\r\n  shape: 32\r\n}\r\nfetch_var {\r\n  name: \"transpose_12.tmp_0\"\r\n  alias_name: \"transpose_12.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 676\r\n  shape: 4\r\n}\r\nfetch_var {\r\n  name: \"transpose_13.tmp_0\"\r\n  alias_name: \"transpose_13.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 676\r\n  shape: 32\r\n}\r\nfetch_var {\r\n  name: \"transpose_14.tmp_0\"\r\n  alias_name: \"transpose_14.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 169\r\n  shape: 4\r\n}\r\nfetch_var {\r\n  name: \"transpose_15.tmp_0\"\r\n  alias_name: \"transpose_15.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 169\r\n  shape: 32\r\n}\r\nfetch_var {\r\n  name: \"transpose_16.tmp_0\"\r\n  alias_name: \"transpose_16.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 49\r\n  shape: 4\r\n}\r\nfetch_var {\r\n  name: \"transpose_17.tmp_0\"\r\n  alias_name: \"transpose_17.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 49\r\n  shape: 32\r\n}\r\n\r\n\r\nconfig.yml 内容如下：\r\ndag:\r\n  is_thread_op: false\r\n  tracer:\r\n    interval_s: 30\r\nhttp_port: 18888\r\nop:\r\n  ppyolo:\r\n    concurrency: 1\r\n\r\n    local_service_conf:\r\n      client_type: local_predictor\r\n      device_type: 1\r\n      devices: '0'\r\n      fetch_list: \r\n      - transpose_17.tmp_0\r\n      model_config: serving_server/\r\nrpc_port: 9998\r\nworker_num: 2\r\n\r\n\r\n",
        "state": "closed",
        "user": "lguowang",
        "closed_by": "lguowang",
        "created_at": "2022-03-07T09:53:37+00:00",
        "updated_at": "2022-03-08T09:15:06+00:00",
        "closed_at": "2022-03-08T09:15:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1699,
        "title": "PicoDet 在 Sering 部署问题",
        "body": "\r\n\r\n你好，我用pp2.3训练了PicoDet，然后pp(develop)导出模型进行sering部署，\r\n但预测时出现主要的错误：{'err_no': 8, 'err_msg': \"(data_id=0 log_id=0) [ppyolo_v3|0] Failed to postprocess: 'scale_factor.lod'\", 'key': [], 'value': [], 'tensors': []}\r\n\r\n\r\n请问我要怎么修改才能让PicoDet在Sering上运行？\r\n\r\n\r\n说明：服务和客户端参考的是Serving/examples/Pipeline/PaddleDetection/ppyolo_mbv3/下的 web_service.py 和pipeline_http_client.py\r\n\r\n\r\n\r\nserving_server 中的 serving_server_conf.prototxt ：\r\n\r\n\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 416\r\n  shape: 416\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_1.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_1.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_2.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_2.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_3.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_3.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_4.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_4.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_5.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_5.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_6.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_6.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_7.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_7.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n\r\n\r\n\r\n\r\n我配置的config.yml:\r\n\r\n\r\ndag:\r\n  is_thread_op: false\r\n  tracer:\r\n    interval_s: 20\r\nhttp_port: 18888\r\nop:\r\n  yolov3:\r\n    concurrency: 10\r\n    local_service_conf:\r\n      client_type: local_predictor\r\n      device_type: 1\r\n      devices: '2'\r\n      fetch_list:\r\n      - save_infer_model/scale_0.tmp_1\r\n      model_config: serving_server/\r\nrpc_port: 9998\r\nworker_num: 2\r\n\r\n\r\n\r\n请问我要怎么修改才能让PicoDet在Sering上运行？\r\n",
        "state": "closed",
        "user": "lguowang",
        "closed_by": "lguowang",
        "created_at": "2022-03-08T09:32:25+00:00",
        "updated_at": "2022-03-11T01:12:35+00:00",
        "closed_at": "2022-03-11T01:12:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1683,
        "title": "serving-gpu 找不到 libnvinfer.so.6",
        "body": "你好\r\n\r\n我照著這篇教程去run paddle serving 的 container\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.8.2/doc/Install_CN.md\r\n\r\n但最後要serve自己的模型的時候，出現找不到`libnvinfer.so.6`的問題\r\n\r\n我的步驟是先從安裝nvidia-docker的runtime環境開始到最後的serving\r\n\r\n## 復現步驟\r\n\r\n- 參照 https://zhuanlan.zhihu.com/p/37519492 ，安裝`nvidia-docker`\r\n\r\n1. 安裝`nvidia-container-toolkit`\r\n\r\n```bash\r\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\r\nsudo apt-get install nvidia-docker2\r\nsudo pkill -SIGHUP dockerd\r\n```\r\n\r\n2. 增加 runtime設定 `\"default-runtime\": \"nvidia\"` 到 `/etc/docker/daemon.json`.\r\n\r\n```json\r\n{\r\n    \"default-runtime\": \"nvidia\",\r\n    \"runtimes\": {\r\n        \"nvidia\": {\r\n            \"path\": \"/usr/bin/nvidia-container-runtime\",\r\n            \"runtimeArgs\": []\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n3. 重啓docker服務，然後確認是否正常\r\n\r\n```bash\r\nsudo systemctl restart docker\r\nsystemctl status docker\r\n```\r\n\r\n4. 測試 nvidia runtime container 是否能夠 work\r\n\r\n```bash\r\ndocker run --runtime=nvidia --rm nvidia/cuda:8.0-cudnn6-runtime-ubuntu16.04 nvidia-smi\r\n```\r\n\r\n到這邊都沒有問題的話，開始pull跟run paddle serving的container\r\n\r\n```bash\r\n# 拉image\r\ndocker pull registry.baidubce.com/paddlepaddle/paddle:2.2.2-gpu-cuda10.2-cudnn7\r\n# 用nvidia runtime去run container 並 bind model 的位置\r\ndocker run --rm --runtime=nvidia -p 9292:9292 -v $(pwd)/model:/model --name test -dit registry.baidubce.com/paddlepaddle/paddle:2.2.2-gpu-cuda10.2-cudnn7 bash\r\n\r\n# 進入container後，clone serving包跟裝依賴\r\ndocker exec -it test bash\r\ngit clone https://github.com/PaddlePaddle/Serving\r\ncd Serving\r\npip3 install -r python/requirements.txt\r\n\r\npip3 install paddle-serving-client==0.8.2\r\npip3 install paddle-serving-app==0.8.2\r\npip3 install paddle-serving-server-gpu==0.8.2.post102 # cuda10.2\r\n\r\n# 開啓服務\r\npython -m paddle_serving_server.serve --model /model/serving_server --port 9292 --gpu_id 0\r\n```\r\n\r\n然後就噴了\r\n\r\n```bash\r\nλ 7aa3453c887f /home python -m paddle_serving_server.serve --model ./serving_server --port 9292 --gpu_id 0\r\n/usr/local/python3.7.0/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nGoing to Run Comand\r\n/usr/local/python3.7.0/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-102-0.8.2/serving -enable_model_toolkit -inferservice_path workdir_9292 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9292 -precision fp32 -use_calib=False -reload_interval_s 10 -resource_path workdir_9292 -resource_file resource.prototxt -workflow_path workdir_9292 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912 \r\n/usr/local/python3.7.0/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-102-0.8.2/serving: error while loading shared libraries: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n```\r\n\r\n請問我的步驟有哪邊出問題了嗎？\r\n\r\n謝謝！\r\n\r\n\r\n\r\n### System spec\r\n\r\nOS: Ubuntu 18.04\r\nCPU: Intel(R) Core(TM) i7-9700F\r\nGPU: Nvidia RTX 2060\r\nGPU Driver Version: 470.103.01\r\nDocker version: 20.10.6, build 370c289\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "NatLee",
        "closed_by": "NatLee",
        "created_at": "2022-03-01T05:49:38+00:00",
        "updated_at": "2022-03-01T06:21:43+00:00",
        "closed_at": "2022-03-01T06:21:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "NatLee"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1703,
        "title": "higherHRNet 模型部署测试出错: ValueError: Wrong feed name: im_shape.",
        "body": "修改配置文件自己训练了一个higherHRNet模型，infer没问题（根据实际修改过），模型导出也没问题，部署测试时报错：\r\n`root@ubuntu:/home/project/strabismus_detection/output_inference/higherhrnet_hrnet_w32_512# python3 ../../deploy/serving/test_client.py ../../deploy/serving/label_list.txt ../../demo/0000015.jpg\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0309 00:16:40.764921 22318 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\n../../deploy/serving/test_client.py ../../deploy/serving/label_list.txt ../../demo/0000015.jpg\r\nTraceback (most recent call last):\r\n  File \"../../deploy/serving/test_client.py\", line 41, in <module>\r\n    fetch_map = client.predict(\r\n  File \"/usr/local/lib/python3.8/dist-packages/paddle_serving_client/client.py\", line 366, in predict\r\n    raise ValueError(\"Wrong feed name: {}.\".format(key))\r\nValueError: Wrong feed name: im_shape.\r\n`\r\n\r\nserving_client_conf.prototxt配置文件如下：\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"bilinear_interp_v2_1.tmp_0\"\r\n  alias_name: \"bilinear_interp_v2_1.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 6\r\n}\r\nfetch_var {\r\n  name: \"top_k_v2_0.tmp_0\"\r\n  alias_name: \"top_k_v2_0.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n  shape: 6\r\n  shape: 30\r\n}\r\nfetch_var {\r\n  name: \"top_k_v2_0.tmp_1\"\r\n  alias_name: \"top_k_v2_0.tmp_1\"\r\n  is_lod_tensor: false\r\n  fetch_type: 0\r\n  shape: 1\r\n  shape: 6\r\n  shape: 30\r\n}\r\nfetch_var {\r\n  name: \"unsqueeze2_0.tmp_0\"\r\n  alias_name: \"unsqueeze2_0.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 6\r\n  shape: 1\r\n}\r\n\r\nlabel_list.txt文件在最后一行添加了自己的类别名\r\n\r\n\r\n环境：虚拟机ubuntu20.04, cpu部署，paddle_serving版本0.8.3，0.7.0均测试过，出现同样问题，请问是什么原因？谢谢\r\n\r\n",
        "state": "closed",
        "user": "Victoria-1",
        "closed_by": "Victoria-1",
        "created_at": "2022-03-09T08:32:09+00:00",
        "updated_at": "2022-03-10T03:17:52+00:00",
        "closed_at": "2022-03-10T03:17:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Victoria-1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1710,
        "title": "关键点检测模型部署",
        "body": "请问是否有关键点检测模型部署样例，或者相关接口信息，现在已获得fetch_map，但不太知道怎么用这个信息获得最终检测结果。感谢大佬",
        "state": "closed",
        "user": "Victoria-1",
        "closed_by": "TeslaZhao",
        "created_at": "2022-03-10T05:26:48+00:00",
        "updated_at": "2022-03-21T09:04:59+00:00",
        "closed_at": "2022-03-21T09:04:59+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1720,
        "title": "预测自建模型时，Serving报错",
        "body": "![image](https://user-images.githubusercontent.com/15975613/158556750-1257a716-13b0-4cbb-94f4-96d0c9cdecaa.png)\r\n![image](https://user-images.githubusercontent.com/15975613/158556970-01efe49a-de2a-44d5-9e44-36f12f7fd314.png)\r\n",
        "state": "closed",
        "user": "yqsoooo",
        "closed_by": "yqsoooo",
        "created_at": "2022-03-16T09:17:26+00:00",
        "updated_at": "2022-03-16T09:35:16+00:00",
        "closed_at": "2022-03-16T09:35:16+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1712,
        "title": "华为昇腾310+X86环境适配疑问",
        "body": "是否支持华为昇腾310和x86 适配？目前框架做了arm+昇腾310的适配，能否支持x86+昇腾310的。",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-11T06:12:41+00:00",
        "updated_at": "2024-03-05T06:50:55+00:00",
        "closed_at": "2024-03-05T06:50:55+00:00",
        "comments_count": [
            "ShiningZhang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1717,
        "title": "关键点检测用serving如何部署",
        "body": "自己训练出HigherHRNet模型，成功导出模型，修改test_client.py也成功获得fetch_map，请问得到fetch_map之后如何获得检测结果，有示例吗？",
        "state": "closed",
        "user": "Victoria-1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-14T04:03:58+00:00",
        "updated_at": "2024-03-05T06:50:56+00:00",
        "closed_at": "2024-03-05T06:50:56+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1721,
        "title": "paddleServing JavaSdk 不可用",
        "body": "paddle-serving-sdk-java-0.0.1.jar\r\n![image](https://user-images.githubusercontent.com/15975613/158598086-7ed28dc6-2544-4368-8c4d-36a1269b8677.png)\r\n\r\n\r\n引用Serving0.8.2版本的java examples里的PaddleServingClientExample.java  但好像这个jar包版本不对啊",
        "state": "closed",
        "user": "yqsoooo",
        "closed_by": "yqsoooo",
        "created_at": "2022-03-16T13:16:45+00:00",
        "updated_at": "2023-12-06T10:03:08+00:00",
        "closed_at": "2022-03-16T13:47:28+00:00",
        "comments_count": [
            "yqsoooo",
            "wangzhedong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1722,
        "title": "0.8.2 版本的JAVA_SDK疑问",
        "body": "通过Java_SDK_CN.md \r\nwget https://paddle-serving.bj.bcebos.com/jar/paddle-serving-sdk-java-0.0.1.jar\r\n\r\n下载下来查看其中的Client对象和examples中的PaddleServingClientExample.java 完全对不上。\r\n",
        "state": "closed",
        "user": "yqsoooo",
        "closed_by": "yqsoooo",
        "created_at": "2022-03-16T14:39:59+00:00",
        "updated_at": "2022-05-23T07:23:02+00:00",
        "closed_at": "2022-03-17T02:24:29+00:00",
        "comments_count": [
            "yqsoooo",
            "ChiTaoGIS"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1723,
        "title": "pipeline serving 0.7.0 rpc服务端启动错误",
        "body": "### 使用pipeline 部署OCR模型，client是local predictor正常执行，选rpc报错\r\n![1](https://user-images.githubusercontent.com/22436644/158752370-f8d6d36a-336a-4433-9aac-cff1dd3bffb9.png)\r\npipeline日志是这样\r\n![2](https://user-images.githubusercontent.com/22436644/158752385-1a3cfea7-8a51-444e-aa14-a79e4722bb8e.png)\r\nunknown client type grpc\r\n\r\n版本是:\r\n-paddlepaddle==2.2.2, cpu\r\n-serving==0.7.0\r\n系统环境是Win10上的linux docker\r\n",
        "state": "closed",
        "user": "zhuangzheng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-17T06:47:40+00:00",
        "updated_at": "2024-03-05T06:50:56+00:00",
        "closed_at": "2024-03-05T06:50:56+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1724,
        "title": "使用Serving預測，顯存異常增長",
        "body": "各位先進跟大佬好\r\n\r\n我照着官方教程並使用docker部署模型，遇到一個問題，就是啓動服務放一天預測，發現顯存會異常增長\r\n\r\n### 操作過程\r\n\r\n-------------------------\r\n\r\n\r\n我用docker-compose建立image並run一個可以使用轉換過的模型進行預測的container\r\n\r\n```Dockerfile\r\nFROM registry.baidubce.com/paddlepaddle/paddle:2.2.2-gpu-cuda10.2-cudnn7\r\n\r\nRUN git clone https://github.com/PaddlePaddle/Serving\r\nRUN bash Serving/tools/paddle_env_install.sh\r\nRUN cd Serving && pip3 install -r python/requirements.txt\r\n\r\nRUN pip3 install paddle-serving-client==0.8.2\r\nRUN pip3 install paddle-serving-app==0.8.2\r\nRUN pip3 install paddle-serving-server-gpu==0.8.2.post102\r\n```\r\n\r\n```docker-compose.yml\r\nversion: \"3.2\"\r\nservices:\r\n  ernie_service:\r\n    container_name: \"ernie_service\"\r\n    runtime: \"nvidia\"\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    restart: always\r\n    volumes:\r\n        - /etc/localtime:/etc/localtime:ro\r\n        - ./serving_server:/serving_server\r\n    ports:\r\n      - 9292:9292\r\n    command: bash -c \"python -m paddle_serving_server.serve --model /serving_server --port 9292 --gpu_id 0 --thread 10\"\r\n```\r\n\r\n> `./serving_server`資料夾內放的是可服務化的`ERNIE-1.0`情感分類模型（就是用官方教程導出的東西）\r\n\r\n經過一段時間預測發現顯存使用量從原本的`1~2 GB`異常增長到`7.6 GB`\r\n\r\n`client.py`的片段代碼如下\r\n\r\n```py\r\nTOKENIZER = ErnieTokenizer.from_pretrained(\"checkpoints/model_100\")\r\nCLIEN_CONFIG_FILE = \"./serving_client/serving_client_conf.prototxt\"\r\nPREDICT_SERVERS = [\"127.0.0.1:9292\"]\r\nMAX_SEQ_LENGTH = 128\r\n\r\nCLIENT = Client()\r\nCLIENT.load_client_config(CLIEN_CONFIG_FILE)\r\nCLIENT.connect(PREDICT_SERVERS)\r\n\r\ninput_ids, token_type_ids = batchify_fn(batch)\r\nfetch_map = CLIENT.predict(\r\n  feed={\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\r\n  fetch=[\"save_infer_model/scale_0.tmp_1\"],\r\n  batch=True\r\n)\r\n```\r\n\r\nService端的就沒有代碼了，因爲是直接使用`paddle_serving_server.serve`啓動的\r\n\r\n-------------------------\r\n\r\n想請問是我姿勢不正確還是遇到什麼問題？\r\n\r\n非常感謝！\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "NatLee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-17T08:51:02+00:00",
        "updated_at": "2024-04-16T09:06:03+00:00",
        "closed_at": "2024-04-16T09:06:03+00:00",
        "comments_count": [
            "TeslaZhao",
            "NatLee"
        ],
        "labels": [
            "显存"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1725,
        "title": "PaddleServingClientExample.java 编译错误",
        "body": "PaddleServingClientExample.java 里面使用的\r\nio.paddle.serving.client.Client 没有setIP，setPort，loadClientConfig函数\r\njar包是paddle-serving-sdk-java-0.0.1.jar，顺便问一下有没有API地址",
        "state": "closed",
        "user": "tfft2126",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-18T02:58:19+00:00",
        "updated_at": "2024-04-16T09:06:04+00:00",
        "closed_at": "2024-04-16T09:06:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "HexToString",
            "tfft2126"
        ],
        "labels": [
            "SDK"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1727,
        "title": "服务端模型预测报错AttributeError: 'paddle.fluid.core_avx.PaddleInferPredictor' object has no attribute 'rrun'",
        "body": "flask 启动的服务器，客户端post请求过来后报错了，请问这是什么原因呢？和版本有关系吗，paddle的版本是2.2.2，paddel-serving-server,paddel-serving-app是0.8.3,\r\n[2022-03-18 14:57:42,669] ERROR in app: Exception on /rec/prediction [POST]\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 356, in run\r\n    return self.get_prediction(request)\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\paddle_serving_server\\web_service.py\", line 301, in get_prediction\r\n    feed=feed, fetch=fetch, batch=is_batch)\r\n  File \"D:\\Program Files\\Python37\\lib\\site-packages\\paddle_serving_app\\local_predict.py\", line 392, in predict\r\n    self.predictor.rrun()\r\nAttributeError: 'paddle.fluid.core_avx.PaddleInferPredictor' object has no attribute 'rrun'",
        "state": "closed",
        "user": "tfft2126",
        "closed_by": "tfft2126",
        "created_at": "2022-03-18T07:13:29+00:00",
        "updated_at": "2022-03-21T09:58:58+00:00",
        "closed_at": "2022-03-21T09:58:58+00:00",
        "comments_count": [
            "TeslaZhao",
            "tfft2126"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1728,
        "title": "paddleserving 配置gpu启动却没有使用gpu",
        "body": "版本信息：\r\n    paddleserving-gpu 版本 0.7.0.post.112\r\n    cuda版本11.4\r\n问题描述：\r\n    我在使用paddleserving pipline部署时，配置文件配置使用gpu，但服务却没有使用我的gpu，于是我把配置文件改成使用cpu，服务启动资源占用情况和配置为gpu时启动一致。貌似配置文件配置使用设备不起作用？\r\n以下是我的配置：\r\n\r\n```\r\nrpc_port: 18091\r\nhttp_port: 9998\r\n\r\nworker_num: 20\r\n\r\nbuild_dag_each_worker: False\r\n\r\ndag:\r\n    is_thread_op: False\r\n    retry: 1\r\n    use_profile: False\r\n    tracer:\r\n        interval_s: 60\r\nop:\r\n    pre:\r\n        concurrency: 10\r\n        local_service_conf:\r\n            model_config: ./models/qwer/qwer__cls_1.4_serving\r\n            devices: \"0\" # \"0,1\"\r\n            client_type: local_predictor\r\n            fetch_list: [\"softmax_1.tmp_0\"] \r\n    imgdet:\r\n        concurrency: 10\r\n        local_service_conf:\r\n            model_config: ./models/qwer/qwer_imgdet_1.0_serving\r\n            devices: \"0\" # \"0,1\"\r\n            client_type: local_predictor\r\n            fetch_list: [\"multiclass_nms3_0.tmp_0\"] \r\n    det:\r\n        concurrency: 4\r\n        local_service_conf:\r\n            client_type: local_predictor\r\n            model_config: ./models/qwer/qwer_det_1.1_serving\r\n            fetch_list: [\"sigmoid_2.tmp_0\"]\r\n            devices: \"0\"\r\n            ir_optim: True\r\n    rec:\r\n        concurrency: 2\r\n        timeout: -1\r\n        retry: 1\r\n        local_service_conf:\r\n            client_type: local_predictor\r\n            model_config: ./models/qwer/qwer_rec_1.4_serving\r\n            fetch_list: [\"softmax_1.tmp_0\"]  \r\n            devices: \"0\"\r\n            ir_optim: True\r\n```\r\n在该配置下的资源占用情况如下：\r\n![gpu_status](https://user-images.githubusercontent.com/65162523/159109216-e4e7ca5e-d45c-4bb3-a691-d581409d1efa.png)\r\n![cpu1](https://user-images.githubusercontent.com/65162523/159109478-3fe92054-dbda-4078-89ec-8a978dd41508.png)\r\n",
        "state": "closed",
        "user": "Enchanted0911",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-19T05:56:10+00:00",
        "updated_at": "2024-04-16T09:06:05+00:00",
        "closed_at": "2024-04-16T09:06:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Enchanted0911",
            "TeslaZhao",
            "TeslaZhao",
            "Enchanted0911"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1726,
        "title": "关于Paddlle无法识别CUDA的问题",
        "body": "第一次使用GPU资源部署PaddleOCR，出现了一些问题，多次尝试无法解决后求助。\r\n\r\n- 宿主机是`ubuntu20.04`,只安装了`nvidia`驱动，没有安装`cuda`和`cudnn`。\r\n- 使用官方镜像：`registry.baidubce.com/paddlepaddle/serving:0.8.0-cuda11.2-cudnn8-runtime`。\r\n\r\n出现如下问题：\r\n\r\n1. 问题一（docker启动时）：\r\n![image](https://user-images.githubusercontent.com/16896367/158934432-792cd2e0-0f02-464c-a573-d097a890ee5c.png)\r\n\r\n2. 问题二（docker启动后，送入图片预测）：\r\n![image](https://user-images.githubusercontent.com/16896367/158934600-757f18fb-7645-4005-ac4c-ad3694396444.png)\r\n\r\n\r\n在容器中执行`nvidia-smi`\r\n![image](https://user-images.githubusercontent.com/16896367/158934723-2cb8ef69-0cba-4949-84c6-96979550311f.png)\r\n镜像里的安装包\r\n![image](https://user-images.githubusercontent.com/16896367/158936832-7c991190-046d-4879-8a27-959d8b3f96c3.png)\r\n\r\n\r\n部分dockerfile代码如下：\r\n```dockerfile\r\nFROM registry.baidubce.com/paddlepaddle/serving:0.8.0-cuda11.2-cudnn8-runtime as serving\r\n\r\nENV TZ=Asia/Shanghai\r\n\r\nRUN ln -s /usr/local/cuda/lib64/libcublas.so.11   /usr/lib/libcublas.so && \\\r\n    ln -s /usr/local/cuda/lib64/libcusolver.so.11   /usr/lib/libcusolver.so && \\\r\n    ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.8 /usr/lib/libcudnn.so\r\n\r\nFROM serving as prepare\r\n\r\nWORKDIR ppocr\r\n\r\nRUN wget https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar -O ch_PP-OCRv2_det_infer.tar && \\\r\n    tar -xf ch_PP-OCRv2_det_infer.tar && \\\r\n    wget https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar -O ch_PP-OCRv2_rec_infer.tar && \\\r\n    tar -xf ch_PP-OCRv2_rec_infer.tar && \\\r\n    ln -s /usr/local/bin/python3.7 /usr/local/bin/python && \\\r\n    python -m paddle_serving_client.convert --dirname ./ch_PP-OCRv2_det_infer/ \\\r\n                                         --model_filename inference.pdmodel          \\\r\n                                         --params_filename inference.pdiparams       \\\r\n                                         --serving_server ./ppocrv2_det_serving/ \\\r\n                                         --serving_client ./ppocrv2_det_client/ && \\\r\n    python -m paddle_serving_client.convert --dirname ./ch_PP-OCRv2_rec_infer/ \\\r\n                                         --model_filename inference.pdmodel          \\\r\n                                         --params_filename inference.pdiparams       \\\r\n                                         --serving_server ./ppocrv2_rec_serving/  \\\r\n                                         --serving_client ./ppocrv2_rec_client/ && \\\r\n    rm -rf *_infer.tar *_infer\r\n```\r\n启动命令尝试使用过如下几种方式，均无法解决；\r\n1. `docker run -itd --gpus all`\r\n2. `docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all`\r\n3. `nvidia-docker run --runtime=nvidia`",
        "state": "closed",
        "user": "byteszard",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-18T04:18:16+00:00",
        "updated_at": "2024-04-16T09:06:05+00:00",
        "closed_at": "2024-04-16T09:06:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ShiningZhang",
            "byteszard"
        ],
        "labels": [
            "环境问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1729,
        "title": "JAVA Client SDK使用问题。",
        "body": "使用paddleServing 0.8.2  C++  部署了一个自训练的PPyolo_mb_Small模型。\r\n使用/Serving/examples/C++/PaddleDetection/ppyolo_r50vd_dcn_1x_coco/test_client.py  修改resize尺寸后，可以正常使用\r\n![image](https://user-images.githubusercontent.com/15975613/159264817-4c1ea9cf-5270-4c80-82ce-7a7dfb808907.png)\r\n![image](https://user-images.githubusercontent.com/15975613/159265091-4e7eac9d-aec4-42cd-8370-abe41b2970e1.png)\r\n\r\n但是使用java client sdk时，遇到一些问题。由于javaSDK只提供了yolov4的模型样例，我尝试修改了feedData。\r\n\r\n代码如下\r\n![image](https://user-images.githubusercontent.com/15975613/159265568-ab14774f-e727-4960-9c5f-0f476aad96d0.png)\r\n\r\nserving_client_conf.prototxt内容\r\n![image](https://user-images.githubusercontent.com/15975613/159265647-39769abb-7ed0-45b9-a6f7-8568202f44f0.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "yqsoooo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-21T12:58:28+00:00",
        "updated_at": "2024-03-05T06:50:57+00:00",
        "closed_at": "2024-03-05T06:50:57+00:00",
        "comments_count": [
            "yqsoooo",
            "HexToString",
            "HexToString",
            "yqsoooo",
            "yqsoooo",
            "yqsoooo",
            "zjyhll"
        ],
        "labels": [
            "多语言client"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1730,
        "title": "Exception: Op type general_reader is not supported right now",
        "body": "在0.83版serving的安装目录paddle-serving-server/dag.py的22行定义的self.op_list = [\r\n            \"GeneralInferOp\",\r\n            \"GeneralReaderOp\",\r\n            \"GeneralResponseOp\",\r\n            \"GeneralTextReaderOp\",\r\n            \"GeneralTextResponseOp\",\r\n            \"GeneralSingleKVOp\",\r\n            \"GeneralDistKVInferOp\",\r\n            \"GeneralDistKVOp\",\r\n            \"GeneralCopyOp\",\r\n            \"GeneralDetectionOp\",\r\n        ]\r\n以前版本定义的是字典格式，由于版本升级，的安装目录paddle-serving-server/pipeline/local_service_handler.py的285行-301行中的函数调用op_maker.create（）仍按照字典方式调用引起错误。\r\n修改方法：\r\nread_op = op_maker.create('GeneralReaderOp')\r\ngeneral_infer_op = op_maker.create('GeneralInferOp')\r\ngeneral_response_op = op_maker.create('GeneralResponseOp')",
        "state": "closed",
        "user": "taoge2222",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-22T08:26:35+00:00",
        "updated_at": "2024-05-21T06:41:05+00:00",
        "closed_at": "2024-05-21T06:41:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "huaxiangsiyi"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1737,
        "title": "使用imagenet ResNet50示例启动cpu服务，执行后的结果感觉不对，麻烦确认下",
        "body": "按 怎样保存用于Paddle Serving的模型 中的示例执行命令，得到了结果，结果如下；请问这个结果是正确的吗，\r\n\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0328 08:56:49.175515   275 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0328 08:56:49.247929   275 general_model.cpp:490] [client]logid=0,client_cost=60.714ms,server_cost=49.595ms.\r\n[3.24698885e-05 7.93178042e-05 9.62620034e-05 4.12459849e-05\r\n 3.98674856e-05 8.67820490e-05 1.45659278e-05 5.74468540e-05\r\n 4.43146346e-05 4.00807403e-05 2.56470739e-05 3.50113587e-05\r\n 3.53085452e-05 4.06770669e-05 2.91560500e-05 3.42102394e-05\r\n 5.38443346e-05 4.68107173e-05 5.96377758e-05 5.64335460e-05\r\n 8.08987897e-05 1.92742082e-05 3.63689214e-05 2.84555917e-05\r\n 5.13064042e-05 2.72626621e-05 4.92231520e-05 2.66120041e-05\r\n 3.48813483e-05 2.34129366e-05 5.40184374e-05 5.66993367e-05\r\n 4.37215676e-05 4.41749689e-05 2.89865129e-05 5.63947578e-05\r\n 6.02711189e-05 3.28871174e-05 7.27602892e-05 8.38519845e-05\r\n 1.24296144e-04 8.01473798e-05 5.26922231e-05 1.21218647e-04\r\n 1.08091437e-04 4.57358547e-05 7.84685180e-05 9.58213568e-05\r\n 7.36703587e-05 4.96563589e-05 3.31699739e-05 3.75911623e-05\r\n 1.11771042e-04 1.78493312e-04 4.23643141e-05 1.66604601e-04\r\n 6.59314537e-05 5.06811593e-05 7.17213843e-05 7.06572027e-05\r\n 1.96380934e-05 2.45120136e-05 2.94239817e-05 2.24434316e-05\r\n 7.50213512e-05 4.11673536e-05 4.78772199e-05 3.11638432e-05\r\n 1.94276545e-05 3.51666640e-05 8.97757855e-05 8.15219319e-05\r\n 5.74754049e-05 7.64334909e-05 8.79074723e-05 5.23645067e-05\r\n 2.87301809e-05 2.65925719e-05 5.35598920e-05 6.81072415e-05\r\n 1.63648219e-04 7.98451947e-05 4.95433087e-05 2.01555722e-05\r\n 6.18846825e-05 8.07198448e-05 9.13813346e-05 3.54274416e-05\r\n 6.61482991e-05 1.36813032e-04 4.84380253e-05 2.27950386e-05\r\n 3.00347801e-05 7.32399567e-05 9.97906900e-05 4.98355948e-05\r\n 2.37945242e-05 4.00249337e-05 7.14305206e-05 3.28136193e-05\r\n 2.78614680e-05 2.77198887e-05 2.79136748e-05 3.93758928e-05\r\n 2.44720723e-05 1.10301378e-04 5.42582056e-05 7.56889640e-05\r\n 9.04967310e-05 5.65815826e-05 2.52104255e-05 2.88218980e-05\r\n 6.41140505e-05 1.67774124e-04 1.12577538e-04 1.12873066e-04\r\n 2.30227106e-05 1.21020777e-04 4.02082660e-05 2.39977544e-05\r\n 2.98675695e-05 3.66755994e-05 1.73700209e-05 1.00396792e-05\r\n 4.71752901e-05 1.30123981e-05 2.33942119e-05 3.80639576e-05\r\n 3.48973190e-05 5.55280640e-05 5.16448345e-05 2.54011029e-05\r\n 7.73844076e-05 4.01225770e-05 3.25131805e-05 3.42333115e-05\r\n 4.01167235e-05 5.45180155e-05 3.22734522e-05 2.85320057e-05\r\n 1.94241347e-05 4.83927761e-05 3.24661742e-05 2.40462759e-05\r\n 4.62969983e-05 3.86779429e-05 3.04979840e-05 3.99543787e-05\r\n 8.59145221e-05 1.05359279e-04 4.60501869e-05 5.35888103e-05\r\n 1.03593360e-04 6.26425826e-05 4.09713612e-05 6.58738936e-05\r\n 6.55334588e-05 4.54341971e-05 6.59879443e-05 1.23569509e-04\r\n 4.78122456e-05 2.90998814e-05 4.17871415e-05 4.68209837e-05\r\n 5.77539249e-05 4.19637363e-05 4.80731323e-05 3.75925956e-05\r\n 6.30890572e-05 2.87887924e-05 2.78384414e-05 6.79604709e-05\r\n 1.60415257e-05 2.43240665e-05 1.26977175e-04 6.02012624e-05\r\n 6.48867572e-05 3.20998988e-05 6.81021120e-05 7.44355420e-05\r\n 4.31067820e-05 4.90873936e-05 7.50276449e-05 1.02071957e-04\r\n 4.47555321e-05 6.20455394e-05 6.34421958e-05 7.82877833e-05\r\n 5.36391235e-05 2.26627726e-05 5.28806995e-05 7.51851549e-05\r\n 4.33401765e-05 2.67598425e-05 7.10583918e-05 4.49098807e-05\r\n 4.91464634e-05 8.56971819e-05 1.16150914e-04 4.15796749e-05\r\n 1.07534783e-04 8.32118239e-05 1.15449322e-04 4.00312711e-05\r\n 6.49769136e-05 3.15737379e-05 3.48882349e-05 8.36995532e-05\r\n 4.27519954e-05 6.07412803e-05 5.88390139e-05 9.79861725e-05\r\n 5.94256089e-05 6.89421431e-05 7.83289215e-05 9.93678113e-05\r\n 2.75663788e-05 5.05700627e-05 4.17175797e-05 5.66420495e-05\r\n 4.71741223e-05 3.85943749e-05 1.35846378e-04 4.84019620e-05\r\n 4.23768361e-05 3.69171394e-05 6.42262239e-05 2.85410406e-05\r\n 4.12279733e-05 6.74218318e-05 3.74157316e-05 3.63162035e-05\r\n 6.20183200e-05 7.11865214e-05 5.80501255e-05 6.89933149e-05\r\n 1.00977297e-04 3.63939034e-05 6.30113063e-05 3.85642415e-05\r\n 1.06501022e-04 4.18422533e-05 4.28144594e-05 4.38887109e-05\r\n 6.71912640e-05 2.97641254e-05 6.87726570e-05 3.66335480e-05\r\n 9.02387110e-05 5.36651678e-05 4.62255302e-05 4.18272502e-05\r\n 5.39384419e-05 5.14465828e-05 7.64902943e-05 1.67499777e-04\r\n 3.98599950e-05 5.54094077e-05 3.54031217e-05 4.56423477e-05\r\n 5.83442234e-05 5.07299483e-05 4.65042285e-05 6.02161344e-05\r\n 2.03318159e-05 5.64923466e-05 4.99474554e-05 2.47089629e-04\r\n 8.30284771e-05 4.37061790e-05 4.99479283e-05 5.00461028e-05\r\n 2.45161282e-05 2.61745263e-05 4.37298622e-05 5.57940039e-05\r\n 3.90687601e-05 9.06329369e-05 4.67570426e-05 5.59772889e-05\r\n 6.24080058e-05 1.29204898e-04 7.09154192e-05 2.25536951e-05\r\n 3.75988311e-05 6.29310816e-05 6.65156476e-05 4.29329521e-05\r\n 8.59497522e-05 7.49318351e-05 6.09117669e-05 3.08329836e-05\r\n 4.35965740e-05 2.23138650e-05 1.04076906e-04 4.70643899e-05\r\n 7.49114624e-05 3.47841815e-05 6.10898423e-05 7.17542207e-05\r\n 7.34060304e-05 3.50133632e-04 7.25263308e-05 1.71887790e-04\r\n 1.29806998e-04 6.84936313e-05 1.22053265e-04 3.24677851e-04\r\n 9.30077513e-04 1.18131517e-03 1.45055028e-03 2.18034213e-04\r\n 1.56797090e-04 6.33738091e-05 2.77654472e-05 7.10533131e-05\r\n 4.16162911e-05 1.85510944e-04 9.28549998e-05 7.07097788e-05\r\n 6.25957109e-05 1.52952343e-04 3.17125086e-04 1.63543678e-04\r\n 2.44913215e-04 2.87183066e-04 4.37478622e-04 7.34211571e-05\r\n 8.77844941e-05 1.24837316e-05 1.47921688e-04 7.15832794e-05\r\n 3.15944926e-05 3.69480258e-05 6.41481165e-05 9.43721388e-05\r\n 1.70245781e-04 9.38658122e-05 3.16344995e-05 7.06283681e-05\r\n 7.05518250e-05 6.67763597e-05 7.63255375e-05 5.51131743e-05\r\n 8.07934703e-05 2.58577438e-05 2.71201152e-05 3.39831713e-05\r\n 5.75442937e-05 1.16451491e-04 5.50997211e-05 5.73270590e-05\r\n 3.46957895e-05 3.69458430e-05 5.44047398e-05 4.82902906e-05\r\n 2.13938110e-05 3.18769562e-05 2.07224257e-05 2.03733325e-05\r\n 2.49249624e-05 6.23378146e-05 4.56032321e-05 6.57735145e-05\r\n 4.74390836e-05 8.82138411e-05 4.17086667e-05 5.55666265e-05\r\n 5.81202003e-05 5.56278683e-05 9.74349896e-05 4.94705018e-05\r\n 2.84637881e-05 3.17437771e-05 5.32805061e-05 9.46143628e-05\r\n 4.44519865e-05 3.59480218e-05 2.63182501e-05 8.13981824e-05\r\n 6.64480613e-05 2.70202054e-05 4.95499226e-05 5.90926902e-05\r\n 8.14832310e-05 4.92627405e-05 4.68851031e-05 5.09179117e-05\r\n 2.48370961e-05 2.52420614e-05 2.90256285e-05 2.39793844e-05\r\n 3.41525083e-05 2.96306716e-05 3.30576113e-05 8.04848532e-05\r\n 5.03645970e-05 6.75463598e-05 5.90382770e-05 4.86968020e-05\r\n 1.17527459e-04 3.73758958e-05 6.37511475e-05 6.05836904e-05\r\n 3.42056082e-05 6.00539533e-05 6.09216440e-05 2.54713050e-05\r\n 6.39668651e-05 3.55988450e-05 9.15502096e-05 3.18431412e-05\r\n 5.79212756e-05 1.76483209e-05 8.53508682e-05 2.43517097e-05\r\n 4.17687734e-05 4.51010928e-05 5.18607158e-05 4.43864119e-05\r\n 2.71090485e-05 4.27387022e-05 1.67791968e-05 4.04302482e-05\r\n 2.43617196e-05 3.63776999e-05 4.92111831e-05 6.23574379e-05\r\n 3.48686444e-05 6.29861388e-05 3.68348083e-05 4.13281232e-05\r\n 5.30328689e-05 1.04351806e-04 1.21111865e-04 2.99732019e-05\r\n 1.98229827e-05 2.64495939e-05 6.09882081e-05 3.16963160e-05\r\n 1.19547658e-04 7.37918599e-05 3.47008208e-05 5.26092226e-05\r\n 6.63609826e-05 7.10354288e-05 1.29212422e-04 4.21974582e-05\r\n 9.44373169e-05 3.59850346e-05 3.75521413e-05 3.28623828e-05\r\n 2.54598534e-04 4.88027072e-05 4.98747722e-05 3.70334310e-05\r\n 5.10361933e-05 1.50130363e-04 1.07541964e-04 4.74935368e-05\r\n 4.03538616e-05 2.47068547e-05 1.34145594e-04 6.19990387e-05\r\n 7.30906322e-05 4.31153785e-05 2.18266705e-05 1.58332859e-05\r\n 3.51308263e-05 1.88411195e-05 3.91372632e-05 4.49556901e-05\r\n 4.02528858e-05 4.11257170e-05 2.66694478e-05 4.56868547e-05\r\n 5.54161707e-05 8.16745378e-05 5.78520485e-05 3.79472112e-05\r\n 6.29092392e-05 4.18859709e-05 2.67846572e-05 2.77266554e-05\r\n 8.00418929e-05 6.01456595e-05 4.02958685e-05 6.27191621e-05\r\n 4.82072392e-05 1.25398190e-04 4.55766676e-05 4.09312961e-05\r\n 4.11183828e-05 6.91551031e-05 5.93010045e-05 3.77976467e-05\r\n 2.30282021e-05 6.36055775e-05 7.87112804e-05 2.95740974e-05\r\n 7.09348969e-05 3.04003879e-05 2.57354732e-05 3.13583987e-05\r\n 5.14747007e-05 4.16894982e-05 7.88783218e-05 3.90385176e-05\r\n 1.25024671e-04 3.91255853e-05 3.37608944e-05 1.78512637e-05\r\n 3.47483110e-05 1.30160537e-04 1.14669914e-04 4.75985071e-05\r\n 6.59073194e-05 5.72106728e-05 3.15150464e-05 7.19552045e-05\r\n 4.41248230e-05 5.54331928e-05 4.13056623e-05 2.98652903e-05\r\n 2.10981962e-05 5.03462543e-05 3.49047077e-05 4.37187300e-05\r\n 4.65055127e-05 1.15799427e-04 5.41915506e-05 3.34709912e-05\r\n 3.53786199e-05 1.42480290e-04 4.40532567e-05 4.29452775e-05\r\n 3.80622841e-05 1.06390777e-04 3.37229249e-05 9.00511222e-05\r\n 1.00006095e-04 2.67414216e-05 3.48397589e-05 1.86702127e-05\r\n 4.78128386e-05 4.84845659e-05 3.47940368e-05 4.71775420e-05\r\n 6.46499393e-05 3.40940278e-05 6.74092298e-05 5.55898405e-05\r\n 3.17587364e-05 8.69064461e-05 2.20635156e-05 1.61761549e-04\r\n 5.63477697e-05 9.34028940e-05 1.19326352e-04 3.92500333e-05\r\n 3.07903720e-05 5.93489276e-05 7.43963683e-05 4.17142364e-05\r\n 4.33768109e-05 2.42521455e-05 4.76334280e-05 1.01043668e-04\r\n 1.22796455e-05 3.88822409e-05 2.87018920e-05 4.43931422e-05\r\n 6.45289620e-05 6.28536436e-05 1.75435169e-04 7.43484197e-05\r\n 7.90497215e-05 1.01459656e-04 1.18819436e-04 1.85181725e-05\r\n 1.24165468e-04 1.03933278e-04 8.69836367e-05 2.30462610e-05\r\n 1.02608356e-04 3.79691483e-05 2.15911732e-05 5.90983254e-05\r\n 8.34662351e-05 3.74288284e-05 1.05429135e-04 5.67742245e-05\r\n 6.71921662e-05 1.06720836e-04 3.00887659e-05 3.90880268e-05\r\n 3.73312578e-05 3.82247090e-05 3.15153156e-05 4.60267402e-05\r\n 8.62758679e-05 1.00846511e-04 1.17138727e-04 9.34031632e-05\r\n 4.98250920e-05 8.27745316e-05 1.00982405e-04 3.42905259e-05\r\n 1.12785907e-04 3.73787807e-05 7.44065183e-05 4.50920597e-05\r\n 4.35017137e-05 6.98529329e-05 2.32243347e-05 1.03187820e-04\r\n 1.82097807e-04 1.30264845e-04 1.10692970e-04 2.07989688e-05\r\n 3.80820748e-05 4.84270313e-05 7.45634316e-05 1.02509570e-04\r\n 2.96623675e-05 2.53203707e-05 3.51807867e-05 3.41253835e-05\r\n 3.91159592e-05 4.82446267e-05 7.98111578e-05 1.52797962e-04\r\n 1.98860464e-04 4.04794773e-05 2.63113980e-05 6.67209824e-05\r\n 3.28911628e-05 7.74292857e-05 5.57682033e-05 4.86770659e-05\r\n 8.45686664e-05 1.02788174e-04 5.65814735e-05 3.95413554e-05\r\n 9.38133671e-05 7.51789848e-05 1.75316327e-05 2.77840154e-05\r\n 7.50846957e-05 5.37181113e-05 9.49826790e-05 3.48706708e-05\r\n 4.76802888e-05 7.22974801e-05 3.29730101e-05 7.00873570e-05\r\n 4.97701440e-05 2.81524917e-05 7.83408032e-05 7.64289653e-05\r\n 1.33154084e-04 2.23519273e-05 4.77707254e-05 7.17533985e-05\r\n 1.08415712e-04 3.02072203e-05 7.28918312e-05 6.12328658e-05\r\n 6.02530745e-05 4.20485412e-05 3.56859309e-05 3.86055690e-05\r\n 2.56172025e-05 7.19615200e-05 3.84582490e-05 6.03692024e-05\r\n 6.35462857e-05 7.69500621e-05 3.99253549e-05 2.44900966e-05\r\n 4.89452395e-05 1.35615715e-04 5.91547105e-05 3.84171544e-05\r\n 4.03022859e-05 7.85306547e-05 4.11010951e-05 3.10607320e-05\r\n 3.85992716e-05 2.87951079e-05 1.12240072e-04 4.95620225e-05\r\n 8.99500155e-05 4.19344506e-05 3.69221380e-05 3.13608216e-05\r\n 4.54255751e-05 2.92207424e-05 2.68103668e-05 4.87487014e-05\r\n 6.62245511e-05 4.09523782e-05 4.55349182e-05 5.16449327e-05\r\n 9.98645701e-05 4.91594437e-05 1.09598215e-04 9.02171960e-05\r\n 9.04645494e-05 4.04334460e-05 4.69499391e-05 6.99391967e-05\r\n 8.23004375e-05 4.11356814e-05 3.27949747e-05 1.32239147e-04\r\n 2.87073810e-04 2.59395201e-05 4.14389069e-05 4.24107602e-05\r\n 9.03058608e-05 5.76465645e-05 6.58449353e-05 5.11699240e-04\r\n 5.86701899e-05 3.38779100e-05 2.06262685e-05 3.78569930e-05\r\n 8.34902748e-05 7.96092572e-05 4.87154721e-05 1.77000165e-05\r\n 5.30272046e-05 6.86679341e-05 3.50871669e-05 1.49375655e-05\r\n 3.14122153e-05 5.86274582e-05 3.49831389e-04 5.41517184e-05\r\n 7.46642618e-05 8.23685987e-05 2.44450148e-05 6.65467305e-05\r\n 2.00652030e-05 2.98768846e-05 3.29640170e-05 2.18276891e-05\r\n 2.43448358e-05 6.33366435e-05 1.95022803e-05 3.77459583e-05\r\n 1.17966978e-04 3.25335568e-05 7.06047285e-05 5.06145516e-05\r\n 3.13889177e-05 7.95936212e-05 4.26858714e-05 7.69538092e-05\r\n 2.20421261e-05 1.37093317e-04 7.56645750e-05 6.26438414e-05\r\n 1.86934085e-05 5.23319541e-05 9.90531680e-06 4.42695637e-05\r\n 1.74555917e-05 6.04979541e-05 1.15835312e-04 5.39805369e-05\r\n 4.44236757e-05 6.54761097e-05 5.89498304e-05 6.38236597e-05\r\n 2.20463098e-05 1.38442829e-05 1.89793700e-05 9.30992901e-05\r\n 5.47559321e-05 4.21828154e-05 1.24905870e-04 9.96458548e-05\r\n 3.54362237e-05 2.96236085e-05 1.44791593e-05 6.09351227e-05\r\n 4.71783933e-05 7.67060046e-05 5.06982215e-05 2.10276303e-05\r\n 8.39630520e-05 7.68681421e-05 5.32231206e-05 7.70888146e-05\r\n 4.41207412e-05 5.14763233e-05 5.50542863e-05 5.27606608e-05\r\n 2.89072868e-05 3.09122697e-05 4.52013337e-05 7.94683947e-05\r\n 1.08023836e-04 2.17278539e-05 3.31693700e-05 9.59651152e-05\r\n 3.38231548e-05 6.06745889e-05 6.97081923e-05 8.37877160e-05\r\n 1.30372107e-04 6.79520526e-05 5.46720257e-05 5.64009424e-05\r\n 1.98839698e-05 6.16257821e-05 2.13934123e-04 4.79545379e-05\r\n 4.07528642e-05 4.55529407e-05 6.82892860e-05 5.54103572e-05\r\n 9.95699065e-06 1.07385778e-04 4.52565037e-05 5.43049537e-05\r\n 4.97699075e-05 2.72433754e-05 3.81183745e-05 2.32814145e-05\r\n 6.66911510e-05 4.43411409e-05 4.41790107e-05 2.20749542e-04\r\n 6.81317324e-05 8.43808739e-05 2.18740883e-04 1.49821508e-05\r\n 1.09996741e-04 3.85499770e-05 2.09415575e-05 5.65522896e-05\r\n 1.14702067e-04 3.04910063e-05 2.31663780e-05 4.43025492e-05\r\n 6.70707814e-05 4.84096236e-05 5.53743848e-05 1.30335306e-04\r\n 7.73757711e-05 3.56241799e-05 3.93316441e-05 8.89291987e-05\r\n 4.41248230e-05 4.88739242e-05 1.24593091e-04 2.72759244e-05\r\n 5.21678157e-05 1.24478116e-04 7.11878165e-05 4.96793800e-05\r\n 5.25605792e-05 3.16482583e-05 6.55303375e-05 5.29886820e-05\r\n 2.10158414e-05 1.26085733e-05 5.36719235e-05 6.75502961e-05\r\n 1.28049491e-04 3.02378867e-05 1.04079678e-04 4.56646376e-05\r\n 4.03329686e-05 1.54009376e-05 5.22995251e-05 3.41964733e-05\r\n 3.17876766e-05 5.70985503e-05 5.01368704e-05 2.79067695e-04\r\n 7.31719556e-05 8.64125614e-05 6.93945185e-05 7.03066034e-05\r\n 4.01795551e-05 6.13056691e-05 5.51183766e-05 3.04967052e-05\r\n 3.29937357e-05 6.12372969e-05 7.05811035e-05 2.40082627e-05\r\n 3.05798822e-05 5.23090530e-05 3.40432453e-05 2.68784406e-05\r\n 2.71457830e-05 2.83264671e-05 2.00382783e-05 3.37000674e-05\r\n 5.33838065e-05 4.15841168e-05 2.16243297e-05 3.54584699e-05\r\n 2.65461258e-05 1.04858475e-04 9.67939995e-05 1.91662639e-05\r\n 1.31289649e-04 7.93386789e-05 4.90267630e-05 4.17282026e-05\r\n 9.05011329e-05 2.65043618e-05 6.10635689e-05 8.14767773e-05\r\n 5.42179660e-05 5.32037848e-05 2.35966345e-05 6.43457097e-05\r\n 5.26646436e-05 1.20829915e-04 3.57951394e-05 5.47114068e-05\r\n 2.83948957e-05 5.92582655e-05 9.93188223e-05 3.83871266e-05\r\n 5.17880144e-05 1.76156318e-05 9.09835871e-05 2.68682161e-05\r\n 8.08964760e-05 3.73792791e-05 4.69392398e-05 7.17280855e-05\r\n 8.13449442e-05 4.86335448e-05 6.11506402e-05 5.53738028e-05\r\n 3.60721024e-05 5.31666556e-05 1.85797253e-04 5.41459340e-05\r\n 8.86813941e-05 3.85400723e-04 1.70980900e-04 1.25850333e-04\r\n 3.85012208e-05 1.21672761e-04 6.34997632e-05 7.90962513e-05\r\n 5.18936140e-05 7.60269540e-05 2.00775015e-04 4.64980185e-05\r\n 3.00514566e-05 2.41865491e-05 4.52518834e-05 4.76954338e-05\r\n 3.84142259e-05 1.45852764e-05 2.07627018e-05 6.61678641e-05\r\n 1.04175902e-04 6.52100789e-05 1.27029140e-04 1.39436277e-04\r\n 5.96854516e-05 2.64704122e-05 8.43198213e-05 1.12228510e-04\r\n 3.11265612e-05 1.11376386e-04 1.11630849e-04 6.66707347e-05\r\n 6.32972151e-05 6.98203003e-05 1.30234781e-04 5.83634792e-05\r\n 4.69373132e-04 9.33418214e-01 1.01784513e-04 1.40044358e-04\r\n 1.02814738e-04 1.73654917e-04 1.41994187e-05 5.31265687e-05\r\n 5.08579797e-05 5.30882244e-05 6.81649399e-05 2.21717753e-04\r\n 3.90415698e-05 2.80919885e-05 1.02586731e-04 1.21871119e-04]\r\n```\r\n\r\n我用paddlex导出的图片分类模型，在cpu环境下转成serving模型后启动服务，获取的结果如下，但这个模型应该返回“bocai”，却没有返回，为什么呢，跟我是cpu环境有关吗\r\n\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0328 09:09:45.377297   482 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0328 09:09:45.464089   482 general_model.cpp:490] [client]logid=0,client_cost=77.353ms,server_cost=66.639ms.\r\n[0.18139979 0.11694741 0.6913014  0.00147739 0.00129413 0.00757991]\r\n```",
        "state": "closed",
        "user": "zhang52104",
        "closed_by": "zhang52104",
        "created_at": "2022-03-29T03:35:20+00:00",
        "updated_at": "2022-03-29T03:59:12+00:00",
        "closed_at": "2022-03-29T03:59:12+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1731,
        "title": "pdserving部署如何返回识别文本坐标信息",
        "body": "我看到[另外一个issue](https://github.com/PaddlePaddle/PaddleOCR/issues/4505)有人问，回复是在`postprocess`里面147行`res = {\"res\": str(res_list)}`这里修改返回。  \r\n\r\n但是我看`fetch_data`里面存的是识别文本内容，请问下这里怎么拿到坐标信息呢？ \r\n\r\n![image](https://user-images.githubusercontent.com/6022635/159438437-2a09e520-dd60-4974-bdd1-ecd02ce7c406.png)\r\n\r\n",
        "state": "closed",
        "user": "luokuncool",
        "closed_by": "luokuncool",
        "created_at": "2022-03-22T08:32:53+00:00",
        "updated_at": "2022-03-22T09:17:56+00:00",
        "closed_at": "2022-03-22T09:17:55+00:00",
        "comments_count": [
            "github-actions[bot]",
            "luokuncool"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1736,
        "title": "使用imagenet ResNet50示例启动cpu服务，执行后的结果感觉不对，麻烦确认下",
        "body": "按 怎样保存用于Paddle Serving的模型 中的示例执行命令，得到了结果，结果如下；请问这个结果是正确的吗，\r\n\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0328 08:56:49.175515   275 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0328 08:56:49.247929   275 general_model.cpp:490] [client]logid=0,client_cost=60.714ms,server_cost=49.595ms.\r\n[3.24698885e-05 7.93178042e-05 9.62620034e-05 4.12459849e-05\r\n 3.98674856e-05 8.67820490e-05 1.45659278e-05 5.74468540e-05\r\n 4.43146346e-05 4.00807403e-05 2.56470739e-05 3.50113587e-05\r\n 3.53085452e-05 4.06770669e-05 2.91560500e-05 3.42102394e-05\r\n 5.38443346e-05 4.68107173e-05 5.96377758e-05 5.64335460e-05\r\n 8.08987897e-05 1.92742082e-05 3.63689214e-05 2.84555917e-05\r\n 5.13064042e-05 2.72626621e-05 4.92231520e-05 2.66120041e-05\r\n 3.48813483e-05 2.34129366e-05 5.40184374e-05 5.66993367e-05\r\n 4.37215676e-05 4.41749689e-05 2.89865129e-05 5.63947578e-05\r\n 6.02711189e-05 3.28871174e-05 7.27602892e-05 8.38519845e-05\r\n 1.24296144e-04 8.01473798e-05 5.26922231e-05 1.21218647e-04\r\n 1.08091437e-04 4.57358547e-05 7.84685180e-05 9.58213568e-05\r\n 7.36703587e-05 4.96563589e-05 3.31699739e-05 3.75911623e-05\r\n 1.11771042e-04 1.78493312e-04 4.23643141e-05 1.66604601e-04\r\n 6.59314537e-05 5.06811593e-05 7.17213843e-05 7.06572027e-05\r\n 1.96380934e-05 2.45120136e-05 2.94239817e-05 2.24434316e-05\r\n 7.50213512e-05 4.11673536e-05 4.78772199e-05 3.11638432e-05\r\n 1.94276545e-05 3.51666640e-05 8.97757855e-05 8.15219319e-05\r\n 5.74754049e-05 7.64334909e-05 8.79074723e-05 5.23645067e-05\r\n 2.87301809e-05 2.65925719e-05 5.35598920e-05 6.81072415e-05\r\n 1.63648219e-04 7.98451947e-05 4.95433087e-05 2.01555722e-05\r\n 6.18846825e-05 8.07198448e-05 9.13813346e-05 3.54274416e-05\r\n 6.61482991e-05 1.36813032e-04 4.84380253e-05 2.27950386e-05\r\n 3.00347801e-05 7.32399567e-05 9.97906900e-05 4.98355948e-05\r\n 2.37945242e-05 4.00249337e-05 7.14305206e-05 3.28136193e-05\r\n 2.78614680e-05 2.77198887e-05 2.79136748e-05 3.93758928e-05\r\n 2.44720723e-05 1.10301378e-04 5.42582056e-05 7.56889640e-05\r\n 9.04967310e-05 5.65815826e-05 2.52104255e-05 2.88218980e-05\r\n 6.41140505e-05 1.67774124e-04 1.12577538e-04 1.12873066e-04\r\n 2.30227106e-05 1.21020777e-04 4.02082660e-05 2.39977544e-05\r\n 2.98675695e-05 3.66755994e-05 1.73700209e-05 1.00396792e-05\r\n 4.71752901e-05 1.30123981e-05 2.33942119e-05 3.80639576e-05\r\n 3.48973190e-05 5.55280640e-05 5.16448345e-05 2.54011029e-05\r\n 7.73844076e-05 4.01225770e-05 3.25131805e-05 3.42333115e-05\r\n 4.01167235e-05 5.45180155e-05 3.22734522e-05 2.85320057e-05\r\n 1.94241347e-05 4.83927761e-05 3.24661742e-05 2.40462759e-05\r\n 4.62969983e-05 3.86779429e-05 3.04979840e-05 3.99543787e-05\r\n 8.59145221e-05 1.05359279e-04 4.60501869e-05 5.35888103e-05\r\n 1.03593360e-04 6.26425826e-05 4.09713612e-05 6.58738936e-05\r\n 6.55334588e-05 4.54341971e-05 6.59879443e-05 1.23569509e-04\r\n 4.78122456e-05 2.90998814e-05 4.17871415e-05 4.68209837e-05\r\n 5.77539249e-05 4.19637363e-05 4.80731323e-05 3.75925956e-05\r\n 6.30890572e-05 2.87887924e-05 2.78384414e-05 6.79604709e-05\r\n 1.60415257e-05 2.43240665e-05 1.26977175e-04 6.02012624e-05\r\n 6.48867572e-05 3.20998988e-05 6.81021120e-05 7.44355420e-05\r\n 4.31067820e-05 4.90873936e-05 7.50276449e-05 1.02071957e-04\r\n 4.47555321e-05 6.20455394e-05 6.34421958e-05 7.82877833e-05\r\n 5.36391235e-05 2.26627726e-05 5.28806995e-05 7.51851549e-05\r\n 4.33401765e-05 2.67598425e-05 7.10583918e-05 4.49098807e-05\r\n 4.91464634e-05 8.56971819e-05 1.16150914e-04 4.15796749e-05\r\n 1.07534783e-04 8.32118239e-05 1.15449322e-04 4.00312711e-05\r\n 6.49769136e-05 3.15737379e-05 3.48882349e-05 8.36995532e-05\r\n 4.27519954e-05 6.07412803e-05 5.88390139e-05 9.79861725e-05\r\n 5.94256089e-05 6.89421431e-05 7.83289215e-05 9.93678113e-05\r\n 2.75663788e-05 5.05700627e-05 4.17175797e-05 5.66420495e-05\r\n 4.71741223e-05 3.85943749e-05 1.35846378e-04 4.84019620e-05\r\n 4.23768361e-05 3.69171394e-05 6.42262239e-05 2.85410406e-05\r\n 4.12279733e-05 6.74218318e-05 3.74157316e-05 3.63162035e-05\r\n 6.20183200e-05 7.11865214e-05 5.80501255e-05 6.89933149e-05\r\n 1.00977297e-04 3.63939034e-05 6.30113063e-05 3.85642415e-05\r\n 1.06501022e-04 4.18422533e-05 4.28144594e-05 4.38887109e-05\r\n 6.71912640e-05 2.97641254e-05 6.87726570e-05 3.66335480e-05\r\n 9.02387110e-05 5.36651678e-05 4.62255302e-05 4.18272502e-05\r\n 5.39384419e-05 5.14465828e-05 7.64902943e-05 1.67499777e-04\r\n 3.98599950e-05 5.54094077e-05 3.54031217e-05 4.56423477e-05\r\n 5.83442234e-05 5.07299483e-05 4.65042285e-05 6.02161344e-05\r\n 2.03318159e-05 5.64923466e-05 4.99474554e-05 2.47089629e-04\r\n 8.30284771e-05 4.37061790e-05 4.99479283e-05 5.00461028e-05\r\n 2.45161282e-05 2.61745263e-05 4.37298622e-05 5.57940039e-05\r\n 3.90687601e-05 9.06329369e-05 4.67570426e-05 5.59772889e-05\r\n 6.24080058e-05 1.29204898e-04 7.09154192e-05 2.25536951e-05\r\n 3.75988311e-05 6.29310816e-05 6.65156476e-05 4.29329521e-05\r\n 8.59497522e-05 7.49318351e-05 6.09117669e-05 3.08329836e-05\r\n 4.35965740e-05 2.23138650e-05 1.04076906e-04 4.70643899e-05\r\n 7.49114624e-05 3.47841815e-05 6.10898423e-05 7.17542207e-05\r\n 7.34060304e-05 3.50133632e-04 7.25263308e-05 1.71887790e-04\r\n 1.29806998e-04 6.84936313e-05 1.22053265e-04 3.24677851e-04\r\n 9.30077513e-04 1.18131517e-03 1.45055028e-03 2.18034213e-04\r\n 1.56797090e-04 6.33738091e-05 2.77654472e-05 7.10533131e-05\r\n 4.16162911e-05 1.85510944e-04 9.28549998e-05 7.07097788e-05\r\n 6.25957109e-05 1.52952343e-04 3.17125086e-04 1.63543678e-04\r\n 2.44913215e-04 2.87183066e-04 4.37478622e-04 7.34211571e-05\r\n 8.77844941e-05 1.24837316e-05 1.47921688e-04 7.15832794e-05\r\n 3.15944926e-05 3.69480258e-05 6.41481165e-05 9.43721388e-05\r\n 1.70245781e-04 9.38658122e-05 3.16344995e-05 7.06283681e-05\r\n 7.05518250e-05 6.67763597e-05 7.63255375e-05 5.51131743e-05\r\n 8.07934703e-05 2.58577438e-05 2.71201152e-05 3.39831713e-05\r\n 5.75442937e-05 1.16451491e-04 5.50997211e-05 5.73270590e-05\r\n 3.46957895e-05 3.69458430e-05 5.44047398e-05 4.82902906e-05\r\n 2.13938110e-05 3.18769562e-05 2.07224257e-05 2.03733325e-05\r\n 2.49249624e-05 6.23378146e-05 4.56032321e-05 6.57735145e-05\r\n 4.74390836e-05 8.82138411e-05 4.17086667e-05 5.55666265e-05\r\n 5.81202003e-05 5.56278683e-05 9.74349896e-05 4.94705018e-05\r\n 2.84637881e-05 3.17437771e-05 5.32805061e-05 9.46143628e-05\r\n 4.44519865e-05 3.59480218e-05 2.63182501e-05 8.13981824e-05\r\n 6.64480613e-05 2.70202054e-05 4.95499226e-05 5.90926902e-05\r\n 8.14832310e-05 4.92627405e-05 4.68851031e-05 5.09179117e-05\r\n 2.48370961e-05 2.52420614e-05 2.90256285e-05 2.39793844e-05\r\n 3.41525083e-05 2.96306716e-05 3.30576113e-05 8.04848532e-05\r\n 5.03645970e-05 6.75463598e-05 5.90382770e-05 4.86968020e-05\r\n 1.17527459e-04 3.73758958e-05 6.37511475e-05 6.05836904e-05\r\n 3.42056082e-05 6.00539533e-05 6.09216440e-05 2.54713050e-05\r\n 6.39668651e-05 3.55988450e-05 9.15502096e-05 3.18431412e-05\r\n 5.79212756e-05 1.76483209e-05 8.53508682e-05 2.43517097e-05\r\n 4.17687734e-05 4.51010928e-05 5.18607158e-05 4.43864119e-05\r\n 2.71090485e-05 4.27387022e-05 1.67791968e-05 4.04302482e-05\r\n 2.43617196e-05 3.63776999e-05 4.92111831e-05 6.23574379e-05\r\n 3.48686444e-05 6.29861388e-05 3.68348083e-05 4.13281232e-05\r\n 5.30328689e-05 1.04351806e-04 1.21111865e-04 2.99732019e-05\r\n 1.98229827e-05 2.64495939e-05 6.09882081e-05 3.16963160e-05\r\n 1.19547658e-04 7.37918599e-05 3.47008208e-05 5.26092226e-05\r\n 6.63609826e-05 7.10354288e-05 1.29212422e-04 4.21974582e-05\r\n 9.44373169e-05 3.59850346e-05 3.75521413e-05 3.28623828e-05\r\n 2.54598534e-04 4.88027072e-05 4.98747722e-05 3.70334310e-05\r\n 5.10361933e-05 1.50130363e-04 1.07541964e-04 4.74935368e-05\r\n 4.03538616e-05 2.47068547e-05 1.34145594e-04 6.19990387e-05\r\n 7.30906322e-05 4.31153785e-05 2.18266705e-05 1.58332859e-05\r\n 3.51308263e-05 1.88411195e-05 3.91372632e-05 4.49556901e-05\r\n 4.02528858e-05 4.11257170e-05 2.66694478e-05 4.56868547e-05\r\n 5.54161707e-05 8.16745378e-05 5.78520485e-05 3.79472112e-05\r\n 6.29092392e-05 4.18859709e-05 2.67846572e-05 2.77266554e-05\r\n 8.00418929e-05 6.01456595e-05 4.02958685e-05 6.27191621e-05\r\n 4.82072392e-05 1.25398190e-04 4.55766676e-05 4.09312961e-05\r\n 4.11183828e-05 6.91551031e-05 5.93010045e-05 3.77976467e-05\r\n 2.30282021e-05 6.36055775e-05 7.87112804e-05 2.95740974e-05\r\n 7.09348969e-05 3.04003879e-05 2.57354732e-05 3.13583987e-05\r\n 5.14747007e-05 4.16894982e-05 7.88783218e-05 3.90385176e-05\r\n 1.25024671e-04 3.91255853e-05 3.37608944e-05 1.78512637e-05\r\n 3.47483110e-05 1.30160537e-04 1.14669914e-04 4.75985071e-05\r\n 6.59073194e-05 5.72106728e-05 3.15150464e-05 7.19552045e-05\r\n 4.41248230e-05 5.54331928e-05 4.13056623e-05 2.98652903e-05\r\n 2.10981962e-05 5.03462543e-05 3.49047077e-05 4.37187300e-05\r\n 4.65055127e-05 1.15799427e-04 5.41915506e-05 3.34709912e-05\r\n 3.53786199e-05 1.42480290e-04 4.40532567e-05 4.29452775e-05\r\n 3.80622841e-05 1.06390777e-04 3.37229249e-05 9.00511222e-05\r\n 1.00006095e-04 2.67414216e-05 3.48397589e-05 1.86702127e-05\r\n 4.78128386e-05 4.84845659e-05 3.47940368e-05 4.71775420e-05\r\n 6.46499393e-05 3.40940278e-05 6.74092298e-05 5.55898405e-05\r\n 3.17587364e-05 8.69064461e-05 2.20635156e-05 1.61761549e-04\r\n 5.63477697e-05 9.34028940e-05 1.19326352e-04 3.92500333e-05\r\n 3.07903720e-05 5.93489276e-05 7.43963683e-05 4.17142364e-05\r\n 4.33768109e-05 2.42521455e-05 4.76334280e-05 1.01043668e-04\r\n 1.22796455e-05 3.88822409e-05 2.87018920e-05 4.43931422e-05\r\n 6.45289620e-05 6.28536436e-05 1.75435169e-04 7.43484197e-05\r\n 7.90497215e-05 1.01459656e-04 1.18819436e-04 1.85181725e-05\r\n 1.24165468e-04 1.03933278e-04 8.69836367e-05 2.30462610e-05\r\n 1.02608356e-04 3.79691483e-05 2.15911732e-05 5.90983254e-05\r\n 8.34662351e-05 3.74288284e-05 1.05429135e-04 5.67742245e-05\r\n 6.71921662e-05 1.06720836e-04 3.00887659e-05 3.90880268e-05\r\n 3.73312578e-05 3.82247090e-05 3.15153156e-05 4.60267402e-05\r\n 8.62758679e-05 1.00846511e-04 1.17138727e-04 9.34031632e-05\r\n 4.98250920e-05 8.27745316e-05 1.00982405e-04 3.42905259e-05\r\n 1.12785907e-04 3.73787807e-05 7.44065183e-05 4.50920597e-05\r\n 4.35017137e-05 6.98529329e-05 2.32243347e-05 1.03187820e-04\r\n 1.82097807e-04 1.30264845e-04 1.10692970e-04 2.07989688e-05\r\n 3.80820748e-05 4.84270313e-05 7.45634316e-05 1.02509570e-04\r\n 2.96623675e-05 2.53203707e-05 3.51807867e-05 3.41253835e-05\r\n 3.91159592e-05 4.82446267e-05 7.98111578e-05 1.52797962e-04\r\n 1.98860464e-04 4.04794773e-05 2.63113980e-05 6.67209824e-05\r\n 3.28911628e-05 7.74292857e-05 5.57682033e-05 4.86770659e-05\r\n 8.45686664e-05 1.02788174e-04 5.65814735e-05 3.95413554e-05\r\n 9.38133671e-05 7.51789848e-05 1.75316327e-05 2.77840154e-05\r\n 7.50846957e-05 5.37181113e-05 9.49826790e-05 3.48706708e-05\r\n 4.76802888e-05 7.22974801e-05 3.29730101e-05 7.00873570e-05\r\n 4.97701440e-05 2.81524917e-05 7.83408032e-05 7.64289653e-05\r\n 1.33154084e-04 2.23519273e-05 4.77707254e-05 7.17533985e-05\r\n 1.08415712e-04 3.02072203e-05 7.28918312e-05 6.12328658e-05\r\n 6.02530745e-05 4.20485412e-05 3.56859309e-05 3.86055690e-05\r\n 2.56172025e-05 7.19615200e-05 3.84582490e-05 6.03692024e-05\r\n 6.35462857e-05 7.69500621e-05 3.99253549e-05 2.44900966e-05\r\n 4.89452395e-05 1.35615715e-04 5.91547105e-05 3.84171544e-05\r\n 4.03022859e-05 7.85306547e-05 4.11010951e-05 3.10607320e-05\r\n 3.85992716e-05 2.87951079e-05 1.12240072e-04 4.95620225e-05\r\n 8.99500155e-05 4.19344506e-05 3.69221380e-05 3.13608216e-05\r\n 4.54255751e-05 2.92207424e-05 2.68103668e-05 4.87487014e-05\r\n 6.62245511e-05 4.09523782e-05 4.55349182e-05 5.16449327e-05\r\n 9.98645701e-05 4.91594437e-05 1.09598215e-04 9.02171960e-05\r\n 9.04645494e-05 4.04334460e-05 4.69499391e-05 6.99391967e-05\r\n 8.23004375e-05 4.11356814e-05 3.27949747e-05 1.32239147e-04\r\n 2.87073810e-04 2.59395201e-05 4.14389069e-05 4.24107602e-05\r\n 9.03058608e-05 5.76465645e-05 6.58449353e-05 5.11699240e-04\r\n 5.86701899e-05 3.38779100e-05 2.06262685e-05 3.78569930e-05\r\n 8.34902748e-05 7.96092572e-05 4.87154721e-05 1.77000165e-05\r\n 5.30272046e-05 6.86679341e-05 3.50871669e-05 1.49375655e-05\r\n 3.14122153e-05 5.86274582e-05 3.49831389e-04 5.41517184e-05\r\n 7.46642618e-05 8.23685987e-05 2.44450148e-05 6.65467305e-05\r\n 2.00652030e-05 2.98768846e-05 3.29640170e-05 2.18276891e-05\r\n 2.43448358e-05 6.33366435e-05 1.95022803e-05 3.77459583e-05\r\n 1.17966978e-04 3.25335568e-05 7.06047285e-05 5.06145516e-05\r\n 3.13889177e-05 7.95936212e-05 4.26858714e-05 7.69538092e-05\r\n 2.20421261e-05 1.37093317e-04 7.56645750e-05 6.26438414e-05\r\n 1.86934085e-05 5.23319541e-05 9.90531680e-06 4.42695637e-05\r\n 1.74555917e-05 6.04979541e-05 1.15835312e-04 5.39805369e-05\r\n 4.44236757e-05 6.54761097e-05 5.89498304e-05 6.38236597e-05\r\n 2.20463098e-05 1.38442829e-05 1.89793700e-05 9.30992901e-05\r\n 5.47559321e-05 4.21828154e-05 1.24905870e-04 9.96458548e-05\r\n 3.54362237e-05 2.96236085e-05 1.44791593e-05 6.09351227e-05\r\n 4.71783933e-05 7.67060046e-05 5.06982215e-05 2.10276303e-05\r\n 8.39630520e-05 7.68681421e-05 5.32231206e-05 7.70888146e-05\r\n 4.41207412e-05 5.14763233e-05 5.50542863e-05 5.27606608e-05\r\n 2.89072868e-05 3.09122697e-05 4.52013337e-05 7.94683947e-05\r\n 1.08023836e-04 2.17278539e-05 3.31693700e-05 9.59651152e-05\r\n 3.38231548e-05 6.06745889e-05 6.97081923e-05 8.37877160e-05\r\n 1.30372107e-04 6.79520526e-05 5.46720257e-05 5.64009424e-05\r\n 1.98839698e-05 6.16257821e-05 2.13934123e-04 4.79545379e-05\r\n 4.07528642e-05 4.55529407e-05 6.82892860e-05 5.54103572e-05\r\n 9.95699065e-06 1.07385778e-04 4.52565037e-05 5.43049537e-05\r\n 4.97699075e-05 2.72433754e-05 3.81183745e-05 2.32814145e-05\r\n 6.66911510e-05 4.43411409e-05 4.41790107e-05 2.20749542e-04\r\n 6.81317324e-05 8.43808739e-05 2.18740883e-04 1.49821508e-05\r\n 1.09996741e-04 3.85499770e-05 2.09415575e-05 5.65522896e-05\r\n 1.14702067e-04 3.04910063e-05 2.31663780e-05 4.43025492e-05\r\n 6.70707814e-05 4.84096236e-05 5.53743848e-05 1.30335306e-04\r\n 7.73757711e-05 3.56241799e-05 3.93316441e-05 8.89291987e-05\r\n 4.41248230e-05 4.88739242e-05 1.24593091e-04 2.72759244e-05\r\n 5.21678157e-05 1.24478116e-04 7.11878165e-05 4.96793800e-05\r\n 5.25605792e-05 3.16482583e-05 6.55303375e-05 5.29886820e-05\r\n 2.10158414e-05 1.26085733e-05 5.36719235e-05 6.75502961e-05\r\n 1.28049491e-04 3.02378867e-05 1.04079678e-04 4.56646376e-05\r\n 4.03329686e-05 1.54009376e-05 5.22995251e-05 3.41964733e-05\r\n 3.17876766e-05 5.70985503e-05 5.01368704e-05 2.79067695e-04\r\n 7.31719556e-05 8.64125614e-05 6.93945185e-05 7.03066034e-05\r\n 4.01795551e-05 6.13056691e-05 5.51183766e-05 3.04967052e-05\r\n 3.29937357e-05 6.12372969e-05 7.05811035e-05 2.40082627e-05\r\n 3.05798822e-05 5.23090530e-05 3.40432453e-05 2.68784406e-05\r\n 2.71457830e-05 2.83264671e-05 2.00382783e-05 3.37000674e-05\r\n 5.33838065e-05 4.15841168e-05 2.16243297e-05 3.54584699e-05\r\n 2.65461258e-05 1.04858475e-04 9.67939995e-05 1.91662639e-05\r\n 1.31289649e-04 7.93386789e-05 4.90267630e-05 4.17282026e-05\r\n 9.05011329e-05 2.65043618e-05 6.10635689e-05 8.14767773e-05\r\n 5.42179660e-05 5.32037848e-05 2.35966345e-05 6.43457097e-05\r\n 5.26646436e-05 1.20829915e-04 3.57951394e-05 5.47114068e-05\r\n 2.83948957e-05 5.92582655e-05 9.93188223e-05 3.83871266e-05\r\n 5.17880144e-05 1.76156318e-05 9.09835871e-05 2.68682161e-05\r\n 8.08964760e-05 3.73792791e-05 4.69392398e-05 7.17280855e-05\r\n 8.13449442e-05 4.86335448e-05 6.11506402e-05 5.53738028e-05\r\n 3.60721024e-05 5.31666556e-05 1.85797253e-04 5.41459340e-05\r\n 8.86813941e-05 3.85400723e-04 1.70980900e-04 1.25850333e-04\r\n 3.85012208e-05 1.21672761e-04 6.34997632e-05 7.90962513e-05\r\n 5.18936140e-05 7.60269540e-05 2.00775015e-04 4.64980185e-05\r\n 3.00514566e-05 2.41865491e-05 4.52518834e-05 4.76954338e-05\r\n 3.84142259e-05 1.45852764e-05 2.07627018e-05 6.61678641e-05\r\n 1.04175902e-04 6.52100789e-05 1.27029140e-04 1.39436277e-04\r\n 5.96854516e-05 2.64704122e-05 8.43198213e-05 1.12228510e-04\r\n 3.11265612e-05 1.11376386e-04 1.11630849e-04 6.66707347e-05\r\n 6.32972151e-05 6.98203003e-05 1.30234781e-04 5.83634792e-05\r\n 4.69373132e-04 9.33418214e-01 1.01784513e-04 1.40044358e-04\r\n 1.02814738e-04 1.73654917e-04 1.41994187e-05 5.31265687e-05\r\n 5.08579797e-05 5.30882244e-05 6.81649399e-05 2.21717753e-04\r\n 3.90415698e-05 2.80919885e-05 1.02586731e-04 1.21871119e-04]\r\n```\r\n\r\n我用paddlex导出的图片分类模型，在cpu环境下转成serving模型后启动服务，获取的结果如下，但这个模型应该返回“bocai”，却没有返回，为什么呢，跟我是cpu环境有关吗\r\n\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0328 09:09:45.377297   482 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0328 09:09:45.464089   482 general_model.cpp:490] [client]logid=0,client_cost=77.353ms,server_cost=66.639ms.\r\n[0.18139979 0.11694741 0.6913014  0.00147739 0.00129413 0.00757991]\r\n```",
        "state": "closed",
        "user": "zhang52104",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-29T02:52:29+00:00",
        "updated_at": "2024-03-05T06:50:58+00:00",
        "closed_at": "2024-03-05T06:50:58+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1740,
        "title": "使用paddlex导出模型，通过paddleserving转换模型后，启动服务，得不到想要的结果",
        "body": "我是用 https://github.com/PaddlePaddle/PaddleX/blob/develop/docs/quick_start_API.md 里面的示例训练模型，测试结果\r\n\r\n```\r\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\r\n  if data.dtype == np.object:\r\n2022-03-29 09:08:42 [INFO]\tModel[MobileNetV3_small] loaded.\r\nPredict Result:  [{'category_id': 0, 'category': 'bocai', 'score': 0.99866307}]\r\n\r\n```\r\n\r\n然后我执行导出\r\n\r\n```\r\npaddlex --export_inference --model_dir=./output/mobilenetv3_small/best_model/ --save_dir=./inference_model\r\n```\r\n\r\n再通过paddle_serving_client转换模型\r\n\r\n```\r\npython3 -m paddle_serving_client.convert --dirname ./inference_model/inference_model --model_filename model.pdmodel --params_filename model.pdiparams --serving_server serving_server --serving_client serving_client\r\n```\r\n\r\n启动服务\r\n\r\n```\r\npython3 -m paddle_serving_server.serve --model serving_server --port 9393\r\n```\r\n\r\n执行调用\r\n\r\n```\r\nfrom paddle_serving_client import Client\r\nfrom paddle_serving_app.reader import Sequential, File2Image, Resize, CenterCrop\r\nfrom paddle_serving_app.reader import RGB2BGR, Transpose, Div, Normalize\r\n\r\nclient = Client()\r\nclient.load_client_config(\r\n    \"serving_client/serving_client_conf.prototxt\")\r\nclient.connect([\"127.0.0.1:9393\"])\r\n\r\nseq = Sequential([\r\n    File2Image(), Transpose((2, 0, 1)),\r\n    Div(255)\r\n])\r\n\r\nimage_file = \"daisy.jpg\"\r\nimg = seq(image_file)\r\nfetch_map = client.predict(feed={\"image\": img}, fetch=[\"softmax_0.tmp_0\"])\r\nprint(fetch_map[\"softmax_0.tmp_0\"].reshape(-1))\r\nprint(fetch_map)\r\n```\r\n\r\n输出结果：\r\n```\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0329 09:58:34.798663  2728 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9393\"): added 1\r\nI0329 09:58:34.847139  2728 general_model.cpp:490] [client]logid=0,client_cost=42.611ms,server_cost=29.807ms.\r\n[0.16750729 0.04058916 0.01292069 0.08925734 0.11853778 0.57118773]\r\n{'softmax_0.tmp_0': array([[0.16750729, 0.04058916, 0.01292069, 0.08925734, 0.11853778,\r\n        0.57118773]], dtype=float32)}\r\n```\r\n\r\n这个结果并没有输出。 “bocai”。请问是我少干啥事了吗，或者少写参数了，困扰我好几天了，也没有解决，请大神帮忙解决，多谢",
        "state": "closed",
        "user": "zhang52104",
        "closed_by": "zhang52104",
        "created_at": "2022-03-29T10:13:59+00:00",
        "updated_at": "2022-04-02T02:46:54+00:00",
        "closed_at": "2022-04-02T02:46:54+00:00",
        "comments_count": [
            "zhang52104",
            "zhang52104"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1738,
        "title": "About stopping  the server problems",
        "body": "在关闭服务的时候，因为是nohup方式启动的，所以最初用了kill -9 XXX的方式\r\n后来用了  `python -m paddle_serving_server.serve stop`方式进行停止，报下面的异常\r\n\r\n`/home/beyond/myDL/miniconda3/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nTraceback (most recent call last):\r\n  File \"/home/beyond/myDL/miniconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/beyond/myDL/miniconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/beyond/myDL/miniconda3/lib/python3.7/site-packages/paddle_serving_server/serve.py\", line 535, in <module>\r\n    result = stop_serving(args.server)\r\n  File \"/home/beyond/myDL/miniconda3/lib/python3.7/site-packages/paddle_serving_server/serve.py\", line 451, in stop_serving\r\n    infoList = load_pid_file(filepath)\r\n  File \"/home/beyond/myDL/miniconda3/lib/python3.7/site-packages/paddle_serving_server/util.py\", line 123, in load_pid_file\r\n    \"ProcessInfo.json file is not exists, All processes of PaddleServing has been stopped.\")\r\nValueError: ProcessInfo.json file is not exists, All processes of PaddleServing has been stopped.`\r\n\r\n后台服务进程依然在\r\n![1648540617(1)](https://user-images.githubusercontent.com/12640462/160562334-e6101c20-50de-46bb-99a7-6df65f5f6186.png)\r\n\r\n\r\n这种情况是不是只能手动Kill 服务了",
        "state": "closed",
        "user": "BeyondYourself",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-29T07:57:38+00:00",
        "updated_at": "2024-04-16T09:06:06+00:00",
        "closed_at": "2024-04-16T09:06:06+00:00",
        "comments_count": [
            "TeslaZhao",
            "BeyondYourself"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1741,
        "title": "ERROR: paddle_serving_client-0.8.3-cp37-none-any.whl is not a supported wheel on this platform.",
        "body": "ERROR: paddle_serving_client-0.8.3-cp37-none-any.whl is not a supported wheel on this platform.\r\n\r\npaddleocr的部署  安装失败\r\n使用阿里云  linux 服务器",
        "state": "closed",
        "user": "mingo-doer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-01T02:29:29+00:00",
        "updated_at": "2024-03-05T06:50:59+00:00",
        "closed_at": "2024-03-05T06:50:59+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "jiangbowen",
            "tingsi",
            "tingsi"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1742,
        "title": "开发机部署时遇到OPENSSL_1.0.1_EC not found",
        "body": "ImportError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: version `OPENSSL_1.0.1_EC' not found (required by /usr/local/lib/python3.7/dist-packages/paddle_serving_client/serving_client.so)\r\n\r\n机器上已经安装openssl 1.1.1, ubuntu 18.04",
        "state": "closed",
        "user": "Cppowboy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-01T11:38:55+00:00",
        "updated_at": "2024-03-05T06:51:00+00:00",
        "closed_at": "2024-03-05T06:51:00+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "install"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1743,
        "title": "/site-packages/paddle_serving_server/serve.py中的binurl已经过期",
        "body": "/site-packages/paddle_serving_server/serve.py 中的binurl使用的是https://paddle-serving.bj.bcebos.com/bin/serving-cpu-avx-mkl-0.8.3.tar.gz，这个地址已经无法访问，现在只能使用https://paddle-serving.bj.bcebos.com/test-dev/bin/serving-cpu-avx-mkl-0.8.3.tar.gz 请及时更新paddle_serving_server，不然第一次启动paddleserving的时候会出现无法下载bin的问题",
        "state": "closed",
        "user": "lightbordwin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-07T09:36:06+00:00",
        "updated_at": "2024-03-05T06:51:01+00:00",
        "closed_at": "2024-03-05T06:51:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1745,
        "title": "服务部署时，提示没有--name参数",
        "body": "python37 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292 --name uci\r\n/usr/local/python3/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but p                     rior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\nserve: error: unrecognized arguments: --name",
        "state": "closed",
        "user": "yafeisong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-10T14:10:27+00:00",
        "updated_at": "2024-03-05T06:51:02+00:00",
        "closed_at": "2024-03-05T06:51:02+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1746,
        "title": "飞腾2000、麒麟V10宿主环境下，docker中编译安装paddleserving 问题",
        "body": "[问题说明.docx](https://github.com/PaddlePaddle/Serving/files/8460597/default.docx)\r\n",
        "state": "closed",
        "user": "zjbit",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-11T02:26:11+00:00",
        "updated_at": "2024-03-05T06:51:03+00:00",
        "closed_at": "2024-03-05T06:51:03+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "硬件适配"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1747,
        "title": "Failed to inference: Service name error.",
        "body": "从0.6.2升级到0.7.0，原来的正常的服务报这样的错，是什么原因呢？",
        "state": "closed",
        "user": "Jnoee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-12T02:09:01+00:00",
        "updated_at": "2024-04-16T09:06:07+00:00",
        "closed_at": "2024-04-16T09:06:07+00:00",
        "comments_count": [
            "TeslaZhao",
            "Jnoee",
            "Jnoee",
            "Jnoee"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1748,
        "title": "Xiao WUJI",
        "body": "Great to see you here",
        "state": "closed",
        "user": "zqq1995",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-13T01:11:52+00:00",
        "updated_at": "2024-03-05T06:51:03+00:00",
        "closed_at": "2024-03-05T06:51:03+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1749,
        "title": "原生Windows系统运行Paddle Serving,支持GPU模式运算吗",
        "body": "原生Windows系统运行Paddle Serving,支持GPU模式运算吗",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-13T02:10:35+00:00",
        "updated_at": "2024-04-16T09:06:08+00:00",
        "closed_at": "2024-04-16T09:06:08+00:00",
        "comments_count": [
            "TeslaZhao",
            "meishitouzhele"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1750,
        "title": "windows系统安装docker  paddleServing可以使用GPU吗？",
        "body": "windows系统安装docker  paddleServing可以使用GPU吗？我看paddleserving中使用docker需要安装nvidia-docker，但是win10系统是没办法安装nvidia-docker吧？  是不是就是说，win10系统如果想通过docker，只能使用cpu模式？\r\n那么，win10系统如果想使用paddleServing的GPU模式，怎么才能行？",
        "state": "closed",
        "user": "meishitouzhele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-13T08:04:47+00:00",
        "updated_at": "2024-04-16T09:06:09+00:00",
        "closed_at": "2024-04-16T09:06:09+00:00",
        "comments_count": [
            "TeslaZhao",
            "meishitouzhele"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1751,
        "title": "AttributeError: 'Program' object has no attribute '_remove_training_info'",
        "body": "My environment:\r\nubuntu 18.0  base on GPU\r\npaddledetection==2.3.0\r\npaddleserving==0.8.3\r\n --------------------------------------------------------------------------------------------\r\nI want to export my model file,so I input:\r\npython tools/export_model.py -c configs/ssd/ssd_mobilenet_v1_300_120e_voc.yml --output_dir=./inference_model -o weights=output/ssd_mobilenet_v1_300_120e_voc/model_final.pdparams --export_serving_model=True\r\n----------------------------------------------------------------------------------------------\r\nThe question I met is as follow:\r\nTraceback (most recent call last):\r\n  File \"tools/export_model.py\", line 115, in <module>\r\n    main()\r\n  File \"tools/export_model.py\", line 111, in main\r\n    run(FLAGS, cfg)\r\n  File \"tools/export_model.py\", line 90, in run\r\n    params_filename=\"model.pdiparams\")\r\n  File \"/home/phm/J/Anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py\", line 334, in inference_model_to_serving\r\n    model_filename, params_filename, show_proto, feed_alias_names, fetch_alias_names)\r\n  File \"/home/phm/J/Anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py\", line 214, in save_model\r\n    new_model_file.write(main_program._remove_training_info(False).desc.serialize_to_string())\r\nAttributeError: 'Program' object has no attribute '_remove_training_info'\r\n----------------------------------------------------------------------------------------------------\r\nThanks\r\n",
        "state": "closed",
        "user": "Roly-Yang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-18T10:30:43+00:00",
        "updated_at": "2024-03-05T06:51:04+00:00",
        "closed_at": "2024-03-05T06:51:04+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Roly-Yang",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1755,
        "title": "边缘端AI如何进行多模型部署",
        "body": "在单机上需要部署两个独立的模型，是启动两个独立的serving服务，还是在一个serving服务中挂在两个模型，两个模型的调用是否有动态调度，是否会有资源抢占的情况？",
        "state": "closed",
        "user": "ChenjieXu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-21T02:56:06+00:00",
        "updated_at": "2024-04-16T09:06:11+00:00",
        "closed_at": "2024-04-16T09:06:11+00:00",
        "comments_count": [
            "TeslaZhao",
            "TeslaZhao",
            "ChenjieXu"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1754,
        "title": "边缘端AI解决方案中视频解码部分如何处理",
        "body": "边缘端AI解决方案中视频解码部分如何处理，比如在jetson上面，解码有NVDEC硬件进行解码，这部分如何与Serving代码进行结合？有没有完整的例子。\r\n微信交流群二维码过期了，能否拉我一下进群讨论？",
        "state": "closed",
        "user": "ChenjieXu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-21T02:04:28+00:00",
        "updated_at": "2024-04-16T09:06:10+00:00",
        "closed_at": "2024-04-16T09:06:10+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "ChenjieXu"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1756,
        "title": "ASR语音模型部署",
        "body": "请问现在支持语音ASR的部署吗？有没有相应的example",
        "state": "closed",
        "user": "Chenwe111",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-21T03:11:56+00:00",
        "updated_at": "2024-03-05T06:51:05+00:00",
        "closed_at": "2024-03-05T06:51:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "Chenwe111",
            "learningpro"
        ],
        "labels": [
            "example"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1757,
        "title": "关于pipeline serving时，返回结果的batch拆分问题",
        "body": "目前使用的模型来自paddlex 训练的 FasterRCNN，在参考官方FasterRCNN例子使用pipeline serving时只返回第一个结果。\r\n\r\n定位了一下问题，官方提供的FasterRCNN serving例子的模型可能来自老版本保存格式为 \\_\\_model__  __params__在调用时会返回lodTensor，并根据lod截取相应的长度\r\n![image](https://user-images.githubusercontent.com/50342689/164361215-5373a05b-208a-4808-835e-e6436e42bda6.png)\r\n\r\n而我们使用paddlex导出的模型为2.x版本导出的推理模型，保存格式为.pdmodel  .pdiparams，调用时返回normal tensor，无法根据lod截取对应的长度，所以只截取了第一个\r\n![image](https://user-images.githubusercontent.com/50342689/164362332-43505758-6f37-4f1c-80ab-6001388c80bd.png)\r\n\r\n对于paddle的FasterRCNN类模型来说，默认的fetch_targets都会有两个值，一个对应识别bbox结果（scale_0.tmp1 与 concat_12.tmp_0），一个对应返回的bbox的数量（scale_1.tmp_1 与 multiclass_nms3_0.tmp2），可以使用对应bbox数量来截取以替代lod做返回结果拆分。但考虑到不同模型的返回不一样，尤其对于返回结果变长需要拆分的，是否有更统一的办法",
        "state": "closed",
        "user": "KRuok",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-21T03:52:22+00:00",
        "updated_at": "2024-03-05T06:51:06+00:00",
        "closed_at": "2024-03-05T06:51:06+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "模型升级"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1758,
        "title": "部署预测运行时，偶发Failed to fetch错误",
        "body": "使用PaddleDetection训练的模型，以PaddleServing参数导出模型部署，导出的模型中serving_client_conf.prototxt内容如下：\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 608\r\n  shape: 608\r\n}\r\nfeed_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"scale_factor\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_0\"\r\n  alias_name: \"multiclass_nms3_0.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_2\"\r\n  alias_name: \"multiclass_nms3_0.tmp_2\"\r\n  is_lod_tensor: false\r\n  fetch_type: 2\r\n}\r\n\r\npython脚本中预测相关代码如下：\r\n             preprocess = DetectionSequential([\r\n                DetectionFile2Image(),\r\n                DetectionNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], True),\r\n                DetectionResize((800, 1333), True, interpolation=cv2.INTER_LINEAR),\r\n                DetectionTranspose((2, 0, 1)),\r\n                DetectionPadStride(128)\r\n            ])\r\n\r\n            postprocess = RCNNPostprocess(\"/home/models/220406/label_list.txt\", output_dir)\r\n            client = Client()\r\n            client.load_client_config(\"/home/models/220406/serving_client/serving_client_conf.prototxt\")\r\n            client.connect(['127.0.0.1:9494'])\r\n\r\n            im, im_info = preprocess(path)\r\n            fetch_map = client.predict(\r\n                feed={\r\n                    \"image\": im,\r\n                    \"im_shape\": np.array(list(im.shape[1:])).reshape(-1),\r\n                    \"scale_factor\": im_info['scale_factor'],\r\n                },\r\n                fetch='multiclass_nms3_0.tmp_0')\r\n            fetch_map[\"image\"] = path\r\n            postprocess(fetch_map)\r\n大部分情况下可以正常预测，但是某些图片会报如下错误：\r\n\r\n2022-04-22 08:26:37 - Failed to fetch, maybe the type of [multiclass_nms3_0.tmp_0] is wrong, please check the model file\r\nTraceback (most recent call last):\r\n  File \"/home/scripts/Watcher_py/prehandler.py\", line 62, in on_created\r\n    fetch='multiclass_nms3_0.tmp_0')\r\n  File \"/usr/local/lib/python3.7/site-packages/paddle_serving_client/client.py\", line 500, in predict\r\n    name))\r\nValueError: Failed to fetch, maybe the type of [multiclass_nms3_0.tmp_0] is wrong, please check the model file\r\n\r\n会报这种错的图片，重复进行预测每次稳定都会报错，这张图片中没有可以检测的目标，但不确定是否所有报错图片都没有目标。\r\n模型训练使用**GPU**训练，部署的服务器使用**CPU**进行预测，和这个有关系吗？\r\n如果需要提供其他文件，请指出。",
        "state": "closed",
        "user": "morrigank12",
        "closed_by": "morrigank12",
        "created_at": "2022-04-22T03:36:19+00:00",
        "updated_at": "2022-05-05T05:54:35+00:00",
        "closed_at": "2022-05-05T05:54:35+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "morrigank12",
            "TeslaZhao",
            "morrigank12",
            "TeslaZhao",
            "morrigank12"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1760,
        "title": "docker部署serving的image_reader.py文件在哪里",
        "body": "谁能告知下 docker 方式 serving下载的环境在哪里啊  paddle_serving_app提供的图像预处理或后期处理结果可能与自己训练的模型存在偏差，需要到python安装包path/to/python3.6/site-packages/paddle_serving_app/reader/下修改image_reader.py文件  找不到这个文件\r\n\r\n![image](https://user-images.githubusercontent.com/60963646/164971445-1038fdf7-b0a5-4f91-bfd9-5b245b5721d4.png)\r\n我想后续处理检测的结果，直接输出检测到的对象和分数应该处理哪里",
        "state": "closed",
        "user": "cutexiaokele",
        "closed_by": "cutexiaokele",
        "created_at": "2022-04-24T10:09:51+00:00",
        "updated_at": "2022-04-26T08:46:05+00:00",
        "closed_at": "2022-04-26T08:46:05+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1764,
        "title": "使用serving部署detection 耗时太长",
        "body": "λ 5bfbf6cb6b15 /home/Serving/examples/C++/PaddleDetection/luoxiang {v0.8.3} python3 test_client.py 1.jpg\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0425 08:03:52.003800 21326 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9292\"): added 1\r\nI0425 08:03:54.618553 21326 general_model.cpp:490] [client]logid=0,client_cost=2530.3ms,server_cost=2486.09ms.\r\nrabbit 0.6221785545349121",
        "state": "closed",
        "user": "cutexiaokele",
        "closed_by": "cutexiaokele",
        "created_at": "2022-04-25T08:09:28+00:00",
        "updated_at": "2022-04-26T08:45:42+00:00",
        "closed_at": "2022-04-26T08:45:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1761,
        "title": "paddleServing部署时，启动http客户端，报错。",
        "body": "ppocr模型中，检测模型是自己训练的，识别模型用的是官方的，将其组合在一起，服务部署的时候，服务端启动正常，客户端Pipeline_http_client启动时，报错，看日志提示是：\r\nERROR 2022-04-24 09:01:22,719 [error_catch.py:125]\r\nLog_id: 0\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/error_catch.py\", line 97, in wrapper\r\n    res = func(*args, **kw)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/operator.py\", line 1156, in postprocess_help\r\n    midped_data, data_id, logid_dict.get(data_id))\r\n  File \"web_service.py\", line 95, in postprocess\r\n    det_out = fetch_dict[\"save_infer_model/scale_0.tmp_1\"]\r\nKeyError: 'save_infer_model/scale_0.tmp_1'\r\nClassname: Op._run_postprocess.<locals>.postprocess_help\r\nFunctionName: postprocess_help\r\n\r\n\r\n我的检测模型输出serving_server_conf.prototxt文件是：\r\n$ cat serving_server_conf.prototxt\r\nfeed_var {\r\n  name: \"x\"\r\n  alias_name: \"x\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"sigmoid_0.tmp_0\"\r\n  alias_name: \"sigmoid_0.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 1\r\n}\r\n\r\n识别模型的 .prototxt文件是：\r\n$ cat serving_server_conf.prototxt\r\nfeed_var {\r\n  name: \"x\"\r\n  alias_name: \"x\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 32\r\n  shape: 100\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 25\r\n  shape: 6625\r\n}\r\n\r\n我是采用docker的方式部署的，请问各位大佬，这种问题咋解决呢。\r\n",
        "state": "closed",
        "user": "penghui-luna",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-24T14:31:29+00:00",
        "updated_at": "2024-03-05T06:51:07+00:00",
        "closed_at": "2024-03-05T06:51:07+00:00",
        "comments_count": [
            "github-actions[bot]",
            "penghui-luna",
            "TeslaZhao",
            "penghui-luna",
            "TeslaZhao",
            "penghui-luna",
            "BeyondYourself",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1762,
        "title": "pp-tracking可以用paddleserving进行部署嘛",
        "body": "我把fairmot_hrnetv2_w18_dlafpn_30e_576x320.pdparams模型导出为paddleserving需要的格式，然后在服务端启动成功了，但是不知道怎么通过客户端把想要跟踪的视频发送给服务端检测跟踪后返回给客户端进行显示。就想问问pp-tracking里面的模型是可以用paddleserving进行部署的嘛",
        "state": "closed",
        "user": "susan-812",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-25T02:22:33+00:00",
        "updated_at": "2024-03-05T06:51:08+00:00",
        "closed_at": "2024-03-05T06:51:08+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1765,
        "title": "使用serving Pipeline 部署 当尺寸为960*1280时报错",
        "body": "InvalidArgumentError: The 3-th dimension of input[0] and input[1] is expected to be equal.But received input[0]'s shape = [1, 160, 40, 32], input[1]'s shape = [1, 160, 40, 31].\r\n      [Hint: Expected inputs_dims[0][j] == inputs_dims[i][j], but received inputs_dims[0][j]:32 != inputs_dims[i][j]:31.] (at /paddle/paddle/fluid/operators/concat_op.h:63)\r\n\r\n不知道什么问题",
        "state": "closed",
        "user": "cutexiaokele",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-25T11:42:12+00:00",
        "updated_at": "2024-03-05T06:51:09+00:00",
        "closed_at": "2024-03-05T06:51:09+00:00",
        "comments_count": [
            "cutexiaokele",
            "cutexiaokele",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1769,
        "title": "pp-tracking里的模型是否可以用paddleserving进行服务器部署",
        "body": null,
        "state": "closed",
        "user": "susan-812",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-06T03:23:36+00:00",
        "updated_at": "2024-03-05T06:51:09+00:00",
        "closed_at": "2024-03-05T06:51:09+00:00",
        "comments_count": [
            "susan-812",
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1770,
        "title": "docker内存不释放",
        "body": "![image](https://user-images.githubusercontent.com/34369626/167063278-0005bcef-bcf7-48ea-a669-561969d80ef5.png)\r\n客户端请求后，服务端内存不释放，多次请求后内存一直累加。\r\nPaddle Server版本：0.8.3\r\n代码：https://github.com/PaddlePaddle/PaddleClas/tree/release/2.3/deploy/paddleserving",
        "state": "closed",
        "user": "charmowen",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-06T03:46:28+00:00",
        "updated_at": "2024-11-19T06:42:10+00:00",
        "closed_at": "2024-11-19T06:42:10+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "charmowen",
            "Juruobudong",
            "wangpy1204"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1775,
        "title": "docker run --gpus all 启动 0.8.0-cuda10.2-cudnn7-devel 容器出错",
        "body": "使用windows的docker desktop，docker version:20.10.13\r\n使用 --gpus all 替代 nvidia-docker\r\ndocker run --gpus all -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:0.8.0-cuda10.2-cudnn7-devel bash\r\n\r\ndocker: Error response from daemon: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: load library failed: /usr/lib/wsl/drivers/nvlt.inf_amd64_b656a48cddc58650/libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.",
        "state": "closed",
        "user": "hq0749a",
        "closed_by": "hq0749a",
        "created_at": "2022-05-09T06:32:51+00:00",
        "updated_at": "2022-05-09T06:46:50+00:00",
        "closed_at": "2022-05-09T06:46:50+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1774,
        "title": "serving是不是不支持VGPU",
        "body": "NVIDIA-DOCKER 部署paddleserving的ocr服务，没有问题。\r\n但是在私有BML上部署的时候，服务调用不了。请问是不是gpu虚拟卡的问题。",
        "state": "closed",
        "user": "ly3197640",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-09T03:08:21+00:00",
        "updated_at": "2024-04-16T09:06:12+00:00",
        "closed_at": "2024-04-16T09:06:12+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ly3197640"
        ],
        "labels": [
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1776,
        "title": "docker run --gpus all 启动 registry.baidubce.com/paddlepaddle/0.8.0-cuda10.2-cudnn7-devel 镜像容器出错",
        "body": "windows \r\ndocker version:20.10.13\r\n使用 --gpus all 替代 nvidia-docker\r\n启动命令：\r\ndocker run --gpus all -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:0.8.0-cuda10.2-cudnn7-devel bash\r\nERROR:\r\ndocker: Error response from daemon: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: load library failed: /usr/lib/wsl/drivers/nvlt.inf_amd64_b656a48cddc58650/libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.\r\n请问这个如何解决，是这个镜像缺少cuda及cudnn相关驱动文件么",
        "state": "closed",
        "user": "hq0749a",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-09T08:11:25+00:00",
        "updated_at": "2024-03-05T06:51:10+00:00",
        "closed_at": "2024-03-05T06:51:10+00:00",
        "comments_count": [
            "TeslaZhao"
        ],
        "labels": [
            "deploy"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1777,
        "title": "pymilvus 2.0.2和PaddleServing底层包不兼容，重新编译PaddleServing包，源码编译出错",
        "body": "详细介绍见链接：https://blog.csdn.net/qq_15821487/article/details/124703947?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22124703947%22%2C%22source%22%3A%22qq_15821487%22%7D&ctrtid=MwBDd\r\n\r\n\r\n报错\r\n`[2022-05-10 12:06:31] The conflict is caused by:\r\n[2022-05-10 12:06:31]     paddle-serving-server-gpu 0.8.3.post101 depends on grpcio<=1.33.2\r\n[2022-05-10 12:06:31]     paddle-serving-client 0.8.3 depends on grpcio<=1.33.2\r\n[2022-05-10 12:06:31]     pymilvus 2.0.2 depends on grpcio==1.37.1\r\n[2022-05-10 12:06:31] \r\n[2022-05-10 12:06:31] To fix this you could try to:\r\n[2022-05-10 12:06:31] 1. loosen the range of package versions you've specified\r\n[2022-05-10 12:06:31] 2. remove package versions to allow pip attempt to solve the dependency\r\n`\r\n\r\n源码编译报错：\r\n`CMakeFiles/Makefile2:757: recipe for target 'CMakeFiles/extern_gflags.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_gflags.dir/all] Error 2\r\nfatal: unable to access 'https://github.com/pybind/pybind11.git/': Failed to connect to github.com port 443: Connection timed out\r\nfatal: unable to access 'https://github.com/madler/zlib.git/': Failed to connect to github.com port 443: Connection timed out\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/Serving/build_server/third_party/pybind/tmp/extern_pybind-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/pybind/pybind11.git'\r\n`\r\n",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-11T11:38:04+00:00",
        "updated_at": "2024-03-05T06:51:11+00:00",
        "closed_at": "2024-03-05T06:51:11+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "AI-Mart",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1780,
        "title": "使用PaddleDetection导出的Serving量化部署模型，开启--use_trt选项运行时报错",
        "body": "使用PaddleDetection，对yolov3_mobilenet_v1模型进行QAT量化。量化后模型经测试，不开启TRT时，PaddleInference可用，PaddleServing可用。开启TRT（Inference添加参数--run_mode=trt_int8、Serving添加参数--use_trt）时，PaddleInference可用，PaddleServing报错。\r\n\r\nPaddleServing运行命令：python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 1 --precision int8 --use_trt\r\n报错如下（会随机报以下两不同错误，出现前提条件不明）：\r\n```\r\n(pdconfig) ubuntu@sunyuke:~/lxd-storage/xzy/PaddleCV/PaddleDetection/inference_model/yolov3_mobilenet_v1_270e_qat_pdserving/yolov3_mobilenet_v1_qat$ python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 1 --precision int8 --use_trt\r\n/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nGoing to Run Comand\r\n/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-101-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision int8 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 31581 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 31581 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 31581 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 31581 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 31581 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 31581 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\nI0513 15:06:24.095415 31585 analysis_predictor.cc:576] TensorRT subgraph engine is enabled\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [conv_affine_channel_fuse_pass]\r\n--- Running IR pass [adaptive_pool2d_convert_global_pass]\r\n--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]\r\n--- Running IR pass [shuffle_channel_detect_pass]\r\n--- Running IR pass [quant_conv2d_dequant_fuse_pass]\r\n--- Running IR pass [delete_quant_dequant_op_pass]\r\nI0513 15:06:24.263115 31585 fuse_pass_base.cc:57] ---  detected 47 subgraphs\r\n--- Running IR pass [delete_quant_dequant_filter_op_pass]\r\nI0513 15:06:24.300499 31585 fuse_pass_base.cc:57] ---  detected 47 subgraphs\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v2]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v3]\r\n--- Running IR pass [skip_layernorm_fuse_pass]\r\n--- Running IR pass [unsqueeze2_eltwise_fuse_pass]\r\n--- Running IR pass [squeeze2_matmul_fuse_pass]\r\n--- Running IR pass [reshape2_matmul_fuse_pass]\r\n--- Running IR pass [flatten2_matmul_fuse_pass]\r\n--- Running IR pass [map_matmul_v2_to_mul_pass]\r\n--- Running IR pass [map_matmul_v2_to_matmul_pass]\r\n--- Running IR pass [map_matmul_to_mul_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\n--- Running IR pass [conv_elementwise_add_fuse_pass]\r\n--- Running IR pass [tensorrt_subgraph_pass]\r\nI0513 15:06:24.358453 31585 tensorrt_subgraph_pass.cc:138] ---  detect a sub-graph with 145 nodes\r\nI0513 15:06:24.391294 31585 tensorrt_subgraph_pass.cc:395] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nUnimplementedError: no OpConverter for optype [nearest_interp_v2]\r\n  [Hint: it should not be null.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:142)\r\n\r\nAborted (core dumped)\r\n```\r\n```\r\n(pdconfig) ubuntu@sunyuke:~/lxd-storage/xzy/PaddleCV/PaddleDetection/inference_model/yolov3_mobilenet_v1_270e_qat_pdserving/yolov3_mobilenet_v1_qat$ python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 1 --precision int8 --use_trt\r\n/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nGoing to Run Comand\r\n/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-101-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision int8 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 31237 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 31237 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 31237 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 31237 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 31237 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 31237 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\nI0513 12:57:24.111806 31240 analysis_predictor.cc:576] TensorRT subgraph engine is enabled\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [conv_affine_channel_fuse_pass]\r\n--- Running IR pass [adaptive_pool2d_convert_global_pass]\r\n--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]\r\n--- Running IR pass [shuffle_channel_detect_pass]\r\n--- Running IR pass [quant_conv2d_dequant_fuse_pass]\r\n--- Running IR pass [delete_quant_dequant_op_pass]\r\nI0513 12:57:24.282593 31240 fuse_pass_base.cc:57] ---  detected 47 subgraphs\r\n--- Running IR pass [delete_quant_dequant_filter_op_pass]\r\nI0513 12:57:24.320891 31240 fuse_pass_base.cc:57] ---  detected 47 subgraphs\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v2]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v3]\r\n--- Running IR pass [skip_layernorm_fuse_pass]\r\n--- Running IR pass [unsqueeze2_eltwise_fuse_pass]\r\n--- Running IR pass [squeeze2_matmul_fuse_pass]\r\n--- Running IR pass [reshape2_matmul_fuse_pass]\r\n--- Running IR pass [flatten2_matmul_fuse_pass]\r\n--- Running IR pass [map_matmul_v2_to_mul_pass]\r\n--- Running IR pass [map_matmul_v2_to_matmul_pass]\r\n--- Running IR pass [map_matmul_to_mul_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\n--- Running IR pass [conv_elementwise_add_fuse_pass]\r\n--- Running IR pass [tensorrt_subgraph_pass]\r\nI0513 12:57:24.429746 31240 tensorrt_subgraph_pass.cc:138] ---  detect a sub-graph with 8 nodes\r\nI0513 12:57:24.433738 31240 tensorrt_subgraph_pass.cc:395] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nW0513 12:57:24.989765 31240 helper.h:107] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32.\r\nE0513 12:57:24.990360 31240 helper.h:111] Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.\r\nE0513 12:57:24.990375 31240 helper.h:111] Builder failed while configuring INT8 mode.\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: Build TensorRT cuda engine failed! Please recheck you configurations related to paddle-TensorRT.\r\n  [Hint: infer_engine_ should not be null.] (at /paddle/paddle/fluid/inference/tensorrt/engine.cc:252)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\n\r\n### 复现环境 Environment\r\n\r\npaddlepaddle-gpu=2.3.0.rc0.post101\r\npaddledet=2.3.0\r\npaddleslim=2.2.2\r\npaddle-serving-server-gpu=0.8.3.post101\r\npaddle-serving-client=0.8.3\r\npaddle-serving-app=0.8.3\r\n\r\nUbuntu 18.04\r\nPython 3.7\r\n\r\nNvidia Driver 430.64\r\nCUDA 10.1\r\ncudnn 7.6.5\r\nTensorRT=6.0.1.5\r\n",
        "state": "closed",
        "user": "Disciple7",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-17T02:45:05+00:00",
        "updated_at": "2024-03-05T06:51:13+00:00",
        "closed_at": "2024-03-05T06:51:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1779,
        "title": "pipeline server init error",
        "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\site-packages\\paddle_serving_server\\pipeline\\error_catch.py\", line 97, in wrapper\r\n    res = func(*args, **kw)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\site-packages\\paddle_serving_server\\pipeline\\error_catch.py\", line 163, in wrapper\r\n    result = function(*args, **kwargs)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\site-packages\\paddle_serving_server\\pipeline\\pipeline_server.py\", line 51, in init_helper\r\n    self._dag_executor = dag.DAGExecutor(response_op, dag_conf, worker_idx)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\site-packages\\paddle_serving_server\\pipeline\\dag.py\", line 85, in __init__\r\n    self._tracer = PerformanceTracer(\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\site-packages\\paddle_serving_server\\pipeline\\profiler.py\", line 45, in __init__\r\n    self._data_buffer = multiprocessing.Manager().Queue()\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\context.py\", line 57, in Manager\r\n    m.start()\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\managers.py\", line 554, in start\r\n    self._process.start()\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\context.py\", line 327, in _Popen\r\n    return Popen(process_obj)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 45, in __init__\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\spawn.py\", line 154, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"C:\\Users\\666\\anaconda3\\lib\\multiprocessing\\spawn.py\", line 134, in _check_not_importing_main\r\n    raise RuntimeError('''\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\nClassname: PipelineServicer.__init__.<locals>.init_helper\r\nFunctionName: init_helper\r\n",
        "state": "closed",
        "user": "wenjin11",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-16T06:49:50+00:00",
        "updated_at": "2024-03-05T06:51:12+00:00",
        "closed_at": "2024-03-05T06:51:12+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "multi-OS platforms"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1784,
        "title": "关于客户端增加其他值传入服务器端",
        "body": "![图片](https://user-images.githubusercontent.com/58407657/168999029-41690e6e-4686-4727-a906-8a308d0eed63.png)\r\n在ocr的paddle serving中 使用的pipeline  如何在上图的data中，添加一个别的数值，然后和图片一起，送入服务器端？",
        "state": "closed",
        "user": "1037419569",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-18T08:54:17+00:00",
        "updated_at": "2024-03-05T06:51:14+00:00",
        "closed_at": "2024-03-05T06:51:14+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1785,
        "title": "{'err_no': 10000, 'err_msg': 'Log_id: 0  Raise_msg: RecOp object has no attribute sorted_boxes  ClassName: Op._run_preprocess.<locals>.preprocess_help  FunctionName: preprocess_help', 'key': [], 'value': [], 'tensors': []}",
        "body": "paddle serving 的pipline中的webservice 文本识别部分之前有一个文本框排序的函数，但是我显示说没有这个东西，但是我这个是把示例复制下来的。\r\n![图片](https://user-images.githubusercontent.com/58407657/169029276-2a7774f1-e353-4b3a-aa06-00cbb98dfb4b.png)\r\n![图片](https://user-images.githubusercontent.com/58407657/169029305-ebaf5660-d0b4-4849-99a2-0c21c139c821.png)\r\n",
        "state": "closed",
        "user": "1037419569",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-18T11:33:11+00:00",
        "updated_at": "2025-06-24T06:46:34+00:00",
        "closed_at": "2025-06-24T06:46:34+00:00",
        "comments_count": [
            "TeslaZhao",
            "2022WPJ",
            "2022WPJ",
            "jackjyq"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1786,
        "title": "关于Serving0.8.3部署OCR模型检测精度下降和识别结果问题",
        "body": "python=3.8\r\npaddlepaddle==2.2.2\r\npaddle-serving-server==0.8.3\r\n服务器是centos7，cpu\r\n\r\n1.paddle_serving_client.convert转换ch_PP-OCRv3_rec_infer和ch_PP-OCRv3_det_infer模型\r\n2.python3 web_service.py\r\n\r\n通过post请求预测接口，同一张图片的预测结果和tools/infer/predict_system.py预测结果不一致，相差在20%\r\n\r\n请问这个问题该如何解决",
        "state": "closed",
        "user": "Yinyihang857",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-21T18:55:32+00:00",
        "updated_at": "2024-04-16T09:06:13+00:00",
        "closed_at": "2024-04-16T09:06:13+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Yinyihang857",
            "TeslaZhao",
            "Yinyihang857"
        ],
        "labels": [
            "help wanted"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1802,
        "title": "serving:0.9.0-runtime 没有发布",
        "body": "0.9.0-devel可以pull\r\n0.9.0-runtime没有这个tag",
        "state": "closed",
        "user": "zzl221000",
        "closed_by": "zzl221000",
        "created_at": "2022-05-25T04:30:29+00:00",
        "updated_at": "2022-05-25T10:46:43+00:00",
        "closed_at": "2022-05-25T10:46:43+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1803,
        "title": "编译时因core不是目录而失败",
        "body": "```bash\r\n$ cmake -DPYTHON_INCLUDE_DIR=$PYTHON_INCLUDE_DIR/ \\\r\n    -DPYTHON_LIBRARIES=$PYTHON_LIBRARIES \\\r\n    -DPYTHON_EXECUTABLE=$PYTHON_EXECUTABLE \\\r\n    -DOpenCV_DIR=../opencv4 \\ \r\n    -DWITH_OPENCV=ON \\\r\n    -DSERVER=ON ..\r\n\r\n...\r\nCMake Error: Cannot open file for write: /root/PaddleOCR/deploy/pdserving/Serving/build_server/core/Makefile.tmp\r\nCMake Error: : System Error: Not a directory\r\nCMake Error: Cannot open file for write: /root/PaddleOCR/deploy/pdserving/Serving/build_server/core/CMakeFiles/CMakeDirectoryInformation.cmake.tmp\r\nCMake Error: : System Error: Not a directory\r\n...\r\n```",
        "state": "closed",
        "user": "thep0y",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-26T01:34:13+00:00",
        "updated_at": "2024-03-05T06:51:15+00:00",
        "closed_at": "2024-03-05T06:51:15+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "编译问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1805,
        "title": "关于Serving编译失败问题",
        "body": "使用serving docker开发镜像版本：registry.baidubce.com/paddlepaddle/serving:0.8.0-devel\r\n编译安装过程中报错如下图：\r\n![image](https://user-images.githubusercontent.com/38742830/170773944-74513866-2c7d-4ebc-9cd9-540f4d73aabd.png)\r\n\r\n报错显示git clone代码失败，网络链接超时\r\n但是我单独使用git clone https://github.com/pybind/pybind11.git 是能正常拉取代码的且网络是和github链接正常。\r\n请问这种问题我能否在make之前单独把依赖程序git clone 下来吗？但是这些clone下的代码要放在那个目录下才能在make时识别到",
        "state": "closed",
        "user": "Yinyihang857",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-27T19:03:46+00:00",
        "updated_at": "2024-04-30T06:43:05+00:00",
        "closed_at": "2024-04-30T06:43:05+00:00",
        "comments_count": [
            "Yinyihang857",
            "self-discipline-zhu",
            "TeslaZhao",
            "pupubushilulu"
        ],
        "labels": [
            "编译问题"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1807,
        "title": "BUG:operator数组越界异常",
        "body": "在尝试本地服务化部署的时候，日志抛出数组越界异常\r\n![image](https://user-images.githubusercontent.com/20200721/171225535-6a64267c-bfa6-4bf7-9ebc-b1b67f84bd3d.png)\r\n代码：\r\n```\r\nmidped_batch = []\r\nfor idx in range(len(feed_batch)):\r\n    predict_res, error_code, error_info = func_timeout.func_timeout(\r\n        self._timeout,\r\n        self.process,\r\n        args=([feed_batch[idx]], typical_logid))\r\n    #midped_batch[idx].append(predict_res)\r\n    #上述写法错误，因改为如下\r\n    midped_batch.append(predict_res)\r\n```",
        "state": "closed",
        "user": "MarsOu1995",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-31T16:28:54+00:00",
        "updated_at": "2024-03-05T06:51:16+00:00",
        "closed_at": "2024-03-05T06:51:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "good first issue"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1809,
        "title": "询问PaddleServing模型是否支持模型并联",
        "body": "Paddleserving OCR例子中，web_service.py 实现了检测模型和识别模型的串联，我想实现首先输入数据通过检测模型，然后将检测结果根据判断条件分成两部分，分别送入不同的两个识别模型，即检测模型串联两个并联的识别模型，根据判断条件分支。想请教各位专家paddleserving是否支持，如何实现？",
        "state": "closed",
        "user": "Andy02070",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-06-01T08:35:29+00:00",
        "updated_at": "2024-03-05T06:51:17+00:00",
        "closed_at": "2024-03-05T06:51:17+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1810,
        "title": "[Python] [serving-gateway] Build script seems not right. 编译脚本似乎有错，1.18高版本的GO会编译不过",
        "body": "问题描述：\r\n\r\nGO 1.18.X --- 会在serving-gateway这里build不过。\r\n\r\n描述：\r\nGo版本差异造成。1.17的go， go mod vendor会自动做类似tidy的工作，不报错。1.18就不会，直接报错。\r\n\r\n<img width=\"678\" alt=\"image\" src=\"https://user-images.githubusercontent.com/97085562/172550653-27e1c34b-9ec7-4695-9a9d-3e678fd72397.png\">\r\n\r\n\r\npython/util.py这里，  应该有点问题。\r\n应该是先go mod tidy，然后再go mod vendor才对的。  \r\n\r\n其实感觉好像不需要vendor啊。 vendor是在本地mod目录下，拷贝依赖进到vendor目录。 不需要魔改vendor的话，没必要这么做。\r\n\r\n",
        "state": "closed",
        "user": "tuimeo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-06-08T06:50:02+00:00",
        "updated_at": "2024-03-05T06:51:18+00:00",
        "closed_at": "2024-03-05T06:51:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1813,
        "title": "C++Serving 是否支持 TRT 模型的直接部署？",
        "body": "1、如题：C++Serving 是否支持 TRT 模型的直接部署？\r\n2、目前 cuda10.2应该是可以支持 TensorRT 8（对 BERT 类模型加速效果好），C++Serving 好像并没有提供对于的镜像，是否支持重新编译对应的C++ Serving（还是因为有问题，所以未提供）？\r\n",
        "state": "closed",
        "user": "yang9112",
        "closed_by": "yang9112",
        "created_at": "2022-06-10T07:26:29+00:00",
        "updated_at": "2022-11-17T02:36:24+00:00",
        "closed_at": "2022-09-16T05:33:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TeslaZhao",
            "safehumeng",
            "yang9112",
            "safehumeng"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1814,
        "title": "failed to create predictor: Log_id: 0  Raise_msg: (External) ",
        "body": "这个项目中 cuda 11.2  \r\n政务问答检索式 FAQ System \r\npython web_sevice.py\r\n请求报如下错误\r\nfailed to create predictor: Log_id: 0  Raise_msg: (External) CUDA error(3), initialization error. \r\n  [Hint: cudaErrorInitializationError. The API call failed because the CUDA driver and runtime could not be initialized. ] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:243)\r\n  ClassName: LocalPredictor.load_model_config.<locals>.create_predictor_check  FunctionName: create_predictor_check\r\nKilled\r\n上课打算用来教学用的，应该怎么办？头大",
        "state": "closed",
        "user": "yangnianen",
        "closed_by": "yangnianen",
        "created_at": "2022-06-21T15:59:08+00:00",
        "updated_at": "2022-06-21T16:14:37+00:00",
        "closed_at": "2022-06-21T16:14:37+00:00",
        "comments_count": [
            "github-actions[bot]",
            "yangnianen"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1815,
        "title": "并发client请求，多线程推理丢失极少部分结果",
        "body": "client端和paddle serving server在同一docker内，4物理核\r\npaddle serving版本0.6.2\r\n```\r\n#serving server 启动参数\r\n  nohup python -m paddle_serving_server.serve \\\r\n   --model ../serving_model/rec_serving_model \\\r\n   --port 15393 \\\r\n   --use_mkl \\\r\n   --ir_optim \\\r\n   --thread 3 \r\n#client配置参数\r\n    serving_host='localhost',\r\n    serving_port='15393',\r\n    serving_rpc_timeout_ms=300000,  # 5 min\r\n```\r\n丢失了接近千分之一到万分之一的结果。多次测试丢失结果的数据随机，这些数据单独发送能正常返回结果。\r\n抓取 serving.INFO 日志，报错时间与丢失结果的时间一致\r\n```\r\nW0628 14:59:28.860103    35 baidu_rpc_protocol.cpp:256] Fail to write into fd=11 SocketId=104@127.0.0.1:40568@15393: Unknown error 1014 [1014]\r\nW0628 15:46:11.570693    36 baidu_rpc_protocol.cpp:256] Fail to write into fd=10 SocketId=106@127.0.0.1:40574@15393: Unknown error 1014 [1014]\r\nW0628 15:57:15.332007    35 baidu_rpc_protocol.cpp:256] Fail to write into fd=12 SocketId=108@127.0.0.1:40578@15393: Unknown error 1014 [1014]\r\n```\r\n请求--> gunicorn(thread： 3) <-----------> paddle_serving_server\r\n请问如何调试或者修改配置可以解决该问题。",
        "state": "closed",
        "user": "jaysontree",
        "closed_by": "jaysontree",
        "created_at": "2022-06-28T08:29:05+00:00",
        "updated_at": "2022-07-06T01:45:43+00:00",
        "closed_at": "2022-07-06T01:45:43+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1817,
        "title": "使用paddle_serving_server.serve启动后，如何使用http调用服务？",
        "body": "我根据文档https://github.com/PaddlePaddle/Serving/blob/v0.9.0/examples/C%2B%2B/PaddleOCR/ocr/README_CN.md\r\n试着部署一个环境\r\n\r\n当我使用下面这个命令启动是，是可以通过地址(127.0.0.1:9292/ocr/prediction)调用服务的\r\n`python3 ocr_web_server.py cpu`\r\n\r\n但是如果我使用下面paddle_serving_server的方式启动，要怎么使用http调用呢？\r\n`python3 -m paddle_serving_server.serve --model ocr_det_model --port 9292`\r\n\r\n如果我继续调用/ocr/prediction，返回的结果是\r\n`[127.0.0.1:9292][E1002]Fail to find method on /ocr/prediction`\r\n\r\n",
        "state": "closed",
        "user": "pierswu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-01T11:25:47+00:00",
        "updated_at": "2024-03-05T06:51:21+00:00",
        "closed_at": "2024-03-05T06:51:21+00:00",
        "comments_count": [
            "github-actions[bot]",
            "genius0182"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1816,
        "title": "build_dag_each_worker 设置为 True 且 worker_num > 1 时，只有一个进程在工作，其他进程在围观",
        "body": "请问 build_dag_each_worker 设置为 True 且 worker_num > 1 时 ， grpc-gateway 会自动做负载均衡吗？  如果不会 ，那么 build_dag_each_worker 为 True 的意义是什么？ 如果会，那么该怎样才能让多个进程同时工作？",
        "state": "closed",
        "user": "shengzhou1216",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-06-29T10:04:33+00:00",
        "updated_at": "2024-11-27T08:19:08+00:00",
        "closed_at": "2024-03-05T06:51:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "wujushan",
            "wang-tf"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1818,
        "title": "请问有golang的版本吗？",
        "body": null,
        "state": "closed",
        "user": "sinhang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-03T03:45:56+00:00",
        "updated_at": "2024-03-05T06:51:22+00:00",
        "closed_at": "2024-03-05T06:51:22+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1819,
        "title": "TypeError: catching classes that do not inherit from BaseException is not allowed",
        "body": "python 3.8 使用C++的方式部署，然后用httpclient报错：\r\n\r\n```\r\ntime to cost :0.007140398025512695 seconds\r\nException ignored in: <function HttpClient.__del__ at 0x7fb8fb38f0d0>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/paddle_serving_client/httpclient.py\", line 583, in __del__\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/requests/sessions.py\", line 737, in close\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/requests/adapters.py\", line 326, in close\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/urllib3/poolmanager.py\", line 223, in clear\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/urllib3/_collections.py\", line 100, in clear\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/urllib3/poolmanager.py\", line 174, in <lambda>\r\n  File \"/root/anaconda3/envs/d2l/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 498, in close\r\nTypeError: catching classes that do not inherit from BaseException is not allowed\r\n```",
        "state": "closed",
        "user": "w5688414",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-04T03:11:51+00:00",
        "updated_at": "2024-04-16T09:06:13+00:00",
        "closed_at": "2024-04-16T09:06:13+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1821,
        "title": "单机多卡推理data race问题",
        "body": "# 环境\r\ndocker: `paddlepaddle/paddle:latest-dev-cuda11.4.1-cudnn8-gcc82     `\r\nGPU:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro RTX 8000     On   | 00000000:12:00.0 Off |                    0 |\r\n| N/A   35C    P0    57W / 250W |   7507MiB / 45556MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro RTX 8000     On   | 00000000:48:00.0 Off |                    0 |\r\n| N/A   33C    P0    57W / 250W |   7507MiB / 45556MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Quadro RTX 8000     On   | 00000000:89:00.0 Off |                    0 |\r\n| N/A   32C    P0    59W / 250W |   6966MiB / 45556MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Quadro RTX 8000     On   | 00000000:C1:00.0 Off |                    0 |\r\n| N/A   33C    P0    58W / 250W |   6966MiB / 45556MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n\r\n```\r\nos:\r\n```\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.6 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n```\r\n# config.yml\r\n```yml\r\nop:\r\n    det:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 32 # 单卡: 8\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n            #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #det模型路径\r\n            model_config: ./ppocr_det_v3_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准，不设置默认取全部输出变量\r\n            #fetch_list: [\"sigmoid_0.tmp_0\"]\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0,1,2,3\"\r\n\r\n            ir_optim: True\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 16 # 单卡: 4\r\n\r\n        #超时时间, 单位ms\r\n        timeout: -1\r\n\r\n        #Serving交互重试次数，默认不重试\r\n        retry: 1\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            model_config: ./ppocr_rec_v3_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准, 不设置默认取全部输出变量\r\n            #fetch_list: \r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0,1,2,3\"\r\n\r\n            ir_optim: True\r\n\r\n```\r\n\r\n\r\n单卡的并发数设置的是: det: 8, rec:4 。 按照文档，4开的并发数设置的是: det:32, rec:4\r\n\r\n# 日志\r\n```\r\nW0706 10:17:39.483583 30589 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483031 30595 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483023 30596 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482869 29609 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483136 30545 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483281 30508 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482638 30588 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483786 30119 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483019 30584 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482604 30602 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483140 30590 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482591 30585 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.491717 29285 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483007 30004 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482746 29063 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.496740 29755 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483481 30266 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482864 30605 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483136 30591 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482599 30592 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.483502 30593 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.497339 30586 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.482615 30587 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.512519 30578 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:39.520423 30583 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:48.994534 30608 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:49.019855 30598 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:47.971360 30591 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.461241 30594 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.470719 29609 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.554875 30605 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.606678 30595 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.610824 30604 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:52.651865 30609 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:17:47.971288 30597 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:24.647740 30622 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:24.647660 30595 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:57.900487 30596 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.545289 30612 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.711550 30508 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.920372 30605 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.923874 30644 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.947504 30622 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.983484 30606 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.984248 30266 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.987569 30604 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:59.995507 30597 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:00.007701 30631 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:00.143644 30624 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:18:24.603538 30590 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:02.079862 30609 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:02.042539 28151 gpu_context.cc:278] Please NOTE: device: 3, GPU Compute Capability: 7.5, Driver API Version: 11.4, Runtime API Version: 11.1\r\nW0706 10:19:42.803803 30597 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:42.815570 30583 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:41.371565 30625 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:41.009637 29285 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:41.015522 30004 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:42.858319 30612 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:42.775671 30623 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:48.490499 30624 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:41.331369 30611 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:37.311604 30629 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:55.682211 30604 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:58.299429 30592 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:58.258342 30624 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:57.263528 30613 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:57.268218 30119 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:19:55.677124 30630 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:00.155541 30589 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:00.151526 30591 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:00.156126 30600 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:08.018196 30600 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:07.848265 29609 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:06.955668 30596 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:08.138383 30591 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:08.143868 30644 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:07.972570 30588 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:06.955510 30586 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:08.452530 30645 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:24.970340 28072 gpu_context.cc:278] Please NOTE: device: 2, GPU Compute Capability: 7.5, Driver API Version: 11.4, Runtime API Version: 11.1\r\nW0706 10:20:30.316375 28151 gpu_context.cc:306] device: 3, cuDNN Version: 8.2.\r\nW0706 10:20:38.178418 30612 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:43.764528 30628 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:43.790513 30644 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:20:45.343868 28072 gpu_context.cc:306] device: 2, cuDNN Version: 8.2.\r\nW0706 10:20:51.734341 30601 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\nW0706 10:21:00.231284 30631 sampler.cpp:139] bvar is busy at sampling for 2 seconds!\r\n```",
        "state": "closed",
        "user": "shengzhou1216",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-06T10:29:22+00:00",
        "updated_at": "2024-04-16T09:06:14+00:00",
        "closed_at": "2024-04-16T09:06:14+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1820,
        "title": "Servering C++ 编译失败",
        "body": "\r\n# 环境\r\ndocker images: `paddlepaddle/paddle:latest-dev-cuda11.4.1-cudnn8-gcc82 `\r\n\r\n# 复现\r\n按照 [如何编译PaddleServing](https://github.com/PaddlePaddle/Serving/blob/v0.8.3/doc/Compile_CN.md#%E6%AD%A3%E5%BC%8F%E7%BC%96%E8%AF%91)文档，在 [编译paddle-serving-server](https://github.com/PaddlePaddle/Serving/blob/v0.8.3/doc/Compile_CN.md#%E7%BC%96%E8%AF%91paddle-serving-server)步骤中 `make -j20` 这一步出错.\r\n\r\n> 编译时，设置了代理，可以从Github上拉取资源\r\n\r\n# 错误信息\r\n```\r\nCMake Error at extern_paddle-stamp/download-extern_paddle.cmake:159 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Uses proxy env variable http_proxy == 'http://10.171.60.50:7890'\r\n    Trying 10.171.60.50:7890...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 10.171.60.50 (10.171.60.50) port 7890 (#0)\r\n\r\n  GET\r\n  http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU//paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-inference-lib.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.65.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         \r\n    \r\n\r\n\r\nCMakeFiles/extern_paddle.dir/build.make:90: recipe for target 'third_party/Paddle/src/extern_paddle-stamp/extern_paddle-download' failed\r\nmake[2]: *** [third_party/Paddle/src/extern_paddle-stamp/extern_paddle-download] Error 1\r\nCMakeFiles/Makefile2:487: recipe for target 'CMakeFiles/extern_paddle.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_paddle.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```",
        "state": "closed",
        "user": "shengzhou1216",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-06T04:12:46+00:00",
        "updated_at": "2024-11-26T06:41:08+00:00",
        "closed_at": "2024-11-26T06:41:08+00:00",
        "comments_count": [
            "shengzhou1216",
            "pupubushilulu",
            "diana-pwf",
            "v587xpt"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1822,
        "title": "昇腾310 编译失败",
        "body": "按照下面的文档使用docker环境操作，用代理和不用代理都试过了，都会失败\r\nhttps://github.com/PaddlePaddle/Serving/blob/develop/doc/Run_On_NPU_CN.md\r\n\r\n报错如下：\r\n[ 35%] Performing configure step for 'extern_prometheus'\r\nCMake Error at /workspace/Serving-develop/build-server-npu/third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-configure-RelWithDebInfo.cmake:16 (message):\r\n  Command failed: 1\r\n\r\n   '/usr/bin/cmake' '-DCMAKE_CXX_COMPILER=/usr/bin/c++' '-DCMAKE_C_COMPILER=/usr/bin/cc' '-DCMAKE_C_FLAGS= -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -Wno-error=literal-suffix -Wno-error=sign-compare -Wno-error=unused-local-typedefs -Wno-error=ignored-attributes -Wno-error=terminate -Wno-error=int-in-bool-context -Wimplicit-fallthrough=0 -Wno-error=maybe-uninitialized' '-DCMAKE_C_FLAGS_DEBUG=-g' '-DCMAKE_C_FLAGS_RELEASE=-O3 -DNDEBUG' '-DCMAKE_CXX_FLAGS= -std=c++11 -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -Wnon-virtual-dtor -Wdelete-non-virtual-dtor -Wno-unused-parameter -Wno-unused-function -Wno-error=literal-suffix -Wno-error=sign-compare -Wno-error=unused-local-typedefs -Wno-error=ignored-attributes -Wno-error=terminate -Wno-error=int-in-bool-context -Wimplicit-fallthrough=0 -Wno-error=maybe-uninitialized' '-DCMAKE_CXX_FLAGS_RELEASE= -O3 -Wall' '-DCMAKE_CXX_FLAGS_DEBUG= -O0 -Wall -g2 -ggdb' '-DCMAKE_INSTALL_PREFIX:PATH=/workspace/Serving-develop/build-server-npu/third_party/install/prometheus' '-DCMAKE_INSTALL_LIBDIR=/workspace/Serving-develop/build-server-npu/third_party/install/prometheus/lib' '-DCMAKE_BUILD_TYPE:STRING=RelWithDebInfo' '-DBUILD_SHARED_LIBS=OFF' '-DENABLE_PUSH=OFF' '-DENABLE_COMPRESSION=OFF' '-DENABLE_TESTING=OFF' '-GUnix Makefiles' '/workspace/Serving-develop/build-server-npu/third_party/prometheus/src/extern_prometheus'\r\n\r\n  See also\r\n\r\n    /workspace/Serving-develop/build-server-npu/third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-configure-*.log\r\n\r\n\r\nCMakeFiles/extern_prometheus.dir/build.make:106: recipe for target 'third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-configure' failed\r\nmake[2]: *** [third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-configure] Error 1\r\nCMakeFiles/Makefile2:362: recipe for target 'CMakeFiles/extern_prometheus.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_prometheus.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n-- Using src='http://paddle-serving.bj.bcebos.com/inferlib/2.3.0/cxx_c/Linux/ASCEND/arm64_gcc7.5_openblas_lite2.10/paddle_inference_install_dir.tgz'\r\nCMake Error at extern_paddle-stamp/download-extern_paddle.cmake:157 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/2.3.0/cxx_c/Linux/ASCEND/arm64_gcc7.5_openblas_lite2.10/paddle_inference_install_dir.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 127.0.0.1...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to 127.0.0.1 (127.0.0.1) port 1080 (#0)\r\n\r\n  GET\r\n  http://paddle-serving.bj.bcebos.com/inferlib/2.3.0/cxx_c/Linux/ASCEND/arm64_gcc7.5_openblas_lite2.10/paddle_inference_install_dir.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-serving.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.58.0\r\n\r\n  Accept: */*\r\n\r\n  Proxy-Connection: Keep-Alive\r\n\r\n  \r\n\r\n  The requested URL returned error: 503 Service Unavailable\r\n\r\n  stopped the pause stream!\r\n\r\n  Closing connection 0\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n",
        "state": "closed",
        "user": "20102710038",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-06T11:41:55+00:00",
        "updated_at": "2025-02-27T10:51:56+00:00",
        "closed_at": "2024-03-05T06:51:22+00:00",
        "comments_count": [
            "github-actions[bot]",
            "kegehe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1823,
        "title": "没有cuda10.2-cudnn8-TensorRT8  的serving安装包吗？",
        "body": "设备jetson nano 2g jetpack4.6.1\r\n没有cuda10.2-cudnn8-TensorRT8  的serving安装包吗？",
        "state": "closed",
        "user": "jo-dean",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-07T09:43:45+00:00",
        "updated_at": "2024-03-05T06:51:23+00:00",
        "closed_at": "2024-03-05T06:51:23+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1826,
        "title": "paddleServing 可以把s2anet和ocr识别串联起来不",
        "body": "pipline的方式要怎么写对应的op和配置",
        "state": "closed",
        "user": "LeeLear",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-15T07:16:06+00:00",
        "updated_at": "2024-04-16T09:06:15+00:00",
        "closed_at": "2024-04-16T09:06:15+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1824,
        "title": "方向分类器可以部署吗",
        "body": null,
        "state": "closed",
        "user": "LeeLear",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-08T01:36:53+00:00",
        "updated_at": "2024-03-05T06:51:24+00:00",
        "closed_at": "2024-03-05T06:51:24+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1825,
        "title": "docker部署paddle的问题",
        "body": "运行\r\ndocker run --gpus all -p 9292:9292 --name test -dit registry.baidubce.com/paddlepaddle/serving:latest-cuda10.2-cudnn8-devel\r\ndocker exec -it test bash\r\n报错\r\nunable to find image 'regitstry.baiducw.com/paddlepaddle/serving:latest-cuda10.2-cudnn8-devel' localy docker: Error response from daemon: Got https://registry.baiduce.com/v2/: dial tcp: lookup registry.baiduce.com:no such host",
        "state": "closed",
        "user": "sweetboxwwy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-08T02:20:06+00:00",
        "updated_at": "2024-03-05T06:51:25+00:00",
        "closed_at": "2024-03-05T06:51:25+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1827,
        "title": "学习部署模型uci_housing_model 提示没有找到模型",
        "body": "python3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292\r\n\r\n/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n\r\nGoing to Run Comand\r\n/home/naldo/.local/lib/python3.6/site-packages/paddle_serving_server/serving-gpu-102-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9292 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 10 -port 9292 -precision fp32 -use_calib=False -reload_interval_s 10 -resource_path workdir_9292 -resource_file resource.prototxt -workflow_path workdir_9292 -workflow_file workflow.prototxt -bthread_concurrency 10 -max_body_size 536870912\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000 19397 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 19397 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 19397 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 19397 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 19397 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 19397 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 19397 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\n\r\n访问的时候提示\r\n[127.0.1.1:9292][E1002]Fail to find method on `/uci/prediction'",
        "state": "closed",
        "user": "thehzzz",
        "closed_by": "thehzzz",
        "created_at": "2022-07-16T02:25:30+00:00",
        "updated_at": "2022-07-22T03:26:08+00:00",
        "closed_at": "2022-07-22T03:26:08+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1828,
        "title": "macos上无法安装serving client whl包",
        "body": "![image](https://user-images.githubusercontent.com/6971044/179549954-4ac0fc2a-54b9-4924-9484-824d4133cf40.png)\r\n如上",
        "state": "closed",
        "user": "frankxyy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-18T15:44:49+00:00",
        "updated_at": "2024-03-05T06:51:26+00:00",
        "closed_at": "2024-03-05T06:51:26+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1829,
        "title": "PaddleServing服务化部署后，服务端和客户端隔几天不定期的报grpc链接超时复位的错误，导致服务不可用",
        "body": "由于已经上生产环境，需要尽快解决，谢谢\r\n\r\n# 环境：\r\nregistry.baidubce.com/paddlepaddle/serving:0.8.3-cuda10.1-cudnn7-runtime作为基础镜像\r\n由于要更改的grpc版本，grpcio==1.37.1和grpcio-tools==1.37.1\r\n如下的三个包是定制的\r\npaddle_serving_app-0.8.3-py3-none-any.whl\r\npaddle_serving_client-0.8.3-cp37-none-any.whl\r\npaddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl\r\n\r\n# 报错\r\nERROR 2022-07-21 03:15:39,100 [app.py:1892] Exception on /nlp/v1/company_policy/policy_res [POST]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1950, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1936, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py\", line 467, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/views.py\", line 89, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py\", line 582, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"/deploy/main_api.py\", line 53, in post\r\n    top_k_)\r\n  File \"/deploy/semantics_match.py\", line 103, in simi_search\r\n    ret = self.client.predict(feed_dict=feed)\r\n  File \"/usr/local/lib/python3.7/site-packages/paddle_serving_server/pipeline/pipeline_client.py\", line 202, in predict\r\n    resp = self._stub.inference(req)\r\n  File \"/usr/local/lib/python3.7/site-packages/grpc/_channel.py\", line 946, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/usr/local/lib/python3.7/site-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\r\n    raise _InactiveRpcError(state)\r\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.UNAVAILABLE\r\n\tdetails = \"Connection reset by peer\"\r\n\tdebug_error_string = \"{\"created\":\"@1658373339.099364704\",\"description\":\"Error received from peer ipv4:193.168.57.222:30088\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1067,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\"\r\n\r\n# 寻找问题根源\r\n和这个链接描述的一样：http://longfan.me/post/devops/2020-07-09\r\n\r\n`sysctl net.ipv4.tcp_keepalive_time net.ipv4.tcp_keepalive_probes net.ipv4.tcp_keepalive_intvl`\r\nnet.ipv4.tcp_keepalive_time = 7200\r\nnet.ipv4.tcp_keepalive_probes = 9\r\nnet.ipv4.tcp_keepalive_intvl = 75\r\n\r\n`ipvsadm -l --timeout`\r\nTimeout (tcp tcpfin udp): 900 120 300\r\n\r\n由于900<7200+9x75所以k8svip会超时复位grpc的服务端和客户端的长链接\r\n\r\n# 问题\r\n参考链接的描述http://longfan.me/post/devops/2020-07-09，\r\n因为grpc的包是内嵌在paddle_serving_app-0.8.3-py3-none-any.whl、\r\npaddle_serving_client-0.8.3-cp37-none-any.whl、paddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl三个包里面，如何通过代码实现grpc本身的超时设置时间，使得服务端和客户端的超时设置小于系统ipvs时间900？还是需要更改paddleServing底层代码重新定制这三个包？\r\n",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-22T02:32:40+00:00",
        "updated_at": "2024-04-16T09:06:16+00:00",
        "closed_at": "2024-04-16T09:06:16+00:00",
        "comments_count": [
            "AI-Mart"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1830,
        "title": "clinet请求服务，得不到响应，服务端日志无报错",
        "body": "![image](https://user-images.githubusercontent.com/6971044/180681452-a6a4f776-52f1-4332-96e0-fd43f84c1f9f.png)\r\npipeline.log日志卡在这, pipeline.log.wf无报错日志",
        "state": "closed",
        "user": "frankxyy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-25T02:29:11+00:00",
        "updated_at": "2024-04-16T09:06:17+00:00",
        "closed_at": "2024-04-16T09:06:17+00:00",
        "comments_count": [
            "frankxyy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1832,
        "title": "重新编译serving后 有些op无法正常加载",
        "body": "重新编译了serving，没有做任何更改，同样使用的0.9.0版本，使用同样的命令启动serving，但自己编译的serving不能正常加载某些op\r\n下图是使用正常的官方提供的serving\r\n![微信图片_20220804093045](https://user-images.githubusercontent.com/62418900/182743684-80cc9919-6727-4d14-9e38-11758eec75bc.png)\r\n\r\n下图是自己编译的serving\r\n![微信图片_20220804093129](https://user-images.githubusercontent.com/62418900/182743765-4cb2e2de-dafd-4446-8d17-9f0d6a9b6adc.png)\r\n可以看到自己编译的serving 中--op Generalpicodetop GeneralFeatureExtractop没有正常加载。\r\n",
        "state": "closed",
        "user": "mcl-stone",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-04T01:32:53+00:00",
        "updated_at": "2024-04-16T09:06:18+00:00",
        "closed_at": "2024-04-16T09:06:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1831,
        "title": "DockerFile报错，找不到setup.py",
        "body": "我按照教程描述，自制了一个DockerFile,但是报错，找不到setup.py，可以帮忙看一下吗？\r\n![image](https://user-images.githubusercontent.com/101552273/182146397-74c161f2-6962-4ea6-9d53-eeeac0e06e1f.png)\r\n\r\n```\r\n#Serving开发镜像\r\nFROM registry.baidubce.com/paddlepaddle/serving:0.9.0-devel\r\nWORKDIR /\r\nRUN git clone --depth 1 https://github.com/PaddlePaddle/Serving\r\nWORKDIR /Serving\r\nRUN pip3 install -r python/requirements.txt\r\n#2.1 在线安装\r\nRUN pip3 install paddle-serving-client==0.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\nRUN pip3 install paddle-serving-app==0.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n# CPU Server\r\nRUN pip3 install paddle-serving-server==0.9.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n# CPU环境请执行\r\nRUN pip3 install paddlepaddle==2.3.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n#安装离线 Wheel 包\r\nRUN wget https://paddle-serving.bj.bcebos.com/offline_wheels/0.9.0/py37_offline_whl_packages.tar\r\n#RUN wget https://paddle-serving.bj.bcebos.com/offline_wheels/0.9.0/py36_offline_whl_packages.tar\r\n#通过运行 install.py 脚本可本地安装 Serving 和 Paddle Wheel 包。install.py 脚本的参数列表如下：\r\nWORKDIR /Serving/python\r\nRUN python3 setup.py install  \\\r\n --python_version : py37 \\\r\n --device : cpu \\ \r\n --serving_version : 0.9.0 \\\r\n--paddle_version : 2.3.0\r\n\r\nRUN python3 -m paddle_serving_server.serve check\r\nEXPOSE 9292 \r\n```",
        "state": "closed",
        "user": "zhaobinchen",
        "closed_by": "zhaobinchen",
        "created_at": "2022-08-01T12:21:36+00:00",
        "updated_at": "2022-08-01T12:42:57+00:00",
        "closed_at": "2022-08-01T12:42:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zhaobinchen"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1833,
        "title": "float16数据类型支持问题",
        "body": "我们在使用paddle serving并适配自定义硬件类型时，对C++ serving中的float16数据类型有一些问题/疑问：\r\n\r\n以`examples/C++/PaddleClas/imagenet`中resnet50 为例\r\n\r\n### client输入float16数据类型问题：\r\n\r\nclient端对实际的numpy输入并没有做检查/转数，如果prototxt中定义了输入输出类型为float16，但是实际输入采用了numpy.float32，会出现精度问题。\r\n\r\nprototxt定义：\r\n```\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 5\r\n  shape: 3\r\n  shape: 224\r\n  shape: 224\r\n}\r\nfetch_var {\r\n  name: \"softmax_0.tmp_0\"\r\n  alias_name: \"score\"\r\n  is_lod_tensor: false\r\n  fetch_type: 5\r\n  shape: 1000\r\n}\r\n```\r\n\r\n实际代码调用：\r\n```\r\n    img = seq(image_file) # img is float32 numpy array here.\r\n    fetch_map = client.predict(\r\n        feed={\"image\": img}, fetch=[\"score\"], batch=False)\r\n```\r\n在client.py中，会根据proto定义的float16数据类型，把`img`转为string数据\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/cf9ad1d9d6667974ecbff6917b0d74c11a25109d/python/paddle_serving_client/client.py#L438\r\n\r\ngeneral_model.cpp中把string数据设置到tensor中，并在之后向server传输。\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/cf9ad1d9d6667974ecbff6917b0d74c11a25109d/core/general-client/src/general_model.cpp#L335\r\n\r\n但是，img本身为float32数据类型，其内存大小为float16的二倍，不能没有经过转数直接传递到server，否则paddle构建出来的tensor数据错误的：\r\ngdb可以看到输入类型指定为float16时，内存大小却仍然是4*3*224*224=602112\r\n```\r\n(gdb) p string_feed[vec_idx].size()\r\n$4 = 602112\r\n(gdb) p string_shape[vec_idx]\r\n$5 = std::vector of length 3, capacity 3 = {3, 224, 224}\r\n(gdb) p 3*224*224\r\n$6 = 150528\r\n```\r\n\r\n### client float16输出问题：\r\n上述案例中，用户代码对输入做转数操作，则会出现pybind解析错误，client代码示例：\r\n```\r\n    img = seq(image_file) # img is float32 numpy array here.\r\n    fetch_map = client.predict(\r\n        feed={\"image\": img.astype(np.float16)}, fetch=[\"score\"], batch=False) # convert img to float16\r\n```\r\n使用报错：\r\n```\r\n'utf-8' codec can't decode byte\r\n```\r\n\r\n因为float16的数据是通过string在client/server中传递的，pybind中要求对C++传递到python端的string数据需要能够被utf-8 decode. 除非用户显示指定返回`py::bytes`不做转换。\r\n\r\n因此，以下代码是不是需要改为：`return py::bytes(self.get_string_by_name_with_rv(model_idx, name));`\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/cf9ad1d9d6667974ecbff6917b0d74c11a25109d/core/general-client/src/pybind_general_model.cpp#L63\r\n\r\npybind参考：https://github.com/pybind/pybind11/blob/master/docs/advanced/cast/strings.rst\r\n\r\n另外想问下，我们有可供参考的float16运行案例吗？",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-04T07:40:34+00:00",
        "updated_at": "2024-04-16T09:06:19+00:00",
        "closed_at": "2024-04-16T09:06:19+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "czr-gc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1834,
        "title": "Serving采用docker部署的方式，返回的结果是空的",
        "body": "![image](https://user-images.githubusercontent.com/20657139/183828094-2a28e6f6-f70b-496e-96ed-86955dc789c0.png)\r\n采用的是docker的方式进行部署的**registry.baidubce.com/paddlepaddle/serving:0.8.3-cuda10.1-cudnn7-runtime**\r\n配置文件config.xml的内容如下：\r\n```\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 18090\r\n\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 9999\r\n\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n##当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 10\r\n\r\n#build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\nbuild_dag_each_worker: False\r\n\r\ndag:\r\n    #op资源类型, True, 为线程模型；False，为进程模型\r\n    is_thread_op: False\r\n\r\n    #重试次数,默认是10\r\n    retry: 1\r\n\r\n    #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n    use_profile: False\r\n    \r\n    tracer:\r\n        interval_s: 10\r\nop:\r\n    det:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 4\r\n        \r\n        #Serving交互重试次数，默认不重试\r\n        #retry: 3\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n            #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #det模型路径\r\n            model_config: ./inference/ppocrv3_det_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准，不设置默认取全部输出变量\r\n            fetch_list: [\"sigmoid_0.tmp_0\"]\r\n            \r\n            # 批量查询Serving的数量, 默认1。batch_size>1要设置auto_batching_timeout，否则不足batch_size时会阻塞\r\n            # batch_size: 16\r\n            # 批量查询超时，与batch_size配合使用\r\n            # auto_batching_timeout: 2000\r\n            \r\n\r\n            # device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu, 5=arm ascend310, 6=arm ascend910\r\n            #device_type: 2\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0\"\r\n            \r\n            # use_mkldnn, 开启mkldnn时，必须同时设置ir_optim=True，否则无效\r\n            # use_mkldnn: True\r\n            \r\n            # thread_num: 2   \r\n            # ir_optim, 开启TensorRT时，必须同时设置ir_optim=True，否则无效\r\n            ir_optim: True\r\n\r\n            # precsion, 预测精度，降低预测精度可提升预测速度\r\n            # GPU 支持: \"fp32\"(default), \"fp16\", \"int8\"\r\n            # precision: \"fp32\"\r\n\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 2\r\n\r\n        #超时时间, 单位ms\r\n        timeout: -1\r\n \r\n        #Serving交互重试次数，默认不重试\r\n        #retry: 3\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            model_config: ./inference/ppocrv3_rec_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准, 不设置默认取全部输出变量\r\n            fetch_list: [\"softmax_5.tmp_0\"]\r\n            \r\n            # 批量查询Serving的数量, 默认1。batch_size>1要设置auto_batching_timeout，否则不足batch_size时会阻塞\r\n            # batch_size: 16\r\n            # 批量查询超时，与batch_size配合使用\r\n            # auto_batching_timeout: 2000\r\n            \r\n            \r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0\"\r\n\r\n            #device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu, 5=arm ascend310, 6=arm ascend910\r\n            #device_type: 2\r\n\r\n            # use_mkldnn, 开启mkldnn时，必须同时设置ir_optim=True，否则无效\r\n            # use_mkldnn: True\r\n            \r\n            # thread_num: 2\r\n            ir_optim: True\r\n\r\n            # precsion, 预测精度，降低预测精度可提升预测速度\r\n            # GPU 支持: \"fp32\"(default), \"fp16\", \"int8\"\r\n            # precision: \"fp32\"\r\n```",
        "state": "closed",
        "user": "xiulianzw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-10T06:14:06+00:00",
        "updated_at": "2024-03-05T06:51:27+00:00",
        "closed_at": "2024-03-05T06:51:27+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1835,
        "title": "docker部署的gpu版本镜像报错",
        "body": "```\r\nC++ Traceback (most recent call last):\r\n\r\nNo stack trace in paddle, may be caused by external reasons.\r\n\r\n\r\nError Message Summary:\r\n\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1660128530 (unix time) try \"date -d @1660128530\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x0) received by PID 4959 (TID 0x7f5b87012700) from PID 0 ***]\r\n```\r\n您好，请问这个问题要怎么排查\r\n",
        "state": "closed",
        "user": "whalefa1I",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-10T10:50:48+00:00",
        "updated_at": "2024-03-05T06:51:28+00:00",
        "closed_at": "2024-03-05T06:51:28+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1836,
        "title": "window 启动serving RuntimeError:     An attempt has been made to start a new process before the   ",
        "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\site-packages\\paddle_serving_server\\pipeline\\error_catch.py\", line 97, in wrapper\r\n    res = func(*args, **kw)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\site-packages\\paddle_serving_server\\pipeline\\error_catch.py\", line 163, in wrapper\r\n    result = function(*args, **kwargs)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\site-packages\\paddle_serving_server\\pipeline\\pipeline_server.py\", line 51, in init_helper\r\n    self._dag_executor = dag.DAGExecutor(response_op, dag_conf, worker_idx)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\site-packages\\paddle_serving_server\\pipeline\\dag.py\", line 86, in __init__\r\n    self._is_thread_op, tracer_interval_s, server_worker_num)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\site-packages\\paddle_serving_server\\pipeline\\profiler.py\", line 45, in __init__\r\n    self._data_buffer = multiprocessing.Manager().Queue()\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\context.py\", line 56, in Manager\r\n    m.start()\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\managers.py\", line 563, in start\r\n    self._process.start()\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\popen_spawn_win32.py\", line 46, in __init__\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"C:\\Users\\CDROG\\paddle_recaptcha\\env\\lib\\multiprocessing\\spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\nClassname: PipelineServicer.__init__.<locals>.init_helper\r\nFunctionName: init_helper",
        "state": "closed",
        "user": "litter-rabbit",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-12T02:43:21+00:00",
        "updated_at": "2024-03-05T06:51:28+00:00",
        "closed_at": "2024-03-05T06:51:28+00:00",
        "comments_count": [
            "github-actions[bot]",
            "w5688414"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1837,
        "title": "请问Serving0.6显存不释放issue#767新版本解决了嘛",
        "body": "registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda10.2-cudnn8-devel的\r\nServing0.6执行完多次推理后，闲时显存不释放",
        "state": "closed",
        "user": "Juruobudong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-13T08:28:36+00:00",
        "updated_at": "2024-04-16T09:06:20+00:00",
        "closed_at": "2024-04-16T09:06:20+00:00",
        "comments_count": [
            "github-actions[bot]",
            "xiulianzw",
            "Juruobudong",
            "Juruobudong",
            "xiulianzw",
            "Juruobudong",
            "xiulianzw",
            "Juruobudong",
            "Juruobudong",
            "xiulianzw",
            "Juruobudong",
            "xiulianzw",
            "Juruobudong",
            "xiulianzw",
            "Juruobudong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1839,
        "title": "同一个服务，不同时间对同一个图片预测，结果不一致",
        "body": "服务是clas detection ocr串联部署。\r\n同一张图片第一次经过服务预测的结果和第二次不一致 (少部分识别文本不同)\r\n两次识别时服务的配置、环境、机器都是一样的。\r\n两次识别期间服务因断电重启过一次\r\n均为本地预测，模型没有改变，启动命令没有改变，配置没有改变\r\n为什么会这样呢，希望得到解答！\r\n",
        "state": "closed",
        "user": "Enchanted0911",
        "closed_by": "Enchanted0911",
        "created_at": "2022-08-22T15:14:13+00:00",
        "updated_at": "2022-08-22T15:38:41+00:00",
        "closed_at": "2022-08-22T15:38:41+00:00",
        "comments_count": [
            "Enchanted0911"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1841,
        "title": "异步接口支持指定固定batch size吗？",
        "body": "看了异步接口batch相关的文档和代码，`--batch_infer_size 32`参数应该只能保证每次infer的batch不超过32，有什么方法能够让每次infer的batch是固定的32吗？",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-25T06:29:39+00:00",
        "updated_at": "2024-04-16T09:06:21+00:00",
        "closed_at": "2024-04-16T09:06:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1840,
        "title": "python pipeline serving 启动报error，但是服务是正常的",
        "body": "环境 linux\r\npaddlepaddle 2.3.0\r\npaddleserving 0.9.0\r\ncuda 11.3 cudnn 8.2.1 tensorRT 8.0.1.6\r\n\r\n报错信息：\r\n[convolutionRunner.cpp::executeConv::458] Error Code 1: Cudnn (CUDNN_STATUS_BAD_PARAM)\r\n![serving-err](https://user-images.githubusercontent.com/65162523/186556887-2bc6a705-55ad-4b68-bed0-2275a975cea2.png)\r\n\r\n\r\n但是我调用这个服务，服务是能正常预测和响应的。\r\n希望得到解答！",
        "state": "closed",
        "user": "Enchanted0911",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-25T01:59:56+00:00",
        "updated_at": "2024-03-05T06:51:29+00:00",
        "closed_at": "2024-03-05T06:51:29+00:00",
        "comments_count": [
            "Enchanted0911",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1842,
        "title": "FR: 是否可以增加feed 传递文件路径的功能",
        "body": "目前想用c++ 版本的Serving 来提升性能,但是目前测试使用python 对文件进行预处理的性能并不理想\r\n因此想在OP中读取本地图片,并进行预处理\r\n目前我们推理图片都会预先传输到推理服务器上, 因此是否可以增加客户端对文件路径的支持",
        "state": "closed",
        "user": "TaChao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-26T10:10:18+00:00",
        "updated_at": "2024-03-05T06:51:30+00:00",
        "closed_at": "2024-03-05T06:51:30+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1843,
        "title": "请问java demo里面怎么加mean和std",
        "body": "java里面怎么加这两个东西\r\nprotected float[] inputMean = new float[]{0.485f, 0.456f, 0.406f};\r\nprotected float[] inputStd = new float[]{0.229f, 0.224f, 0.225f};",
        "state": "closed",
        "user": "zjyhll",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-29T09:48:07+00:00",
        "updated_at": "2024-03-05T06:51:31+00:00",
        "closed_at": "2024-03-05T06:51:31+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1844,
        "title": "0.9.0版本windowsclient端：No module named 'paddle_serving_client.serving_client'",
        "body": null,
        "state": "closed",
        "user": "hq0749a",
        "closed_by": "hq0749a",
        "created_at": "2022-08-31T06:32:29+00:00",
        "updated_at": "2022-10-19T08:55:00+00:00",
        "closed_at": "2022-10-19T08:55:00+00:00",
        "comments_count": [
            "hq0749a"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1845,
        "title": "win10启动paddleseg的模型服务报错",
        "body": "WIN10执行 python -m paddle_serving_server.serve --model serving_server --thread 10 --port 9292 --ir_optim\r\n报错如下\r\n\r\n\r\nle_serving_server.serve --model serving_server --thread 10 --port 9292 --ir_optim\r\nD:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'pa\r\nddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\ngpu_ids not set, going to run cpu service.\r\n子目录或文件 -p 已经存在。\r\n处理: -p 时出错。\r\n子目录或文件 workdir 已经存在。\r\n处理: workdir 时出错。\r\n命令语法不正确。\r\n'touch' 不是内部或外部命令，也不是可运行的程序\r\n或批处理文件。\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\site-packages\\paddle_serving_server\\serve.py\", line 395, in <module>\r\n    start_multi_card(args)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\site-packages\\paddle_serving_server\\serve.py\", line 278, in start_multi_card\r\n    start_gpu_card_model(-1, -1, serving_port, args)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\site-packages\\paddle_serving_server\\serve.py\", line 249, in start_gpu_card_model\r\n    use_encryption_model=args.use_encryption_model)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\site-packages\\paddle_serving_server\\server.py\", line 437, in prepare_server\r\n    self._prepare_resource(workdir, cube_conf)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\PaddleServing\\lib\\site-packages\\paddle_serving_server\\server.py\", line 227, in _prepare_resource\r\n    \"w\") as fout:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'workdir/general_infer_0/general_model.prototxt'\r\n",
        "state": "closed",
        "user": "xinyujituan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-05T09:00:43+00:00",
        "updated_at": "2024-03-05T06:51:32+00:00",
        "closed_at": "2024-03-05T06:51:32+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1846,
        "title": "请问kie推理模型可以集成到pdserving，用rpc请求预测结果吗？",
        "body": "问题：\r\n目前使用paddleocr-release2.6已部署一套pdserving，采用rpc访问，模型使用ch_PP-OCRv3_det_slim和ch_PP-OCRv3_rec_slim，现在需要识别发票图片，需要使用kie相关功能，如何能将kie模型集成到pdserving中呢？",
        "state": "closed",
        "user": "sybest1259",
        "closed_by": "sybest1259",
        "created_at": "2022-09-06T04:03:48+00:00",
        "updated_at": "2022-09-13T00:56:55+00:00",
        "closed_at": "2022-09-13T00:56:55+00:00",
        "comments_count": [
            "github-actions[bot]",
            "sybest1259"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1848,
        "title": "prometheus配置",
        "body": "我尝试使用prometheus，按照官方demo添加了对应参数，但是服务启动就立即报错了。\r\n系统 linux\r\npdserving版本: 0.9.0\r\ncuda 11.2\r\ncudnn 8.2.1\r\ntensorrt 8.0.3.4\r\npaddlepaddle 2.3\r\n以下是我的报错信息\r\n![image](https://user-images.githubusercontent.com/65162523/189322921-b53fbfeb-fdcc-4ef1-94fc-f421242c4e5a.png)\r\n以下是我添加的配置\r\n![image](https://user-images.githubusercontent.com/65162523/189323021-26c34bf5-a207-4d98-9a58-f33861c27930.png)\r\n",
        "state": "closed",
        "user": "Enchanted0911",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-09T09:50:26+00:00",
        "updated_at": "2024-03-05T06:51:33+00:00",
        "closed_at": "2024-03-05T06:51:33+00:00",
        "comments_count": [
            "Enchanted0911",
            "DreamJokerMJ"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1849,
        "title": "请问kie推理模型可以集成到pdserving，用rpc请求预测结果吗？",
        "body": "问题：\r\n目前使用paddleocr-release2.6已部署一套pdserving，采用rpc访问，模型使用ch_PP-OCRv3_det_slim和ch_PP-OCRv3_rec_slim，现在需要识别发票图片，需要使用kie相关功能，如何能将kie模型（比如SER、RE相关模型）集成到pdserving中呢？",
        "state": "closed",
        "user": "sybest1259",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-13T00:56:31+00:00",
        "updated_at": "2024-03-12T06:40:47+00:00",
        "closed_at": "2024-03-12T06:40:47+00:00",
        "comments_count": [
            "francisol",
            "danaodai",
            "Tanmay98"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1847,
        "title": "如何修改客户端的内容? 有配置文档可以参考吗? 如果想用其他model而不在example里面的时候, 客户端的py文件要如何修改?",
        "body": "如何修改Serving中的test_client.py要怎么改?可以指导一下吗?https://github.com/PaddlePaddle/PaddleGAN/blob/develop/deploy/serving/test_client.py",
        "state": "closed",
        "user": "timousT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-07T01:55:37+00:00",
        "updated_at": "2024-03-05T06:51:33+00:00",
        "closed_at": "2024-03-05T06:51:32+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1850,
        "title": "paddle.inference开启TensorRT量化加速推理报错",
        "body": "基于文本4分类的bert模型，然后通过x2paddle转化为paddle的模型，使用CPU和原生GPU推理都是正常的，启用tensorrt量化加速，出现如下错误：ValueError: (InvalidArgument) After flatten the input tensor X and Y to 2-D dimensions matrix X1 and Y1, the matrix X1's width must be equal with matrix Y1's height. But received X's shape = [1, 1, 768], X1's shape = [1, 768], X1's width = 768; Y's shape = [4, 768], Y1's shape = [4, 768], Y1's height = 4.[Hint: Expected x_mat_dims[1] == y_mat_dims[0], but received x_mat_dims[1]:768 != y_mat_dims[0]:4.] (at /paddle/paddle/phi/infermeta/binary.cc:1460) [operator < mul > error]",
        "state": "closed",
        "user": "bowencarry",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-15T08:45:36+00:00",
        "updated_at": "2024-03-05T06:51:35+00:00",
        "closed_at": "2024-03-05T06:51:35+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1851,
        "title": "如何提高 paddle serving 转 TRT 模型效率",
        "body": "使用 paddle serving (Pipeline 模式) 转TRT模型的时候，转换速度非常慢（大概需要几分钟），且打印大量的：\r\n\r\nTry increasing the workspace size to 4194304 bytes to get better performance.\r\n\r\nconfig.yml配置如下：\r\n```\r\n    local_service_conf:\r\n      # client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n      client_type: local_predictor\r\n      #ir_optim\r\n      ir_optim: True\r\n      # device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n      device_type: 2\r\n      # 计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n      devices: '0'\r\n      # Fetch结果列表，以client_config中fetch_var的alias_name为准, 如果没有设置则全部返回\r\n      # fetch_list: \"output\"\r\n      pricision: \"fp16\"\r\n      # 模型路径\r\n      model_config: serving_server\r\n```",
        "state": "closed",
        "user": "yang9112",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-15T11:18:29+00:00",
        "updated_at": "2024-04-16T09:06:21+00:00",
        "closed_at": "2024-04-16T09:06:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1852,
        "title": "基于HTTP 接口请求 PaddleSeving 服务，未识别 post 参数中body 的内容",
        "body": "使用以下配置启动服务，使用 http 接口请求服务时：\r\n\r\n<img width=\"980\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6162257/190552756-ef3cbf4c-ec62-4b11-a80c-631bd3c491b4.png\">\r\n\r\n查看web_server.py 中的 input_dict打印结果，发现没有传入任何参数，这个是为什么？\r\n\r\n{'@DAGExecutor': {}}\r\n\r\n```\r\n# worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n# 当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 10\r\n# build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\nbuild_dag_each_worker: false\r\n\r\ndag:\r\n  # op资源类型, True, 为线程模型；False，为进程模型\r\n  is_thread_op: False\r\n  # 使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n  tracer:\r\n    interval_s: 10\r\n\r\n# http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 18082\r\n# rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 8088\r\n\r\nop:\r\n  bert:\r\n    # 并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n    concurrency: 2\r\n\r\n    # 当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n    local_service_conf:\r\n      # client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n      client_type: local_predictor\r\n      #ir_optim\r\n      ir_optim: True\r\n      # device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n      device_type: 2\r\n      # 计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n      devices: '0'\r\n      # Fetch结果列表，以client_config中fetch_var的alias_name为准, 如果没有设置则全部返回\r\n      # fetch_list: \"output\"\r\n      pricision: \"fp16\"\r\n      # 模型路径\r\n      model_config: serving_server\r\n```",
        "state": "closed",
        "user": "yang9112",
        "closed_by": "yang9112",
        "created_at": "2022-09-16T03:51:46+00:00",
        "updated_at": "2022-12-28T09:42:18+00:00",
        "closed_at": "2022-09-16T06:26:25+00:00",
        "comments_count": [
            "yang9112",
            "XS170411105276"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1853,
        "title": "paddle serving是否已经支持paddlets",
        "body": "paddle serving是否已经支持paddlets",
        "state": "closed",
        "user": "jiangxinufo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-24T06:04:34+00:00",
        "updated_at": "2024-03-05T06:51:36+00:00",
        "closed_at": "2024-03-05T06:51:36+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1856,
        "title": "文档书写错误",
        "body": "https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Python_Pipeline/Pipeline_Design_CN.md#3.1\r\n\r\n一 网络服务层\r\n网络服务层包括了 gRPC-gateway 和 gRPC Server。gPRC gateway 接收 HTTP 请求\r\n\r\ngPRC gateway？什么👻",
        "state": "closed",
        "user": "hbo-lambda",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-27T05:40:18+00:00",
        "updated_at": "2024-04-16T09:06:23+00:00",
        "closed_at": "2024-04-16T09:06:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1854,
        "title": "serving:0.9.0-devel 无法运行",
        "body": "linux 环境\r\n拉取registry.baidubce.com/paddlepaddle/serving:0.9.0-runtime后可以正常启动\r\n拉取 registry.baidubce.com/paddlepaddle/serving:0.9.0-devel \r\ndocker run -p 9292:9292 --name paddle-test -dit registry.baidubce.com/paddlepaddle/serving:0.9.0-devel  无法正常启动\r\n\r\n",
        "state": "closed",
        "user": "kg-nlp",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-26T08:53:58+00:00",
        "updated_at": "2024-04-16T09:06:22+00:00",
        "closed_at": "2024-04-16T09:06:22+00:00",
        "comments_count": [
            "github-actions[bot]",
            "kg-nlp"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1855,
        "title": "http api url",
        "body": "paddle serving通过如下命令启动后：\r\n1.\r\nocr_rec_model ocr_det_model都是通过python -m.... 命令获得的，解压后，执行\r\n```\r\npython -m paddle_serving_server.serve --model ocr_rec_model ocr_det_model --thread 10 --port 9292\r\n```\r\n通过http的方式，客户端发请求时的url从哪儿得知呢？\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "hbo-lambda",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-27T01:04:10+00:00",
        "updated_at": "2024-05-07T06:40:53+00:00",
        "closed_at": "2024-05-07T06:40:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Jaccica",
            "Yan-Ke",
            "XS170411105276",
            "zyzz1974",
            "XS170411105276",
            "roomeo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1857,
        "title": "如何获取op的多个输出？",
        "body": "改造了Serving/core/general-server/op/general_detection_op.cpp增加了输出boxes的代码。但是client端获取不到增加的输出。请问应该怎么改prototxt或者client端实现获取多个输出？\r\n增加部分如下：\r\n```//generate boxes output\r\n    int box_size_out = box_num*4*2*sizeof(int);\r\n    void *box_data_out = MempoolWrapper::instance().malloc(box_size_out);\r\n    if (!box_data_out) {\r\n      LOG(ERROR) << \"Malloc failed, size: \" << box_size_out;\r\n      return -1;\r\n    }\r\n    \r\n    int* databuf_int = reinterpret_cast<int*>(box_data_out);\r\n    for (int i = 0; i < box_num; ++i) {\r\n      for (int row = 0; row < 4; row++){\r\n        *databuf_int = boxes[i][row][0];\r\n        databuf_int++;\r\n        *databuf_int = boxes[i][row][1];\r\n        databuf_int++;\r\n      }\r\n    }\r\n\r\n    // memcpy(databuf_data_out, &boxes, databuf_size_out);\r\n    char *box_char_out = reinterpret_cast<char*>(box_data_out);\r\n    paddle::PaddleBuf paddleBuf_out_2(box_char_out, box_size_out);\r\n    paddle::PaddleTensor tensor_out_2;\r\n\r\n    tensor_out_2.name = \"boxes\";\r\n    tensor_out_2.dtype = paddle::PaddleDType::INT32;\r\n    tensor_out_2.shape = {box_num, 4, 2};\r\n    tensor_out_2.data = paddleBuf_out_2;\r\n    out->push_back(tensor_out_2);\r\n```\r\n\r\n获取serving输出的det_cpp_client.py如下：\r\n\r\n```from paddle_serving_client import Client\r\nimport sys\r\nimport numpy as np\r\nimport base64\r\nimport os\r\nimport cv2\r\nfrom paddle_serving_app.reader import Sequential, URL2Image, ResizeByFactor\r\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\r\nfrom paddle_serving_app.reader import OCRReader\r\nfrom paddle_serving_app.reader import DBPostProcess, FilterBoxes, GetRotateCropImage, SortedBoxes\r\n\r\n\r\nclient = Client()\r\nclient.load_client_config(sys.argv[1:])\r\nclient.connect([\"127.0.0.1:9293\"])\r\nfetch_list = client.get_fetch_names()\r\nprint(fetch_list)\r\n\r\ntest_img_dir = \"imgs/\"\r\nimg_file = '1.jpg'\r\n\r\ndef cv2_to_base64(image):\r\n    return base64.b64encode(image)  #data.tostring()).decode('utf8')\r\n\r\nwith open(os.path.join(test_img_dir, img_file), 'rb') as file:\r\n    image_data = file.read()\r\n    image = cv2_to_base64(image_data)\r\n\r\ndet_out = client.predict(\r\n        feed={\"x\": image},\r\n        fetch=[\"x\",\"boxes\"],\r\n        batch=True)\r\n\r\nprint(det_out)\r\n```\r\n\r\nserving启动代码如下：\r\n\r\n`python3 -m paddle_serving_server.serve --model ocr_det_model --op GeneralDetectionOp --port 9293`\r\n\r\nclient启动代码如下：\r\n\r\n```python3 det_cpp_client.py ocr_det_client```",
        "state": "closed",
        "user": "XinyuDu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-27T06:29:16+00:00",
        "updated_at": "2024-07-09T06:41:01+00:00",
        "closed_at": "2024-07-09T06:41:01+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ooxoxx"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1858,
        "title": "一个服务包含两个模型部署时Client怎么区分调用的是哪个模型？",
        "body": "文档中提到（https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Serving_Configure_CN.md）：\r\n**当您的一个服务包含两个模型部署时.**\r\n`python3 -m paddle_serving_server.serve --model serving_model_1 serving_model_2 --thread 10 --port 9292`\r\n\r\n请问client端怎么区分调用哪个模型进行inference呢？貌似模型的serving_client_conf.prototxt里面也没有模型的名称。\r\n```\r\nclient = Client()\r\nclient.load_client_config(sys.argv[1:])\r\nclient.connect([\"127.0.0.1:9292\"])\r\nfetch_map = client.predict(feed=feed, fetch=[], batch=True)\r\n```",
        "state": "closed",
        "user": "XinyuDu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-30T09:09:59+00:00",
        "updated_at": "2024-03-05T06:51:37+00:00",
        "closed_at": "2024-03-05T06:51:37+00:00",
        "comments_count": [
            "pinngan",
            "XS170411105276"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1859,
        "title": "可以部署其他深度学习框架吗 ？",
        "body": "可以部署其他深度学习框架吗 ",
        "state": "closed",
        "user": "renoyuan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-08T03:07:00+00:00",
        "updated_at": "2024-03-05T06:51:38+00:00",
        "closed_at": "2024-03-05T06:51:38+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1860,
        "title": "Serving升级0.8.3版本提示nitialization error",
        "body": "你好，在部署分类服务service [https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/text_classification/multi_class/deploy/paddle_serving](url)\r\n将serving升级到0.8.3版本提示\r\n`failed to create predictor: Log_id: 0  Raise_msg: (External) CUDA error(3), initialization error. \r\n  [Hint: cudaErrorInitializationError. The API call failed because the CUDA driver and runtime could not be initialized. ] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:243)\r\n  ClassName: LocalPredictor.load_model_config.<locals>.create_predictor_check  FunctionName: create_predictor_check`\r\n\r\n将from paddlenlp.transformers import AutoTokenizer移到init_op中没有提示错误，请问具体是什么原因？",
        "state": "closed",
        "user": "jiangliqin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-10T02:04:42+00:00",
        "updated_at": "2024-03-05T06:51:39+00:00",
        "closed_at": "2024-03-05T06:51:39+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1865,
        "title": "增加Graphcore IPU支持",
        "body": "paddlepaddle中已经支持使用Graphcore IPU，我们希望在paddle serving中也对IPU硬件做进一步的支持。\r\n\r\n该功能分成了以下三个PR做提交，还请百度的同事review一下，谢谢！\r\n\r\nhttps://github.com/PaddlePaddle/Serving/pull/1862\r\nhttps://github.com/PaddlePaddle/Serving/pull/1863\r\nhttps://github.com/PaddlePaddle/Serving/pull/1864",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-12T05:29:19+00:00",
        "updated_at": "2024-04-16T09:06:24+00:00",
        "closed_at": "2024-04-16T09:06:24+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1866,
        "title": "请问serving0.9，部署后推理模型时显存无法释放怎么解决？",
        "body": "显卡：GTX3050\r\n镜像：registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda11.2-cudnn8-runtime\r\n环境：\r\nubuntu20.04，cuda:11.7.1-cudnn8，serving0.9，paddlepaddle-gpu==2.3.2.post116，paddle-serving-server-gpu==0.9，Docker version 20.10.18\r\n\r\n执行完多次推理后，显存不释放\r\n设置以下属性也无效\r\nexport FLAGS_eager_delete_tensor_gb=0\r\nexport FLAGS_memory_fraction_of_eager_deletion=1\r\nexport FLAGS_fast_eager_deletion_mode=True",
        "state": "closed",
        "user": "hq0749a",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-19T08:30:40+00:00",
        "updated_at": "2024-04-16T09:06:25+00:00",
        "closed_at": "2024-04-16T09:06:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1861,
        "title": "关于Serving部署OCR服务性能问题的疑问",
        "body": "服务器系统：Ubuntu16.04\r\nCPU核数：16\r\n内存：64G\r\n镜像：registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda10.1-cudnn7-runtime\r\nOCR模型：PPOCR-V3\r\n配置文件如下：\r\n```\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 18090\r\n\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 9999\r\n\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n##当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 20\r\n\r\n#build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\nbuild_dag_each_worker: False\r\n\r\ndag:\r\n    #op资源类型, True, 为线程模型；False，为进程模型\r\n    is_thread_op: False\r\n\r\n    #重试次数\r\n    retry: 3\r\n\r\n    #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n    use_profile: False\r\n    \r\n    tracer:\r\n        interval_s: 10\r\nop:\r\n    det:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 4\r\n\r\n        timeout: -1\r\n        retry: 10\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n            #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #det模型路径\r\n            model_config: ./ppocr_det_v3_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准，不设置默认取全部输出变量\r\n            #fetch_list: [\"sigmoid_0.tmp_0\"]\r\n            \r\n            #batch_size: 4\r\n            #auto_batching_timeout: 100\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0\"\r\n            #开启文字检测的TensorRT加速\r\n            #device_type: 2            \r\n            #precision: \"fp16\"            \r\n\r\n            #use_mkldnn, 开启mkldnn时，必须同时设置ir_optim=True，否则无效\r\n            #use_mkldnn: True\r\n            mem_optim: True\r\n            ir_optim: True\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 4\r\n\r\n        #超时时间, 单位ms\r\n        timeout: -1\r\n \r\n        #Serving交互重试次数，默认不重试\r\n        retry: 10\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            model_config: ./ppocr_rec_v3_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准, 不设置默认取全部输出变量\r\n            #fetch_list: [\"softmax_5.tmp_0\"]\r\n            \r\n            #batch_size: 4\r\n            #auto_batching_timeout: 1000\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0\"\r\n            \r\n            #开启文字识别的TensorRT加速\r\n            #device_type: 2\r\n            #precision: \"fp16\"\r\n            \r\n            #use_mkldnn, 开启mkldnn时，必须同时设置ir_optim=True，否则无效\r\n            #use_mkldnn: True\r\n            mem_optim: True\r\n            ir_optim: True\r\n```\r\n测试发现的问题，因为服务器上有两块显卡，当我使用1块卡部署模型的时候FPS能达到30，如果开两个服务时，FPS就直接掉了16，这样两个服务和一个服务没有区别，很好奇为啥会有这么大的差距？",
        "state": "closed",
        "user": "xiulianzw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-12T03:12:10+00:00",
        "updated_at": "2024-07-02T06:40:41+00:00",
        "closed_at": "2024-07-02T06:40:41+00:00",
        "comments_count": [
            "dizhenx"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1867,
        "title": "docker采用paddle serving进行了服务部署，局域网如何调用docker内的服务？",
        "body": "运行环境：\r\ndocker run paddle serving 0.9\r\npaddlepaddle2.3.2\r\npaddle-serving-client==0.9.0 paddle-serving-app==0.9.0 paddle-serving-server9==0.9.0\r\n\r\n启动服务：\r\ncd paddleocr/deploy/pdserving\r\npython3.7 web_service.py &>log.txt &\r\n\r\n 客户端调用：\r\npython3.7 pipeline_http_client.py\r\n\r\n在docker内paddleocr的示例运行服务和调用服务没有问题，现在的问题是，局域网内如何调用部署好了的docker服务？非常感谢大佬的解答。谢谢！",
        "state": "closed",
        "user": "Jaccica",
        "closed_by": "Jaccica",
        "created_at": "2022-10-21T08:25:46+00:00",
        "updated_at": "2022-11-21T08:03:22+00:00",
        "closed_at": "2022-11-21T08:01:39+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1868,
        "title": "在ma c上运行Serving/examples/Pipeline/PaddleDetection/faster_rcnn异常",
        "body": "\r\n```\r\npaddle_serving_server.pipeline.error_catch.CustomException: \r\n\texception_code: 3003\r\n\texception_type: INIT_ERROR\r\n\terror_msg: pipeline server init error\r\n\tis_send_to_user: False\r\nTraceback (most recent call last):\r\n  File \"web_service.py\", line 77, in <module>\r\n    fasterrcnn_service.run_service()\r\n  File \"/Users/geng/opt/miniconda3/envs/ppdet/lib/python3.8/site-packages/paddle_serving_server/web_service.py\", line 69, in run_service\r\n    self._server.run_server()\r\n  File \"/Users/geng/opt/miniconda3/envs/ppdet/lib/python3.8/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 314, in run_server\r\n    PipelineServicer(self._name, self._response_op, self._conf),\r\n  File \"/Users/geng/opt/miniconda3/envs/ppdet/lib/python3.8/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 57, in __init__\r\n    raise CustomException(CustomExceptionCode.INIT_ERROR, \"pipeline server init error\")\r\npaddle_serving_server.pipeline.error_catch.CustomException: \r\n\texception_code: 3003\r\n\texception_type: INIT_ERROR\r\n\terror_msg: pipeline server init error\r\n\tis_send_to_user: False\r\n```",
        "state": "closed",
        "user": "ddgetget",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-22T00:29:15+00:00",
        "updated_at": "2024-03-12T06:40:48+00:00",
        "closed_at": "2024-03-12T06:40:48+00:00",
        "comments_count": [
            "github-actions[bot]",
            "SunVenus"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1870,
        "title": "是否支持pulc中 PPLCNet_x1_0推理模型的部署",
        "body": "1、是否支持pulc中 PPLCNet_x1_0推理模型的serving部署\r\n2、如果支持，是否有参考例子。我参考paddlecls中resnet50的serving部署方式部署人员属性模型 时遇到问题\r\n![image](https://user-images.githubusercontent.com/51695758/197969004-18b6aae2-54ea-4397-bc6e-2aff2cb5618d.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "gitmhg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-26T08:00:44+00:00",
        "updated_at": "2024-03-05T06:51:40+00:00",
        "closed_at": "2024-03-05T06:51:40+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1869,
        "title": "serving v0.9.0 的 java 代码里，缺少 baidu.paddle_serving.predictor.general_model",
        "body": "文件 java/src/main/java/io/paddle/serving/client/Client.java 里的\r\n```\r\nimport io.paddle.serving.configure.*;\r\nimport baidu.paddle_serving.predictor.general_model.*;\r\n```\r\n找不到这两个依赖",
        "state": "closed",
        "user": "758915145",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-24T15:47:05+00:00",
        "updated_at": "2024-04-16T09:06:26+00:00",
        "closed_at": "2024-04-16T09:06:26+00:00",
        "comments_count": [
            "github-actions[bot]",
            "758915145"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1871,
        "title": "请问我在进行http部署启动之后，我的预测路径是什么呢？",
        "body": "![image](https://user-images.githubusercontent.com/50902619/197973460-3bddd866-86f3-4b18-acad-ee4dc1974c22.png)\r\n\r\n这里的路径启动也没有显示，换成自己的模型就不知道后边该写什么了",
        "state": "closed",
        "user": "1084667371",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-26T08:22:14+00:00",
        "updated_at": "2024-03-05T06:51:40+00:00",
        "closed_at": "2024-03-05T06:51:40+00:00",
        "comments_count": [
            "github-actions[bot]",
            "758915145",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1873,
        "title": "c++ Serving 中cpu 使用mkldnn 加速不起作用",
        "body": "python -m paddle_serving_server. serve --model ./model/mode_serving_rec/ --thread 10 --port 9201 --ir_optim True --use_mkl    请问是这样使用吗，为什么没有一点加速",
        "state": "closed",
        "user": "pinngan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-27T15:50:31+00:00",
        "updated_at": "2024-07-30T06:42:10+00:00",
        "closed_at": "2024-07-30T06:42:10+00:00",
        "comments_count": [
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1872,
        "title": "麻烦问下pipeline部署可以多模型并联吗，比如说有多个检测模型，能通过入参的flag确定使用哪一个模型进行推理吗？",
        "body": null,
        "state": "closed",
        "user": "pinngan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-26T09:15:43+00:00",
        "updated_at": "2024-03-05T06:51:41+00:00",
        "closed_at": "2024-03-05T06:51:41+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1874,
        "title": "昇腾310 编译失败",
        "body": "#看样子像是地址失效了\r\n```shell\r\nCMake Error at extern_paddle-stamp/download-extern_paddle.cmake:170 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc7.5_openblas_lite2.10/paddle_inference_install_dir.tgz' failed\r\n          status_code: 22\r\n          status_string: \"HTTP response code said error\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n            Trying 103.235.46.61:80...\r\n\r\n  Connected to paddle-serving.bj.bcebos.com (103.235.46.61) port 80 (#0)\r\n\r\n  GET\r\n  /inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc7.5_openblas_lite2.10/paddle_inference_install_dir.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-serving.bj.bcebos.com\r\n\r\n  User-Agent: curl/7.81.0\r\n\r\n  Accept: */*\r\n\r\n\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  HTTP/1.1 404 Not Found\r\n\r\n  Date: Thu, 27 Oct 2022 15:56:15 GMT\r\n\r\n  Content-Type: application/json; charset=utf-8\r\n\r\n  Content-Length: 117\r\n\r\n  Connection: keep-alive\r\n\r\n  Server: BceBos\r\n\r\n  x-bce-debug-id:\r\n  KzZu5wj88xo0rq28URBIXobE0Xic3i3BtuUOHCZl4GyxMq4Xgdc2hyOX3tJjm+UM7UW4eKjalW9W4UjdcLqxIw==\r\n\r\n\r\n  x-bce-request-id: 73f43ae5-7652-44c2-9558-84533950cba8\r\n\r\n  x-bce-restore-cache: -\r\n\r\n  x-bce-restore-tier: -\r\n\r\n  The requested URL returned error: 404\r\n\r\n  Closing connection 0\r\n\r\n\r\n\r\n          --- LOG END ---\r\n\r\n```",
        "state": "closed",
        "user": "MyOnlyCat",
        "closed_by": "MyOnlyCat",
        "created_at": "2022-10-27T15:58:21+00:00",
        "updated_at": "2025-02-27T10:55:55+00:00",
        "closed_at": "2023-04-27T07:54:28+00:00",
        "comments_count": [
            "MyOnlyCat",
            "github-actions[bot]",
            "danyXu",
            "kegehe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1875,
        "title": "win部署问题",
        "body": "python ocr_debugger_server.py cpu &\r\nThis API will be deprecated later. Please do not use it\r\nI1028 09:21:40.265446 10692 analysis_predictor.cc:985] ir_optim is turned off, no IR pass will be executed\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_graph_clean_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : relu_2.tmp_0  size: 29491200\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : batch_norm_31.tmp_3  size: 29491200\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : tmp_2  size: 22118400\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : x  size: 11059200\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : conv2d_186.tmp_0  size: 5529600\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : relu_1.tmp_0  size: 7372800\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : elementwise_add_3  size: 1382400\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : elementwise_add_7  size: 806400\r\nI1028 09:21:40.296638 10692 memory_optimize_pass.cc:216] Cluster name : conv2d_179.tmp_0  size: 345600\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI1028 09:21:40.327919 10692 analysis_predictor.cc:1035] ======= optimize end =======\r\nI1028 09:21:40.327919 10692 naive_executor.cc:102] ---  skip [feed], feed -> x\r\nI1028 09:21:40.327919 10692 naive_executor.cc:102] ---  skip [save_infer_model/scale_0.tmp_1], fetch -> fetch\r\nThis API will be deprecated later. Please do not use it\r\nweb service address:\r\nhttp://192.168.1.107:9292/ocr/prediction\r\nThis API will be deprecated later. Please do not use it\r\n * Serving Flask app \"paddle_serving_server.web_service\" (lazy loading)\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: off\r\n然后就不动了。。。死住了\r\n\r\n安装的paddlegpu2.3.2, cuda11.6 \r\n小白按照Windows平台使用Paddle Serving指导 操作，到python ocr_debugger_server.py cpu & 就卡住了。\r\n请问什么问题？怎么解决",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-28T01:29:45+00:00",
        "updated_at": "2024-04-16T09:06:27+00:00",
        "closed_at": "2024-04-16T09:06:27+00:00",
        "comments_count": [
            "github-actions[bot]",
            "cunjing56",
            "cunjing56"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1876,
        "title": "python client端多线程并发请求报错：inference call failed, message: [E125]1/1 channels failed, fail_limit=1 [C0][E125]Operation canceled @ip:port",
        "body": "python client多线程请求server时会偶发性报错，需要调整 _api.thrd_clear() 的位置到发起rpc请求前\r\n\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.9.0/core/general-client/src/general_model.cpp#L460",
        "state": "closed",
        "user": "barrierye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-28T06:42:41+00:00",
        "updated_at": "2024-04-16T09:06:28+00:00",
        "closed_at": "2024-04-16T09:06:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1879,
        "title": "BPRC客户端支持windows吗？",
        "body": "本项目的C文件都是基于linux，没有看到基于windows的，有人有搞过吗？有没有例子程序可参考？",
        "state": "closed",
        "user": "weida008",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-07T11:09:56+00:00",
        "updated_at": "2024-03-05T06:51:43+00:00",
        "closed_at": "2024-03-05T06:51:43+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1878,
        "title": "使用pdserving框架时候，导入paddleocr，服务无法启动，该如何处理",
        "body": "代码如下：\r\n`# encoding: utf-8\r\nfrom paddle_serving_server.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nfrom paddle_serving_app.reader import Sequential, Resize, NormalizeImage, Transpose\r\nfrom functools import reduce\r\nimport os\r\nimport faiss\r\nimport pickle\r\nfrom detop import DetOp\r\nfrom orientation_op import ImageOrientationOp\r\nimport yaml\r\nfrom paddleocr import PaddleOCR\r\nfrom res_postprocess import PredsPosProcess, NpEncoder\r\nimport json`\r\n服务启动部分代码\r\n`class RecService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        orientation_op = ImageOrientationOp(name='ImageOrientation', input_ops=[read_op])\r\n        det_op = DetOp(name=\"det\", input_ops=[orientation_op])\r\n        rec_op = RecOp(name=\"rec\", input_ops=[det_op])\r\n        return rec_op    \r\n\r\nif __name__ == '__main__':\r\n    uci_service = RecService(name=\"rec\")\r\n    uci_service.prepare_pipeline_config('config.yml')\r\n    uci_service.run_service()`\r\n运行代码后的结果：\r\n![70c151fa66e8c791a323e1788f100f4](https://user-images.githubusercontent.com/22832382/199950888-73a02dd1-1c05-4fb8-af6e-847a4b28ff42.png)\r\n卡在这里，如果将”import paddleocr“注释掉，服务可以正常启动",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-04T10:20:24+00:00",
        "updated_at": "2024-03-05T06:51:42+00:00",
        "closed_at": "2024-03-05T06:51:42+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1880,
        "title": "jetson support.",
        "body": "from the official release notes, I see serving support jetson. \r\n![image](https://user-images.githubusercontent.com/40784619/200532406-ce16ed42-a75f-4fe4-8164-50fbbf84761c.png)\r\nany doc illustrate the build process for jetson",
        "state": "closed",
        "user": "ted8201",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-08T09:58:20+00:00",
        "updated_at": "2024-04-16T09:06:29+00:00",
        "closed_at": "2024-04-16T09:06:29+00:00",
        "comments_count": [
            "github-actions[bot]",
            "ted8201",
            "ted8201"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1881,
        "title": "jetson 源码编译过程中遇到 No rule to make target '/lib/libnvinfer.so",
        "body": "编译过程中遇到“No rule to make target '/usr/lib//lib/libnvinfer.so'”的问题，请问在jetpack4.6上如何设置TENSORRT_LIBRARY_PATH\r\n? libnvinfer.so 在/usr/lib/aarch64-linux-gnu/路径下，但是找不到tensorrt这个文件夹。\r\n![image](https://user-images.githubusercontent.com/40784619/201055153-5743e588-bb0c-425d-94de-88f47ced81a4.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "ted8201",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-10T09:39:23+00:00",
        "updated_at": "2024-04-16T09:06:30+00:00",
        "closed_at": "2024-04-16T09:06:30+00:00",
        "comments_count": [
            "ted8201"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1882,
        "title": "bash benchmark.sh bert_seq128_model bert_seq128_client 出错",
        "body": "根据Serving/examples/C++/PaddleNLP/bert/README_CN.md操作\r\n模型获取采用方法2\r\n生成的profile_log_bert_seq128_model中batch_size > 1时报错\r\nmodel_name:bert_seq128_model\r\nbatch_size:4\r\nCPU_UTILIZATION: 99.1\r\nMAX_GPU_MEMORY: 5939\r\nGPU_UTILIZATION: 0\r\nthread_num: 1\r\nquery_count: 1\r\nbert_pre cost: 0.001778s in each thread\r\nTraceback (most recent call last):\r\n  File \"benchmark.py\", line 132, in <module>\r\n    \"turns\": turns})\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_client/utils/__init__.py\", line 64, in run\r\n    return_result = result_list[0].get()\r\n  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 608, in get\r\n    raise self._value\r\nValueError: input is a list, but we got 0 or 2+ feed_var, don`t know how to divide the feed list",
        "state": "closed",
        "user": "safehumeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-14T09:35:25+00:00",
        "updated_at": "2024-03-05T06:51:44+00:00",
        "closed_at": "2024-03-05T06:51:44+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1883,
        "title": "使用Kserve部署yolov3时，推理结果只有一个尺度的输出（正常为三个尺度）",
        "body": "我根据官网的那个林业虫类识别的案例，将模型动态转为静态后进行部署，在输入都正确的情况下，请求返回的**predictions只有[1,36,20,20]一个尺度的输出，正常应该是三个尺度[[1,36,20,20], [1,36,40,40], [1,36,80,80]]**。在本地使用paddle.jit.load()加载模型，然后能够正确输出三个尺度，但是部署之后就会出错，只有一个。\r\n\r\n后来看了paddleserver中model.py源码，发现**默认输出第一个位置**self.output_tensor = self.predictor.get_output_handle(output_names[0])\r\n\r\n然后我就想着给输出再包一层，但又发现文档里说：“暂不支持依赖 Tensor 的控制流中，使用多层嵌套的 list.append 操作，因为框架底层的 LoDTensorArray = std::vector< LoDTensor > ，不支持两层以上 vector 嵌套”。\r\n\r\n虽然能够去改一下model.py源码，但是总不能每启起来一个服务就去手动改一下，而且我也没有root权限还改不了。。\r\n\r\n有没有大神能教教什么操作能再给输出包一层。。",
        "state": "closed",
        "user": "zyf950120",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-15T03:50:40+00:00",
        "updated_at": "2024-03-05T06:51:45+00:00",
        "closed_at": "2024-03-05T06:51:45+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1884,
        "title": "执行python3 pipeline_http_client.py报错",
        "body": "服务器安装环境：\r\npaddle-bfloat         0.1.2\r\npaddle-serving-app    0.9.0\r\npaddle-serving-client 0.9.0\r\npaddle-serving-server 0.9.0\r\npaddlepaddle          2.0.0rc0\r\n\r\n房产价格那个测试可用通过，但是验证ocr这个执行python3 pipeline_http_client.py就报错\r\nINFO 2022-11-15 07:16:31,534 [pipeline_server.py:64] (log_id=0) inference request name:ocr self.name:ocr time:1668496591.5340307\r\nINFO 2022-11-15 07:16:31,534 [operator.py:1826] RequestOp unpack one request. log_id:0, clientip:             name:ocr, method:prediction, time:1668496591.5349097\r\nINFO 2022-11-15 07:16:31,535 [dag.py:379] (data_id=1 log_id=0) Succ Generate ID \r\nERROR 2022-11-15 07:16:31,659 [error_catch.py:106] \r\nLog_id: 1\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/error_catch.py\", line 97, in wrapper\r\n    res = func(*args, **kw)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/error_catch.py\", line 160, in wrapper\r\n    raise CustomException(CustomExceptionCode.INPUT_PARAMS_ERROR, \"invalid arg list: {}\".format(invalid_argument_list), True)\r\npaddle_serving_server.pipeline.error_catch.CustomException: \r\n\texception_code: 5000\r\n\texception_type: INPUT_PARAMS_ERROR\r\n\terror_msg: invalid arg list: ['feed_batch', 'fetch_list']\r\n\tis_send_to_user: True\r\nClassname: Op.process.<locals>.feed_fetch_list_check_helper\r\nFunctionName: feed_fetch_list_check_helper\r\nArgs: ([{'x': array([[[[ 2.2489083 ,  2.2489083 ,  2.2489083 , ..., -1.5699117 ,\r\n          -1.5699117 , -1.5699117 ],\r\n         [ 2.2489083 ,  2.2489083 ,  2.2489083 , ..., -1.7582841 ,\r\n          -1.7411593 , -1.689785  ],\r\n         [ 2.2489083 ,  2.2489083 ,  2.2489083 , ..., -1.6726602 ,\r\n          -1.5870365 , -1.5185375 ],\r\n         ...,\r\n         [ 0.27956173,  0.19393796,  0.07406469, ..., -0.86779684,\r\n          -0.93629587, -0.9534206 ],\r\n         [ 0.41655976,  0.29668647,  0.17681322, ..., -0.85067207,\r\n          -0.9020463 , -0.93629587],\r\n         [ 0.45080924,  0.38231024,  0.33093598, ..., -0.83354735,\r\n          -0.86779684, -0.8849216 ]],\r\n\r\n        [[ 2.4285715 ,  2.4285715 ,  2.4285715 , ..., -1.352941  ,\r\n          -1.352941  , -1.352941  ],\r\n         [ 2.4285715 ,  2.4285715 ,  2.4285715 , ..., -1.5455183 ,\r\n          -1.5280112 , -1.4754901 ],\r\n         [ 2.4285715 ,  2.4285715 ,  2.4285715 , ..., -1.4579831 ,\r\n          -1.3704481 , -1.30042   ],\r\n         ...,\r\n         [ 0.62535024,  0.5378152 ,  0.41526622, ..., -0.687675  ,\r\n          -0.757703  , -0.77521   ],\r\n         [ 0.7303923 ,  0.6078432 ,  0.48529422, ..., -0.67016804,\r\n          -0.72268903, -0.77521   ],\r\n         [ 0.74789923,  0.6778712 ,  0.62535024, ..., -0.652661  ,\r\n          -0.687675  , -0.72268903]],\r\n\r\n        [[ 2.6399999 ,  2.6399999 ,  2.6399999 , ..., -0.95041394,\r\n          -0.95041394, -0.9678431 ],\r\n         [ 2.6399999 ,  2.6399999 ,  2.6399999 , ..., -1.1595643 ,\r\n          -1.142135  , -1.0898474 ],\r\n         [ 2.6399999 ,  2.6399999 ,  2.6399999 , ..., -1.0724183 ,\r\n          -0.98527235, -0.89812636],\r\n         ...,\r\n         [ 1.0539434 ,  0.9667975 ,  0.84479314, ..., -0.37525052,\r\n          -0.44496727, -0.46239647],\r\n         [ 1.1585187 ,  1.0365143 ,  0.93193907, ..., -0.35782132,\r\n          -0.4275381 , -0.46239647],\r\n         [ 1.193377  ,  1.1236603 ,  1.0713726 , ..., -0.3403921 ,\r\n          -0.37525052, -0.4101089 ]]]], dtype=float32)}], ['save_infer_model/scale_0.tmp_1'])\r\nERROR 2022-11-15 07:16:31,660 [operator.py:1101] [det] failed to predict. Log_id: 1  Raise_msg:  invalid arg list  ClassName: Op.process.<locals>.feed_fetch_list_check_helper  FunctionName: feed_fetch_list_check_helper. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details.\r\nINFO 2022-11-15 07:16:31,660 [operator.py:1488] prometheus inf count +1\r\nERROR 2022-11-15 07:16:31,665 [dag.py:420] (data_id=1 log_id=0) Failed to predict: [det] failed to predict. Log_id: 1  Raise_msg:  invalid arg list  ClassName: Op.process.<locals>.feed_fetch_list_check_helper  FunctionName: feed_fetch_list_check_helper. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details.\r\n",
        "state": "closed",
        "user": "guomengf",
        "closed_by": "guomengf",
        "created_at": "2022-11-15T07:27:28+00:00",
        "updated_at": "2022-11-15T09:12:57+00:00",
        "closed_at": "2022-11-15T09:11:58+00:00",
        "comments_count": [
            "github-actions[bot]",
            "guomengf"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1886,
        "title": "请问有部署多个模型串联的pipeline模式的文档或者范例吗？用ocr那个不起作用，卡了两天了",
        "body": "目前我是想实现paddlex 范例中的meter_reader 的det + seg 两个模型的pipeline模式的部署，det的通了，但是到seg 这里的时候，后处理总是报错，试了很多方法都不行，希望能获得帮助，也在aistudio和谷歌上搜了很多，都没找到这样的范例，希望能有个文档或者例子都可以\r\n下面是我的webservice.py文件\r\n\r\n```\r\nfrom paddle_serving_server.web_service import WebService, Op\r\nimport numpy as np\r\nimport cv2\r\nfrom paddle_serving_app.reader import *\r\nimport base64\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\n\r\nclass MeterReader(Op):\r\n    def init_op(self):\r\n        # 这里 compose 一些需要的预处理操作\r\n        self.img_preprocess = Sequential([\r\n            BGR2RGB(), Div(255.0),\r\n            Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5], False),\r\n            Resize((512, 512)), Transpose((2, 0, 1))\r\n        ])\r\n\r\n        self.img_postprocess = SegPostprocess(3)\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        #print(\"keys\", input_dict.keys())\r\n        for key in input_dict.keys():\r\n            if input_dict[key].startswith('data:image'):\r\n                image_str = input_dict[key]\r\n                image_str = image_str[image_str.find(';base64,') + 8:]\r\n            data = base64.b64decode(image_str.encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            \r\n            im = self.img_preprocess(im)\r\n            imgs.append({\r\n                \"image\": im[np.newaxis, :]\r\n            })\r\n        feed_dict = {\r\n            \"image\": np.concatenate(\r\n                [x[\"image\"] for x in imgs], axis=0)\r\n        }\r\n        image = im[np.newaxis, :]\r\n        # print(image)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\r\n        print(fetch_dict)\r\n        fetch_dict['filename'] = \"/home/output.jpg\"\r\n        res_dict = {\r\n            \"bbox_result\":\r\n            str(self.img_postprocess(\r\n                image_with_result=fetch_dict))\r\n        }\r\n        return res_dict, None, \"\"\r\n\r\n# 其余的都是固定的操作\r\nclass MeterSetService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        meter_set_op = MeterReader(name=\"meter_seg\", input_ops=[read_op])\r\n        return meter_set_op\r\n\r\n# define the service class\r\nuci_service = MeterSetService(name=\"meter_seg\")\r\n# load config and prepare the service\r\nuci_service.prepare_pipeline_config(\"config.yml\")\r\n# start the service\r\nuci_service.run_service()\r\n```\r\n\r\nserving_server_conf.prototxt\r\n```\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 3\r\n}\r\n\r\n```\r\n\r\n\r\nconfig.yml\r\n```\r\ndag:\r\n  #op资源类型, True, 为线程模型；False，为进程模型\r\n  is_thread_op: false\r\n  #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n  tracer:\r\n    interval_s: 30\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 18082\r\nop:\r\n  meter_seg:\r\n    #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n    concurrency: 10\r\n    local_service_conf:\r\n      #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n      client_type: local_predictor\r\n      # device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n      device_type: 0\r\n      #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n      devices: ''\r\n      #Fetch结果列表，以bert_seq128_model中fetch_var的alias_name为准, 如果没有设置则全部返回\r\n      fetch_list:\r\n      - save_infer_model/scale_0.tmp_1\r\n      #模型路径\r\n      model_config: /home/models/meter_reader/models/meter_seg_model/server/\r\n  \r\n\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\n#rpc_port: 9998\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n#当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 20\r\n```\r\n\r\n求帮助，真的是没有办法了，能找的方法都找了。",
        "state": "closed",
        "user": "dlmkaq55240",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-16T03:52:39+00:00",
        "updated_at": "2024-03-05T06:51:46+00:00",
        "closed_at": "2024-03-05T06:51:46+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "ClassmateXiaoyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1887,
        "title": "采用paddleserving部署OCR服务，只需要输出识别的字符串，如何修改代码？",
        "body": "       采用paddleserving部署好了OCR服务，现在输出的结果包含识别的字符串和框信息，我现在只需要输出识别的字符串，请问怎么修改代码？\r\n       我查了下web_service.py的代码，recOp返回的是字典，而不是字符。\r\n![图片](https://user-images.githubusercontent.com/17899115/202600975-c7961466-37e6-425e-b80c-97febabd8dd4.png)\r\n\r\n",
        "state": "closed",
        "user": "Jaccica",
        "closed_by": "Jaccica",
        "created_at": "2022-11-18T02:14:40+00:00",
        "updated_at": "2022-11-21T10:46:24+00:00",
        "closed_at": "2022-11-21T10:46:24+00:00",
        "comments_count": [
            "HexToString",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1889,
        "title": "Serving Op是否支持ErnieGen文本生成类应用？",
        "body": "请问自定义Op是否支持ErnieGen文本生成类应用？demo中bert应该只是提取特征吧？\r\n建议Op类运行模型的接口支持自定义实现\r\nBert demo示例：\r\n`\r\nclass BertOp(Op):\r\n    def init_op(self):\r\n        self.reader = ChineseBertReader({\r\n            \"vocab_file\": \"vocab.txt\",\r\n            \"max_seq_len\": 128\r\n        })\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        print(\"input dict\", input_dict)\r\n        batch_size = len(input_dict.keys())\r\n        feed_res = []\r\n        for i in range(batch_size):\r\n            feed_dict = self.reader.process(input_dict[str(i)].encode(\"utf-8\"))\r\n            for key in feed_dict.keys():\r\n                feed_dict[key] = np.array(feed_dict[key]).reshape(\r\n                    (1, len(feed_dict[key]), 1))\r\n            feed_res.append(feed_dict)\r\n        feed_dict = {}\r\n        for key in feed_res[0].keys():\r\n            feed_dict[key] = np.concatenate([x[key] for x in feed_res], axis=0)\r\n            print(key, feed_dict[key].shape)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\r\n        new_dict = {}\r\n        new_dict[\"pooled_output\"] = str(fetch_dict[\"pooled_output\"])\r\n        new_dict[\"sequence_output\"] = str(fetch_dict[\"sequence_output\"])\r\n        return new_dict, None, \"\"\r\n\r\n\r\nclass BertService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        bert_op = BertOp(name=\"bert\", input_ops=[read_op])\r\n        return bert_op\r\n\r\n\r\nbert_service = BertService(name=\"bert\")\r\nbert_service.prepare_pipeline_config(\"config.yml\")\r\nbert_service.run_service()\r\n`",
        "state": "closed",
        "user": "GoneWithTheCloud",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-24T09:53:08+00:00",
        "updated_at": "2024-04-16T09:06:31+00:00",
        "closed_at": "2024-04-16T09:06:31+00:00",
        "comments_count": [
            "github-actions[bot]",
            "GoneWithTheCloud"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1890,
        "title": "在op添加set_dynamic_shape_info函数指定输入大小，还是会报错",
        "body": "我在DetOp中添加set_dynamic_shape_info函数，具体如下：\r\n```\r\n    def set_dynamic_shape_info(self):\r\n        min_input_shape = {\r\n            \"x\": [1, 3, 64, 64],\r\n            \"batch_norm_23.tmp_3.tmp\": [1, 512, 8, 8],\r\n            \"relu_5.tmp_0\": [1, 512, 8, 8],\r\n            \"batch_norm_10.tmp_3.tmp\": [1, 256, 16, 16],\r\n            \"relu_1.tmp_0\": [1, 256, 16, 16],\r\n            \"conv2d_120.tmp_0.tmp\": [1, 256, 2, 2],\r\n            \"conv2d_121.tmp_0.tmp\": [1, 256, 4, 4],\r\n            \"conv2d_122.tmp_0.tmp\": [1, 256, 8, 8],\r\n            \"conv2d_123.tmp_0.tmp\": [1, 256, 16, 16],\r\n            \"conv2d_124.tmp_0.tmp\": [1, 64, 2, 2],\r\n            \"conv2d_125.tmp_0.tmp\": [1, 64, 4, 4],\r\n            \"conv2d_126.tmp_0.tmp\": [1, 64, 8, 8],\r\n            \"conv2d_127.tmp_0.tmp\": [1, 64, 16, 16],\r\n            \"batch_norm_42.tmp_3.tmp\": [1, 1024, 4, 4],\r\n            \"relu_11.tmp_0\": [1, 1024, 4, 4],\r\n            \"nearest_interp_v2_24501.tmp_0\": [1, 64, 16, 16],\r\n            \"nearest_interp_v2_24502.tmp_0\": [1, 64, 16, 16],\r\n            \"nearest_interp_v2_24503.tmp_0\": [1, 64, 16, 16],\r\n            \"sigmoid_16.tmp_0.tmp\": [1, 4, 16, 16],\r\n            \"batch_norm_10.tmp_3\": [1, 256, 16, 16],\r\n        }\r\n        max_input_shape = {\r\n            \"x\": [1, 3, 960, 960],\r\n            \"batch_norm_23.tmp_3.tmp\": [1, 512, 120, 120],\r\n            \"relu_5.tmp_0\": [1, 512, 120, 120],\r\n            \"batch_norm_10.tmp_3.tmp\": [1, 256, 240, 240],\r\n            \"relu_1.tmp_0\": [1, 256, 240, 240],\r\n            \"conv2d_120.tmp_0.tmp\": [1, 256, 30, 30],\r\n            \"conv2d_121.tmp_0.tmp\": [1, 256, 60, 60],\r\n            \"conv2d_122.tmp_0.tmp\": [1, 256, 120, 120],\r\n            \"conv2d_123.tmp_0.tmp\": [1, 256, 240, 240],\r\n            \"conv2d_124.tmp_0.tmp\": [1, 64, 30, 30],\r\n            \"conv2d_125.tmp_0.tmp\": [1, 64, 60, 60],\r\n            \"conv2d_126.tmp_0.tmp\": [1, 64, 120, 120],\r\n            \"conv2d_127.tmp_0.tmp\": [1, 64, 240, 240],\r\n            \"batch_norm_42.tmp_3.tmp\": [1, 1024, 60, 60],\r\n            \"relu_11.tmp_0\": [1, 1024, 60, 60],\r\n            \"nearest_interp_v2_24501.tmp_0\": [1, 64, 240, 240],\r\n            \"nearest_interp_v2_24502.tmp_0\": [1, 64, 240, 240],\r\n            \"nearest_interp_v2_24503.tmp_0\": [1, 64, 240, 240],\r\n            \"sigmoid_16.tmp_0.tmp\": [1, 4, 240, 240],\r\n            \"batch_norm_10.tmp_3\": [1, 256, 240, 240],\r\n        }\r\n        opt_input_shape = {\r\n            \"x\": [1, 3, 64, 64],\r\n            \"batch_norm_23.tmp_3.tmp\": [1, 512, 8, 8],\r\n            \"relu_5.tmp_0\": [1, 512, 8, 8],\r\n            \"batch_norm_10.tmp_3.tmp\": [1, 256, 16, 16],\r\n            \"relu_1.tmp_0\": [1, 256, 16, 16],\r\n            \"conv2d_120.tmp_0.tmp\": [1, 256, 2, 2],\r\n            \"conv2d_121.tmp_0.tmp\": [1, 256, 4, 4],\r\n            \"conv2d_122.tmp_0.tmp\": [1, 256, 8, 8],\r\n            \"conv2d_123.tmp_0.tmp\": [1, 256, 16, 16],\r\n            \"conv2d_124.tmp_0.tmp\": [1, 64, 2, 2],\r\n            \"conv2d_125.tmp_0.tmp\": [1, 64, 4, 4],\r\n            \"conv2d_126.tmp_0.tmp\": [1, 64, 8, 8],\r\n            \"conv2d_127.tmp_0.tmp\": [1, 64, 16, 16],\r\n            \"batch_norm_42.tmp_3.tmp\": [1, 1024, 4, 4],\r\n            \"relu_11.tmp_0\": [1, 1024, 4, 4],\r\n            \"nearest_interp_v2_24501.tmp_0\": [1, 64, 16, 16],\r\n            \"nearest_interp_v2_24502.tmp_0\": [1, 64, 16, 16],\r\n            \"nearest_interp_v2_24503.tmp_0\": [1, 64, 16, 16],\r\n            \"sigmoid_16.tmp_0.tmp\": [1, 4, 16, 16],\r\n            \"batch_norm_10.tmp_3\": [1, 256, 16, 16],\r\n        }\r\n        self.dynamic_shape_info = {\r\n            \"min_input_shape\": min_input_shape,\r\n            \"max_input_shape\": max_input_shape,\r\n            \"opt_input_shape\": opt_input_shape,\r\n        }\r\n```\r\n使用tensorrt和int8进行推理，config文件内容如下：\r\n```\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 18091\r\n\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 9998\r\n\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n##当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 32\r\n\r\n#build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\nbuild_dag_each_worker: False\r\n\r\ndag:\r\n    #op资源类型, True, 为线程模型；False，为进程模型\r\n    is_thread_op: False\r\n\r\n    #重试次数\r\n    retry: 10\r\n\r\n    #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n    use_profile: True\r\n    \r\n    tracer:\r\n        interval_s: 10\r\nop:\r\n    det:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 16\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n            #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #det模型路径\r\n            model_config: ./ppocr_det_db++_qt_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准，不设置默认取全部输出变量\r\n            #fetch_list: [\"sigmoid_0.tmp_0\"]\r\n            #计算硬件类型: 空缺时由devices决定(CPU/GPU)，0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n            device_type: 2\r\n\r\n            precision: \"int8\"\r\n            use_calib: True\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0,1,2,3\"\r\n            \r\n            ir_optim: True\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        concurrency: 8\r\n\r\n        #超时时间, 单位ms\r\n        timeout: -1\r\n \r\n        #Serving交互重试次数，默认不重试\r\n        retry: 1\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            model_config: ./ppocr_rec_svtr_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准, 不设置默认取全部输出变量\r\n            #fetch_list: \r\n\r\n            #计算硬件类型: 空缺时由devices决定(CPU/GPU)，0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n            device_type: 0\r\n\r\n            # precision: \"int8\"\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            devices: \"0,1,2,3\"\r\n            \r\n            ir_optim: True\r\n\r\n```\r\n\r\n运行客户端python文件，还是会报下面的错误：\r\nI1126 09:08:41.038064 25636 tensorrt_engine_op.h:421] This process is generating calibration table for Paddle TRT int8...\r\nI1126 09:08:41.049059 27658 tensorrt_engine_op.h:301] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  (InvalidArgument) The input [batch_norm_10.tmp_3.tmp] shape of trt subgraph is [-1,256,-1,-1], please enable trt dynamic_shape mode by SetTRTDynamicShapeInfo. (at /paddle/paddle/fluid/inference/tensorrt/engine.h:104)\r\n\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   std::thread::_State_impl<std::thread::_Invoker<std::tuple<paddle::operators::TensorRTEngineOp::RunCalibration(paddle::framework::Scope const&, phi::Place const&) const::{lambda()#1}> > >::_M_run()\r\n1   paddle::operators::TensorRTEngineOp::PrepareTRTEngine(paddle::framework::Scope const&, paddle::inference::tensorrt::TensorRTEngine*) const\r\n2   paddle::inference::tensorrt::OpConverter::ConvertBlockToTRTEngine(paddle::framework::BlockDesc*, paddle::framework::Scope const&, std::vector<std::string, std::allocator<std::string > > const&, std::unordered_set<std::string, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > > const&, paddle::inference::tensorrt::TensorRTEngine*)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Process abort signal` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1669453721 (unix time) try \"date -d @1669453721\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGABRT (@0x6424) received by PID 25636 (TID 0x7fda69ffb700) from PID 25636 ***]\r\n\r\n\r\n请问这是为什么呢？batch_norm_10.tmp_3.tmp 这个模块的大小上面已经指定了",
        "state": "closed",
        "user": "heyuhhh",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-26T09:13:20+00:00",
        "updated_at": "2024-03-05T06:51:47+00:00",
        "closed_at": "2024-03-05T06:51:47+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1891,
        "title": "paddle serving RPC部署UIE模型，提示输入的数据维度不正确，",
        "body": "probuf 如下:\r\n\r\nfeed_var {\r\n  name: \"input_ids\"\r\n  alias_name: \"input_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n}\r\nfeed_var {\r\n  name: \"token_type_ids\"\r\n  alias_name: \"token_type_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n}\r\nfeed_var {\r\n  name: \"pos_ids\"\r\n  alias_name: \"pos_ids\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n}\r\nfeed_var {\r\n  name: \"att_mask\"\r\n  alias_name: \"att_mask\"\r\n  is_lod_tensor: false\r\n  feed_type: 0\r\n}\r\nfetch_var {\r\n  name: \"sigmoid_1.tmp_0\"\r\n  alias_name: \"sigmoid_1.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"sigmoid_2.tmp_0\"\r\n  alias_name: \"sigmoid_2.tmp_0\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\n\r\n\r\nclient  代码：\r\ninput_dict = {\r\n                \"input_ids\":\r\n                encoded_inputs['input_ids'][l:r].astype('int64'),\r\n                \"token_type_ids\":\r\n                encoded_inputs['token_type_ids'][l:r].astype('int64'),\r\n                \"pos_ids\":\r\n                encoded_inputs['position_ids'][l:r].astype('int64'),\r\n                \"att_mask\":\r\n                encoded_inputs[\"attention_mask\"][l:r].astype('int64')\r\n            }\r\n            pprint(texts)\r\n            print(input_dict['input_ids'].shape, type(input_dict['input_ids']))  # shape = 1*512\r\n            print(input_dict['token_type_ids'].shape, type(input_dict['token_type_ids']))  # shape = 1*512\r\n            print(input_dict['pos_ids'].shape, type(input_dict['pos_ids']))  # shape = 1*512\r\n            print(input_dict['att_mask'].shape, type(input_dict['att_mask']))  # shape = 1*512\r\n            feed = input_dict#{\"words\": word_ids}\r\n            #fetch = [\"layer_norm_12.tmp_2\",\"gelu_1.tmp_0\"]#[\"acc\", \"cost\", \"prediction\"]\r\n            fetch = [\"sigmoid_1.tmp_0\", \"sigmoid_2.tmp_0\"]  # [\"acc\", \"cost\", \"prediction\"]\r\n            fetch_map = client.predict(feed=feed, fetch=fetch, batch=False)\r\n\r\nInvalidArgumentError: The first dimension value of Input(Scale) must equal to be thesecond dimension value of the flattened 2D matrix of Input(X),But                                   received the first dimension value of Input(Scale) is[312], the second dimension value of the flattened 2D matrix of Input(Scale) is [159744].\r\n  [Hint: Expected ctx->GetInputDim(\"Scale\")[0] == right, but received ctx->GetInputDim(\"Scale\")[0]:312 != right:159744.] (at /paddle/paddle/fluid/oper                                  ators/layer_norm_op.cc:72)\r\n  [operator < layer_norm > error]\r\nAborted (core dumped)\r\n\r\n\r\n使用的是UIE nano； 我能了解到的是159744/512=312的字向量维度，但为何这样写服务端不支持了",
        "state": "closed",
        "user": "yangxuan14nlp",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-29T06:01:00+00:00",
        "updated_at": "2024-03-05T06:51:48+00:00",
        "closed_at": "2024-03-05T06:51:48+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1893,
        "title": "使用Paddle Serving部署OCR Pipeline在线服务，空格无法识别",
        "body": "是下载的ocr_rec.tar.gz，ocr_det.tar.gz这两个模型不支持空格的识别吗",
        "state": "closed",
        "user": "guomengf",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-05T09:49:25+00:00",
        "updated_at": "2024-04-16T09:06:32+00:00",
        "closed_at": "2024-04-16T09:06:31+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1895,
        "title": "run paddle serving docker fail with \"source: not found\"",
        "body": "```bash\r\n$ docker run --net=host --name paddle_serving_test -dit registry.baidubce.com/paddlepaddle/serving:0.9.0-devel\r\n$ docker container ls -a |grep paddle_serving_test\r\n2fe8056b9bba        registry.baidubce.com/paddlepaddle/serving:0.9.0-devel                                  \"/bin/sh -c 'source …\"   33 seconds ago      Exited (127) 30 seconds ago                               paddle_serving_test\r\n\r\n$ docker logs paddle_serving_test\r\n/bin/sh: 1: source: not found\r\n```\r\n\r\n",
        "state": "closed",
        "user": "CH7auAI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-20T12:40:31+00:00",
        "updated_at": "2024-04-16T09:06:33+00:00",
        "closed_at": "2024-04-16T09:06:32+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1892,
        "title": "不同的模型的web_service是不是都是不一样的？",
        "body": "下载官方的例子，可以正常运行，现在换了个模型文件，再测试就会提示参数错误。请问每个模型对应的web_service都是需要自己重新实现的吗？",
        "state": "closed",
        "user": "Yan-Ke",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-30T01:47:06+00:00",
        "updated_at": "2024-03-05T06:51:49+00:00",
        "closed_at": "2024-03-05T06:51:49+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1894,
        "title": "为什么RPC服务和Pipeline服务的输出不一样",
        "body": "遇见了很奇怪的情况：\r\n1. fairmot-608*1088-vehicle模型，部署RPC服务，输出bbox_result维度是对的：(500, 6)\r\n![image](https://user-images.githubusercontent.com/56629915/206353842-c689d54f-7839-4e2a-b473-5bf9990c5ff5.png)\r\n\r\n2. fairmot-608*1088-vehicle模型，部署pipeline服务，输出bbox_result维度是错的：(1,  6)\r\n![image](https://user-images.githubusercontent.com/56629915/206353793-2226e47b-707a-456c-a307-4a0ac32f02c4.png)\r\n\r\n3. yolov3-darknet53模型，部署pipeline服务，输出bbox_result维度也是对的：(42,  6)\r\n![image](https://user-images.githubusercontent.com/56629915/206353903-03669cf5-df94-4f2f-826c-56a3038945b9.png)\r\n\r\n模型文件一样，图片一样，前后处理也一样，就是部署pipeline服务的fairmot模型输出不对，只能输出一个。。。\r\n\r\n有没有大佬帮忙解决下，感谢",
        "state": "closed",
        "user": "zyf950120",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-08T04:05:36+00:00",
        "updated_at": "2024-03-05T06:51:50+00:00",
        "closed_at": "2024-03-05T06:51:50+00:00",
        "comments_count": [
            "vividzhang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1896,
        "title": "KeyError: 'concat_8.tmp_0.lod'",
        "body": "使用PaddleDetection套件，数据集使用VOC2007转换成COCO数据集后，采用fast_RCNN模型进行训练，导出模型后，在Serving的docker环境下部署，采用PaddleDetection/C++的方式，服务端能够启动，客户端启动之后出现\r\n```\r\nλ 0697a0a4f727 /home/inference_model/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco python3 test_client.py 009046.jpg \r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI1220 18:03:05.789144   325 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9494\"): added 1\r\nI1220 18:03:10.932313   325 general_model.cpp:490] [client]logid=0,client_cost=5119.43ms,server_cost=5086.37ms.\r\nTraceback (most recent call last):\r\n  File \"test_client.py\", line 54, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'concat_8.tmp_0.lod'\r\n```",
        "state": "closed",
        "user": "ddgetget",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-20T14:37:50+00:00",
        "updated_at": "2024-03-05T06:51:51+00:00",
        "closed_at": "2024-03-05T06:51:51+00:00",
        "comments_count": [
            "fanruifeng",
            "ddgetget",
            "fanruifeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1897,
        "title": "昇腾NPU编译时错误",
        "body": "error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/paddle-lite-unknown/paddle_inference_install_dir.tgz' failed\r\n没有这个文件",
        "state": "closed",
        "user": "cycle1113",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-24T15:45:34+00:00",
        "updated_at": "2024-03-05T06:51:52+00:00",
        "closed_at": "2024-03-05T06:51:52+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1898,
        "title": "cpu环境检查,FileNotFoundError: [Errno 2] No such file or directory: '/dev/null'",
        "body": "(paddlecpu) D:\\pkg\\Serving>python -m paddle_serving_server.serve check\r\nD:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\runpy.py:127: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nWelcome to the check env shell.Type help to list commands.\r\n\r\n(Cmd) check_all\r\nTraceback (most recent call last):\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\site-packages\\paddle_serving_server\\serve.py\", line 715, in <module>\r\n    Check_Env_Shell().cmdloop()\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\cmd.py\", line 138, in cmdloop\r\n    stop = self.onecmd(line)\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\cmd.py\", line 217, in onecmd\r\n    return func(arg)\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\site-packages\\paddle_serving_server\\serve.py\", line 674, in do_check_all\r\n    check_env(\"all\")\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\site-packages\\paddle_serving_server\\env_check\\run.py\", line 104, in check_env\r\n    run_test_cases(inference_test_cases, \"PaddlePaddle\", is_open_std)\r\n  File \"D:\\soft\\anaconda3\\envs\\paddlecpu\\lib\\site-packages\\paddle_serving_server\\env_check\\run.py\", line 63, in run_test_cases\r\n    sys.stdout = open('/dev/null', 'w')\r\nFileNotFoundError: [Errno 2] No such file or directory: '/dev/null'\r\n\r\n\r\n在此之前，安装包的时候报错pip install -r python/requirements.txt\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nsahi 0.11.9 requires click==8.0.4, but you have click 7.1.2 which is incompatible.\r\nsahi 0.11.9 requires opencv-python>=4.2.0.32, but you have opencv-python 3.4.17.61 which is incompatible.\r\n\r\n我升级click就报错\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nsahi 0.11.9 requires opencv-python>=4.2.0.32, but you have opencv-python 3.4.17.61 which is incompatible.\r\npaddle-serving-server 0.9.0 requires click==7.1.2, but you have click 8.0.4 which is incompatible.\r\nflask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.4 which is incompatible.\r\n\r\n请问应该怎么办？",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-27T01:40:22+00:00",
        "updated_at": "2024-04-16T09:06:33+00:00",
        "closed_at": "2024-04-16T09:06:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1900,
        "title": "C++ gpu environment running failure， 环境安装问题",
        "body": "![image](https://user-images.githubusercontent.com/112528302/209891979-81e0e3a7-82e1-4c0b-ba24-b8edcb30eafe.png)\r\n我先安了cuda11.2, 又配置了cudnn8.2.1, 安了docker, nvidia container,然后拉的paddle镜像（因为是ubuntu18）\r\n![image](https://user-images.githubusercontent.com/112528302/209892444-e7728b8b-a27b-40dd-b8ba-81eefc2bce2f.png)\r\n唯一报错-我没管，接着执行的。到了环境check就报错了。。。\r\n![image](https://user-images.githubusercontent.com/112528302/209892459-e6b78e56-07fe-466a-aa02-6f855264e122.png)\r\n\r\n请问我现在是怎么修复这个问题，还是说，我重新搞个ubuntu ,安个docker, nvidia container然后就可以直接拉镜像了？ubuntu18是不是只能拉paddle镜像？\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-29T01:40:40+00:00",
        "updated_at": "2024-04-16T09:06:35+00:00",
        "closed_at": "2024-04-16T09:06:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1899,
        "title": "使用http请求如何填写参数格式呢",
        "body": "![1672221585806](https://user-images.githubusercontent.com/49724146/209794401-0934d1f4-777f-470f-9f0c-9b82d01e8a72.png)\r\n![1672221585806](https://user-images.githubusercontent.com/49724146/209794621-9486fd4c-665d-45fd-af31-a4eb59d94047.png)\r\npython给的客户端例子传参是这样，我想用http发送请求，参数部分要写成什么样的格式呢",
        "state": "closed",
        "user": "XS170411105276",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-28T10:02:02+00:00",
        "updated_at": "2024-04-16T09:06:34+00:00",
        "closed_at": "2024-04-16T09:06:34+00:00",
        "comments_count": [
            "github-actions[bot]",
            "XS170411105276"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1901,
        "title": "快速开始示例中的HTTP服务，客户端不能正常获得返回值的问题",
        "body": "已完成docker中的Paddle Serving安装，版本是CPU的0.9.0-devel，环境检查显示安装成功。\r\n但是在跑快速示例的时候，一直不能跑通，求助大神帮忙看看。\r\n![image](https://user-images.githubusercontent.com/119045531/210330509-efa758e7-ec6e-424d-8d5d-8fcf90f53b84.png)\r\n![image](https://user-images.githubusercontent.com/119045531/210331027-552a4421-19cd-43ad-ad98-f96ad6fcd380.png)\r\n\r\n![image](https://user-images.githubusercontent.com/119045531/210331329-9a320f4f-9987-4362-a97a-8f5ab138fae5.png)\r\n![image](https://user-images.githubusercontent.com/119045531/210331560-1df4fe0b-3264-447a-a123-24f5b74c52ca.png)\r\n\r\n![image](https://user-images.githubusercontent.com/119045531/210331726-4c92a123-7eaf-4666-a97c-478729a40aac.png)\r\n![image](https://user-images.githubusercontent.com/119045531/210332039-10146f9c-0e1f-4144-bf84-f46f672f564a.png)\r\n",
        "state": "closed",
        "user": "QileLeo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-03T09:45:44+00:00",
        "updated_at": "2024-03-05T06:51:52+00:00",
        "closed_at": "2024-03-05T06:51:52+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1902,
        "title": "pipeline部署模型，出现lod报错，_get_bbox_result无法返回bbox_results",
        "body": "```\r\n环境\r\nCUDA 11.7\r\ncudnn 8.4.1\r\n显卡：GTX 1070 \r\npython 3.8.13\r\nPaddlePaddle 2.4.1.post117\r\npaddle-serving-server-gpu 0.9.0\r\npaddle_serving_app 0.9.0\r\n\r\n用paddleX训练的PPYOLOv2模型，通过python -m paddle_serving_client.convert --dirname  --model_filename  --params_filename  --serving_server serving_server --serving_client serving_client命令将inference模型转为了server模型。\r\n发现一个问题，同一个模型用不同的方式部署后，会出现lod报错。具体如下：\r\n1、当我用pipeline方式部署，fetch_dict中没有fetch_name.lod这个键，fetch_dict:  {'save_infer_model/scale_0.tmp_1': array([[  0.        ,   0.85202295, 216.68979   ,  64.207535  ,        436.6143    , 332.37054   ]], dtype=float32)}。\r\n也就是没有lod信息，client与server通讯时，出现报错\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/paddle38/lib/python3.8/site-packages/paddle_serving_server/pipeline/error_catch.py\", line 97, in wrapper\r\n    res = func(*args, **kw)\r\n  File \"/root/anaconda3/envs/paddle38/lib/python3.8/site-packages/paddle_serving_server/pipeline/operator.py\", line 1179, in postprocess_help\r\n    postped_data, prod_errcode, prod_errinfo = self.postprocess(\r\n  File \"pipeline_web_service_linux.py\", line 72, in postprocess\r\n    self.img_postprocess(\r\n  File \"/root/anaconda3/envs/paddle38/lib/python3.8/site-packages/paddle_serving_app/reader/image_reader.py\", line 426, in __call__\r\n    bbox_result = self._get_bbox_result(image_with_bbox, fetch_name,\r\n  File \"/root/anaconda3/envs/paddle38/lib/python3.8/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'save_infer_model/scale_0.tmp_1.lod'\r\nClassname: Op._run_postprocess.<locals>.postprocess_help\r\nFunctionName: postprocess_help\r\n\r\n2、当我用非pipeline方式部署时，fetch_map则有fetch_name.lod这个键，fetch_map:{'save_infer_model/scale_0.tmp_1': array([[0.0000000e+00, 6.3646980e-02, 5.2615891e+00, 1.2278875e+02,\r\n        1.6876831e+02, 3.5357916e+02],\r\n       [0.0000000e+00, 4.2369448e-02, 6.6680511e+01, 6.9318405e+01,\r\n        6.0023975e+02, 5.3855756e+02],\r\n       [0.0000000e+00, 1.8086428e-02, 1.2872772e+02, 1.4232706e+02,\r\n        2.9876392e+02, 3.3751181e+02],\r\n       [0.0000000e+00, 1.5854711e-02, 1.8734198e+02, 3.1824486e+01,\r\n        3.5457477e+02, 1.9962274e+02],\r\n       [0.0000000e+00, 1.5454855e-02, 2.1284140e+02, 1.9946268e+02,\r\n        3.9645621e+02, 4.0698849e+02],\r\n       [0.0000000e+00, 1.4058443e-02, 1.5301871e+02, 2.4853967e+02,\r\n        3.2183228e+02, 4.3125073e+02],\r\n       [0.0000000e+00, 1.2545503e-02, 1.1664839e+02, 2.4064153e+02,\r\n        3.0317432e+02, 4.3767188e+02],\r\n       [0.0000000e+00, 1.1161749e-02, 3.8942078e+01, 1.3401808e+02,\r\n        1.8269760e+02, 3.4691406e+02],\r\n       [0.0000000e+00, 1.0988280e-02, 1.4913477e+02, 1.8804048e+02,\r\n        3.2895029e+02, 3.5642706e+02],\r\n       [0.0000000e+00, 1.0884989e-02, 1.5156635e+02, 2.1480481e+02,\r\n        3.2716016e+02, 3.9296497e+02]], dtype=float32), 'save_infer_model/scale_0.tmp_1.lod': array([ 0, 10])}。\r\nclient与server通讯时则没有报错，可以正常返回预测结果。\r\n{'result': [{'bbox': [5.261589050292969, 122.78874969482422, 164.50672149658203, 231.79041290283203], 'category_id': 0, 'score': 0.06364697962999344}]}\r\n\r\n请问技术同学，为何同一个模型，不同部署方式，会出现lod缺失的问题，这个问题该如何处理呀，谢谢！\r\n```\r\n",
        "state": "closed",
        "user": "ClassmateXiaoyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-04T08:59:19+00:00",
        "updated_at": "2024-10-29T06:44:02+00:00",
        "closed_at": "2024-10-29T06:44:02+00:00",
        "comments_count": [
            "ClassmateXiaoyu",
            "fanruifeng",
            "ClassmateXiaoyu",
            "fanruifeng",
            "wjplove8",
            "HuiHuiSun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1903,
        "title": "paddle Serving 部署模型，出现KeyError报错",
        "body": "环境\r\nCUDA 11.6\r\ncudnn 8.4.1\r\n显卡：GTX 1080 \r\npython 3.6\r\npaddle-serving-app        0.7.0\r\npaddle-serving-client     0.7.0\r\npaddle-serving-server-gpu 0.7.0.post112\r\npaddlepaddle-gpu          2.3.2.post116\r\n\r\n用paddleDetection训练的PicoDet模型，通过python tools/export_model.py -c configs/picodet/application/mainbody_detection/picodet_lcnet_x2_5_640_mainbody.yml -o weights=output/picodet_lcnet_x2_5_640_mainbody/best_model --export_serving_model=True命令将inference模型转为了server模型。\r\n然后在服务端成功启动服务; 命令如下: python3 -m paddle_serving_server.serve --model serving_server --port 50982 --gpu_ids 1\r\n在启动客户端 test_client.py代码时\r\n发现一个问题，会出现lod报错。具体如下：\r\nTraceback (most recent call last):\r\n  File \"../../deploy/serving/test_client.py\", line 49, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'transpose_7.tmp_0.lod'\r\n 我的fetch_map 没找到这个key， 我检查了下serving_client_conf.prototxt 文件, 没有这个name , \r\n请问技术同学，为何会出现lod缺失的问题，这个问题该如何处理呀，谢谢！",
        "state": "closed",
        "user": "fanruifeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-06T08:40:35+00:00",
        "updated_at": "2024-04-16T09:06:36+00:00",
        "closed_at": "2024-04-16T09:06:36+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString",
            "fanruifeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1904,
        "title": "开启 opencv 选项时编译失败",
        "body": "使用的 cmake 命令：\r\n```bash\r\ncmake -DPYTHON_INCLUDE_DIR=$PYTHON_INCLUDE_DIR \\\r\n                                                -DPYTHON_LIBRARIES=$PYTHON_LIBRARIES \\\r\n                                                -DPYTHON_EXECUTABLE=$PYTHON_EXECUTABLE \\\r\n                                                -DCUDA_TOOLKIT_ROOT_DIR=$CUDA_PATH \\\r\n                                                -DCUDNN_LIBRARY=$CUDNN_LIBRARY \\\r\n                                                -DCUDA_CUDART_LIBRARY=$CUDA_CUDART_LIBRARY \\\r\n                                                -DTENSORRT_ROOT=$TENSORRT_LIBRARY_PATH \\\r\n                                                -DSERVER=ON \\\r\n                                                -DOPENCV_DIR=$OPENCV_DIR \\\r\n                                                -DWITH_OPENCV=ON \\\r\n                                                -DWITH_GPU=ON ..\r\n```\r\n日志：\r\n<details>\r\n<summary>展开查看</summary>\r\n<pre><code>\r\n-- Found Paddle host system: ubuntu, version: 20.04.5\r\n-- Found Paddle host system's CPU: 32 cores\r\n-- CXX compiler: /usr/bin/c++, version: GNU 9.4.0\r\n-- C compiler: /usr/bin/cc, version: GNU 9.4.0\r\n-- Compile Version Tag for wheel: 0.0.0\r\n-- Use PADDLE_ON_INFERENCE\r\n-- Do not have AVX2 intrinsics and disabled MKL-DNN\r\nCMake Error at cmake/external/zlib.cmake:61 (SET_PROPERTY):\r\n  SET_PROPERTY could not find TARGET zlib.  Perhaps it has not yet been\r\n  created.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:104 (include)\r\n\r\n\r\nCMake Error at cmake/external/zlib.cmake:62 (ADD_DEPENDENCIES):\r\n  Cannot add target-level dependencies to non-existent target \"zlib\".\r\n\r\n  The add_dependencies works for top-level logical targets created by the\r\n  add_executable, add_library, or add_custom_target commands.  If you want to\r\n  add file-level dependencies see the DEPENDS option of the add_custom_target\r\n  and add_custom_command commands.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:104 (include)\r\n\r\n\r\n-- BOOST_TAR: boost_1_74_0, BOOST_URL: http://paddlepaddledeps.bj.bcebos.com/boost_1_74_0.tar.gz\r\nCMake Warning at /usr/share/cmake-3.16/Modules/FindProtobuf.cmake:499 (message):\r\n  Protobuf compiler version doesn't match library version 3.6.1\r\nCall Stack (most recent call first):\r\n  cmake/external/protobuf.cmake:18 (FIND_PACKAGE)\r\n  CMakeLists.txt:106 (include)\r\n\r\n\r\nCMake Error at /usr/share/cmake-3.16/Modules/ExternalProject.cmake:2962 (get_property):\r\n  get_property could not find TARGET zlib.  Perhaps it has not yet been\r\n  created.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake-3.16/Modules/ExternalProject.cmake:3239 (_ep_add_configure_command)\r\n  cmake/external/protobuf.cmake:272 (ExternalProject_Add)\r\n  cmake/external/protobuf.cmake:301 (build_protobuf)\r\n  CMakeLists.txt:106 (include)\r\n\r\n\r\n-- Protobuf protoc executable: /home/ocr/Serving/build_server/third_party/install/protobuf/bin/protoc\r\n-- Protobuf-lite library: /home/ocr/Serving/build_server/third_party/install/protobuf/lib/libprotobuf-lite.a\r\n-- Protobuf library: /home/ocr/Serving/build_server/third_party/install/protobuf/lib/libprotobuf.a\r\n-- Protoc library: /home/ocr/Serving/build_server/third_party/install/protobuf/lib/libprotoc.a\r\n-- Protobuf version: 3.1\r\n-- ssl:/usr/lib/x86_64-linux-gnu/libssl.so\r\n-- crypto:/usr/lib/x86_64-linux-gnu/libcrypto.so\r\n-- Current cuDNN header is /usr/include/cudnn.h. Current cuDNN version is v8.1. \r\npaddle install dir: /home/ocr/Serving/build_server/third_party/install/Paddle/\r\nWITH_GPU = ON\r\nCUDA: 11.2, CUDNN_MAJOR_VERSION: 8\r\n-- PADDLE_LIB_PATH=http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/GPU/x86-64_gcc8.2_avx_mkl_cuda11.2_cudnn8.2.1_trt8.0.3.4/paddle_inference.tgz\r\npaddle serving source dir: /home/ocr/Serving\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/__init__.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/analyse.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/channel.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/dag.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/error_catch.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/__init__.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/gateway.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/any.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/api.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/compiler\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/compiler/plugin.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/descriptor.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/duration.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/empty.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/field_mask.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/source_context.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/struct.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/timestamp.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/type.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/wrappers.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proxy_server.go\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/local_service_handler.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/logger.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/operator.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/pipeline_client.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/pipeline_server.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/profiler.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/prometheus_metrics.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/__init__.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/pipeline_service.proto\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/run_codegen.py\r\n-- Up-to-date: /home/ocr/Serving/build_server/python/paddle_serving_server/pipeline/util.py\r\npython env: \r\n-- Configuring incomplete, errors occurred!\r\n</code></pre>\r\n</details>\r\n\r\n错误日志：\r\n<details>\r\n<summary>展开查看</summary>\r\n<pre><code>\r\nPerforming C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_c22ef/fast && /usr/bin/make -f CMakeFiles/cmTC_c22ef.dir/build.make CMakeFiles/cmTC_c22ef.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_c22ef.dir/src.c.o\r\n/usr/bin/cc   -DCMAKE_HAVE_LIBC_PTHREAD   -o CMakeFiles/cmTC_c22ef.dir/src.c.o   -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.c\r\nLinking C executable cmTC_c22ef\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_c22ef.dir/link.txt --verbose=1\r\n/usr/bin/cc  -DCMAKE_HAVE_LIBC_PTHREAD    -rdynamic CMakeFiles/cmTC_c22ef.dir/src.c.o  -o cmTC_c22ef \r\n/usr/bin/ld: CMakeFiles/cmTC_c22ef.dir/src.c.o: in function `main':\r\nsrc.c:(.text+0x46): undefined reference to `pthread_create'\r\n/usr/bin/ld: src.c:(.text+0x52): undefined reference to `pthread_detach'\r\n/usr/bin/ld: src.c:(.text+0x63): undefined reference to `pthread_join'\r\ncollect2: error: ld returned 1 exit status\r\nmake[1]: *** [CMakeFiles/cmTC_c22ef.dir/build.make:87: cmTC_c22ef] Error 1\r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nmake: *** [Makefile:121: cmTC_c22ef/fast] Error 2\r\n\r\n\r\nSource file was:\r\n#include <pthread.h>\r\n\r\nvoid* test_func(void* data)\r\n{\r\n  return data;\r\n}\r\n\r\nint main(void)\r\n{\r\n  pthread_t thread;\r\n  pthread_create(&thread, NULL, test_func, NULL);\r\n  pthread_detach(thread);\r\n  pthread_join(thread, NULL);\r\n  pthread_atfork(NULL, NULL, NULL);\r\n  pthread_exit(NULL);\r\n\r\n  return 0;\r\n}\r\n\r\nDetermining if the function pthread_create exists in the pthreads failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_b0981/fast && /usr/bin/make -f CMakeFiles/cmTC_b0981.dir/build.make CMakeFiles/cmTC_b0981.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_b0981.dir/CheckFunctionExists.c.o\r\n/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create   -o CMakeFiles/cmTC_b0981.dir/CheckFunctionExists.c.o   -c /usr/share/cmake-3.16/Modules/CheckFunctionExists.c\r\nLinking C executable cmTC_b0981\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_b0981.dir/link.txt --verbose=1\r\n/usr/bin/cc  -DCHECK_FUNCTION_EXISTS=pthread_create    -rdynamic CMakeFiles/cmTC_b0981.dir/CheckFunctionExists.c.o  -o cmTC_b0981  -lpthreads \r\n/usr/bin/ld: cannot find -lpthreads\r\ncollect2: error: ld returned 1 exit status\r\nmake[1]: *** [CMakeFiles/cmTC_b0981.dir/build.make:87: cmTC_b0981] Error 1\r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nmake: *** [Makefile:121: cmTC_b0981/fast] Error 2\r\n\r\n\r\n\r\nPerforming C++ SOURCE FILE Test AVX512F_FOUND failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_0051d/fast && /usr/bin/make -f CMakeFiles/cmTC_0051d.dir/build.make CMakeFiles/cmTC_0051d.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding CXX object CMakeFiles/cmTC_0051d.dir/src.cxx.o\r\n/usr/bin/c++    -DAVX512F_FOUND -mavx512f   -o CMakeFiles/cmTC_0051d.dir/src.cxx.o -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.cxx\r\nLinking CXX executable cmTC_0051d\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_0051d.dir/link.txt --verbose=1\r\n/usr/bin/c++   -DAVX512F_FOUND -mavx512f    -rdynamic CMakeFiles/cmTC_0051d.dir/src.cxx.o  -o cmTC_0051d \r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\n\r\n\r\n...and run output:\r\nIllegal instruction\r\nReturn value: FAILED_TO_RUN\r\nSource file was:\r\n\r\n#include <immintrin.h>\r\nint main()\r\n{\r\n    __m512i a = _mm512_set_epi32 (-1, 2, -3, 4, -1, 2, -3, 4,\r\n                                  13, -5, 6, -7, 9, 2, -6, 3);\r\n    __m512i result = _mm512_abs_epi32 (a);\r\n    return 0;\r\n}\r\nPerforming C SOURCE FILE Test C_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_cefd7/fast && /usr/bin/make -f CMakeFiles/cmTC_cefd7.dir/build.make CMakeFiles/cmTC_cefd7.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_cefd7.dir/src.c.o\r\n/usr/bin/cc   -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -DC_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor   -Wnon-virtual-dtor -o CMakeFiles/cmTC_cefd7.dir/src.c.o   -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.c\r\ncc1: warning: command line option '-Wnon-virtual-dtor' is valid for C++/ObjC++ but not for C\r\nLinking C executable cmTC_cefd7\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_cefd7.dir/link.txt --verbose=1\r\n/usr/bin/cc  -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -DC_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor    -rdynamic CMakeFiles/cmTC_cefd7.dir/src.c.o  -o cmTC_cefd7 \r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\n\r\n\r\nSource file was:\r\nint main(void) { return 0; }\r\nPerforming C SOURCE FILE Test C_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_2f405/fast && /usr/bin/make -f CMakeFiles/cmTC_2f405.dir/build.make CMakeFiles/cmTC_2f405.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_2f405.dir/src.c.o\r\n/usr/bin/cc   -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -DC_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor   -Wdelete-non-virtual-dtor -o CMakeFiles/cmTC_2f405.dir/src.c.o   -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.c\r\ncc1: warning: command line option '-Wdelete-non-virtual-dtor' is valid for C++/ObjC++ but not for C\r\nLinking C executable cmTC_2f405\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_2f405.dir/link.txt --verbose=1\r\n/usr/bin/cc  -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -DC_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor    -rdynamic CMakeFiles/cmTC_2f405.dir/src.c.o  -o cmTC_2f405 \r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\n\r\n\r\nSource file was:\r\nint main(void) { return 0; }\r\nPerforming C SOURCE FILE Test C_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_3e149/fast && /usr/bin/make -f CMakeFiles/cmTC_3e149.dir/build.make CMakeFiles/cmTC_3e149.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_3e149.dir/src.c.o\r\n/usr/bin/cc   -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -Wno-unused-parameter -Wno-unused-function -Wno-error=literal-suffix -Wno-error=sign-compare -Wno-error=unused-local-typedefs -DC_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality   -Wno-error=parentheses-equality -o CMakeFiles/cmTC_3e149.dir/src.c.o   -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.c\r\ncc1: error: '-Werror=parentheses-equality': no option -Wparentheses-equality\r\nmake[1]: *** [CMakeFiles/cmTC_3e149.dir/build.make:66: CMakeFiles/cmTC_3e149.dir/src.c.o] Error 1\r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nmake: *** [Makefile:121: cmTC_3e149/fast] Error 2\r\n\r\n\r\nSource file was:\r\nint main(void) { return 0; }\r\nPerforming C++ SOURCE FILE Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality failed with the following output:\r\nChange Dir: /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/make cmTC_ffe96/fast && /usr/bin/make -f CMakeFiles/cmTC_ffe96.dir/build.make CMakeFiles/cmTC_ffe96.dir/build\r\nmake[1]: Entering directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nBuilding CXX object CMakeFiles/cmTC_ffe96.dir/src.cxx.o\r\n/usr/bin/c++    -std=c++11 -D__const__= -DUSE_PTHREAD -fPIC -fno-omit-frame-pointer -Wall -Wextra -Wnon-virtual-dtor -Wdelete-non-virtual-dtor -Wno-unused-parameter -Wno-unused-function -Wno-error=literal-suffix -Wno-error=sign-compare -Wno-error=unused-local-typedefs -DCXX_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality   -Wno-error=parentheses-equality -o CMakeFiles/cmTC_ffe96.dir/src.cxx.o -c /home/ocr/Serving/build_server/CMakeFiles/CMakeTmp/src.cxx\r\ncc1plus: error: '-Werror=parentheses-equality': no option -Wparentheses-equality\r\nmake[1]: *** [CMakeFiles/cmTC_ffe96.dir/build.make:66: CMakeFiles/cmTC_ffe96.dir/src.cxx.o] Error 1\r\nmake[1]: Leaving directory '/home/ocr/Serving/build_server/CMakeFiles/CMakeTmp'\r\nmake: *** [Makefile:121: cmTC_ffe96/fast] Error 2\r\n\r\n\r\nSource file was:\r\nint main() { return 0; }\r\n</code></pre>\r\n</details>",
        "state": "closed",
        "user": "thep0y",
        "closed_by": "thep0y",
        "created_at": "2023-01-06T09:14:01+00:00",
        "updated_at": "2023-01-09T10:03:02+00:00",
        "closed_at": "2023-01-09T10:03:02+00:00",
        "comments_count": [
            "thep0y"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1906,
        "title": "paddleserving 部署 picodet模型, 客户端报错",
        "body": "环境\r\nCUDA 11.6\r\ncudnn 8.4.1\r\n显卡：GTX 1080 \r\npython 3.6\r\npaddle-bfloat             0.1.7\r\npaddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server-gpu 0.9.0.post112\r\npaddlelite                2.9\r\npaddlepaddle-gpu          2.3.2.post116\r\n\r\n用paddle训练的Picodet模型，通过python -m paddle_serving_client.convert --dirname  --model_filename  --params_filename  --serving_server serving_server --serving_client serving_client命令将inference模型转为了server模型。\r\n发现一个问题会出现lod报错。具体如下：\r\n1、当我用pipeline方式部署，fetch_dict中没有fetch_name.lod这个键，{'transpose_7.tmp_0': array([[[ 2.0223362 ,  1.5683302 , -0.40058073, ...,  0.0507356 ,\r\n          0.8766475 ,  0.8922676 ],\r\n        [ 0.34861326,  1.7860167 ,  1.4869571 , ...,  0.00460189,\r\n          0.7187939 ,  0.8137586 ],\r\n        [-0.44164556, -0.7165884 ,  1.3838978 , ...,  0.16071278,\r\n          0.63282794,  0.6942027 ],\r\n        ...,\r\n        [-0.20754977, -0.4660724 , -0.4744862 , ..., -0.3195387 ,\r\n          0.04248828,  0.16745234],\r\n        [-0.13426918, -0.3694974 , -0.39638266, ..., -0.26385802,\r\n          0.04389518,  0.20333196],\r\n        [-0.18678631, -0.376009  , -0.3708608 , ..., -0.18855351,\r\n          0.13031027,  0.27454522]]], dtype=float32)}\r\n\r\n也就是没有lod信息，client与server通讯时，出现报错\r\nTraceback (most recent call last):\r\n  File \"../../deploy/serving/test_client.py\", line 49, in <module>\r\n    postprocess(fetch_map)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n    self.clsid2catid)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\n    lod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'transpose_7.tmp_0.lod'\r\n\r\n2、当我用非pipeline方式部署时，\r\nserver通讯时可以正常启动, 客户端也是后处理部分报错{'err_no': 10000, 'err_msg': 'Log_id: 10000  Raise_msg: transpose_0.tmp_0  ClassName: Op._run_postprocess.<locals>.postprocess_help  FunctionName: postprocess_help', 'key': [], 'value': [], 'tensors': []} \r\n\r\n\r\n请教下各位同学,第一次部署服务, 按照文档调整,不知道是什么问题导致的, 请各位帮忙看下",
        "state": "closed",
        "user": "fanruifeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-10T02:37:51+00:00",
        "updated_at": "2024-04-16T09:06:37+00:00",
        "closed_at": "2024-04-16T09:06:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1905,
        "title": "No module named 'paddle_serving_server'",
        "body": "pip list查看paddle_serving_server是存在的\r\n![image](https://user-images.githubusercontent.com/99126288/211184039-0ee0b8b4-91de-4246-a79f-3adb39c7ef09.png)\r\n[root@192 fit_a_line]# python -m paddle_serving_server.serve --model serving_server\r\n/usr/bin/python: Error while finding module specification for 'paddle_serving_server.serve' (ModuleNotFoundError: No module named 'paddle_serving_server')\r\n[root@192 fit_a_line]# \r\n",
        "state": "closed",
        "user": "zyzz1974",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-08T06:47:16+00:00",
        "updated_at": "2024-03-05T06:51:53+00:00",
        "closed_at": "2024-03-05T06:51:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HexToString"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1908,
        "title": "Error response from daemon: manifest for paddlepaddle/serving:0.8.0-devel not found: manifest unknown: manifest unknown",
        "body": "已经下好了，为什么还是无法启动？![image](https://user-images.githubusercontent.com/99126288/211518168-525ef8ce-cdf8-4b18-9426-7fe258a8585f.png)\r\n",
        "state": "closed",
        "user": "zyzz1974",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-10T09:50:07+00:00",
        "updated_at": "2024-04-16T09:06:38+00:00",
        "closed_at": "2024-04-16T09:06:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1909,
        "title": "paddleServing 部署目标检测模型 , 报错问题",
        "body": "[](url)我安装的环境如下: \r\npaddle-bfloat                  0.1.7\r\npaddle-serving-app             0.9.0\r\npaddle-serving-client          0.9.0\r\npaddle-serving-server-gpu      0.9.0.post112\r\npaddle2onnx                    1.0.5\r\npaddlefsl                      1.1.0\r\npaddlehub                      2.3.1\r\npaddlenlp                      2.4.9\r\npaddlepaddle-gpu               2.3.2.post116\r\npaddleslim                     2.2.1\r\npaddlex                        2.1.0\r\n我的问题是: \r\npaddleServing  部署 picodet模型 ； 目前我训练结束后, 转成inference model 来推理时没问题的, 转成paddleServing方式后, 加载报错了, 错误信息如下；  feed_dict的输出维度以及信息不了解是啥意思, 后处理报错;\r\n![aa](https://user-images.githubusercontent.com/42561039/212000152-4847cf89-b0a9-47be-a8c4-4cd481d6da43.jpg)\r\n\r\n我的config.yml如下:\r\n\r\n-             #uci模型路径\r\n-             model_config: \"serving_server\"\r\n- \r\n-             #计算硬件类型: 空缺时由devices决定(CPU/GPU)，0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n-             device_type: 1\r\n- \r\n-             #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n-             devices: \"0\" # \"0,1\"\r\n- \r\n-             #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n-             client_type: local_predictor\r\n- \r\n-             #Fetch结果列表，以client_config中fetch_var的alias_name为准\r\n-             fetch_list: ['transpose_0.tmp_0', 'transpose_1.tmp_0', 'transpose_2.tmp_0', 'transpose_3.tmp_0', 'transpose_4.tmp_0', 'transpose_5.tmp_0', 'transpose_6.tmp_0', 'transpose_7.tmp_0']\r\n我的  serving_server_conf.prototxt 如下: \r\n![bb](https://user-images.githubusercontent.com/42561039/211999720-10e690fd-6c7b-4801-8370-ba8469579ef9.jpg)\r\n我锁定的错误信息位置:\r\n![cc](https://user-images.githubusercontent.com/42561039/212000092-21d34f2b-33e7-40c2-af58-f82c6b887f68.jpg)\r\n\r\n感觉是 由于模型转化后的 feed_dict输出不对, 我输出出来的不是 bbox_num  和bbox的值, 请老师们看下是哪里的问题, 帮忙解决下\r\n\r\n",
        "state": "closed",
        "user": "fanruifeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-12T07:07:48+00:00",
        "updated_at": "2024-04-16T09:06:39+00:00",
        "closed_at": "2024-04-16T09:06:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1907,
        "title": "[Security] [Bug] Unsafe deserialization is prone to RCE",
        "body": "https://github.com/PaddlePaddle/Serving/blob/bdf4ada65e40c9d8146b9aac14a8cf406d9ba37e/python/pipeline/operator.py#L1753\r\n\r\n`np_data = np.load(byte_data, allow_pickle=True)` can trigger pickle deserialization\r\n\r\nTracing its call chain is as follows:\r\n\r\n```\r\n(paddle_serving_server/pipeline)\r\noperator.py:1753 np_data = np.load(byte_data, allow_pickle=True)\r\noperator.py:1763 unpack_request_package(self, request)\r\ndag.py:799 unpack_func = op.unpack_request_package (in _build_dag)\r\ndag.py:814 build(self)\r\ndag.py:94 (in_channel, out_channel, pack_rpc_func,unpack_rpc_func) = self._dag.build()\r\ndag.py:306 dictdata, log_id, prod_errcode, prod_errinfo = self._unpack_rpc_func(rpc_request)\r\ndag.py:374 req_channeldata = self._pack_channeldata(rpc_request, data_id) (in call)\r\npipeline_server.py:73 resp = self._dag_executor.call(request)\r\n```\r\nIt is speculated that Pickle deserialization can be triggered by constructing the `tensor` field in the request.\r\n\r\npoc:\r\n\r\n```\r\ncurl -X POST -k http://localhost:18082/uci/prediction -d '{\"key\": [\"x\"], \"value\": [\"0.0137, -0.1136, 0.2553, -0.0692, 0.0582, -0.0727, -0.1583, -0.0584, 0.6283, 0.4919, 0.1856, 0.0795, -0.0332\"],\"tensors\":[{name:\"A\",elem_type: \"13\",byte_data: payload,}]}'\r\n```\r\n\r\nThe payload is generated by pickle.dumps",
        "state": "closed",
        "user": "crazymanarmy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-10T07:26:11+00:00",
        "updated_at": "2024-03-05T06:51:54+00:00",
        "closed_at": "2024-03-05T06:51:54+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1910,
        "title": "Runtime镜像中没有Python",
        "body": "`registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda11.2-cudnn8-runtime` 中没装python吗？ \r\n```\r\nroot@ecs-cd51:~# docker run --rm -i registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda11.2-cudnn8-runtime python -V\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown.\r\nroot@ecs-cd51:~# docker run --rm -i registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda11.2-cudnn8-runtime python3 -V\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"python3\": executable file not found in $PATH: unknown\r\n```",
        "state": "closed",
        "user": "shengzhou1216",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-16T10:28:47+00:00",
        "updated_at": "2024-04-16T09:06:39+00:00",
        "closed_at": "2024-04-16T09:06:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1916,
        "title": " 采用paddleserving部署好了服务，启动了web_service，如何连续保存图片？",
        "body": "采用paddleserving部署好了服务，启动了web_service，如何连续保存图片？连续保存的图片我可用来扩增图片样本数据。谢谢！",
        "state": "closed",
        "user": "Jaccica",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T02:30:28+00:00",
        "updated_at": "2024-04-16T09:06:41+00:00",
        "closed_at": "2024-04-16T09:06:41+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1911,
        "title": "Windows上无法使用Serving部署Detection模型",
        "body": "### 系统环境：\r\n- OS: Windows10\r\n- Python: 3.8.10\r\n- Paddle: paddlepaddle-gpu 2.3.0.post112\r\n- Serving: 0.9.0\r\n- paddle-serving-app: 0.9.0\r\n- paddle-serving-client: 0.9.0\r\n- paddle-serving-server: 0.9.0\r\n### 操作步骤\r\nWindows上按照[示例](https://github.com/PaddlePaddle/Serving/tree/v0.9.0/examples/C%2B%2B/PaddleDetection/yolov4)使用Serving部署Detection服务失败。\r\n执行 `python3 -m paddle_serving_server.serve --model yolov4_model --port 9393 --gpu_ids 0` 启动服务\r\n首先会提示touch和mkdir -p命令在windows中无法正常执行，故根据代码直接创建相关目录和文件夹。\r\n创建目录和文件后执行启动服务命令报“name 'fcntl' is not defined”错误，故注释掉fcntl的相关代码，再次执行报如下错误：\r\n![image](https://user-images.githubusercontent.com/16732930/215301240-c1a0a893-b3cf-479f-9dce-8b117124da33.png)\r\n发现serving-cpu-noavx-openblas-0.9.0中的内容如下，是linux的动态库\r\n![image](https://user-images.githubusercontent.com/16732930/215301274-58cdccf0-00dd-4d1c-91a0-d4e825d980ad.png)",
        "state": "closed",
        "user": "YishuiLi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-29T02:46:50+00:00",
        "updated_at": "2024-03-05T06:51:55+00:00",
        "closed_at": "2024-03-05T06:51:55+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1912,
        "title": "在AI studio上部署paddle3d目标检测模型报错",
        "body": "服务端正常启动\r\nGoing to Run Comand\r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision fp32 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912 \r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralDetectionOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralFeatureExtractOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralPicodetOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralRemoteOp\r\nI0100 00:00:00.000000 13001 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 13001 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 13001 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 13001 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 13001 general_model_service.pb.h:1650] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 13001 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 13001 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nC++ Serving service started successfully!\r\n\r\n\r\n\r\n使用http方式进行预测时，报错\r\naistudio@jupyter-3310911-5426973:~/Serving$ curl -XPOST http://0.0.0.0:9393/GeneralModelService/inference -d ' {\"x\":\"/home/aistudio/000004.png\"}'\r\n[10.36.12.114:9393][E-5100]InferService inference failed!aistudio@jupyter-3310911-5426973:~/Serving$ \r\n\r\n支持https方式的调用吗？",
        "state": "closed",
        "user": "TreasureYi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-06T05:33:29+00:00",
        "updated_at": "2024-04-16T09:06:41+00:00",
        "closed_at": "2024-04-16T09:06:40+00:00",
        "comments_count": [
            "github-actions[bot]",
            "TreasureYi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1917,
        "title": "自动构建ocr测试用例执行失败",
        "body": "请修复一下/test_ocr_concate.py。导致全部pr过不了\r\n",
        "state": "closed",
        "user": "guojiahuiEmily",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T03:55:49+00:00",
        "updated_at": "2024-04-16T09:06:42+00:00",
        "closed_at": "2024-04-16T09:06:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1913,
        "title": "ppocrv3_det ppocrv3_rec model C++ Serving TensorRT start failed",
        "body": "![image](https://user-images.githubusercontent.com/58454582/217485204-842cb026-eca2-4d65-b72f-5368f40a8d40.png)\r\n![image](https://user-images.githubusercontent.com/58454582/217485237-0e8d0ef0-5fad-4127-8ad6-14a957fef0e1.png)\r\n",
        "state": "closed",
        "user": "liuwqiang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T09:10:29+00:00",
        "updated_at": "2025-01-14T06:40:57+00:00",
        "closed_at": "2025-01-14T06:40:57+00:00",
        "comments_count": [
            "github-actions[bot]",
            "allen20200111"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1919,
        "title": "按照PaddleDetection中Paddle Serving服务化部署快速示例，成功部署了服务，但是在客户端没有得到正确的检测结果。",
        "body": "#启动服务端模型预测服务\r\npython deploy/serving/python/web_service.py --model_dir output_inference/yolov3_darknet53_270e_coco &\r\n![image](https://user-images.githubusercontent.com/75315116/220002349-b93585ab-7956-4455-b90e-ac5ad3b5af74.png)\r\n#客户端得到的结果，只有在demo图像中检测到了一个person，但是通过python tools/infer.py可以得到多个检测结果：\r\n![image](https://user-images.githubusercontent.com/75315116/220002464-7dc81231-5929-48c6-b5fb-441b9afb6e7a.png)\r\n\r\n",
        "state": "closed",
        "user": "LiuHaodongdong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-20T03:26:05+00:00",
        "updated_at": "2024-03-05T06:51:56+00:00",
        "closed_at": "2024-03-05T06:51:56+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1920,
        "title": "serving的重大bug",
        "body": "当serving的server端接收到client发送的数据转换不成所需要的数据类型时，server会自动退出，这种bug不可理喻！",
        "state": "closed",
        "user": "whtwhtw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-21T02:57:45+00:00",
        "updated_at": "2024-03-12T06:40:49+00:00",
        "closed_at": "2024-03-12T06:40:49+00:00",
        "comments_count": [
            "github-actions[bot]",
            "wjplove8",
            "xuxiaobao-3"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1921,
        "title": "docker镜像编译失败啊",
        "body": "我用 serving的docker镜像编译serving竟然编译失败了！我使用的镜像是 registry.baidubce.com/paddlepaddle/serving:0.9.0-devel\r\n\r\n这是我的日志 \r\n[编译.txt](https://github.com/PaddlePaddle/Serving/files/10799546/default.txt)\r\n",
        "state": "closed",
        "user": "liuwqiang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T02:41:17+00:00",
        "updated_at": "2024-03-05T06:51:57+00:00",
        "closed_at": "2024-03-05T06:51:57+00:00",
        "comments_count": [
            "liuwqiang",
            "liuwqiang",
            "delayk",
            "delayk"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1922,
        "title": "两个OP组合; AttributeError: 'VirtualOp' object has no attribute 'model_config'",
        "body": "模型串并联问题，如下图，先目标检测，然后根据检测结果再对图片进行分割，分割之后进行图像分类；然后和另一个目标检测的结果进行结果组合。最后一个OP不进行模型预测，只进行前面input_ops的预测结果进行整合。\r\n![image](https://user-images.githubusercontent.com/36287503/220563803-8d9b2515-53c8-4815-acb5-9e1e13c4cf3d.png)\r\n启动报错\r\n![da715c481a09f476538da87fca49744](https://user-images.githubusercontent.com/36287503/220565010-f408e2f5-455c-4f09-8126-bb18bea0d556.png)\r\n组合模型的代码\r\n```\r\nclass CombineOp(Op):\r\n\r\n    def preprocess(self, input_data, data_id, log_id):\r\n        out_data = {}\r\n        for op_name, data in input_data.items():\r\n            if 'bbox_result' in data.keys():\r\n                _LOGGER.info(\"{}: {}\".format(op_name, data['bbox_result']))\r\n                out_data[op_name] = str(data['bbox_result'])\r\n            else:\r\n                data_str = json.dumps(data).replace('\"', \"'\").replace(\"'[\", \"[\").replace(\"]'\", \"]\")\r\n                _LOGGER.info(\"{}: {}\".format(op_name, data_str))\r\n                out_data[op_name] = data_str\r\n        return out_data, True, None, \"\"\r\n```\r\n当图像分类OP的input_ops中包含read_op也会报同样错误.\r\n关键代码\r\n```\r\n            if type == \"start\":\r\n                op_map[id] = read_op\r\n                # continue\r\n            elif type == \"end\":\r\n                input_ops = []\r\n                for input_id in inputIds:\r\n                    input_ops.append(op_map[input_id])\r\n                res_op = CombineOp(\"combine\", input_ops=input_ops)\r\n                op_map[id] = res_op\r\n            else:\r\n                modelId = op[\"modelId\"]\r\n                input_ops = []\r\n                op_temp = None\r\n                if inputIds is None:\r\n                    input_ops.append(read_op)\r\n                else:\r\n                    for input_id in inputIds:\r\n                        input_ops.append(op_map[input_id])\r\n                if type == \"classification\":\r\n                    op_temp = ClassificationOp(name=modelId, input_ops=input_ops)\r\n                else:\r\n                    op_temp = DetectionOp(name=modelId, input_ops=input_ops)\r\n                op_map[id] = op_temp\r\n```\r\n起始节点为read_op, 结束节点为CombineOp(组合OP,不进行模型预测)\r\n目标检测OP根据前置OP组合input_ops\r\nop_map为一个字典{OP的id: 对应OP}",
        "state": "closed",
        "user": "zhengwangwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T08:50:06+00:00",
        "updated_at": "2024-04-16T09:06:43+00:00",
        "closed_at": "2024-04-16T09:06:43+00:00",
        "comments_count": [
            "github-actions[bot]",
            "smallfish45",
            "zhengwangwang",
            "zhengwangwang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1924,
        "title": "【急】pipeline serving如何将自定义的额外的数据从preprocess传到postprocess",
        "body": "【急】pipeline serving如何将自定义的额外的数据从preprocess传到postprocess\r\n我想将preprocess中经过清洗的文本数据传到postprocess中，请问如何实现\r\n![图片](https://user-images.githubusercontent.com/44657953/222112902-0aad3af7-4377-4e23-b25e-ca89869f20f9.png)\r\n",
        "state": "closed",
        "user": "geekChinaMaster",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T10:25:53+00:00",
        "updated_at": "2024-03-05T06:51:58+00:00",
        "closed_at": "2024-03-05T06:51:58+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1923,
        "title": "这项目是不是停了？",
        "body": "RT，没什么动静的样子",
        "state": "closed",
        "user": "sunzhaoyang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T07:20:14+00:00",
        "updated_at": "2024-04-30T06:43:06+00:00",
        "closed_at": "2024-04-30T06:43:06+00:00",
        "comments_count": [
            "github-actions[bot]",
            "HaoLiuHust"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1925,
        "title": "java如何解析返回值tensor",
        "body": "java client 返回值如何解析，可以给个例子吗\r\n{\"outputs\":[{\"tensor\":[{\"float_data\":[0.0,0.930903971195221,272.0629577636719,133.62464904785157,362.3904113769531,210.66978454589845,0.0,0.04165499284863472,224.37013244628907,126.29287719726563,407.8487548828125,198.01980590820313,0.0,0.027022194117307664,301.24371337890627,148.58856201171876,341.6492919921875,227.12991333007813,0.0,0.02605859749019146,267.3560485839844,174.3739013671875,375.1983337402344,203.81365966796876,0.0,0.020890625193715097,19.798187255859376,96.92759704589844,84.05364227294922,138.19598388671876,0.0,0.016766920685768129,280.56549072265627,148.9947052001953,352.3643798828125,175.04298400878907,0.0,0.00988149642944336,27.438291549682618,49.757843017578128,80.58512878417969,86.267333984375,0.0,0.007814896292984486,133.08984375,57.31978988647461,253.93695068359376,120.90846252441406,1.0,0.13379301130771638,281.61187744140627,143.43833923339845,354.6566162109375,209.21778869628907,1.0,0.032523706555366519,248.6391143798828,135.3167724609375,379.3636474609375,237.5391845703125,1.0,0.027705401182174684,19.798187255859376,96.92759704589844,84.05364227294922,138.19598388671876,1.0,0.01427733339369297,267.3560485839844,174.3739013671875,375.1983337402344,203.81365966796876,1.0,0.013393879868090153,27.438291549682618,49.757843017578128,80.58512878417969,86.267333984375,1.0,0.009086854755878449,89.14093780517578,84.71559143066406,149.53675842285157,150.86851501464845,1.0,0.007297317963093519,224.37013244628907,126.29287719726563,407.8487548828125,198.01980590820313,1.0,0.006580862682312727,299.963134765625,173.16738891601563,344.46258544921877,188.04067993164063,1.0,0.0057264771312475208,45.78583526611328,107.68647003173828,60.17835235595703,128.91468811035157,1.0,0.00501543004065752,133.08984375,57.31978988647461,253.93695068359376,120.90846252441406],\"elem_type\":1,\"shape\":[18,6],\"name\":\"multiclass_nms3_0.tmp_0\",\"alias_name\":\"multiclass_nms3_0.tmp_0\"}],\"engine_name\":\"GeneralInferOp_0\"}],\"profile_time\":[1677727313592448,1677727313701123]}\r\n",
        "state": "closed",
        "user": "shiyun123456789",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T03:32:05+00:00",
        "updated_at": "2024-03-05T06:51:59+00:00",
        "closed_at": "2024-03-05T06:51:58+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1926,
        "title": "docker pull serving:0.9.0-cuda10.1-cudnn7-devel失败",
        "body": "执行命令 docker pull registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda10.1-cudnn7-devel\r\n报错结果：failed to register layer: ApplyLayer exit status 1 stdout:  stderr: unlinkat //build_scripts: invalid argument",
        "state": "closed",
        "user": "yanzhelee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T09:17:01+00:00",
        "updated_at": "2024-03-05T06:52:00+00:00",
        "closed_at": "2024-03-05T06:52:00+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1927,
        "title": "Adding table recognition model to pdserving",
        "body": "Hi,\r\n\r\nThe quick start document shows deployment of paddleocr detection and recognition models. Is it possible to deploy table recognition model as well ? following similar steps as for det and rec models?\r\n\r\n",
        "state": "closed",
        "user": "Tanmay98",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T06:18:22+00:00",
        "updated_at": "2024-03-12T06:40:50+00:00",
        "closed_at": "2024-03-12T06:40:50+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1929,
        "title": "paddleserving可以只当一个服务接口用吗？",
        "body": "我想将fasttext用paddleserving部署，只是借用paddleserving的高并发处理能力，不知道该怎么部署，能指点下吗？",
        "state": "closed",
        "user": "ingale726",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-10T06:43:49+00:00",
        "updated_at": "2024-03-12T06:40:51+00:00",
        "closed_at": "2024-03-12T06:40:51+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1928,
        "title": "部署paddlenlp的mbart模型报错",
        "body": "python -m paddle_serving_server.serve --model SERVING_SERVER_DIR --thread 10 --port 9393 --gpu_id 0\r\n执行上面命令报以下错误\r\n\r\nError Message Summary:\r\n----------------------\r\nNotFoundError: Operator (fusion_mbart_decoding) is not registered.\r\n  [Hint: op_info_ptr should not be null.] (at /paddle/paddle/fluid/framework/op_info.h:151)\r\n\r\n",
        "state": "closed",
        "user": "Amy234543",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-09T05:23:24+00:00",
        "updated_at": "2024-03-26T06:41:18+00:00",
        "closed_at": "2024-03-26T06:41:18+00:00",
        "comments_count": [
            "github-actions[bot]",
            "whisky-12"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1930,
        "title": "PaddleOCR C++ Serving core dump!!!",
        "body": "paddleocr的serving部署方式下，发起预测触发core dump报错\r\n![image](https://user-images.githubusercontent.com/11955508/225275639-9ea26d4d-26f8-4f4f-be4d-8fb28a9eaee7.png)\r\n\r\n\r\n环境：\r\n`docker image:paddlepaddle/paddle:2.4.2-gpu-cuda11.2-cudnn8.2-trt8.0`\r\n\r\n驱动信息：\r\n![image](https://user-images.githubusercontent.com/11955508/225267252-736da64e-f089-403a-81e4-5ae35e115adc.png)\r\n\r\n```\r\npaddle版本：\r\npaddle-bfloat             0.1.2          \r\npaddle-serving-app        0.9.0          \r\npaddle-serving-client     0.9.0          \r\npaddle-serving-server-gpu 0.9.0.post112  \r\npaddlepaddle-gpu          2.3.0.post112\r\n```\r\n\r\n运行参数\r\n```\r\nDir：Serving/examples/C++/PaddleOCR/ocr\r\nserving：python3 -m paddle_serving_server.serve --model ocr_det_model ocr_rec_model --op GeneralDetectionOp GeneralRecOp --thread 4 --port 9293 --gpu_ids 0\r\nclient：python3 ocr_cpp_client.py ocr_det_client ocr_rec_client\r\n\r\nserving_client_conf.prototxt配置\r\nfeed_var {\r\n  name: \"x\"\r\n  alias_name: \"x\"\r\n  is_lod_tensor: false\r\n  feed_type: 20\r\n  shape: 1\r\n}\r\n```",
        "state": "closed",
        "user": "yuanhaisu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T10:03:39+00:00",
        "updated_at": "2024-03-26T06:41:19+00:00",
        "closed_at": "2024-03-26T06:41:19+00:00",
        "comments_count": [
            "github-actions[bot]",
            "whisky-12"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1931,
        "title": "多个独立模型启动serving服务",
        "body": "paddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server     0.9.0\r\npaddlepaddle              2.4.2\r\nLinux ubuntu 20.04\r\n你好，我现在有100个独立的模型，我想为这100个独立的模型同时启动推理服务，我尝试了pipline和web service,但是还是占用资源太大了，模型本身并不大，只是模型之外的框架解释器之类的占用太多了，我应该怎末启动服务才能最节省内存资源？期待您的回复",
        "state": "closed",
        "user": "gitmhg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-20T07:29:38+00:00",
        "updated_at": "2024-03-26T06:41:20+00:00",
        "closed_at": "2024-03-26T06:41:20+00:00",
        "comments_count": [
            "whisky-12"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1932,
        "title": "【急急急！！！】InvalidArgumentError: Broadcast dimension mismatch.  使用window做客户端，centos为服务端 ，输入尺寸问题",
        "body": "InvalidArgumentError: Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 361, 1024] and the shape of Y = [1, 400, 1024]. Received [361] in X is not equal to [400] in Y at i:1.\r\n![image](https://user-images.githubusercontent.com/79358023/227481787-38403955-d175-42d1-88c2-38d0133b76aa.png)\r\n![image](https://user-images.githubusercontent.com/79358023/227481875-6d49c7c7-1c1f-4327-b28e-20a34927df5d.png)\r\n![image](https://user-images.githubusercontent.com/79358023/227481962-44e9cf7d-cbac-4504-a3a6-2c0af1aa2bdf.png)\r\n![image](https://user-images.githubusercontent.com/79358023/227482140-62c98a01-ec25-440a-85b4-817e9263f8ca.png)\r\n\r\n以上为报错及相关代码信息\r\nwindow做客户端，centos为服务端 ，请求发出后， 服务端报错， 个人认为是preprocess对图像处理不符合模型输入的问题\r\n但未找到解决办法，官方为找到关于perprocess配置详细说明文档 ， 模型为paddleDetetion导出的serving模型， 服务端部署启动正常\r\n以下为模型信息\r\n![image](https://user-images.githubusercontent.com/79358023/227483003-750ca723-73df-4271-9b90-da82e9e07fde.png)\r\n![image](https://user-images.githubusercontent.com/79358023/227483060-bdc2c439-b066-4814-8608-88d11e7e561a.png)\r\n\r\n请各位有过相关经验的同学 ，帮忙指点原因 ，多谢 ，辛苦了！！！\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "whisky-12",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-24T09:39:46+00:00",
        "updated_at": "2024-03-26T06:41:20+00:00",
        "closed_at": "2024-03-26T06:41:20+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1936,
        "title": "paddleserving有没有提供部署方向分类器的接口？",
        "body": "各位大佬，我想请教下，我想解决旋转不变性的角度适应性问题，就是各个角度拍照都能成功的识别。想用方向分类器解决，请问paddleserving有没有提供部署方向分类器的接口？谢谢！",
        "state": "closed",
        "user": "Jaccica",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T02:50:26+00:00",
        "updated_at": "2024-04-16T09:06:44+00:00",
        "closed_at": "2024-04-16T09:06:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1933,
        "title": "pipeline的detection demo一个都没跑通？",
        "body": "paddle相关安装包如下：\r\n/home/pdemo pip3 list|grep paddle\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\npaddle-bfloat             0.1.7\r\npaddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server-gpu 0.9.0.post112\r\npaddlepaddle-gpu          2.4.2.post112\r\n\r\n\r\n以/Serving/tree/v0.9.0/examples/Pipeline/ppyolo_mbv3/为例，web_service.py输出如下：\r\n\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\nW0325 12:45:47.886139 26989 analysis_predictor.cc:1395] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0325 12:45:47.921669 26989 analysis_predictor.cc:1243] ir_optim is turned off, no IR pass will be executed.\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\nI0325 12:45:47.983028 26989 ir_params_sync_among_devices_pass.cc:89] Sync params from CPU to GPU\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\nI0325 12:45:48.037250 26989 memory_optimize_pass.cc:219] Cluster name : cast_0.tmp_0  size: 8\r\nI0325 12:45:48.037276 26989 memory_optimize_pass.cc:219] Cluster name : pool2d_10.tmp_0  size: 64000\r\nI0325 12:45:48.037286 26989 memory_optimize_pass.cc:219] Cluster name : conv2d_73.tmp_0  size: 6553600\r\nI0325 12:45:48.037292 26989 memory_optimize_pass.cc:219] Cluster name : im_shape  size: 8\r\nI0325 12:45:48.037300 26989 memory_optimize_pass.cc:219] Cluster name : batch_norm_4.tmp_3  size: 6553600\r\nI0325 12:45:48.037304 26989 memory_optimize_pass.cc:219] Cluster name : hardswish_0.tmp_0  size: 1638400\r\nI0325 12:45:48.037312 26989 memory_optimize_pass.cc:219] Cluster name : elementwise_add_7  size: 179200\r\nI0325 12:45:48.037314 26989 memory_optimize_pass.cc:219] Cluster name : scale_factor  size: 8\r\nI0325 12:45:48.037317 26989 memory_optimize_pass.cc:219] Cluster name : image  size: 1228800\r\nI0325 12:45:48.037321 26989 memory_optimize_pass.cc:219] Cluster name : batch_norm_9.tmp_3  size: 614400\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI0325 12:45:48.201012 26989 analysis_predictor.cc:1318] ======= optimize end =======\r\nI0325 12:45:48.209089 26989 naive_executor.cc:110] ---  skip [feed], feed -> scale_factor\r\nI0325 12:45:48.209123 26989 naive_executor.cc:110] ---  skip [feed], feed -> image\r\nI0325 12:45:48.209134 26989 naive_executor.cc:110] ---  skip [feed], feed -> im_shape\r\nI0325 12:45:48.214416 26989 naive_executor.cc:110] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch\r\nI0325 12:45:48.214444 26989 naive_executor.cc:110] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch\r\n[OP Object] init success\r\nweb_service.py:38: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\r\n  data = np.fromstring(data, np.uint8)\r\nimage (1, 3, 320, 320)\r\nim_shape (1, 2)\r\nscale_factor (1, 2)\r\nW0325 12:45:51.099383 26989 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 11.2\r\nW0325 12:45:51.101948 26989 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.\r\nW0325 12:45:51.103163 26989 gpu_resources.cc:217] WARNING: device: . The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\r\n{'save_infer_model/scale_0.tmp_1': array([[  0.        ,   0.96408874, 104.77782   , 220.04672   ,\r\n        130.5245    , 313.18912   ]], dtype=float32)}\r\n\r\n可以看到从process输出的fetch_dict永远只有一个array，我手动修改operator.py，从client predict出来的结果是完整的，但不知道为什么operator.py下面的逻辑会只取[0:1]的结果。搜索相关case只有这个：https://github.com/PaddlePaddle/Serving/issues/975\r\n",
        "state": "closed",
        "user": "tommyfgj",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-25T12:50:00+00:00",
        "updated_at": "2024-03-26T06:41:21+00:00",
        "closed_at": "2024-03-26T06:41:21+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1934,
        "title": "使用paddleserver部署paddleCls，如何输出top5？",
        "body": "使用paddleserver部署paddleCls，如何输出top5？\r\n\r\n使用python predict_cls.py可以输出top5，但使用paddlerserver部署后只能输出top1，部署命令是python classification_web_service.py 。   在哪里可以修改输出类别数？？",
        "state": "closed",
        "user": "chengshaoxi0",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-31T03:09:45+00:00",
        "updated_at": "2024-04-16T06:41:51+00:00",
        "closed_at": "2024-04-16T06:41:51+00:00",
        "comments_count": [
            "github-actions[bot]",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1935,
        "title": "pdServing是否支持自定义的Model？",
        "body": "pdServing是否支持自定义的Model？ 还是只支持 模型库 https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Model_Zoo_CN.md？\r\n\r\n怎样开发来支持自定义的Model",
        "state": "closed",
        "user": "luoshenfu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-01T13:03:50+00:00",
        "updated_at": "2024-04-02T06:41:40+00:00",
        "closed_at": "2024-04-02T06:41:40+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1937,
        "title": "如何关闭服务日志",
        "body": "怎样关闭模型预测时的服务日志\r\nPipelineClient::predict pack_data time:1680858497.491558\r\n\r\n环境:\r\npaddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server-gpu 0.8.3.post102",
        "state": "open",
        "user": "kg-nlp",
        "closed_by": null,
        "created_at": "2023-04-07T09:12:31+00:00",
        "updated_at": "2023-04-18T02:15:53+00:00",
        "closed_at": null,
        "comments_count": [
            "kg-nlp"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1938,
        "title": "PaddleVideo的FootballAction使用Pipeline部署问题",
        "body": "环境：\r\nPaddleVideo版本：V2.2.2\r\npaddle-serving-app：0.9.0\r\npaddle-serving-client ： 0.9.0   \r\npaddle-serving-server-gpu： 0.9.0.post1028 \r\npaddlepaddle-gpu ： 2.4.2.post116\r\n项目：\r\n当前使用FootballAction进行模型训练，训练lstm使用train_lstm，导出使用train_lstm下的export_inference_model.py。最终，模型导出视图如下：\r\n![58e3b151cc2076962b606412309a1c8](https://user-images.githubusercontent.com/35001756/231431816-fb90c2df-3104-45a5-a450-f35aadbd89a8.png)\r\n使用paddle_serving_client.convert去将这个模型转换成paddle_serving_server要求的模型。模型信息如下：\r\n![1a09895bddcac9918baeb3bbf56e93a](https://user-images.githubusercontent.com/35001756/231431916-69461878-bd21-4937-9c88-eeba6e9ee314.png)\r\n在部署Pipeline后，预测报出的错误如下：\r\n![853bd5f82adfa7755061bd5023c2962](https://user-images.githubusercontent.com/35001756/231431990-330c7430-7b86-42cb-805e-362ae2dda2ab.png)\r\n![28a992cdab3c6885c05a12f25d0099a](https://user-images.githubusercontent.com/35001756/231432009-34003a82-9ce6-4951-9850-1c49f668c173.png)\r\n",
        "state": "closed",
        "user": "AvaMins",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-12T10:32:33+00:00",
        "updated_at": "2024-04-12T08:20:48+00:00",
        "closed_at": "2024-04-12T08:20:48+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1940,
        "title": "examples/Pipeline/PaddleDetection中的yolov3",
        "body": "使用[链接](https://github.com/PaddlePaddle/Serving/blob/v0.9.0/examples/Pipeline/PaddleDetection/yolov3/README_CN.md)中官方给的模型，无法跑通yolov3：先启动web_service.py（启动正常），然后启动pipeline_http_client.py，将报错如下：\r\n```\r\nTraceback (most recent call last):\r\nFile \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/error_catch.py\", line 97, in wrapper\r\nres = func(*args, **kw)\r\nFile \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/operator.py\", line 1181, in postprocess_help\r\nlogid_dict.get(data_id))\r\nFile \"web_service.py\", line 64, in postprocess\r\nfetch_dict, visualize=False))\r\nFile \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 427, in __call__\r\n self.clsid2catid)\r\nFile \"/usr/local/lib/python3.6/site-packages/paddle_serving_app/reader/image_reader.py\", line 344, in _get_bbox_result\r\nlod = [fetch_map[fetch_name + '.lod']]\r\nKeyError: 'save_infer_model/scale_0.tmp_1.lod'\r\nClassname: Op._run_postprocess.<locals>.postprocess_help\r\nFunctionName: postprocess_help\r\nERROR 2023-04-16 07:48:33,891 [dag.py:420] (data_id=0 log_id=0) Failed to predict: Log_id: 0  Raise_msg: save_infer_model/scale_0.tmp_1.lod     \r\nClassName: Op._run_postprocess.<locals>.postprocess_help  FunctionName: postprocess_help](url)\r\n```\r\n请问这是什么原因，是因为模型代码太老了吗？我看代码是两年前提交的，我的环境如下：\r\n```\r\npaddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server-gpu 0.9.0.post112\r\npaddle2onnx               1.0.6\r\npaddlefsl                 1.1.0\r\npaddlehub                 2.3.1\r\npaddlenlp                 2.4.3\r\npaddlepaddle              2.4.2\r\n\r\n```\r\n",
        "state": "closed",
        "user": "xxxdxxdxd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-16T07:37:50+00:00",
        "updated_at": "2024-04-16T06:41:52+00:00",
        "closed_at": "2024-04-16T06:41:52+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1945,
        "title": "启动pipeline web service一直卡在[OP Object] init success，不报错也不运行",
        "body": "<img width=\"491\" alt=\"79a0f9ad5b047e98bf7cd0cf651c51e\" src=\"https://github.com/PaddlePaddle/Serving/assets/58201310/1ecded8d-5860-406e-98ca-62c97337bb89\">\r\n",
        "state": "open",
        "user": "Karenlyw",
        "closed_by": null,
        "created_at": "2023-05-15T09:43:53+00:00",
        "updated_at": "2023-05-15T09:43:53+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1941,
        "title": "运行模型是出现error_msg: invalid arg list: ['feed_batch']",
        "body": "运行自己的文本检测模型时出现了这个错误",
        "state": "closed",
        "user": "HaoLiuHust",
        "closed_by": "HaoLiuHust",
        "created_at": "2023-04-17T09:43:35+00:00",
        "updated_at": "2023-04-18T08:24:54+00:00",
        "closed_at": "2023-04-18T08:24:54+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1942,
        "title": "NVIDIA A30部署服务，压力测试异常，内存溢出",
        "body": "pip列表：\r\npaddle-bfloat             0.1.7\r\npaddle-serving-app        0.3.0\r\npaddle-serving-client     0.5.0\r\npaddle-serving-server     0.5.0\r\npaddle-serving-server-gpu 0.5.0.post11\r\npaddlepaddle-gpu          2.3.2.post116\r\n压力测试一段时间之后，内存溢出",
        "state": "open",
        "user": "view6view",
        "closed_by": null,
        "created_at": "2023-04-20T08:27:55+00:00",
        "updated_at": "2023-04-20T10:17:30+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "view6view"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1944,
        "title": "ubuntu20.04 按照Docker安装Paddle Serving环境检查一直有错误。 xxx gpu environment running failure",
        "body": "折腾两天了，无论在线安装还是离线安装Wheel 包，环境检查都有错误\r\n117a5d2df849 /paddle/Serving {v0.9.0} python3 -m paddle_serving_server.serve check\r\n/usr/local/lib/python3.6/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nWelcome to the check env shell.Type help to list commands.\r\n\r\n(Cmd) check_all\r\nPaddlePaddle inference environment running success\r\nC++ cpu environment running success\r\nC++ gpu environment running failure, if you need this environment, please refer to https://github.com/PaddlePaddle/Serving/blob/develop/doc/Install_CN.md\r\nPipeline cpu environment running success\r\nPipeline gpu environment running failure, if you need this environment, please refer to https://github.com/PaddlePaddle/Serving/blob/develop/doc/Install_CN.md\r\n(Cmd) exit\r\nCheck Environment Shell Exit\r\n![2023-05-11 14-43-14 的屏幕截图](https://github.com/PaddlePaddle/Serving/assets/52989381/2fe7f0b1-e8cb-4ccd-ae6e-3ae5d132057b)\r\n",
        "state": "closed",
        "user": "fanxinkeji",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-11T06:54:59+00:00",
        "updated_at": "2024-09-03T06:42:10+00:00",
        "closed_at": "2024-09-03T06:42:10+00:00",
        "comments_count": [
            "github-actions[bot]",
            "anomousA"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1943,
        "title": "PaddleServing 在请求时返回的报错信息为‘RecOp' object has no attribute sorted boxes ",
        "body": "pipeline_http_client.py \r\n运行时返回：\r\n\r\npeline/error_catch.py , ine 97,in wrapperres = func(*args，**kw)File \"/home/xieming/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle_serving_server/poeline/operator.py , line 866，in preprocess helppreped_data，is_skip_process，prod_errcode,prod_errinfo = self.preprocess(File \"deploy/pdserving/web_service.py ，line 83，in preprocessself.dt_list = self.sorted boxes(self.dt_list)AttributeError: RecOp' object has no attribute sorted_boxes Classname: Op._run_preprocess.<locals>.preprocess_helpunctionName: preprocess_helpERROR 2023-05-09 07:49:19,551 [dagpy:418] (data_id-20 lgid=0) Failed to predict: Log-id: 0 RaisRecOp object has no attribute sorted_boxes ClassName: p._run_preprocess.<locals>.preprocesssg:helpFunctionName: preprocess_helpINF 2023-05-09 07:49:19,563 [pipeline-server.py:62] (log-id=0) inference request name:ocr self.namocrt ime:1683618559.5639737INFO2023-05-09 07:49:19,564[operator.py:1788] RequestOp unpack one request. log_id:0,clientip:name:ocr，method:prediction，time:1683618559.564209INFO 2023-05-09 07:49:19,564 [dag.py:378] (data_id=21 10g_id=0) Succ Generate IDINFO 2023-05-09 07:49:19,740 [operator.py:1454] prometheus inf count +1ERROR 2023-05-09 07:49:19,754 [error_catch.py:125]og_id:0Traceback (most recent call last):File \"/home/xieming/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle_serving_server/ppeline/error_catch.py\"，ine 97，in wrapperres = func(*args，**kw)File \"/home/xieming/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle_serving_server/ppeline/operator.py\"， line 866，in preprocess_helppreped_data，is_skip_process，prod_errcode， prod_errinfo = self.preprocess(File deploy/pdserving/web_service.py ，line 83，in preprocessself.dt_list = self.sorted boxes(self.dt_list)AttributeError:RecOp' object has no attribute 'sorted_boxes'Classname: Op.run_preprocess.<locals>.preprocess _helpFunctionName: preprocess_helpERROR 2023-05-09 07:49:19,757 [dag.py:418] (data_id=21 l0g-id=0) Failed to predict: Log-id: 0 Raisimsg: Rec0p object has no attribute sorted boxes ClassName: 0p.-run_preprocess.\r\n\r\n这个是为什么呢",
        "state": "closed",
        "user": "Myzr1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-09T09:30:26+00:00",
        "updated_at": "2024-05-14T06:42:29+00:00",
        "closed_at": "2024-05-14T06:42:29+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1946,
        "title": "可以多卡部署chatglm6b模型，进行推理？",
        "body": "没有找到一个模型分布式部署进行推理（目前有4张12g的显存卡，所以可以支持分布式推理吗）",
        "state": "closed",
        "user": "liuzhipengchd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-25T08:45:11+00:00",
        "updated_at": "2024-05-28T06:38:51+00:00",
        "closed_at": "2024-05-28T06:38:51+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1947,
        "title": "关于Ernie 3.0 百亿参数模型部署案例问题，20g模型怎么分成4个部分的",
        "body": "下载的模型，包括rank0-3，分成4个部分，而且还是静态图，20g的模型是怎么切割的",
        "state": "open",
        "user": "liuzhipengchd",
        "closed_by": null,
        "created_at": "2023-05-25T14:20:07+00:00",
        "updated_at": "2023-05-25T14:20:07+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1948,
        "title": "关于ppyoloe serving pipeline部署只输出一个检测框的bug问题及临时修补方法。",
        "body": "在/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/operator.py文件中， \r\nmidped_batch, error_code, error_info = self.process(\r\n                        feed_batch, typical_logid)\r\n通过上述命令获得的预测结果midped_batch正常，所有检测框数据都在，但在\r\n# normal tensor\r\n                    for idx, data_id in enumerate(data_ids):\r\n                        start = input_offset_dict[data_id][0]\r\n                        end = input_offset_dict[data_id][1]\r\n                        midped_data_dict[data_id][name] = value[start:end]\r\n进行筛选时，由于输入部分，只输入一张图片，data_ids=[0]，\r\n            for data_id in data_ids:\r\n                start = cur_offset\r\n                for key, val in preped_data_dict[data_id].items():\r\n                    if isinstance(val, (list, np.ndarray)):\r\n                        cur_offset += len(val)\r\n                    else:\r\n                        cur_offset += 1\r\n                    break\r\n                input_offset_dict[data_id] = [start, cur_offset]\r\n导致input_offset_dict是一个[0,1]的列表，则检测框取值只取了第一个检测框。\r\n具体产生原因没搞懂= =\r\n改成midped_data_dict[data_id][name] = value则解决此问题。\r\nresults [{'bbox': (array([[  0.       ,   0.8911667, 358.67282  ,  21.874176 , 487.72583  ,\r\n        590.9016   ]], dtype=float32), [6]), 'im_id': array([[0]])}]\r\n-------》\r\nresults [{'bbox': (array([[0.0000000e+00, 8.9116669e-01, 3.5867282e+02, 2.1874176e+01,\r\n        4.8772583e+02, 5.9090161e+02],\r\n       [0.0000000e+00, 8.7308830e-01, 5.3001239e+02, 3.1271460e+02,\r\n        5.9980963e+02, 5.2793616e+02],\r\n       [0.0000000e+00, 8.1745487e-01, 9.1202881e+01, 2.7218240e+02,\r\n        1.6436841e+02, 5.3688599e+02],\r\n       ...,\r\n       [1.9000000e+01, 6.6376023e-02, 3.6651331e+02, 2.2170178e+02,\r\n        3.9852289e+02, 2.4392216e+02],\r\n       [1.9000000e+01, 5.9004810e-02, 3.7924310e+02, 2.1985689e+02,\r\n        3.9876065e+02, 2.3381659e+02],\r\n       [1.9000000e+01, 5.6407396e-02, 3.4724228e+02, 2.5580838e+02,\r\n        3.8182556e+02, 2.7566129e+02]], dtype=float32), [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), 'im_id': array([[0]])}]\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "wj920291253",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-01T06:34:12+00:00",
        "updated_at": "2024-06-04T06:41:16+00:00",
        "closed_at": "2024-06-04T06:41:16+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1949,
        "title": "能不能增加一下java客户端调用GRPCserver 端OCR的实例",
        "body": null,
        "state": "closed",
        "user": "polarisunny",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-11T14:37:06+00:00",
        "updated_at": "2024-06-11T06:41:36+00:00",
        "closed_at": "2024-06-11T06:41:36+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1950,
        "title": "图片方向分类模型serving部署报错：{'err_no': 5, 'err_msg': '(log_id=2) imagenet failed to predict.', 'key': [], 'value': [], 'tensors': []}",
        "body": "将图片文字方向模型部署为serving服务。\r\n模型参考文档：https://github.com/PaddlePaddle/PaddleClas/blob/release/2.5/docs/zh_CN/models/PULC/PULC_text_image_orientation.md\r\n对方向分类模型进行了微调，inference模型预测正常。\r\n部署参考文档：https://github.com/PaddlePaddle/PaddleClas/tree/release/2.5/deploy/paddleserving#readme\r\n采用docker部署方式，没有GPU；文档中的imagenet部署成功；方向分类器始终报错。\r\n\r\nPaddleClas版本以及PaddlePaddle版本：\r\npaddle-serving-app 0.7.0\r\npaddle-serving-client 0.7.0\r\npaddle-serving-server 0.7.0\r\npaddleclas 2.5.1\r\npaddlepaddle 2.2.0\r\na. 具体操作系统：Linux docker\r\nb. Python版本号，如Python3.7\r\nc. CUDA/cuDNN版本， CPU预测\r\n完整的代码(相比于repo中代码，有改动的地方)、详细的错误信息及相关log\r\nlog.txt内容：\r\n![image](https://github.com/PaddlePaddle/Serving/assets/4609649/7b757051-4708-4c4b-ae04-8f084bb43ded)\r\npython web服务启动代码，仅修改了读取标签部分：\r\n![image](https://github.com/PaddlePaddle/Serving/assets/4609649/79055e50-d221-4ace-a919-7c702d6454c8)\r\n![image](https://github.com/PaddlePaddle/Serving/assets/4609649/aa398ba6-e7df-4459-8c8f-6f9b31cbdbc6)\r\n标签文件pplcnet.label内容：\r\n![image](https://github.com/PaddlePaddle/Serving/assets/4609649/89886e38-96b4-442f-af4a-205ad18e62eb)\r\n配置文件config.yum内容，仅修改了serving模型目录：\r\n![image](https://github.com/PaddlePaddle/Serving/assets/4609649/202cc194-4a86-4e5f-93f2-30c2f65fe5e6)\r\n\r\n预测命令：python3.7 pipeline_http_client.py\r\n报错信息为：{'err_no': 5, 'err_msg': '(log_id=0) imagenet failed to predict.', 'key': [], 'value': [], 'tensors': []}\r\n\r\n请大牛们指教",
        "state": "closed",
        "user": "MarkHe735",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-19T09:22:51+00:00",
        "updated_at": "2024-06-25T06:41:24+00:00",
        "closed_at": "2024-06-25T06:41:24+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1951,
        "title": "paddeleserving服务化部署后怎么添加CORS跨域配置？",
        "body": "paddleserving部署paddleocr后，运行web_service.py, 使用命令行请求可以访问，但是前端页面发送post请求报错，不能跨域请求，访问失败。\r\n这个web_service怎么添加CORS跨域配置，让前端能够访问？",
        "state": "closed",
        "user": "dizhenx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-29T02:58:47+00:00",
        "updated_at": "2024-07-02T06:40:42+00:00",
        "closed_at": "2024-07-02T06:40:42+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1952,
        "title": "按照PaddleClas2.5中的PaddleClas/deploy/paddleserving/recognition下的Readme部署C++Serving，部署后请求失败(inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of Socket...)",
        "body": "环境：ubuntu20.04；python3.8；paddlepaddle==2.3.2；PaddleClas2.5；cmake：3.16.3；gcc：9.4.0\r\n使用CPU，与原build_server.sh相比，删除了CUDA相关命令，并对一些命令做出调整\r\ncmake命令如下，\r\n![cmake命令](https://github.com/PaddlePaddle/Serving/assets/101171150/50ba0815-dd8b-4d63-bda3-705e5139889c)\r\n执行后的输出，\r\n![cmake_1](https://github.com/PaddlePaddle/Serving/assets/101171150/269809ae-2412-4e39-8415-5536da13d599)\r\n![cmake_2](https://github.com/PaddlePaddle/Serving/assets/101171150/97d89d04-d235-4f0d-967f-6f8258e0696f)\r\n![cmake_3](https://github.com/PaddlePaddle/Serving/assets/101171150/2a7778b9-7208-4cfe-b391-5373ad24a58d)\r\n![cmake_4](https://github.com/PaddlePaddle/Serving/assets/101171150/957c4ad3-b42d-4138-aadc-2a708a066df0)\r\n![cmake_5](https://github.com/PaddlePaddle/Serving/assets/101171150/22f656cf-d9b2-4a17-9665-052754c11f1f)\r\n存在几处Failed和not found，忽略。\r\nmake后的部分输出，\r\n![make_1](https://github.com/PaddlePaddle/Serving/assets/101171150/fcd9f826-a9f2-4c5b-91c9-bec462e7dbff)\r\n![make_2](https://github.com/PaddlePaddle/Serving/assets/101171150/981632e3-cf30-49d7-85ab-39a1613f4c23)\r\n![make_3](https://github.com/PaddlePaddle/Serving/assets/101171150/e6e5d138-2396-4aad-854f-af96ae4a1744)\r\n![make_4](https://github.com/PaddlePaddle/Serving/assets/101171150/eb84a17c-459e-4dbf-9cae-274a43ba8f55)\r\n![make_5](https://github.com/PaddlePaddle/Serving/assets/101171150/8d008072-81a4-4a05-b6ca-dc3deb097269)\r\n![make_6](https://github.com/PaddlePaddle/Serving/assets/101171150/af6497c4-517b-4ae2-ac31-c14539408895)\r\n开启服务后，生成的logPPShiTu.txt如下，\r\n![log_PPShiTu](https://github.com/PaddlePaddle/Serving/assets/101171150/ad9d9ce1-adbb-4efc-b02b-032a9efc2b8f)\r\n执行test_cpp_serving_client.py发送识别请求后，\r\nlogPPShiTu.txt如下，\r\n![log_PPShiTu_发送请求后](https://github.com/PaddlePaddle/Serving/assets/101171150/b5a66c54-ce92-46af-a951-f27d91b09470)\r\n终端显示如下，\r\n![发送请求后的终端_1](https://github.com/PaddlePaddle/Serving/assets/101171150/261eaefb-9176-4531-8111-6f4ec601978e)\r\n![发送请求后的终端_2](https://github.com/PaddlePaddle/Serving/assets/101171150/40741117-d0e9-453c-a554-faea16f5c033)\r\n报错如下：inference call failed, message: [E112]1/1 channels failed, fail_limit=1 [C0][E1014]Got EOF of Socket{id=1 fd=4 addr=127.0.0.1:9400:45838} (0x0x561c4ca330b0) [R1][E111]Fail to connect Socket{id=8589934594 addr=127.0.0.1:9400} (0x0x561c4ca332b0): Connection refused [R2][E112]Fail to select server from list://127.0.0.1:9400 lb=la\r\nE0703 21:52:48.302803 79137 general_model.cpp:370] failed call predictor with req: tensor\r\n请问是cmake或make部分出现错误导致识别失败，还是什么原因导致？",
        "state": "closed",
        "user": "yguofirst",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-04T06:21:14+00:00",
        "updated_at": "2024-07-09T06:41:02+00:00",
        "closed_at": "2024-07-09T06:41:02+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1954,
        "title": "TypeError: cannot pickle '_thread.lock' object",
        "body": "mac os : 10.15.7\r\npython: 3.9.17\r\npaddlepaddle: 2.5.0\r\npaddle-serving-app    0.8.3\r\npaddle-serving-client 0.8.3\r\npaddle-serving-server 0.8.3\r\n安装的是cpu版本的，按照https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/deploy/pdserving/README_CN.md操作，执行到python3 web_service.py --config=config.yml &>log.txt &的时候报下面的错误\r\n\r\n[PipelineServicer] succ init\r\nTraceback (most recent call last):\r\n  File \"/Users/xxxxx/projectPath/PaddleOCR/deploy/pdserving/web_service.py\", line 169, in <module>\r\n    uci_service.run_service()\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/site-packages/paddle_serving_server/web_service.py\", line 69, in run_service\r\n    self._server.run_server()\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 318, in run_server\r\n    self._run_grpc_gateway(\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 148, in _run_grpc_gateway\r\n    self._proxy_server.start()\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/Users/xxxxx/anaconda/envs/myEnv/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nTypeError: cannot pickle '_thread.lock' object",
        "state": "closed",
        "user": "AndyYue99",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-17T14:55:09+00:00",
        "updated_at": "2024-07-23T06:41:21+00:00",
        "closed_at": "2024-07-23T06:41:21+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1953,
        "title": "微信群过期了",
        "body": "微信群二维码过期了",
        "state": "closed",
        "user": "chaichaivi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-13T04:45:55+00:00",
        "updated_at": "2024-07-16T06:41:10+00:00",
        "closed_at": "2024-07-16T06:41:10+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1955,
        "title": "ERROR: Could not build wheels for grpcio, grpcio-tools, which is required to install pyproject.toml-based projects",
        "body": "[input]: \r\npython -m pip install --user -U paddle_serving_server paddle_serving_client paddle_serving_app \r\n\r\n[end of output]:\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for grpcio-tools\r\n  Running setup.py clean for grpcio-tools\r\nFailed to build grpcio grpcio-tools\r\nERROR: Could not build wheels for grpcio, grpcio-tools, which is required to install pyproject.toml-based projects\r\n\r\n描述：Windows10下Serving无法安装，Python=3.9.17，C++工具已安装，paddlepadlle=2.5.0",
        "state": "closed",
        "user": "X17exe",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-21T10:09:01+00:00",
        "updated_at": "2024-07-23T06:41:22+00:00",
        "closed_at": "2024-07-23T06:41:22+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1956,
        "title": "PaddleDetection模型使用PaddleServing推理结果不一致",
        "body": "![1690340208326](https://github.com/PaddlePaddle/Serving/assets/5021119/f8d7aa85-86ca-46f8-bac3-b0f324b35b54)\r\n",
        "state": "closed",
        "user": "cool8413m",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-26T02:57:36+00:00",
        "updated_at": "2024-07-30T06:42:11+00:00",
        "closed_at": "2024-07-30T06:42:11+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1958,
        "title": "libproxy_server.so加载失败，提示OSError: dlopen: cannot load any more object with static TLS",
        "body": "执行 python pipeline_service.py，弹出如下报错\r\n但是  /ssd2/workspace/libai/python_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/gateway/libproxy_server.so 这个so是存在的，求助\r\nPython 版本 3.7.16 (default, Jan 17 2023, 22:20:44)\r\n\r\n----------\r\n[PipelineServicer] succ init\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/ssd2/workspace/libai/python_env/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\r\n    self.run()\r\n  File \"/ssd2/workspace/libai/python_env/lib/python3.7/multiprocessing/process.py\", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/ssd2/workspace/libai/python_env/lib/python3.7/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 121, in _grpc_gateway\r\n    proxy_server = cdll.LoadLibrary(lib_path)\r\n  File \"/ssd2/workspace/libai/python_env/lib/python3.7/ctypes/__init__.py\", line 442, in LoadLibrary\r\n    return self._dlltype(name)\r\n  File \"/ssd2/workspace/libai/python_env/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: dlopen: cannot load any more object with static TLS\r\n\r\n\r\n",
        "state": "closed",
        "user": "bdbaigc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-31T07:56:21+00:00",
        "updated_at": "2024-08-06T06:40:20+00:00",
        "closed_at": "2024-08-06T06:40:19+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1957,
        "title": "serving和prometheus可以共用同一个端口吗",
        "body": "在python server的config文件里同时配置了http_port=30100，prometheus_port=30100，启动时报错：Failed to run grpc-gateway: prot 30100 is already used，请问有办法通过同一个端口实现服务和监控么",
        "state": "closed",
        "user": "jinxiwang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-27T06:30:54+00:00",
        "updated_at": "2024-07-30T06:42:11+00:00",
        "closed_at": "2024-07-30T06:42:11+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1962,
        "title": "ernie模型和FasterTransformer冲突？报错symbol runGemmShortApi, version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference",
        "body": "1、python pipeline_service.py 启动服务，一切正常\r\n\r\n2023/08/08 23:46:45 start proxy service\r\n\r\n2、curl 访问算子\r\npipeline_service.py直接挂掉，同时std_out打印如下日志：\r\n\r\nW0808 23:46:51.424680 206179 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.2, Runtime API Version: 11.1\r\nW0808 23:46:51.427105 206179 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.\r\npython: relocation error: /home/work/paddle_env/cuda_pkgs/cuda-11.1/lib64/libcublas.so: symbol runGemmShortApi, version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference\r\n\r\n现象：\r\nload(\"FasterTransformer\", build_dir=\"/home/work/paddle_env/ppnlp_home/extensions\", verbose=True)\r\n这行注释掉程序运行OK，但是这行注释掉会影响其他算子的运行，所以不能注释。\r\n\r\n不知道ernie模型和FasterTransformer有啥冲突。\r\n\r\n\r\n附录：\r\n1）pipeline_service.py 部分代码如下\r\nif __name__ == \"__main__\":\r\n    service = ModelService(name=\"model_service\")\r\n    script_dir = os.path.dirname(os.path.abspath(__file__))\r\n    root_dir = os.path.dirname(script_dir)\r\n    load(\"FasterTransformer\", build_dir=\"/home/work/paddle_env/ppnlp_home/extensions\", verbose=True)\r\n    config_file = os.path.join(script_dir, \"conf/config.yml\")\r\n    service.prepare_pipeline_config(config_file)\r\n    service.run_service()\r\n\r\n2）算子初始化代码如下：\r\nimport json\r\nfrom paddle_serving_server.web_service import Op\r\nimport numpy as np\r\nfrom collections import namedtuple\r\nclass TextQualityDetectOp(Op):\r\n    \"\"\"文本去噪模型\"\"\"\r\n\r\n    def init_op(self):\r\n        \"\"\"初始化\r\n        \"\"\"\r\n        from paddlenlp.transformers import AutoTokenizer\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\"ernie-3.0-medium-zh\",\r\n                                                        use_fast=True)\r\n        self.fetch_names = [\r\n            \"linear_147.tmp_1\",\r\n        ]",
        "state": "open",
        "user": "bdbaigc",
        "closed_by": null,
        "created_at": "2023-08-08T15:57:05+00:00",
        "updated_at": "2023-08-08T15:57:05+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1960,
        "title": "uci_housing_model ",
        "body": "使用Paddle Serving部署深度模型，看https://github.com/PaddlePaddle/Serving/tree/develop/examples/Pipeline/simple_web_service\r\n例子，使用命令 python3 -m paddle_serving_server.serve --model uci_housing_model --thread 10 --port 9292\r\n其中uci_housing_model这个文件目录是怎么生成的呢",
        "state": "closed",
        "user": "wanzhixiao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-02T03:24:46+00:00",
        "updated_at": "2024-08-06T06:40:20+00:00",
        "closed_at": "2024-08-06T06:40:20+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1959,
        "title": "C++编译报错",
        "body": "\r\n![image](https://github.com/PaddlePaddle/Serving/assets/17562936/a41f3cfe-d664-4719-ad7f-a3354a8986d1)\r\n使用Docker 0.9.0-devel镜像编译，执行到这一步骤时报错 package google.golang.org/grpc is not a main package.\r\n![image](https://github.com/PaddlePaddle/Serving/assets/17562936/55e62e3e-6c0c-4e65-8cf9-dddbd5750ea7)\r\n",
        "state": "closed",
        "user": "Ryanznoco",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-31T14:06:14+00:00",
        "updated_at": "2025-06-03T06:46:16+00:00",
        "closed_at": "2025-06-03T06:46:16+00:00",
        "comments_count": [
            "github-actions[bot]",
            "PlayerJian",
            "allen20200111",
            "123Yujiahang",
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1961,
        "title": "源码编译错误",
        "body": "[ 37%] Built target extern_glog\r\nScanning dependencies of target utils\r\n[ 37%] Building CXX object core/util/CMakeFiles/utils.dir/src/timer.cc.o\r\ncc1plus: warning: /usr/include/python3.8/Python.h: not a directory\r\n[ 38%] Linking CXX static library libutils.a\r\n[ 38%] Built target utils\r\nfatal: the remote end hung up unexpectedly\r\nfatal: early EOF\r\nfatal: index-pack failed\r\nCloning into 'extern_protobuf'...\r\n\r\n\r\n\r\n^Cmake[2]: *** [CMakeFiles/extern_protobuf.dir/build.make:91: third_party/protobuf/src/extern_protobuf-stamp/extern_protobuf-download] Interrupt\r\nmake[1]: *** [CMakeFiles/Makefile2:623: CMakeFiles/extern_protobuf.dir/all] Interrupt\r\nmake: *** [Makefile:130: all] Interrupt\r\n\r\n",
        "state": "closed",
        "user": "stilong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-03T09:12:06+00:00",
        "updated_at": "2024-08-06T06:40:21+00:00",
        "closed_at": "2024-08-06T06:40:21+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1963,
        "title": "client编译报错",
        "body": "google/protobuf/descriptor.proto: File not found.\r\npds_option.proto: Import \"google/protobuf/descriptor.proto\" was not found or had errors.\r\npds_option.proto:19:8: \"google.protobuf.FieldOptions\" is not defined.\r\npds_option.proto:23:8: \"google.protobuf.ServiceOptions\" is not defined.\r\ncore/pdcodegen/CMakeFiles/pdcodegen.dir/build.make:62: recipe for target 'core/pdcodegen/pds_option.pb.h' failed\r\nmake[2]: *** [core/pdcodegen/pds_option.pb.h] Error 1\r\nCMakeFiles/Makefile2:771: recipe for target 'core/pdcodegen/CMakeFiles/pdcodegen.dir/all' failed\r\nmake[1]: *** [core/pdcodegen/CMakeFiles/pdcodegen.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....",
        "state": "closed",
        "user": "ancenye",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-09T07:53:13+00:00",
        "updated_at": "2024-08-13T06:43:08+00:00",
        "closed_at": "2024-08-13T06:43:08+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1964,
        "title": "A10的GPU上运行模型，过一段时间就会报CUDA error(700), an illegal memory access was encountered，机器看不出任何问题",
        "body": "ERROR 2023-08-18 22:36:56,242 [operator.py:1079] [text_quality] failed to predict. (data_id=517045 log_id=517045) [text_quality|6] Failed to process(batch: [517045]): (External) CUDA error(700), an illegal memory access was encountered.\r\n  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:252)\r\n. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details.\r\nINFO 2023-08-18 22:36:56,242 [operator.py:1454] prometheus inf count +1\r\nERROR 2023-08-18 22:36:56,247 [dag.py:420] (data_id=517045 log_id=0) Failed to predict: [text_quality] failed to predict. (data_id=517045 log_id=517045) [text_quality|6] Failed to process(batch: [517045]): (External) CUDA error(700), an illegal memory access was encountered.\r\n  [Hint: Please search for the error code(700) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:252)\r\n. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details\r\n\r\n\r\nnvidia-smi\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  A10                 On   | 00000000:5E:00.0 Off |                    0 |\r\n|  0%   44C    P0    57W / 150W |   6606MiB / 22731MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n\r\n没看出任何问题，\r\n",
        "state": "closed",
        "user": "bdbaigc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-18T15:49:29+00:00",
        "updated_at": "2024-10-08T06:42:01+00:00",
        "closed_at": "2024-10-08T06:42:01+00:00",
        "comments_count": [
            "xinj7"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1965,
        "title": "AttributeError: 'TestFitALine' object has no attribute 'truth_val'",
        "body": "the branch is v0.8.3\r\npython3.8.10\r\nclient build success\r\napp build success\r\nserver build success\r\npip3 install success\r\n\r\nbut when run the cmd: `python3 -m paddle_serving_server.serve check`\r\n\r\nerror occur: \r\n```\r\n../../usr/local/lib/python3.8/dist-packages/paddle_serving_server/env_check/test_fit_a_line.py\r\nAttributeError: 'TestFitALine' object has no attribute 'truth_val'\r\n```\r\n",
        "state": "closed",
        "user": "LuLu2kANG",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-22T08:41:16+00:00",
        "updated_at": "2024-08-27T06:42:28+00:00",
        "closed_at": "2024-08-27T06:42:28+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1966,
        "title": "泣血力荐，千万不要在centos7上编译，一堆的坑，头皮发麻，大家还是在ubuntu上搞好一点",
        "body": null,
        "state": "closed",
        "user": "LuLu2kANG",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-22T08:49:47+00:00",
        "updated_at": "2025-02-11T06:44:41+00:00",
        "closed_at": "2025-02-11T06:44:41+00:00",
        "comments_count": [
            "RogerYu123",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1967,
        "title": "使用yolov3_darknet53_270e_coco示例服务化部署，返回的结果只有一个框",
        "body": "参考文档 PaddleDetection-release-2.6/deploy/serving/python/README.md\r\n通过命令进行模型下载，转换，以及启动服务，都很顺利，但是运行`python deploy/serving/python/pipeline_http_client.py --image_file demo/000000014439.jpg` ，返回结果不报错，但是只返回一个检测框，即使图中有多个目标。目前没有深入去看不知道是哪里的问题， 走到函数parse_detection_result()这里的fetch_dict就只有一个数组。",
        "state": "closed",
        "user": "Plwy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-23T06:13:24+00:00",
        "updated_at": "2025-02-11T06:44:42+00:00",
        "closed_at": "2025-02-11T06:44:42+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1970,
        "title": "服务部署成功，测试py文件也通过。postman调用返回 10000",
        "body": "![image](https://github.com/PaddlePaddle/Serving/assets/102728667/0b1e1661-c3f2-406d-b2a7-91dea834e964)\r\n![image](https://github.com/PaddlePaddle/Serving/assets/102728667/c9f48730-e4f0-43d6-bfc5-6f0a0b8db738)\r\n有没有大佬知道这是哪儿出错了\r\n",
        "state": "closed",
        "user": "JL9503",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-30T06:27:26+00:00",
        "updated_at": "2025-01-21T06:41:06+00:00",
        "closed_at": "2025-01-21T06:41:06+00:00",
        "comments_count": [
            "github-actions[bot]",
            "BurdenRaxs"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1968,
        "title": "file not exists",
        "body": "进行gpu编译的时候，报错这个文件不存在，http://paddle-inference-lib.bj.bcebos.com/2.2.2/cxx_c/Linux/GPU//paddle_inference.tgz",
        "state": "closed",
        "user": "LuLu2kANG",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-23T10:19:55+00:00",
        "updated_at": "2025-02-11T06:44:43+00:00",
        "closed_at": "2025-02-11T06:44:43+00:00",
        "comments_count": [
            "LuLu2kANG",
            "LuLu2kANG",
            "jiekechoo",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1969,
        "title": "Serving模式推理异常",
        "body": "    我在使用飞桨的框架实现模型训练和推理过程中发生了一些问题，有人了解的吗？麻烦帮忙分析解决，谢谢！\r\n\r\n    问题：同一批图片进行推理多次推理时，偶尔出现结果不一样的情况，关闭TRT加速后暂未发现，但关闭后无法实现推理加速。\r\n\r\n    服务端启动命令：python3 -m paddle_serving_server.serve --model serving_server  --runtime_thread_num 6 --batch_infer_size 32  --port 9595  --gpu_ids 0 --gpu_multi_stream --use_trt\r\n\r\n    环境：服务端在Ubuntu系统，运行Serving模式进行推理。客户端在windows端（采用GRPC协议，异步通信方式）。",
        "state": "open",
        "user": "weida008",
        "closed_by": null,
        "created_at": "2023-08-29T09:28:21+00:00",
        "updated_at": "2023-09-08T02:55:21+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "weida008",
            "jiangjiajun",
            "weida008",
            "jiangjiajun",
            "weida008",
            "jiangjiajun",
            "weida008",
            "weida008"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1971,
        "title": "编译失败",
        "body": "hi 这边按照文档在ascend 310机器上编译，cann的版本是6.0.1，python3.9.2,报错如下，看起来是依赖一个预编译的库；serving是一定要依赖于固定的依赖库的版本吗，比如cann3.0, python3.7, gcc7xx ? 感谢回答。\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Run_On_NPU_CN.md\r\n![image](https://github.com/PaddlePaddle/Serving/assets/9452272/bd1635fe-97ea-434b-95ad-b3de31e53c14)\r\n\r\n",
        "state": "closed",
        "user": "lichangW",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-01T09:13:22+00:00",
        "updated_at": "2025-02-11T06:44:44+00:00",
        "closed_at": "2025-02-11T06:44:44+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1972,
        "title": "目前这个项目还在维护吗？",
        "body": "看到issue没人答复，很久没用更新代码，是不是被废弃了？",
        "state": "closed",
        "user": "jiekechoo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-04T08:27:41+00:00",
        "updated_at": "2025-02-11T06:44:44+00:00",
        "closed_at": "2025-02-11T06:44:44+00:00",
        "comments_count": [
            "github-actions[bot]",
            "RogerYu123",
            "jiekechoo",
            "bdbaigc",
            "jiekechoo",
            "HexToString",
            "allen20200111",
            "heavengate",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1974,
        "title": "paddleServing部署电表检测模型，不确定预期输出",
        "body": "1、前期模型训练按照下面的项目方式进行：\r\n\r\n[https://aistudio.baidu.com/projectdetail/3429765?channelType=0&channel=0](https://mailshield.baidu.com/check?q=AlEwhtE5zdzlkKzKpXr5RNbSCGSPA1DPqTt6pRDbyt76zl6VLgvykIxfaprvmj3wj70BAoOTI6d80KBFuPIhfk5FHRBZeVx5bhHPTA==)\r\n\r\n2、通过PaddleOCR导出模型\r\n\r\npython tools/export_model.py -c ./configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_student.yml -o Global.pretrained_model=./output/dianbiao_ch_PP-OCR_V3_det/best_accuracy Global.save_inference_dir=./inference/det_db\r\n\r\n3、模型转换成serving可以识别的\r\n\r\n    python -m paddle_serving_client.convert --dirname ./det_db/ --model_filename inference.pdmodel --params_filename inference.pdiparams --serving_server ./ppocr_det_v3_serving/ --serving_client ./ppocr_det_v3_client/\r\n\r\n4、启动模型Serving服务\r\n\r\n    python3 -m paddle_serving_server.serve --model /opt/ammeter_identify_mode/ppocr_det_v3_serving/ --port 9812\r\n\r\n \r\n\r\n5、配置文件serving_client_conf.prototxt\r\n\r\n`    feed_var {\r\n\r\nname: \"x\"\r\nalias_name: \"x\"\r\nis_lod_tensor: false\r\nfeed_type: 1\r\nshape: 3\r\n}\r\nfetch_var {\r\nname: \"sigmoid_0.tmp_0\"\r\nalias_name: \"sigmoid_0.tmp_0\"\r\nis_lod_tensor: false\r\nfetch_type: 1\r\nshape: 1\r\n}`\r\n\r\n6、客户端调用代码：\r\n\r\n   ` from paddle_serving_app.reader import *\r\n\r\nfrom paddle_serving_client import Client\r\nimport cv2, json, datetime, os,numpy as np\r\nimport numpy as np\r\n\r\nparent_directory = os.path.dirname(os.path.abspath(__file__))\r\n\r\nclient = Client()\r\n#client.load_client_config(os.path.join(parent_directory, 'serving_client_conf.prototxt'))\r\nclient.load_client_config('./serving_det_client_conf.prototxt')\r\nclient.connect(['****:9812'])\r\n\r\n\r\npreprocess = Sequential([\r\nFile2Image(), BGR2RGB(),\r\nResize(\r\n(960, 960), interpolation=cv2.INTER_LANCZOS4), Div(255.0),Transpose((2, 0, 1))\r\n])\r\n\r\nim = preprocess('./P23030907100210.jpg')\r\n\r\nfetch_map = client.predict(\r\nfeed={\r\n\"x\": im,\r\n},\r\nfetch=[\"sigmoid_0.tmp_0\"],\r\nbatch=False)\r\n\r\ndetections = fetch_map[\"sigmoid_0.tmp_0\"]`\r\n\r\n \r\n\r\n输出：\r\n\r\n>>> print(fetch_map)\r\n{'sigmoid_0.tmp_0': array([[[[6.7720975e-08, 1.3140065e-07, 1.9094442e-08, ...,\r\n2.3576363e-09, 7.7910184e-10, 4.4165565e-09],\r\n[6.3383972e-08, 1.4166790e-07, 3.6551061e-08, ...,\r\n4.6193835e-09, 1.3099265e-09, 4.5741602e-09],\r\n[2.5273311e-08, 1.4597879e-07, 3.7196685e-08, ...,\r\n6.0767689e-09, 7.5343232e-10, 1.7367194e-08],\r\n...,\r\n[4.4501288e-08, 3.6136807e-08, 7.8183229e-09, ...,\r\n5.6374336e-08, 1.7245652e-08, 2.9781738e-08],\r\n[1.5184209e-08, 7.0195000e-08, 1.4452498e-08, ...,\r\n5.4933416e-08, 9.6213251e-09, 1.3786045e-07],\r\n[7.2296395e-08, 3.9937358e-08, 2.8997006e-08, ...,\r\n1.5954635e-07, 4.4072682e-08, 2.0925140e-07]]]], dtype=float32)}\r\n\r\n \r\n\r\n我疑问的是这个结果是否正确？这个检测模型为什么没有告诉我检测的四点坐标？",
        "state": "closed",
        "user": "gubinjie",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-10T08:55:28+00:00",
        "updated_at": "2025-02-11T06:44:45+00:00",
        "closed_at": "2025-02-11T06:44:45+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1975,
        "title": "Failed building wheel for grpcio",
        "body": "Env:\r\ndocker image: paddlepaddle/paddle:2.5.1-gpu-cuda12.0-cudnn8.9-trt8.6\r\npython: python10\r\n\r\nIn this dokcer, `pip3 install https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server-0.9.0-py3-none-any.whl` will failed by `grpcio<=1.33.2`",
        "state": "closed",
        "user": "annidy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-17T03:12:33+00:00",
        "updated_at": "2025-02-11T06:44:46+00:00",
        "closed_at": "2025-02-11T06:44:46+00:00",
        "comments_count": [
            "github-actions[bot]",
            "zhh8689",
            "FesonX",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1977,
        "title": "undefined symbol: _dl_sym, version GLIBC_PRIVATE",
        "body": "/root/anaconda3/envs/py39/lib/python3.9/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving: symbol lookup error: /root/anaconda3/envs/py39/lib/python3.9/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving: undefined symbol: _dl_sym, version GLIBC_PRIVATE\r\n\r\n\r\nPaddleDetection+Serving+CPU版本部署时，运行的时候出现如上的提示，请问是ubuntu22.04里边的glibc版本过高，还是什么原因？\r\n如是版本原因，请问需要安装glibc的哪个版本，谢谢",
        "state": "closed",
        "user": "yootou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-15T07:36:50+00:00",
        "updated_at": "2025-02-11T06:44:48+00:00",
        "closed_at": "2025-02-11T06:44:48+00:00",
        "comments_count": [
            "github-actions[bot]",
            "yootou",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1976,
        "title": "想关闭Paddle OCR的paddle-serving 的Pipeline错误日志怎么做呢，因为RabbitMQ断网一下产生了几个G的错误日志",
        "body": "大佬们请问下，我想关闭Paddle OCR的paddle-serving 的Pipeline错误日志怎么做呢，下面文档找不到，因为RabbitMQ断网一下产生了几个G的错误日志\r\n<img width=\"931\" alt=\"c84f10cd60ba0b980fe6010892fb68e\" src=\"https://github.com/PaddlePaddle/Serving/assets/20828404/4ca6ab1c-5b39-4db3-b764-025a2ff6efdc\">\r\n\r\n错误日志\r\n![image](https://github.com/PaddlePaddle/Serving/assets/20828404/dd4bfe1f-e285-46d6-83eb-0392d715f727)\r\n",
        "state": "closed",
        "user": "Juruobudong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-31T13:34:01+00:00",
        "updated_at": "2025-02-11T06:44:47+00:00",
        "closed_at": "2025-02-11T06:44:47+00:00",
        "comments_count": [
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1979,
        "title": "Paddle Serving C++ 部署 启动失败  pdserving.cpp:195] Failed initialize workflow manager",
        "body": "千辛万苦编译完成，启动失败了。找了一圈没有相关错误，怎么搞⛏\r\n```\r\npython3.7 -m paddle_serving_server.serve --model ppocr_det_v3_serving ppocr_rec_v3_serving --op GeneralDetectionOp GeneralInferOp --port 8181\r\n\r\n\r\n/usr/local/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\nUse local bin : /data/deploy/pdserving/Serving/build_server/core/general-server/serving\r\nGoing to Run Comand\r\n/data/deploy/pdserving/Serving/build_server/core/general-server/serving -enable_model_toolkit -inferservice_path workdir_8181 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 8181 -precision fp32 -use_calib=False -reload_interval_s 10 -resource_path workdir_8181 -resource_file resource.prototxt -workflow_path workdir_8181 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912 \r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralRemoteOp\r\nI0100 00:00:00.000000 147697 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\nI0100 00:00:00.000000 147697 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 147697 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]\r\nI0100 00:00:00.000000 147697 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!\r\nI0100 00:00:00.000000 147697 general_model_service.pb.h:1650] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]\r\nI0100 00:00:00.000000 147697 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE\r\nW0100 00:00:00.000000 147697 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!\r\nE1129 07:16:42.677270 147697 op_repository.cpp:29] Try to create unknown op[GeneralDetectionOp]\r\nE1129 07:16:42.677465 147697 dag.cpp:146] Failed to get_op, op type[GeneralDetectionOp]\r\nE1129 07:16:42.677500 147697 workflow.cpp:29] Failed initialize dag: workflow1\r\nE1129 07:16:42.677532 147697 manager.h:73] Failed init item: workflow1 at:0!\r\nE1129 07:16:42.677592 147697 pdserving.cpp:195] Failed initialize workflow manager, conf:workdir_8181/workflow.prototxt\r\n```\r\n\r\n环境使用的官方开发镜像  `registry.baidubce.com/paddlepaddle/serving:0.9.0-cuda11.2-cudnn8-devel`，\r\n根据官方文档\r\n(https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/deploy/pdserving/README_CN.md)\r\n的命令运行。\r\n编译根据官方文档 (https://github.com/PaddlePaddle/Serving/blob/v0.8.3/doc/Compile_CN.md) 的命令运行",
        "state": "closed",
        "user": "limfa",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-29T07:24:22+00:00",
        "updated_at": "2025-05-20T06:45:01+00:00",
        "closed_at": "2025-05-20T06:45:01+00:00",
        "comments_count": [
            "heavengate",
            "123Yujiahang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1978,
        "title": "PaddleDetection+Serving部署，运行时提示：undefined symbol: _dl_sym, version GLIBC_PRIVATE",
        "body": "Starting program: /root/anaconda3/envs/py39/lib/python3.9/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralDetectionOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralFeatureExtractOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralInferOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralPicodetOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralRecOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralRemoteOp\r\nI0100 00:00:00.000000  2187 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp\r\n/root/anaconda3/envs/py39/lib/python3.9/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving: symbol lookup error: /root/anaconda3/envs/py39/lib/python3.9/site-packages/paddle_serving_server/serving-cpu-avx-mkl-0.9.0/serving: undefined symbol: _dl_sym, version GLIBC_PRIVATE\r\n[Inferior 1 (process 2187) exited with code 0177]\r\n(gdb) bt\r\nNo stack.\r\n",
        "state": "closed",
        "user": "yootou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-15T12:09:12+00:00",
        "updated_at": "2025-02-11T06:44:49+00:00",
        "closed_at": "2025-02-11T06:44:48+00:00",
        "comments_count": [
            "yootou",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1980,
        "title": "当客户端通过GRPC发过来的数据包的模型输入信息和图片大小和Serving上跑的模型不一致时，Sering服务会挂掉",
        "body": "当客户端通过GRPC发过来的数据包的模型输入信息和图片大小和Serving上跑的模型不一致时，Sering服务会挂掉。\r\n查看general_reader_op.cpp中没有相关的信息，有人遇到类似的问题吗？如何解决？",
        "state": "closed",
        "user": "weida008",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-01T07:40:56+00:00",
        "updated_at": "2025-02-11T06:44:49+00:00",
        "closed_at": "2025-02-11T06:44:49+00:00",
        "comments_count": [
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1982,
        "title": "安装serving失败, ERROR: Could not build wheels for grpcio, grpcio-tools",
        "body": "环境：\r\nwindows10 使用 Docker Desktop虚拟Linux\r\ndocker镜像： paddlepaddle/paddle:2.6.0\r\npython版本： python3.10.13\r\n\r\n无法成功安装PaddleServing\r\n```\r\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server_gpu-0.9.0.post102-py3-none-any.whl\r\npip3 install paddle_serving_server_gpu-0.9.0.post102-py3-none-any.whl\r\n````\r\n\r\n报错， 0.8.3版也是一样的报这个错误。\r\n```\r\nERROR: Failed building wheel for grpcio-tools\r\nFailed to build grpcio grpcio-tools\r\nERROR: Could not build wheels for grpcio, grpcio-tools, which is required to install pyproject.toml-based projec\r\n```\r\n\r\n安装时第一次失败的地方：\r\n```\r\n     distutils.errors.CompileError: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\r\n      \r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for grpcio\r\n  Building wheel for grpcio-tools (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Building wheel for grpcio-tools (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [2056 lines of output]\r\n      <string>:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n```\r\n\r\n第二次报错： \r\n```\r\nx86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DHAVE_PTHREAD=1 -I. -Igrpc_root -Igrpc_root/include -Ithird_party/protobuf/src -I/usr/include/python3.10 -c third_party/protobuf/src/google/protobuf/descriptor.pb.cc -o build/temp.linux-x86_64-cpython-310/third_party/protobuf/src/google/protobuf/descriptor.pb.o -std=c++11 -fno-wrapv -frtti\r\n      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for grpcio-tools\r\nFailed to build grpcio grpcio-tools\r\nERROR: Could not build wheels for grpcio, grpcio-tools, which is required to install pyproject.toml-based projects\r\n```",
        "state": "open",
        "user": "beike6688",
        "closed_by": null,
        "created_at": "2024-01-02T06:52:52+00:00",
        "updated_at": "2024-07-22T13:14:45+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "magicleo",
            "heavengate",
            "dp9212",
            "Gavinfornever"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1981,
        "title": "After executing client = Client(), an exception FatalError: `Segmentation fault` is detected by the operating system.",
        "body": "paddle-serving-client==0.7.0\r\npaddle-serving-server==0.7.0\r\npaddle-serving-app==0.7.0\r\n\r\n![1703754867377](https://github.com/PaddlePaddle/Serving/assets/37869445/a4ac65ea-493e-4dde-9550-e0fd2cc4d40d)\r\n\r\nWhen the recognition task is executed to the Client() line, an exception message is thrown:\r\n`--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   gevent_run_callbacks\r\n1   gevent_loop_run_callbacks\r\n2   gevent_call\r\n3   greenlet::UserGreenlet::g_switch()\r\n4   greenlet::UserGreenlet::g_initialstub(void*)\r\n5   greenlet::UserGreenlet::inner_bootstrap(greenlet::refs::_OwnedGreenlet<_greenlet, &greenlet::refs::GreenletChecker>&, greenlet::refs::OwnedReference<_object, &greenlet::refs::NoOpChecker>&)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1703754648 (unix time) try \"date -d @1703754648\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x1b8) received by PID 26260 (TID 0x7f4b4f5b6740) from PID 440 ***]\r\n\r\nSegmentation fault\r\n`\r\nIs this what is the reason?\r\n",
        "state": "closed",
        "user": "gubinjie",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-28T09:16:07+00:00",
        "updated_at": "2025-02-11T06:44:50+00:00",
        "closed_at": "2025-02-11T06:44:50+00:00",
        "comments_count": [
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1983,
        "title": "更新",
        "body": "Paddle Serving 是不是停止更新了？以后都不维护了。",
        "state": "closed",
        "user": "huaxiangsiyi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-03T03:53:29+00:00",
        "updated_at": "2025-02-11T06:44:51+00:00",
        "closed_at": "2025-02-11T06:44:51+00:00",
        "comments_count": [
            "github-actions[bot]",
            "allen20200111",
            "heavengate",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1985,
        "title": "No module named 'paddle.fluid' ",
        "body": "paddle-serving-app        0.9.0\r\npaddle-serving-client     0.9.0\r\npaddle-serving-server-gpu 0.9.0.post112\r\npaddlepaddle-gpu          2.6.0.post112\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/storage/anaconda3/envs/paddle/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/mnt/storage/anaconda3/envs/paddle/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/storage/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle_serving_client/convert.py\", line 21, in <module>\r\n    from .io import inference_model_to_serving\r\n  File \"/mnt/storage/anaconda3/envs/paddle/lib/python3.9/site-packages/paddle_serving_client/io/__init__.py\", line 16, in <module>\r\n    from paddle.fluid import Executor\r\nModuleNotFoundError: No module named 'paddle.fluid'\r\n\r\n新版本的paddle已经没有fluid了。serving什么时候能更新下啊",
        "state": "open",
        "user": "magicleo",
        "closed_by": null,
        "created_at": "2024-01-17T07:15:19+00:00",
        "updated_at": "2025-01-10T09:29:55+00:00",
        "closed_at": null,
        "comments_count": [
            "heavengate",
            "FrozenZero",
            "entrehuihui",
            "winkinwong",
            "yazheng0307",
            "fatsheep2020",
            "wolfplan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1984,
        "title": "什么时候支持cuda11.7 11.8 12.0",
        "body": "请问这个项目还在维护吗？什么时候能支持cuda11.7 cuda11.8 cuda12.0",
        "state": "closed",
        "user": "magicleo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-16T06:38:36+00:00",
        "updated_at": "2025-05-06T06:45:36+00:00",
        "closed_at": "2025-05-06T06:45:36+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate",
            "C4a15Wh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1986,
        "title": "python/pipeline/operator.py line:1073 bug导致报错 IndexError: list index out of range",
        "body": "设置超时时间后，会到这部分代码\r\n![image](https://github.com/PaddlePaddle/Serving/assets/24838269/33fbe4c9-7042-42f6-8c72-6597812e24a4)\r\n不设置超时时间是没问题的\r\n![image](https://github.com/PaddlePaddle/Serving/assets/24838269/82d0df86-d6f1-479d-831f-3b4d0d20787b)\r\n",
        "state": "closed",
        "user": "BurdenRaxs",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-17T07:48:25+00:00",
        "updated_at": "2025-02-11T06:44:52+00:00",
        "closed_at": "2025-02-11T06:44:52+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1987,
        "title": "如何Debug c++ serving ",
        "body": "我自定了OP，c++ 服务的启动方式是\r\n```\r\npython3 -m paddle_serving_server.serve --model serving_model --port 9393\r\n```\r\n\r\n我如何在OP中Debug C++ 代码？ 求教",
        "state": "closed",
        "user": "huotong1212",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-27T11:13:22+00:00",
        "updated_at": "2025-02-11T06:44:53+00:00",
        "closed_at": "2025-02-11T06:44:53+00:00",
        "comments_count": [
            "github-actions[bot]",
            "heavengate"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1988,
        "title": "prometheus监控，启动后未发现监控指标",
        "body": "启动命令\r\n<img width=\"1135\" alt=\"image\" src=\"https://github.com/PaddlePaddle/Serving/assets/31182767/69804030-dd48-4aa0-8b0f-c4e79a7cc2db\">\r\n为什么启动后的请求的参数如下：\r\n<img width=\"956\" alt=\"image\" src=\"https://github.com/PaddlePaddle/Serving/assets/31182767/8327ba30-d06c-4f87-ae9a-7f973761d97d\">\r\n\r\n是否需要自己编译c++ 的serving才可以？",
        "state": "closed",
        "user": "huotong1212",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-02-17T07:02:54+00:00",
        "updated_at": "2025-03-04T06:49:07+00:00",
        "closed_at": "2025-03-04T06:49:07+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1989,
        "title": "pp-shituv2 使用paddleserving部署后启动服务运行python3.7 pipeline_http_client.py报/home/aistudio/PaddleClas/deploy/paddleserving/recognition {'err_no': 8, 'err_msg': \"(data_id=0 log_id=0) [det|0] Failed to postprocess: 'scale_factor.lod'\", 'key': [], 'value': [], 'tensors': []}",
        "body": "paddleServing部署时，启动http客户端，报错\r\n\r\n/home/aistudio/PaddleClas/deploy/paddleserving/recognition {'err_no': 8, 'err_msg': \"(data_id=0 log_id=0) [det|0] Failed to postprocess: 'scale_factor.lod'\", 'key': [], 'value': [], 'tensors': []}\r\n\r\n识别推理模型serving_server_conf.prototxt文件是：\r\nfeed_var {\r\n  name: \"x\"\r\n  alias_name: \"x\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 224\r\n  shape: 224\r\n}\r\nfetch_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"features\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 512\r\n}\r\n\r\n通用检测模型.prototxt文件是：\r\nfeed_var {\r\n  name: \"im_shape\"\r\n  alias_name: \"im_shape\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 416\r\n  shape: 416\r\n}\r\nfeed_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"scale_factor\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_0.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_0.tmp_1\"\r\n  is_lod_tensor: true\r\n  fetch_type: 1\r\n  shape: -1\r\n}\r\nfetch_var {\r\n  name: \"save_infer_model/scale_1.tmp_1\"\r\n  alias_name: \"save_infer_model/scale_1.tmp_1\"\r\n  is_lod_tensor: false\r\n  fetch_type: 2\r\n}\r\n\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "sloyqi",
        "closed_by": null,
        "created_at": "2024-03-18T11:31:57+00:00",
        "updated_at": "2024-03-19T00:03:59+00:00",
        "closed_at": null,
        "comments_count": [
            "sloyqi",
            "github-actions[bot]",
            "sloyqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1990,
        "title": "serving报维度匹配错误",
        "body": "<img width=\"1326\" alt=\"image\" src=\"https://github.com/PaddlePaddle/Serving/assets/6964842/e060ba30-76f2-42cb-9bb5-efe5809cf694\">\r\n搭建paddle service的时候遇见问题，export model的时候我的输入设定是[None, None]，代表[batch, token_size]，在service中，preprocess函数中也是给的这个维度的数据，但是最后报这个错",
        "state": "open",
        "user": "PassStory",
        "closed_by": null,
        "created_at": "2024-03-19T03:24:54+00:00",
        "updated_at": "2024-03-20T08:33:55+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "Jiang-Jia-Jun",
            "PassStory"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1991,
        "title": "ModuleNotFoundError: No module named 'paddle_serving_server.proto",
        "body": "An error occurs when running command ：\r\n``λ 71fd011ae915 /home/Serving/python {v0.9.0} python3 -m paddle_serving_server.serve check\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/local/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/Serving/python/paddle_serving_server/__init__.py\", line 22, in <module>\r\n    from paddle_serving_server import (\r\n  File \"/home/Serving/python/paddle_serving_server/server.py\", line 20, in <module>\r\n    from .proto import server_configure_pb2 as server_sdk\r\nModuleNotFoundError: No module named 'paddle_serving_server.proto'\r\n\r\nUse pip list to check the current environment. Paddle-serving-client ，paddle-serving-server and paddle-serving-server are installed.\r\n`λ 71fd011ae915 /home/Serving/python {v0.9.0} pip list\r\nPackage               Version\r\n--------------------- -----------\r\nattrs                 22.2.0\r\nav                    8.0.3\r\nbeautifulsoup4        4.12.3\r\ncertifi               2024.2.2\r\ncharset-normalizer    2.0.12\r\nclick                 7.1.2\r\ndecord                0.4.2\r\nFlask                 1.1.4\r\nfunc-timeout          4.3.5\r\ngoogle                3.0.0\r\ngoogle-cloud          0.34.0\r\ngrpcio                1.33.2\r\ngrpcio-tools          1.33.2\r\nidna                  3.6\r\nimportlib-metadata    4.8.3\r\niniconfig             1.1.1\r\nitsdangerous          1.1.0\r\nJinja2                2.11.3\r\nMarkupSafe            1.1.1\r\nnumpy                 1.19.5\r\nopencv-python         3.4.17.61\r\npackaging             21.3\r\npaddle-serving-app    0.9.0\r\npaddle-serving-client 0.9.0\r\npaddle-serving-server 0.9.0\r\nPillow                8.4.0\r\npip                   21.3.1\r\npluggy                1.0.0\r\nprometheus-client     0.12.0\r\nprotobuf              3.19.6\r\npy                    1.11.0\r\npyclipper             1.2.1\r\npyparsing             3.0.7\r\npytest                7.0.1\r\nPyYAML                6.0.1\r\nrequests              2.27.1\r\nsentencepiece         0.1.96\r\nsetuptools            59.6.0\r\nShapely               1.8.0\r\nSimpleITK             2.1.1.2\r\nsix                   1.16.0\r\nsoupsieve             2.3.2.post1\r\ntomli                 1.2.3\r\ntyping_extensions     4.1.1\r\nujson                 4.3.0\r\nurllib3               1.26.18\r\nWerkzeug              1.0.1\r\nwheel                 0.34.2\r\nzipp                  3.6.0`\r\n\r\nCurrently python is 3.6\r\n`λ 71fd011ae915 /home/Serving/python {v0.9.0} python3 -V Python 3.6.0 `",
        "state": "closed",
        "user": "UserShen-001",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-01T09:47:19+00:00",
        "updated_at": "2025-04-08T06:44:19+00:00",
        "closed_at": "2025-04-08T06:44:19+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1996,
        "title": "海光X86+昆仑芯RG800,请问 registry.baidubce.com/paddlepaddle/serving:xpu-x86连xpu显卡都看不到",
        "body": "FROM registry.baidubce.com/paddlepaddle/serving:xpu-x86\r\n\r\n容器内连xpu显卡都看不到，请问正常吗\r\n\r\n![image](https://github.com/PaddlePaddle/Serving/assets/20828404/15f3a718-bf90-46c6-a97d-3571dfcac18c)\r\n外面终端输命令就行，3块RG800\r\n![image](https://github.com/PaddlePaddle/Serving/assets/20828404/bf381526-62e8-4d93-9f4c-dec2d6d42782)\r\n\r\n![image](https://github.com/PaddlePaddle/Serving/assets/20828404/ce623876-bd53-4179-b1d5-efe8aa5f37be)\r\n",
        "state": "open",
        "user": "Juruobudong",
        "closed_by": null,
        "created_at": "2024-05-14T11:52:26+00:00",
        "updated_at": "2024-05-14T11:52:31+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1995,
        "title": "想同时封装一个paddlepaddle paddlenlp paddleserving的镜像，非常难 你们没有测过么",
        "body": "registry.baidubce.com/paddlepaddle/paddle    2.6.1                                 2f3e2fc3a97c   7 weeks ago     4.66GB\r\nregistry.baidubce.com/paddlepaddle/paddle    2.4.0-cpu                             d69975c73b6d   17 months ago   4.59GB\r\nregistry.baidubce.com/paddlepaddle/serving   0.9.0-runtime                         c2f920785302   23 months ago   4.33GB\r\n\r\n想装上全家桶，不管基于哪个，安装官方的步骤，都是搞不定的。实际项目这三个都可能用得到，还有serving你们官方的docker，居然python只支持3.6，ubuntu16",
        "state": "closed",
        "user": "xinheblue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-07T05:46:26+00:00",
        "updated_at": "2025-05-20T06:45:02+00:00",
        "closed_at": "2025-05-20T06:45:02+00:00",
        "comments_count": [
            "github-actions[bot]",
            "xinheblue",
            "xinheblue",
            "Juruobudong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1997,
        "title": "serving编译失败",
        "body": "λ 9fab183dbb12 /home/ppocr/Serving/build_server {v0.9.0} cmake -DPYTHON_INCLUDE_DIR=$PYTHON_INCLUDE_DIR/ \\\r\n> -DPYTHON_LIBRARIES=$PYTHON_LIBRARIES \\\r\n> -DPYTHON_EXECUTABLE=$PYTHON_EXECUTABLE \\\r\n> -DOPENCV_DIR=$OPENCV_DIR \\\r\n> -DWITH_OPENCV=ON \\\r\n> -DSERVER=ON ..\r\n-- Found Paddle host system: ubuntu, version: 16.04.7\r\n-- Found Paddle host system's CPU: 4 cores\r\n-- The CXX compiler identification is GNU 8.2.0\r\n-- The C compiler identification is GNU 8.2.0\r\n-- Check for working CXX compiler: /usr/local/gcc-8.2/bin/c++\r\n-- Check for working CXX compiler: /usr/local/gcc-8.2/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: /usr/local/gcc-8.2/bin/gcc\r\n-- Check for working C compiler: /usr/local/gcc-8.2/bin/gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- CXX compiler: /usr/local/gcc-8.2/bin/c++, version: GNU 8.2.0\r\n-- C compiler: /usr/local/gcc-8.2/bin/gcc, version: GNU 8.2.0\r\n-- Found Git: /usr/bin/git (found version \"2.7.4\") \r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Performing Test MMX_FOUND\r\n-- Performing Test MMX_FOUND - Success\r\n-- Performing Test SSE2_FOUND\r\n-- Performing Test SSE2_FOUND - Success\r\n-- Performing Test SSE3_FOUND\r\n-- Performing Test SSE3_FOUND - Success\r\n-- Performing Test AVX_FOUND\r\n-- Performing Test AVX_FOUND - Success\r\n-- Performing Test AVX2_FOUND\r\n-- Performing Test AVX2_FOUND - Success\r\n-- Performing Test AVX512F_FOUND\r\n-- Performing Test AVX512F_FOUND - Failed\r\n-- Compile Version Tag for wheel: 0.0.0\r\n-- Found OpenCV: /home/opencv-3.4.7/opencv3 (found version \"3.4.7\") \r\n-- Use PADDLE_ON_INFERENCE\r\n-- Do not have AVX2 intrinsics and disabled MKL-DNN\r\n-- BOOST_TAR: boost_1_74_0, BOOST_URL: http://paddlepaddledeps.bj.bcebos.com/boost_1_74_0.tar.gz\r\nCMake Warning at /home/cmake-3.16.0-Linux-x86_64/share/cmake-3.16/Modules/FindProtobuf.cmake:499 (message):\r\n  Protobuf compiler version doesn't match library version 2.6.1\r\nCall Stack (most recent call first):\r\n  cmake/external/protobuf.cmake:18 (FIND_PACKAGE)\r\n  CMakeLists.txt:106 (include)\r\n\r\n\r\n-- Protobuf protoc executable: /home/ppocr/Serving/build_server/third_party/install/protobuf/bin/protoc\r\n-- Protobuf-lite library: /home/ppocr/Serving/build_server/third_party/install/protobuf/lib/libprotobuf-lite.a\r\n-- Protobuf library: /home/ppocr/Serving/build_server/third_party/install/protobuf/lib/libprotobuf.a\r\n-- Protoc library: /home/ppocr/Serving/build_server/third_party/install/protobuf/lib/libprotoc.a\r\n-- Protobuf version: 3.1\r\n-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"1.0.2g\")  \r\n-- ssl:/usr/lib/x86_64-linux-gnu/libssl.so\r\n-- crypto:/usr/lib/x86_64-linux-gnu/libcrypto.so\r\n-- Found PythonInterp: /usr/local/bin/python3.7 (found version \"3.7\") \r\n-- Found PythonLibs: /usr/local/lib/libpython3.7m.so (found version \"3.7.0\") \r\n-- Looking for UINT64_MAX\r\n-- Looking for UINT64_MAX - found\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of pthread_spinlock_t\r\n-- Check size of pthread_spinlock_t - done\r\n-- Check size of pthread_barrier_t\r\n-- Check size of pthread_barrier_t - done\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__D__const___\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__D__const___ - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__D__const___\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__D__const___ - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__DUSE_PTHREAD\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__DUSE_PTHREAD - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__DUSE_PTHREAD\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__DUSE_PTHREAD - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__fPIC\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__fPIC - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__fPIC\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__fPIC - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__fno_omit_frame_pointer\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__fno_omit_frame_pointer - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__fno_omit_frame_pointer\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__fno_omit_frame_pointer - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wall\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wall - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wall\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wall - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wextra\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wextra - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wextra\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wextra - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor - Failed\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wnon_virtual_dtor - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor - Failed\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wdelete_non_virtual_dtor - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_unused_parameter\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_unused_parameter - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_unused_parameter\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_unused_parameter - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_unused_function\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_unused_function - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_unused_function\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_unused_function - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_literal_suffix\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_literal_suffix - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_literal_suffix\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_literal_suffix - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_sign_compare\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_sign_compare - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_sign_compare\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_sign_compare - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_unused_local_typedefs\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_unused_local_typedefs - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_unused_local_typedefs\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_unused_local_typedefs - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality - Failed\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_parentheses_equality - Failed\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_ignored_attributes\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_ignored_attributes - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_ignored_attributes\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_ignored_attributes - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_terminate\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_terminate - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_terminate\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_terminate - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_int_in_bool_context\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_int_in_bool_context - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_int_in_bool_context\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_int_in_bool_context - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wimplicit_fallthrough_0\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wimplicit_fallthrough_0 - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wimplicit_fallthrough_0\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wimplicit_fallthrough_0 - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_maybe_uninitialized\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_maybe_uninitialized - Success\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_maybe_uninitialized\r\n-- Performing Test CXX_COMPILER_SUPPORT_FLAG__Wno_error_maybe_uninitialized - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_unused_function\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_unused_function - Success\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_array_bounds\r\n-- Performing Test C_COMPILER_SUPPORT_FLAG__Wno_error_array_bounds - Success\r\npaddle install dir: /home/ppocr/Serving/build_server/third_party/install/Paddle/\r\nWITH_GPU = OFF\r\n-- PADDLE_LIB_PATH=http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/CPU/gcc8.2_openblas/paddle_inference.tgz\r\npaddle serving source dir: /home/ppocr/Serving\r\n-- The Golang compiler identification is go1.17.2 linux/amd64\r\n-- Check for working Golang compiler: /usr/local/go/bin/go\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/__init__.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/analyse.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/channel.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/dag.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/error_catch.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/__init__.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/gateway.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/any.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/api.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/compiler\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/compiler/plugin.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/descriptor.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/duration.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/empty.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/field_mask.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/source_context.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/struct.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/timestamp.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/type.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proto/include/google/protobuf/wrappers.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/gateway/proxy_server.go\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/local_service_handler.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/logger.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/operator.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/pipeline_client.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/pipeline_server.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/profiler.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/prometheus_metrics.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/__init__.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/pipeline_service.proto\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/proto/run_codegen.py\r\n-- Installing: /home/ppocr/Serving/build_server/python/paddle_serving_server/pipeline/util.py\r\npython env: \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ppocr/Serving/build_server\r\nλ 9fab183dbb12 /home/ppocr/Serving/build_server {v0.9.0} make -j10\r\nScanning dependencies of target extern_pybind\r\nScanning dependencies of target extern_gflags\r\nScanning dependencies of target extern_boost\r\nScanning dependencies of target extern_snappy\r\nScanning dependencies of target extern_paddle\r\nScanning dependencies of target extern_zlib\r\nScanning dependencies of target extern_jsoncpp\r\n[  0%] Creating directories for 'extern_gflags'\r\n[  0%] Creating directories for 'extern_pybind'\r\nScanning dependencies of target extern_prometheus\r\nScanning dependencies of target transfer-logex\r\nScanning dependencies of target extern_utf8proc\r\n[  1%] Creating directories for 'extern_snappy'\r\n[  1%] Creating directories for 'extern_boost'\r\n[  1%] Creating directories for 'extern_jsoncpp'\r\n[  2%] Creating directories for 'extern_zlib'\r\n[  2%] Creating directories for 'extern_paddle'\r\n[  2%] Creating directories for 'extern_prometheus'\r\n[  3%] Creating directories for 'extern_utf8proc'\r\n[  3%] Performing download step (git clone) for 'extern_pybind'\r\n[  3%] Performing download step (git clone) for 'extern_gflags'\r\n[  3%] Performing download step (git clone) for 'extern_snappy'\r\n[  3%] Performing download step (download, verify and extract) for 'extern_boost'\r\n[  3%] Performing download step (git clone) for 'extern_jsoncpp'\r\n[  3%] Performing download step (git clone) for 'extern_zlib'\r\nCloning into 'extern_pybind'...\r\n-- Downloading...\r\n   dst='/home/ppocr/Serving/build_server/third_party/boost/src/extern_boost/boost_1_74_0.tar.gz'\r\n   timeout='none'\r\n-- Using src='http://paddlepaddledeps.bj.bcebos.com/boost_1_74_0.tar.gz'\r\nCloning into 'extern_snappy'...\r\nCloning into 'extern_gflags'...\r\n[  3%] Performing download step (download, verify and extract) for 'extern_paddle'\r\n[  3%] Performing download step (git clone) for 'extern_utf8proc'\r\n[  3%] Performing download step (git clone) for 'extern_prometheus'\r\nCloning into 'extern_zlib'...\r\nCloning into 'extern_jsoncpp'...\r\n-- Downloading...\r\n   dst='/home/ppocr/Serving/build_server/third_party/Paddle/src/extern_paddle/paddle_inference.tgz'\r\n   timeout='none'\r\n-- Using src='http://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/cxx_c/Linux/CPU/gcc8.2_openblas/paddle_inference.tgz'\r\nCloning into 'extern_prometheus'...\r\nCloning into 'extern_utf8proc'...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 2% complete]\r\n# cd .; git clone -- https://github.com/Badangel/logex /home/ppocr/Serving/core/cube/cube-transfer/src/github.com/Badangel/logex\r\nCloning into '/home/ppocr/Serving/core/cube/cube-transfer/src/github.com/Badangel/logex'...\r\nfatal: unable to access 'https://github.com/Badangel/logex/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\npackage github.com/Badangel/logex: exit status 128\r\ncore/cube/cube-transfer/CMakeFiles/transfer-logex.dir/build.make:57: recipe for target 'core/cube/cube-transfer/CMakeFiles/transfer-logex' failed\r\nmake[2]: *** [core/cube/cube-transfer/CMakeFiles/transfer-logex] Error 1\r\nCMakeFiles/Makefile2:1047: recipe for target 'core/cube/cube-transfer/CMakeFiles/transfer-logex.dir/all' failed\r\nmake[1]: *** [core/cube/cube-transfer/CMakeFiles/transfer-logex.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\nfatal: unable to access 'https://github.com/jupp0r/prometheus-cpp.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nfatal: unable to access 'https://github.com/pybind/pybind11.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nCloning into 'extern_prometheus'...\r\nCloning into 'extern_pybind'...\r\nfatal: unable to access 'https://github.com/JuliaStrings/utf8proc.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nfatal: unable to access 'https://github.com/google/snappy/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nCloning into 'extern_utf8proc'...\r\nfatal: unable to access 'https://github.com/open-source-parsers/jsoncpp/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nCloning into 'extern_snappy'...\r\nfatal: unable to access 'https://github.com/madler/zlib.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nCloning into 'extern_zlib'...\r\nCloning into 'extern_jsoncpp'...\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/ppocr/Serving/build_server/third_party/boost/src/extern_boost/boost_1_74_0.tar.gz'\r\n     dst='/home/ppocr/Serving/build_server/third_party/boost/src/extern_boost'\r\n-- extracting... [tar xfz]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 7% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 13% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\nfatal: unable to access 'https://github.com/JuliaStrings/utf8proc.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\nCloning into 'extern_utf8proc'...\r\n-- [download 17% complete]\r\n-- [download 18% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 23% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 29% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 34% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  3%] No patch step for 'extern_boost'\r\n[  4%] No update step for 'extern_boost'\r\n[  5%] No configure step for 'extern_boost'\r\n[  5%] No build step for 'extern_boost'\r\n[  6%] No install step for 'extern_boost'\r\n[  6%] Completed 'extern_boost'\r\n[  6%] Built target extern_boost\r\n-- [download 40% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 45% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 51% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 56% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 61% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 67% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 72% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 78% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 89% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 94% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/ppocr/Serving/build_server/third_party/Paddle/src/extern_paddle/paddle_inference.tgz'\r\n     dst='/home/ppocr/Serving/build_server/third_party/Paddle/src/extern_paddle'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  6%] No patch step for 'extern_paddle'\r\n[  7%] No update step for 'extern_paddle'\r\n[  8%] No configure step for 'extern_paddle'\r\n[  8%] No build step for 'extern_paddle'\r\n[  9%] Performing install step for 'extern_paddle'\r\n[  9%] Completed 'extern_paddle'\r\n[  9%] Built target extern_paddle\r\nfatal: unable to access 'https://github.com/open-source-parsers/jsoncpp/': Failed to connect to github.com port 443: Connection timed out\r\nfatal: unable to access 'https://github.com/jupp0r/prometheus-cpp.git/': Failed to connect to github.com port 443: Connection timed out\r\nfatal: unable to access 'https://github.com/google/snappy/': Failed to connect to github.com port 443: Connection timed out\r\nCloning into 'extern_jsoncpp'...\r\nCloning into 'extern_snappy'...\r\nCloning into 'extern_prometheus'...\r\nfatal: unable to access 'https://github.com/madler/zlib.git/': Failed to connect to github.com port 443: Connection timed out\r\nCloning into 'extern_zlib'...\r\nfatal: unable to access 'https://github.com/JuliaStrings/utf8proc.git/': Failed to connect to github.com port 443: Connection timed out\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/utf8proc/tmp/extern_utf8proc-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/JuliaStrings/utf8proc.git'\r\n\r\n\r\nCMakeFiles/extern_utf8proc.dir/build.make:90: recipe for target 'third_party/utf8proc/src/extern_utf8proc-stamp/extern_utf8proc-download' failed\r\nmake[2]: *** [third_party/utf8proc/src/extern_utf8proc-stamp/extern_utf8proc-download] Error 1\r\nCMakeFiles/Makefile2:622: recipe for target 'CMakeFiles/extern_utf8proc.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_utf8proc.dir/all] Error 2\r\nerror: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.\r\nfatal: The remote end hung up unexpectedly\r\nfatal: early EOF\r\nfatal: index-pack failed\r\nCloning into 'extern_gflags'...\r\nfatal: unable to access 'https://github.com/pybind/pybind11.git/': GnuTLS recv error (-54): Error in the pull function.\r\nCloning into 'extern_pybind'...\r\nfatal: unable to access 'https://github.com/jupp0r/prometheus-cpp.git/': Failed to connect to github.com port 443: Connection timed out\r\nfatal: unable to access 'https://github.com/open-source-parsers/jsoncpp/': Failed to connect to github.com port 443: Connection timed out\r\nfatal: unable to access 'https://github.com/google/snappy/': Failed to connect to github.com port 443: Connection timed out\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/prometheus/tmp/extern_prometheus-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/jupp0r/prometheus-cpp.git'\r\n\r\n\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/jsoncpp/tmp/extern_jsoncpp-gitclone.cmake:31 (message):\r\n  Failed to clone repository:\r\n  'https://github.com/open-source-parsers/jsoncpp'\r\n\r\n\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMakeFiles/extern_jsoncpp.dir/build.make:90: recipe for target 'third_party/jsoncpp/src/extern_jsoncpp-stamp/extern_jsoncpp-download' failed\r\nCMakeFiles/extern_prometheus.dir/build.make:90: recipe for target 'third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-download' failed\r\nmake[2]: *** [third_party/jsoncpp/src/extern_jsoncpp-stamp/extern_jsoncpp-download] Error 1\r\nmake[2]: *** [third_party/prometheus/src/extern_prometheus-stamp/extern_prometheus-download] Error 1\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/snappy/tmp/extern_snappy-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/google/snappy'\r\n\r\n\r\nCMakeFiles/Makefile2:514: recipe for target 'CMakeFiles/extern_jsoncpp.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_jsoncpp.dir/all] Error 2\r\nCMakeFiles/Makefile2:568: recipe for target 'CMakeFiles/extern_prometheus.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_prometheus.dir/all] Error 2\r\nCMakeFiles/extern_snappy.dir/build.make:90: recipe for target 'third_party/snappy/src/extern_snappy-stamp/extern_snappy-download' failed\r\nmake[2]: *** [third_party/snappy/src/extern_snappy-stamp/extern_snappy-download] Error 1\r\nCMakeFiles/Makefile2:676: recipe for target 'CMakeFiles/extern_snappy.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_snappy.dir/all] Error 2\r\nfatal: unable to access 'https://github.com/madler/zlib.git/': Failed to connect to github.com port 443: Connection timed out\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/zlib/tmp/extern_zlib-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/madler/zlib.git'\r\n\r\n\r\nCMakeFiles/extern_zlib.dir/build.make:90: recipe for target 'third_party/zlib/src/extern_zlib-stamp/extern_zlib-download' failed\r\nmake[2]: *** [third_party/zlib/src/extern_zlib-stamp/extern_zlib-download] Error 1\r\nCMakeFiles/Makefile2:730: recipe for target 'CMakeFiles/extern_zlib.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_zlib.dir/all] Error 2\r\nfatal: unable to access 'https://github.com/gflags/gflags.git/': Failed to connect to github.com port 443: Connection timed out\r\nCloning into 'extern_gflags'...\r\nfatal: unable to access 'https://github.com/pybind/pybind11.git/': Failed to connect to github.com port 443: Connection timed out\r\n-- Had to git clone more than once:\r\n          3 times.\r\nCMake Error at /home/ppocr/Serving/build_server/third_party/pybind/tmp/extern_pybind-gitclone.cmake:31 (message):\r\n  Failed to clone repository: 'https://github.com/pybind/pybind11.git'\r\n\r\n\r\nCMakeFiles/extern_pybind.dir/build.make:91: recipe for target 'third_party/pybind/src/extern_pybind-stamp/extern_pybind-download' failed\r\nmake[2]: *** [third_party/pybind/src/extern_pybind-stamp/extern_pybind-download] Error 1\r\nCMakeFiles/Makefile2:541: recipe for target 'CMakeFiles/extern_pybind.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_pybind.dir/all] Error 2\r\n-- Had to git clone more than once:\r\n          3 times.\r\nNote: checking out '77592648e3f3be87d6c7123eb81cbad75f9aef5a'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 7759264... repair wrong namespace problem\r\nSubmodule 'doc' (https://github.com/gflags/gflags.git) registered for path 'doc'\r\nCloning into 'doc'...\r\nSubmodule path 'doc': checked out '971dd2a4fadac9cdab174c523c22df79efd63aa5'\r\n[  9%] No patch step for 'extern_gflags'\r\n[ 10%] No update step for 'extern_gflags'\r\n[ 11%] Performing configure step for 'extern_gflags'\r\n-- extern_gflags configure command succeeded.  See also /home/ppocr/Serving/build_server/third_party/gflags/src/extern_gflags-stamp/extern_gflags-configure-*.log\r\n[ 11%] Performing build step for 'extern_gflags'\r\nScanning dependencies of target gflags_nothreads_static\r\nScanning dependencies of target gflags_static\r\n[ 12%] Building CXX object CMakeFiles/gflags_static.dir/src/gflags.cc.o\r\n[ 25%] Building CXX object CMakeFiles/gflags_nothreads_static.dir/src/gflags.cc.o\r\n[ 50%] Building CXX object CMakeFiles/gflags_nothreads_static.dir/src/gflags_reporting.cc.o\r\n[ 62%] Building CXX object CMakeFiles/gflags_static.dir/src/gflags_reporting.cc.o\r\n[ 75%] Building CXX object CMakeFiles/gflags_nothreads_static.dir/src/gflags_completions.cc.o\r\n[ 37%] Building CXX object CMakeFiles/gflags_static.dir/src/gflags_completions.cc.o\r\n[ 87%] Linking CXX static library lib/libgflags.a\r\n[100%] Linking CXX static library lib/libgflags_nothreads.a\r\n[100%] Built target gflags_static\r\n[100%] Built target gflags_nothreads_static\r\n[ 12%] Performing install step for 'extern_gflags'\r\n[100%] Built target gflags_nothreads_static\r\n[100%] Built target gflags_static\r\nInstall the project...\r\n-- Install configuration: \"Release\"\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/libgflags.a\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/libgflags_nothreads.a\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/include/gflags/gflags.h\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/include/gflags/gflags_declare.h\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/include/gflags/gflags_completions.h\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/include/gflags/gflags_gflags.h\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/cmake/gflags/gflags-config.cmake\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/cmake/gflags/gflags-config-version.cmake\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/cmake/gflags/gflags-targets.cmake\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/cmake/gflags/gflags-targets-release.cmake\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/bin/gflags_completions.sh\r\n-- Installing: /home/ppocr/Serving/build_server/third_party/install/gflags/lib/pkgconfig/gflags.pc\r\n-- Installing: /root/.cmake/packages/gflags/023c382eb49d526dba039aa80c04d9e8\r\n[ 12%] Completed 'extern_gflags'\r\n[ 12%] Built target extern_gflags\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nλ 9fab183dbb12 /home/ppocr/Serving/build_server {v0.9.0} \r\n\r\n编译serving时，make -j一直失败，这是什么原因。\r\n",
        "state": "open",
        "user": "123Yujiahang",
        "closed_by": null,
        "created_at": "2024-05-15T01:16:42+00:00",
        "updated_at": "2024-05-15T01:18:22+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "123Yujiahang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1998,
        "title": "Is there a simple way for Healthcheck?",
        "body": "I deploied a det model and a rec model using PD-Serving. \r\nCan I get the status using a rest api ?",
        "state": "closed",
        "user": "lvxqjp",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-06T01:31:07+00:00",
        "updated_at": "2025-06-10T06:50:24+00:00",
        "closed_at": "2025-06-10T06:50:24+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 1999,
        "title": "TypeError: prepare_pipeline_config() got an unexpected keyword argument 'yml_dict'",
        "body": " python3 web_service.py -c config.yml\r\nargs config: {'rpc_port': 18091, 'http_port': 9998, 'worker_num': 10, 'build_dag_each_worker': False, 'dag': {'is_thread_op': False, 'retry': 10, 'use_profile': True, 'tracer': {'interval_s': 10}}, 'op': {'det': {'concurrency': 8, 'local_service_conf': {'client_type': 'local_predictor', 'model_config': './ppocr_det_v3_serving', 'devices': '0', 'ir_optim': True}}, 'rec': {'concurrency': 4, 'timeout': -1, 'retry': 1, 'local_service_conf': {'client_type': 'local_predictor', 'model_config': './ppocr_rec_v3_serving', 'devices': '0', 'ir_optim': True}}}}\r\nTraceback (most recent call last):\r\n  File \"web_service.py\", line 177, in <module>\r\n    uci_service.prepare_pipeline_config(yml_dict=FLAGS.conf_dict)\r\nTypeError: prepare_pipeline_config() got an unexpected keyword argument 'yml_dict'",
        "state": "closed",
        "user": "alanOO7",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-19T01:03:37+00:00",
        "updated_at": "2025-06-24T06:46:35+00:00",
        "closed_at": "2025-06-24T06:46:35+00:00",
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2000,
        "title": "如何通过docker+paddel  serving 部署表格识别模型，能给出一些参考代码吗？",
        "body": "如何通过docker+paddel  serving 部署表格识别模型，能给出一些资料指导吗？",
        "state": "open",
        "user": "RockL888",
        "closed_by": null,
        "created_at": "2024-07-05T08:36:18+00:00",
        "updated_at": "2025-02-12T01:42:11+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "pioneer12345"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2001,
        "title": "执行 pip3 install -r python/requirements.txt  报错",
        "body": "Building wheels for collected packages: av\r\n  Building wheel for av (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/local/bin/python3.6 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-2di0ikr3\r\n       cwd: /tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/\r\n  Complete output (48 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/av\r\n  copying av/__main__.py -> build/lib.linux-x86_64-3.6/av\r\n  copying av/__init__.py -> build/lib.linux-x86_64-3.6/av\r\n  copying av/datasets.py -> build/lib.linux-x86_64-3.6/av\r\n  copying av/deprecation.py -> build/lib.linux-x86_64-3.6/av\r\n  creating build/lib.linux-x86_64-3.6/av/container\r\n  copying av/container/__init__.py -> build/lib.linux-x86_64-3.6/av/container\r\n  creating build/lib.linux-x86_64-3.6/av/subtitles\r\n  copying av/subtitles/__init__.py -> build/lib.linux-x86_64-3.6/av/subtitles\r\n  creating build/lib.linux-x86_64-3.6/av/codec\r\n  copying av/codec/__init__.py -> build/lib.linux-x86_64-3.6/av/codec\r\n  creating build/lib.linux-x86_64-3.6/av/filter\r\n  copying av/filter/__init__.py -> build/lib.linux-x86_64-3.6/av/filter\r\n  creating build/lib.linux-x86_64-3.6/av/audio\r\n  copying av/audio/__init__.py -> build/lib.linux-x86_64-3.6/av/audio\r\n  creating build/lib.linux-x86_64-3.6/av/data\r\n  copying av/data/__init__.py -> build/lib.linux-x86_64-3.6/av/data\r\n  creating build/lib.linux-x86_64-3.6/av/video\r\n  copying av/video/__init__.py -> build/lib.linux-x86_64-3.6/av/video\r\n  creating build/lib.linux-x86_64-3.6/av/sidedata\r\n  copying av/sidedata/__init__.py -> build/lib.linux-x86_64-3.6/av/sidedata\r\n  running build_ext\r\n  running config\r\n  PyAV: 8.0.3 (unknown commit)\r\n  Python: 3.6.0 (default, Mar  4 2022, 05:45:55) \\n[GCC 8.2.0]\r\n  platform: Linux-5.4.0-150-generic-x86_64-with-debian-stretch-sid\r\n  extension_extra:\r\n        include_dirs: [b'include']\r\n        libraries: []\r\n        library_dirs: []\r\n        define_macros: []\r\n        runtime_library_dirs: []\r\n  config_macros:\r\n        PYAV_COMMIT_STR=\"unknown-commit\"\r\n        PYAV_VERSION=8.0.3\r\n        PYAV_VERSION_STR=\"8.0.3\"\r\n  Could not find libavformat with pkg-config.\r\n  Could not find libavcodec with pkg-config.\r\n  Could not find libavdevice with pkg-config.\r\n  Could not find libavutil with pkg-config.\r\n  Could not find libavfilter with pkg-config.\r\n  Could not find libswscale with pkg-config.\r\n  Could not find libswresample with pkg-config.\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for av\r\n  Running setup.py clean for av\r\nFailed to build av\r\nInstalling collected packages: av\r\n    Running setup.py install for av ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/local/bin/python3.6 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-8oce82tt/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6m/av\r\n         cwd: /tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/\r\n    Complete output (50 lines):\r\n    running install\r\n    /usr/local/lib/python3.6/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n      setuptools.SetuptoolsDeprecationWarning,\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-x86_64-3.6\r\n    creating build/lib.linux-x86_64-3.6/av\r\n    copying av/__main__.py -> build/lib.linux-x86_64-3.6/av\r\n    copying av/__init__.py -> build/lib.linux-x86_64-3.6/av\r\n    copying av/datasets.py -> build/lib.linux-x86_64-3.6/av\r\n    copying av/deprecation.py -> build/lib.linux-x86_64-3.6/av\r\n    creating build/lib.linux-x86_64-3.6/av/container\r\n    copying av/container/__init__.py -> build/lib.linux-x86_64-3.6/av/container\r\n    creating build/lib.linux-x86_64-3.6/av/subtitles\r\n    copying av/subtitles/__init__.py -> build/lib.linux-x86_64-3.6/av/subtitles\r\n    creating build/lib.linux-x86_64-3.6/av/codec\r\n    copying av/codec/__init__.py -> build/lib.linux-x86_64-3.6/av/codec\r\n    creating build/lib.linux-x86_64-3.6/av/filter\r\n    copying av/filter/__init__.py -> build/lib.linux-x86_64-3.6/av/filter\r\n    creating build/lib.linux-x86_64-3.6/av/audio\r\n    copying av/audio/__init__.py -> build/lib.linux-x86_64-3.6/av/audio\r\n    creating build/lib.linux-x86_64-3.6/av/data\r\n    copying av/data/__init__.py -> build/lib.linux-x86_64-3.6/av/data\r\n    creating build/lib.linux-x86_64-3.6/av/video\r\n    copying av/video/__init__.py -> build/lib.linux-x86_64-3.6/av/video\r\n    creating build/lib.linux-x86_64-3.6/av/sidedata\r\n    copying av/sidedata/__init__.py -> build/lib.linux-x86_64-3.6/av/sidedata\r\n    running build_ext\r\n    running config\r\n    PyAV: 8.0.3 (unknown commit)\r\n    Python: 3.6.0 (default, Mar  4 2022, 05:45:55) \\n[GCC 8.2.0]\r\n    platform: Linux-5.4.0-150-generic-x86_64-with-debian-stretch-sid\r\n    extension_extra:\r\n        include_dirs: [b'include']\r\n        libraries: []\r\n        library_dirs: []\r\n        define_macros: []\r\n        runtime_library_dirs: []\r\n    config_macros:\r\n        PYAV_COMMIT_STR=\"unknown-commit\"\r\n        PYAV_VERSION=8.0.3\r\n        PYAV_VERSION_STR=\"8.0.3\"\r\n    Could not find libavformat with pkg-config.\r\n    Could not find libavcodec with pkg-config.\r\n    Could not find libavdevice with pkg-config.\r\n    Could not find libavutil with pkg-config.\r\n    Could not find libavfilter with pkg-config.\r\n    Could not find libswscale with pkg-config.\r\n    Could not find libswresample with pkg-config.\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/local/bin/python3.6 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-33ucddr6/av_a8ed30cb6adf4834b63b8b918a77b42a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-8oce82tt/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6m/av Check the logs for full command output.",
        "state": "open",
        "user": "RockL888",
        "closed_by": null,
        "created_at": "2024-07-10T07:02:06+00:00",
        "updated_at": "2025-04-02T07:40:59+00:00",
        "closed_at": null,
        "comments_count": [
            "Ryanznoco",
            "pjd206",
            "LuMitchell"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2002,
        "title": "求教：怎么得到中文单字符的位置？",
        "body": "求教：怎么得到中文单字符的位置？",
        "state": "open",
        "user": "March-Wind",
        "closed_by": null,
        "created_at": "2024-07-15T11:13:55+00:00",
        "updated_at": "2024-07-15T11:14:20+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2003,
        "title": "Compilation failure of paddle serving deployment based on Rise 910B",
        "body": "ERROR:\r\n`λ localhost /work/Serving/build-server-npu {v0.9.0} make TARGET=ARMV8 -j16\r\n[  3%] Built target extern_gflags\r\n[  9%] Built target extern_snappy\r\n[  9%] Built target extern_zlib\r\n[ 13%] Performing download step (download, verify and extract) for 'extern_paddle'\r\n[ 14%] Built target extern_boost\r\n[ 17%] Built target extern_utf8proc\r\n[ 27%] Built target extern_prometheus\r\n[ 27%] Built target extern_pybind\r\n[ 27%] Built target extern_jsoncpp\r\n-- File already exists but no hash specified (use URL_HASH):\r\n  file='/work/Serving/build-server-npu/third_party/Paddle/src/extern_paddle/archive.tar'\r\nOld file will be removed and new file downloaded from URL.\r\n-- Downloading...\r\n   dst='/work/Serving/build-server-npu/third_party/Paddle/src/extern_paddle/archive.tar'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz'\r\n[ 27%] Built target general_model_config_py_proto_init\r\n[ 30%] Built target extern_leveldb\r\n[ 30%] Built target boost\r\n[ 33%] Built target extern_protobuf\r\n[ 36%] Built target extern_glog\r\n[ 36%] Built target general_model_service_py_proto_init\r\n[ 36%] Built target server_config_py_proto_init\r\ncore/pdcodegen/CMakeFiles/pdcodegen.dir/build.make:73: *** target pattern contains no '%'.  Stop.\r\nmake[1]: *** [CMakeFiles/Makefile2:1383: core/pdcodegen/CMakeFiles/pdcodegen.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\nCopy generated general_model_config proto file into directory paddle_serving_server/proto.\r\n[ 39%] Built target extern_brpc\r\n[ 40%] Built target utils\r\n[ 40%] Running Python grpc protocol buffer compiler on proto/general_model_service.proto\r\nCopy generated python proto into directory paddle_serving_server/proto.\r\n[ 41%] Built target general_model_config_py_proto\r\n/usr/bin/python3: Error while finding module specification for 'grpc_tools.protoc' (ModuleNotFoundError: No module named 'grpc_tools')\r\n[ 42%] Built target server_config_py_proto\r\nmake[2]: *** [core/configure/CMakeFiles/general_model_service_py_proto.dir/build.make:73: core/configure/general_model_service_pb2_grpc.py] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:1276: core/configure/CMakeFiles/general_model_service_py_proto.dir/all] Error 2\r\n[ 42%] Built target transfer-rfw\r\n[ 42%] Built target transfer-docopt-go\r\n[ 42%] Built target agent-pipeline\r\n[ 42%] Built target transfer-logex\r\n[ 42%] Built target agent-logex\r\n[ 42%] Built target agent-docopt-go\r\n-- Retrying...\r\n-- Using src='http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz'\r\n-- Retry after 5 seconds (attempt #2) ...\r\n-- Using src='http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz'\r\n-- [download 0% complete]\r\nCMake Error at extern_paddle-stamp/download-extern_paddle.cmake:170 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz' failed\r\n          status_code: 6\r\n          status_string: \"Couldn't resolve host name\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n          timeout on name lookup is not supported\r\n\r\n  getaddrinfo(3) failed for paddle-serving.bj.bcebos.com:80\r\n\r\n  Could not resolve host: paddle-serving.bj.bcebos.com\r\n\r\n  Closing connection 0\r\n\r\n\r\n\r\n          --- LOG END ---\r\n          error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz' failed\r\n          status_code: 6\r\n          status_string: \"Couldn't resolve host name\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n          timeout on name lookup is not supported\r\n\r\n  getaddrinfo(3) failed for paddle-serving.bj.bcebos.com:80\r\n\r\n  Could not resolve host: paddle-serving.bj.bcebos.com\r\n\r\n  Closing connection 0\r\n\r\n\r\n\r\n          --- LOG END ---\r\n          error: downloading 'http://paddle-serving.bj.bcebos.com/inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz' failed\r\n          status_code: 22\r\n          status_string: \"HTTP response code said error\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n          timeout on name lookup is not supported\r\n    Trying 36.110.192.178:80...\r\n\r\n  Connected to paddle-serving.bj.bcebos.com (36.110.192.178) port 80 (#0)\r\n\r\n  GET\r\n  /inferlib/2.3.0-rc0/cxx_c/Linux/ASCEND/arm64_gcc8.2_openblas/paddle_inference.tgz\r\n  HTTP/1.1\r\n\r\n  Host: paddle-serving.bj.bcebos.com\r\n\r\n  User-Agent: curl/8.1.2\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  HTTP/1.1 404 Not Found\r\n\r\n  Date: Thu, 18 Jul 2024 05:28:13 GMT\r\n\r\n  Content-Type: application/json; charset=utf-8\r\n\r\n  Content-Length: 117\r\n\r\n  Connection: keep-alive\r\n\r\n  Server: BceBos\r\n\r\n  X-Bce-Flow-Control-Type: -1\r\n\r\n  X-Bce-Is-Transition: false\r\n\r\n  x-bce-debug-id:\r\n  bg0f34msmT7m8r0KJJxOoV4guk+389TQ4ZojqMLRbSUKKq5MvgNJwFXdW0hpC7fCuFmESMiiA68ehKStIAUgYg==\r\n\r\n\r\n  x-bce-flow-control-type: -1\r\n\r\n  x-bce-is-transition: false\r\n\r\n  x-bce-request-id: e55b9176-be2c-4be0-8f16-8e7d3c91a977\r\n\r\n  The requested URL returned error: 404\r\n\r\n  Closing connection 0\r\n\r\n\r\n\r\n          --- LOG END ---\r\n          \r\n    \r\n\r\n\r\nmake[2]: *** [CMakeFiles/extern_paddle.dir/build.make:99: third_party/Paddle/src/extern_paddle-stamp/extern_paddle-download] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:759: CMakeFiles/extern_paddle.dir/all] Error 2\r\nmake: *** [Makefile:136: all] Error 2\r\n`\r\n\r\n\r\nScene Reproduction：\r\n`IMAGE: registry.baidubce.com/device/paddle-npu:cann80RC1-ubuntu20-aarch64-gcc84-py39\r\nAfter completing the installation of the paddles：（reference）\r\nhttps://www.paddlepaddle.org.cn/documentation/docs/zh/guides/hardware_support/npu/install_cn.html\r\n\r\nReference links to compiling serving：\r\nhttps://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Run_On_NPU_CN.md\r\n\r\n4、compiling server\r\n\r\nmkdir build-server-npu && cd build-server-npu\r\ncmake -DPYTHON_INCLUDE_DIR=$PYTHON_INCLUDE_DIR/ \\\r\n    -DPYTHON_LIBRARIES=$PYTHON_LIBRARIES \\\r\n    -DPYTHON_EXECUTABLE=$PYTHON_EXECUTABLE \\\r\n    -DCMAKE_INSTALL_PREFIX=./output \\\r\n    -DWITH_ASCEND_CL=ON \\\r\n    -DSERVER=ON ..\r\nmake TARGET=ARMV8 -j16\r\n\r\n`",
        "state": "open",
        "user": "JasonFlyBeauty",
        "closed_by": null,
        "created_at": "2024-07-18T06:41:40+00:00",
        "updated_at": "2025-01-23T08:23:53+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]",
            "danyXu",
            "JasonFlyBeauty",
            "danyXu",
            "JasonFlyBeauty",
            "cgq0816"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2004,
        "title": "How does the inputs and outputs work. What is scale factor and can I disable it?",
        "body": "Full disclosure, I am not an ML expert and barely understand the fundamentals so I apologize if this is ignorant. I can get around the tool chains, read directions, and solve dependency issues. \r\n\r\nMost yolo tool chains, have an input and output. When exporting paddle models it has input, scale factor, and two scale factor outputs. I don't know how any of the layers work, I just know that scale factor seems unique to paddlepaddle, and I use fixed size everything, I don't even want that. I'm not scaling anything. \r\n\r\nIs there a way to disable the use of scalefactor and export a simple in/out model?",
        "state": "closed",
        "user": "rlewkowicz",
        "closed_by": "rlewkowicz",
        "created_at": "2024-08-09T21:47:43+00:00",
        "updated_at": "2024-08-09T21:48:13+00:00",
        "closed_at": "2024-08-09T21:48:12+00:00",
        "comments_count": [
            "github-actions[bot]",
            "rlewkowicz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2005,
        "title": "pipelineserver 335 行写了两个 grpc.max_send_message_length 配置，",
        "body": "![image](https://github.com/user-attachments/assets/2fc02c7d-a8eb-4c94-9a9b-84cf6e8fc660)\r\n",
        "state": "open",
        "user": "LeanFly",
        "closed_by": null,
        "created_at": "2024-08-20T06:54:07+00:00",
        "updated_at": "2024-08-20T06:54:30+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2006,
        "title": "支持M系列的Mac产品吗,我是m3",
        "body": "<img width=\"1512\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fcc89af1-3611-4fb9-9806-56c5faecc8b1\">\r\n",
        "state": "open",
        "user": "gracefullliam",
        "closed_by": null,
        "created_at": "2024-09-02T08:07:05+00:00",
        "updated_at": "2024-09-02T08:07:30+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2007,
        "title": "readme_cn.md 和 readme.md 两个文件弄反了",
        "body": "中英文 README 放反了。",
        "state": "open",
        "user": "Hsiayukoo",
        "closed_by": null,
        "created_at": "2024-10-24T01:42:16+00:00",
        "updated_at": "2024-10-24T01:42:38+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2008,
        "title": "[ernie] failed to predict. (data_id=3 log_id=3) [ernie|0] Failed to process(batch: [3]): In user code:",
        "body": "请问以下报错是什么原因？\r\n[ernie] failed to predict. (data_id=3 log_id=3) [ernie|0] Failed to process(batch: [3]): In user code:\r\n\r\n    File \"/home/aistudio/work/PaddleNLP/legacy/applications/neural_search/recall/in_batch_negative/export_model.py\", line 56, in <module>\r\n      paddle.jit.save(model, save_path)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/api.py\", line 752, in wrapper\r\n      func(layer, path, input_spec, **configs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/dygraph/base.py\", line 75, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/api.py\", line 1043, in save\r\n      static_func.concrete_program_specify_input_spec(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py\", line 709, in concrete_program_specify_input_spec\r\n      concrete_program, _ = self.get_concrete_program(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py\", line 589, in get_concrete_program\r\n      concrete_program, partial_program_layer = self._program_cache[\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py\", line 1249, in __getitem__\r\n      self._caches[item_id] = self._build_once(item)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py\", line 1193, in _build_once\r\n      concrete_program = ConcreteProgram.from_func_spec(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/dygraph/base.py\", line 75, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/program_translator.py\", line 1063, in from_func_spec\r\n      outputs = static_func(*inputs)\r\n    File \"/home/aistudio/work/PaddleNLP/legacy/applications/neural_search/recall/in_batch_negative/base_model.py\", line 111, in get_pooled_embedding\r\n      _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids, attention_mask)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py\", line 1256, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py\", line 1235, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/ernie/modeling.py\", line 331, in forward\r\n      if attention_mask is None:\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/convert_operators.py\", line 352, in convert_ifelse\r\n      out = _run_py_ifelse(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/dy2static/convert_operators.py\", line 429, in _run_py_ifelse\r\n      py_outs = true_fn() if pred else false_fn()\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddlenlp/transformers/ernie/modeling.py\", line 332, in forward\r\n      attention_mask = paddle.unsqueeze(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/layers/math_op_patch.py\", line 445, in __impl__\r\n      current_block(self).append_op(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/framework.py\", line 4013, in append_op\r\n      op = Operator(\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/fluid/framework.py\", line 2781, in __init__\r\n      for frame in traceback.extract_stack():\r\n\r\n    InvalidArgumentError: Axis should be less than 2, but received axis is 2.\r\n      [Hint: Expected axis < max_dim, but received axis:2 >= max_dim:2.] (at /paddle/paddle/phi/kernels/funcs/common_shape.h:53)\r\n      [operator < equal > error]. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details.\r\n      \r\n      \r\n运行的环境是：\r\n# python -V\r\nPython 3.8.10\r\n# nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\r\nCuda compilation tools, release 10.2, V10.2.89\r\n# pip list | grep paddle\r\npaddle-bfloat                  0.1.7\r\npaddle-serving-app             0.9.0\r\npaddle-serving-client          0.9.0\r\npaddle-serving-server-gpu      0.9.0.post1028\r\npaddle2onnx                    1.2.11\r\npaddlefsl                      1.1.0\r\npaddlenlp                      2.4.2\r\npaddlepaddle-gpu               2.4.2\r\n\r\n按照这个例子训练了一个召回模型\r\nhttps://github.com/PaddlePaddle/PaddleNLP/tree/develop/slm/applications/neural_search/recall/in_batch_negative\r\n\r\n现在是部署推理阶段报错了",
        "state": "open",
        "user": "lisdoo",
        "closed_by": null,
        "created_at": "2024-10-25T06:55:52+00:00",
        "updated_at": "2024-10-25T06:56:15+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2009,
        "title": "在容器中编译Paddle Serving C++失败",
        "body": "参考[doc/Compile_CN.md](https://github.com/PaddlePaddle/Serving/blob/v0.8.3/doc/Compile_CN.md)文档尝试编译Serving，一直失败，错误信息如下：\r\n\r\npython/CMakeFiles/paddle_python.dir/build.make:75: recipe for target '.timestamp' failed\r\nmake[2]: *** [.timestamp] Error 1\r\nCMakeFiles/Makefile2:1669: recipe for target 'python/CMakeFiles/paddle_python.dir/all' failed\r\nmake[1]: *** [python/CMakeFiles/paddle_python.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2",
        "state": "open",
        "user": "ouerum",
        "closed_by": null,
        "created_at": "2024-10-31T07:57:34+00:00",
        "updated_at": "2024-10-31T07:58:51+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2010,
        "title": "使用容器编译c++ Serving失败",
        "body": "错误信息如下：\r\n\r\npython/CMakeFiles/paddle_python.dir/build.make:75: recipe for target '.timestamp' failed\r\nmake[2]: *** [.timestamp] Error 1\r\nCMakeFiles/Makefile2:1669: recipe for target 'python/CMakeFiles/paddle_python.dir/all' failed\r\nmake[1]: *** [python/CMakeFiles/paddle_python.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2",
        "state": "open",
        "user": "ouerum",
        "closed_by": null,
        "created_at": "2024-11-04T09:10:56+00:00",
        "updated_at": "2024-11-21T09:12:35+00:00",
        "closed_at": null,
        "comments_count": [
            "ouerum"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2013,
        "title": "训练的图片多标签识别模型，使用paddleserving部署模型，预测结果和在本地使用命令行预测结果不一致",
        "body": "PaddleClas：2.5.2 PaddleServing：paddlepaddle/serving：0.7.0-devel\r\n以下是classification_web_service.py脚本内容\r\n\r\nclass ImagenetOp(Op):\r\ndef init_op(self):\r\nself.seq = Sequential([\r\nResize(256), CenterCrop(224), RGB2BGR(), Transpose((2, 0, 1)),\r\nDiv(255), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],\r\nTrue)\r\n])\r\nself.label_dict = {}\r\nlabel_idx = 0\r\nwith open(\"gas.label\") as fin:\r\nfor line in fin:\r\nself.label_dict[label_idx] = line.strip()\r\nlabel_idx += 1\r\n\r\ndef preprocess(self, input_dicts, data_id, log_id):\r\n    (_, input_dict), = input_dicts.items()\r\n    batch_size = len(input_dict.keys())\r\n    imgs = []\r\n    for key in input_dict.keys():\r\n        data = base64.b64decode(input_dict[key].encode('utf8'))\r\n        data = np.fromstring(data, np.uint8)\r\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n        img = self.seq(im)\r\n        imgs.append(img[np.newaxis, :].copy())\r\n    input_imgs = np.concatenate(imgs, axis=0)\r\n    return {\"x\": input_imgs}, False, None, \"\"\r\n\r\ndef postprocess(self, input_dicts, fetch_dict, data_id, log_id):\r\n    score_list = fetch_dict[\"prediction\"]\r\n    result = {\"scores\": str(score_list)}\r\n    return result, None, \"\"\r\nclass ImageService(WebService):\r\ndef get_pipeline_response(self, read_op):\r\nimage_op = ImagenetOp(name=\"imagenet\", input_ops=[read_op])\r\nreturn image_op\r\n\r\nuci_service = ImageService(name=\"imagenet\")\r\nuci_service.prepare_pipeline_config(\"config.yml\")\r\nuci_service.run_service()",
        "state": "open",
        "user": "syge",
        "closed_by": null,
        "created_at": "2024-11-19T06:29:03+00:00",
        "updated_at": "2024-11-19T06:29:26+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2014,
        "title": "开启GPU，推理的时候报错。",
        "body": "W1125 12:07:24.788084   312 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.2, Runtime API Version: 10.2\r\nW1125 12:07:24.792352   312 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\nNo stack trace in paddle, may be caused by external reasons.\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1732536447 (unix time) try \"date -d @1732536447\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x0) received by PID 312 (TID 0x7f5cd301c700) from PID 0 ***]\r\n\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'ascii' codec can't encode character '\\u2019' in position 576: ordinal not in range(128)\r\nCall stack:\r\n  File \"/usr/local/lib/python3.6/threading.py\", line 884, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/usr/local/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/local/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.6/concurrent/futures/thread.py\", line 66, in _worker\r\n    work_item.run()\r\n  File \"/usr/local/lib/python3.6/concurrent/futures/thread.py\", line 55, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/grpc/_server.py\", line 553, in _unary_response_in_pool\r\n    argument, request_deserializer)\r\n  File \"/usr/local/lib/python3.6/site-packages/grpc/_server.py\", line 435, in _call_behavior\r\n    response_or_iterator = behavior(argument, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/pipeline_server.py\", line 73, in inference\r\n    resp = self._dag_executor.call(request)\r\n  File \"/usr/local/lib/python3.6/site-packages/paddle_serving_server/pipeline/dag.py\", line 420, in call\r\n    resp_channeldata.error_info))\r\nMessage: \"(data_id=1 log_id=0) Failed to predict: [det] failed to predict. (data_id=1 log_id=1) [det|0] Failed to process(batch: [1]): (External) CUSOLVER error(7). \\n  [Hint: 'CUSOLVER_STATUS_INTERNAL_ERROR'. An internal cuSolver operation failed. This error is usually caused by a cudaMemcpyAsync() failure.To correct: check that the hardware, an appropriate version of the driver, and the cuSolver library are correctly installed. Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine\\u2019s completion.] (at /paddle/paddle/phi/backends/gpu/gpu_context.cc:548)\\n. Please check the input dict and checkout PipelineServingLogs/pipeline.log for more details.\"\r\nArguments: ()",
        "state": "open",
        "user": "dongwentao1",
        "closed_by": null,
        "created_at": "2024-11-25T12:08:17+00:00",
        "updated_at": "2024-11-25T12:08:41+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Serving",
        "number": 2015,
        "title": "Error running service: cannot pickle '_thread.lock' object 请问如何解决？",
        "body": "windows部署报错：\r\n> python web_service.py --config=config.yml\r\nargs config: {'rpc_port': 18091, 'http_port': 9998, 'worker_num': 1, 'build_dag_each_worker': False, 'dag': {'is_thread_op': True, 'retry': 3, 'use_profile': False, 'tracer': {'interval_s': -1}}, 'op': {'det': {'concurrency': 1, 'local_service_conf': {'client_type': 'local_predictor', 'model_config': './ppocr_det_v4_serving', 'devices': '', 'ir_optim': False}}, 'rec': {'concurrency': 1, 'timeout': 3000, 'retry': 1, 'local_service_conf': {'client_type': 'local_predictor', 'model_config': './ppocr_rec_v4_serving', 'devices': '', 'ir_optim': False}}}}\r\n[DAG] Succ init\r\nI0113 11:20:45.652944 10512 analysis_predictor.cc:1626] MKLDNN is enabled\r\nI0113 11:20:45.652944 10512 analysis_predictor.cc:1740] Ir optimization is turned off, no ir pass will be executed.\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\nI0113 11:20:45.652944 10512 executor.cc:187] Old Executor is Running.\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[1me[35m--- Running analysis [save_optimized_model_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0113 11:20:45.668565 10512 memory_optimize_pass.cc:118] The persistable params in main graph are : 10.2689MB\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : linear_170.tmp_1  size: 26500\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : shape_5.tmp_0_slice_1  size: 4\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : conv2d_198.tmp_1  size: 11520\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : linear_170.tmp_0  size: 26500\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : pool2d_3.tmp_0_clone_0  size: 1920\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : batch_norm_5.tmp_0  size: 1920\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : x  size: 576\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : transpose_44.tmp_0_slice_2  size: 480\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : fill_constant_17.tmp_0  size: 4\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : fill_constant_19.tmp_0  size: 4\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : shape_3.tmp_0_slice_1  size: 4\r\nI0113 11:20:45.684191 10512 memory_optimize_pass.cc:246] Cluster name : reshape2_27.tmp_1  size: 0\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0113 11:20:45.736836 10512 analysis_predictor.cc:1838] ======= optimize end =======\r\nI0113 11:20:45.736836 10512 naive_executor.cc:200] ---  skip [feed], feed -> x\r\nI0113 11:20:45.736836 10512 naive_executor.cc:200] ---  skip [softmax_11.tmp_0], fetch -> fetch\r\n[OP Object] init success\r\nI0113 11:20:45.762605 10512 analysis_predictor.cc:1626] MKLDNN is enabled\r\nI0113 11:20:45.762605 10512 analysis_predictor.cc:1740] Ir optimization is turned off, no ir pass will be executed.\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[1me[35m--- Running analysis [save_optimized_model_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:118] The persistable params in main graph are : 4.47025MB\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_85  size: 1536\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_21  size: 192\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_94  size: 1536\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_118  size: 96\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : relu_1.tmp_0  size: 384\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : hardswish_79.tmp_0  size: 1536\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_35  size: 384\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : tmp_73  size: 768\r\nI0113 11:20:45.782716 10512 memory_optimize_pass.cc:246] Cluster name : x  size: 12\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0113 11:20:45.830435 10512 analysis_predictor.cc:1838] ======= optimize end =======\r\nI0113 11:20:45.830435 10512 naive_executor.cc:200] ---  skip [feed], feed -> x\r\nI0113 11:20:45.830435 10512 naive_executor.cc:200] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[OP Object] init success\r\n[PipelineServicer] succ init\r\nError running service: cannot pickle '_thread.lock' object\r\n\r\n\r\n\r\n当前环境为：windows的anaconda环境下使用pip命令\r\npython 3.8\r\n\r\nName: paddlepaddle\r\nVersion: 2.6.2\r\nName: paddleocr\r\nVersion: 2.9.1\r\n\r\n使用线程，config.yml如下：\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 18091\r\n\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\n# 原来没有注释掉：http_port: 9998\r\nhttp_port: 9998\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n##当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\n#原来为：worker_num: 10\r\nworker_num: 1\r\n\r\n#build_dag_each_worker, False，框架在进程内创建一条DAG；True，框架会每个进程内创建多个独立的DAG\r\n#原来为：build_dag_each_worker: False\r\nbuild_dag_each_worker: False\r\n\r\ndag:\r\n    #op资源类型, True, 为线程模型；False，为进程模型\r\n    #原来为：is_thread_op: False\r\n    is_thread_op: False\r\n\r\n    #重试次数\r\n    #原来为：retry: 10\r\n    retry: 3\r\n\r\n    #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n    #原来为：use_profile: True\r\n    use_profile: False\r\n    \r\n    tracer:\r\n        #原来为：interval_s: 10\r\n        interval_s: -1\r\n\r\nop:\r\n    det:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        #原来为：concurrency: 8\r\n        concurrency: 1\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n            #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #det模型路径\r\n            #model_config: ./ppocr_det_v3_serving\r\n            model_config: ./ppocr_det_v4_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准，不设置默认取全部输出变量\r\n            #fetch_list: [\"sigmoid_0.tmp_0\"]\r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            #原来为：devices: \"0\"\r\n            devices: \"\"\r\n            \r\n            #原来为：ir_optim: True\r\n            ir_optim: False\r\n    rec:\r\n        #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n        #原来为：concurrency: 4\r\n        concurrency: 1\r\n\r\n        #超时时间, 单位ms\r\n        #原来为：timeout: -1\r\n        timeout: 3000\r\n \r\n        #Serving交互重试次数，默认不重试\r\n        retry: 1\r\n\r\n        #当op配置没有server_endpoints时，从local_service_conf读取本地服务配置\r\n        local_service_conf:\r\n\r\n            #client类型，包括brpc, grpc和local_predictor。local_predictor不启动Serving服务，进程内预测\r\n            client_type: local_predictor\r\n\r\n            #rec模型路径\r\n            #model_config: ./ppocr_rec_v3_serving\r\n            model_config: ./ppocr_rec_v4_serving\r\n\r\n            #Fetch结果列表，以client_config中fetch_var的alias_name为准, 不设置默认取全部输出变量\r\n            #fetch_list: \r\n\r\n            #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n            #原来为：devices: \"0\"\r\n            devices: \"\"\r\n            \r\n            #原来为：ir_optim: True\r\n            ir_optim: False\r\n\r\n\r\n部署命令运行后报错\r\n",
        "state": "open",
        "user": "eunij-peanut",
        "closed_by": null,
        "created_at": "2025-01-13T06:40:01+00:00",
        "updated_at": "2025-01-13T06:40:23+00:00",
        "closed_at": null,
        "comments_count": [
            "github-actions[bot]"
        ],
        "labels": []
    }
]