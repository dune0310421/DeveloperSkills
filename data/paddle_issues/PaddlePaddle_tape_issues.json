[
    {
        "repo": "PaddlePaddle/tape",
        "number": 30,
        "title": "Fill API",
        "body": "Do not use `fill(variable)`, because\r\n\r\n```c++\r\n...\r\nb = op(a);\r\nfill(b)\r\n...\r\n```\r\n\r\nIn the backward, `b@grad` should NO be assigned to `a@grad`, since `b` has been overwritten by `fill` hence is independent of `a`. However, the current implementation of tape would `a@grad = op_grad(b@grad)`\r\n\r\nSolution: fill shouldn't take input, use `b = fill();` instead.",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "kexinzhao",
        "created_at": "2018-06-23T07:36:27+00:00",
        "updated_at": "2018-06-26T22:19:48+00:00",
        "closed_at": "2018-06-26T22:19:48+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 23,
        "title": "Operator is stateless, so is non-parametric layer",
        "body": "A paddle fluid operator is stateless. It only performs computation, doesn't store any intermediate value in its instance. Consider `c = mul(a, b)` and `e = mul(c, d)`, the first `mul` and second `mul` can be the same operator instance.\r\n\r\nWe can think of a neural net as a stack of layers. A layer may or may not contain parameters.\r\n\r\nA non-parametric layer e.g. `softmax`, `activation`, `fill_constant`, is also stateless. Hence a non-parametric layer could be written as a c++ function. On the other hand, a parametric layer, e.g. `fc`, `conv`, should be instantiated explicitly. Because the layer owns its parameters, and therefore stateful.",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-21T19:09:00+00:00",
        "updated_at": "2018-06-27T21:51:14+00:00",
        "closed_at": "2018-06-27T21:51:14+00:00",
        "comments_count": [],
        "labels": [
            "design"
        ]
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 11,
        "title": "Implementing var->value() in tape",
        "body": "In Tape, we want to provide the lazy evaluation feature, so that we can use the following code to compute the value of a variable:\r\n\r\n```C++\r\nauto loss = softmax(linear2(linear1(input)), label); // compile time InferShape & InferVarType\r\nLOG(INFO) << loss.value(); // Run forward up to loss\r\n```\r\n\r\nWhen `value()` is called upon a variable, the variable must know the location of the operator handle in the global tape so that tape run from the current op position up to the location of the target op handle. \r\n\r\nThere are two ways of adding the needed info in variable class for lazy exe:\r\n\r\n1. Add a `int op_position_` field\r\n\r\n  - Pros: tape is implemented as a vector of op handles and has a built-in current_position field. A `op_position_` field in variable is easy to implement and understand by comparing it with current_location and overall size of the tape.\r\n\r\n  - Cons: `OpHandle` does not store its position in the tape. If we later want to do kernel fusion or something similar to change the `vector<ophandle>` in tape, the position of the ops needed to updated for variables generated by many ops.\r\n\r\n2. Add a `weak_ptr<OpHandle> op_` field. (we don't consider shared_ptr here because there maybe cyclic referencing issue between OpHandle and Variable)\r\n  - Pros:  If there is change in `vector<ophandle>` in tape,  we mostly don't need to change Variable.\r\n  \r\n  - Cons: \r\n    - weak_ptr<OpHandle> may occupy more memory than `size_t`.\r\n    - Hard to implement and error prone: we need to use `while` to compare the current OPHandle and the target OpHandle and run it until we hit the target position, but what if the variable's correponding OP is before the current location or there is no such OpHandle in the current tape.\r\n\r\nThanks @tonyyang-svail for the suggestion on simply run the tape forward method when calling the value() function in the Variable. It is simple, straightforward and thus preferred.",
        "state": "closed",
        "user": "kexinzhao",
        "closed_by": "kexinzhao",
        "created_at": "2018-06-19T03:10:11+00:00",
        "updated_at": "2018-06-19T21:23:47+00:00",
        "closed_at": "2018-06-19T21:23:47+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 27,
        "title": "Operator is stateless, But OperatorHandle on the tape can contain state",
        "body": "@kexinzhao one example is `dropout`.\r\n\r\nIn training, `dropout` has an intermediate output called `mask`. It is used at backward `X@grad = mask * Y@grad`.",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-22T23:41:48+00:00",
        "updated_at": "2018-06-27T21:51:07+00:00",
        "closed_at": "2018-06-27T21:51:07+00:00",
        "comments_count": [],
        "labels": [
            "design"
        ]
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 21,
        "title": "Delete VarDesc in Variable",
        "body": "https://github.com/PaddlePaddle/tape/blob/develop/src/variable.h#L84\r\n\r\n`Tensor` and `SelectedRow` already have `dim` and `dtype`, doesn't need duplicated information `VarDesc`.\r\n\r\nThe problem is:\r\n1. Without `OpDesc`, InferShape only exists in `OperatorKernel`. So maybe only `OperatorWithKernel` can be added to tape.\r\n1. `InferVarType` only exists in the `Desc` level. There is no runtime `InferVarType`.",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-20T21:45:00+00:00",
        "updated_at": "2018-06-22T22:44:40+00:00",
        "closed_at": "2018-06-22T22:44:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 10,
        "title": "Tape MileStone",
        "body": "- [ ] 1. Vision Net benchmark\r\n   - [x] 1. Move to a new repo, PaddlePaddle/Tape (tony)\r\n   - [x] 2. Implement loss->value(), very useful for debugging (kexin) #11 \r\n   - [x] 3. Expose Operators\r\n      - [x] 1. Xavier initializer (tony)\r\n      - [x] 2. Reader (kexin)\r\n      - [x] 3. softmax, mul, conv2d (tony)\r\n      - [x] 4. dropout, pool2d, batch_norm (kexin)\r\n      - [x] 5. relu, use something like F.relu (tony)\r\n      - [x] 6. enable default attribute (kexin)\r\n   - [x] 4. Test Backward (3 days, tony) #38\r\n      - [x] 1. Write a graph drawer to print GraphViz #39\r\n   - [x] 5. Test Adam optimizer (2 days, kexin) #36 \r\n   - [x] 6. Save/Load Model ( 1 day) #54\r\n      - [x] Add `Parameter` derived from `Variable` #44 \r\n      - [x] Add `ParameterCollection` #44\r\n      - [x] Expose Save and Load #47\r\n   - [x] 7. Inference (1 day, kexin) #40 \r\n      - [x] 1. Accuracy\r\n   - [x] 8. GPU (2 day) #45\r\n      - [x] Code clean up `CPUPlace`\r\n      - [x] Reader\r\n      - [x] `<< Variable`\r\n   - [ ] 9. Benchmark benchmark/vgg\r\n      - [x] DoubleBuffer\r\n      - [ ] Without ScopeWrapper, direct InferShape and Operator::Run\r\n- [ ] 2. Control Flow demo\r\n   - [ ] 1. if/else\r\n   - [ ] 2. while/for\r\n   - [ ] 3. Nested while/for\r\n      - [ ] 1. nested \r\n   - [ ] 4. GAN demo \r\n   - [ ] 5. Imitation demo\r\n- [ ] 3. Mobile Deployment\r\n   - [ ] 1. Support training\r\n   - [ ] 2. Prune binary file",
        "state": "open",
        "user": "tonyyang-svail",
        "closed_by": null,
        "created_at": "2018-06-19T03:09:11+00:00",
        "updated_at": "2018-07-19T18:26:10+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 12,
        "title": "Depend on Paddle via ExternalProject_Add",
        "body": "The current solution uses git submodule\r\n\r\nhttps://github.com/PaddlePaddle/tape/blob/bb1fc27436c2d70c3d87a34cae1711a22440ab78/.gitmodules#L1-L3\r\n\r\nThis causes that Tape relies on Paddle's CMake build rules.  However, there are many hacks in Paddle's build rules that lead to confusing build rules of Tape.  For example:\r\n\r\n- https://github.com/PaddlePaddle/tape/pull/8#discussion_r196283014\r\n\r\nand\r\n\r\nhttps://github.com/PaddlePaddle/tape/blob/bb1fc27436c2d70c3d87a34cae1711a22440ab78/src/CMakeLists.txt#L23-L41",
        "state": "open",
        "user": "wangkuiyi",
        "closed_by": null,
        "created_at": "2018-06-19T03:38:46+00:00",
        "updated_at": "2018-06-19T03:47:56+00:00",
        "closed_at": null,
        "comments_count": [
            "tonyyang-svail"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 24,
        "title": "Change ReadNext API",
        "body": "Change to\r\n```c++\r\nstd::vector<VariableHandle> ReadNext(VariableHandle reader) {\r\n  PADDLE_ENFORCE_EQ(reader->Desc().GetType(),\r\n                    paddle::framework::proto::VarType::READER);\r\n\r\n  std::vector<VariableHandle> rval;\r\n\r\n  paddle::framework::LoDTensorArray data_holder;\r\n  reader->GetMutable<paddle::framework::ReaderHolder>()->ReadNext(&data_holder);\r\n  PADDLE_ENFORCE(!data_holder.empty(), \"There is no next data.\");\r\n  for (size_t i = 0; i < data_holder.size(); ++i) {\r\n    rval.emplace_back(new Variable(\"data\" + std::to_string(i)));\r\n    auto* lod_tensor = rval.back()->GetMutable<framework::LoDTensor>();\r\n    lod_tensor->ShareDataWith(data_holder[i]);\r\n    lod_tensor->set_lod(data_holder[i].lod());\r\n  }\r\n\r\n  return rval;\r\n}\r\n```",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-22T18:46:44+00:00",
        "updated_at": "2018-06-25T18:04:28+00:00",
        "closed_at": "2018-06-25T18:04:28+00:00",
        "comments_count": [
            "tonyyang-svail"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 29,
        "title": "Print Variable after tape has been backwarded",
        "body": "There is a common user scenario that user may want to print the value of a grad var after running backward pass. \r\n\r\n``` cpp\r\ntape.Backward(loss)\r\nLOG(INFO) << input->Grad().value();\r\n```\r\n\r\nHowever, since Value() will invoke `tape.Forward()` and Foward() will throw when the tape has been forwarded, so the above code will throw. \r\n",
        "state": "closed",
        "user": "kexinzhao",
        "closed_by": "kexinzhao",
        "created_at": "2018-06-23T00:25:48+00:00",
        "updated_at": "2018-06-26T00:11:13+00:00",
        "closed_at": "2018-06-26T00:11:13+00:00",
        "comments_count": [
            "tonyyang-svail"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 49,
        "title": "Save Adam's beta power when saving models",
        "body": "The following should be parameters, so that they will be saved\r\n\r\nhttps://github.com/PaddlePaddle/tape/blob/fa0919e33936a3a8def6a0e687f1bca38f7ca1da/src/function.h#L217-L218\r\n\r\n",
        "state": "open",
        "user": "tonyyang-svail",
        "closed_by": null,
        "created_at": "2018-07-03T02:34:45+00:00",
        "updated_at": "2018-07-03T02:42:03+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 37,
        "title": "Adding Parameter as a derived class of Variable",
        "body": "`Parameter` is a `Variable`. So `Parameter` should be a derived class of Variable.\r\n\r\nThere are some advantages of adding a `Parameter` class:\r\n\r\n1. We can book-keeping a set of all parameters. Then a user can do:\r\n```c++\r\nfor (auto w : AllParameters()) {\r\n    sgd.Update(w);\r\n}\r\n```\r\n2. In multi-GPU or distributed training, only a parameter's gradient needs to be allreduced. A tape needs a way to tell `Variable` and `Parameter` apart.\r\n\r\nIn Tape, we use smart pointers to pass around variables. Thanks to the [covariance](http://cpptruths.blogspot.com/2015/11/covariance-and-contravariance-in-c.html?m=1) support of smart pointers. We are able to put `std::shared_ptr<Parameter>` to `std::shared_ptr<Variable>` API.\r\n\r\n```c++\r\nclass Variable {\r\n public:\r\n  virtual void bar() {\r\n    LOG(INFO) << \"Variable\";\r\n  }\r\n};\r\n\r\nclass Parameter : public Variable {\r\n public:\r\n  virtual void bar() override {\r\n    LOG(INFO) << \"Parameter\";\r\n  }\r\n};\r\n\r\nvoid foo(std::shared_ptr<Variable> ptr) {\r\n  ptr->bar();\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  std::shared_ptr<Parameter> d_ptr(new Parameter);\r\n  foo(d_ptr); // prints \"Parameter\"\r\n  return 0;\r\n}\r\n```",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "kexinzhao",
        "created_at": "2018-06-26T23:45:38+00:00",
        "updated_at": "2018-06-29T16:50:34+00:00",
        "closed_at": "2018-06-29T16:50:34+00:00",
        "comments_count": [],
        "labels": [
            "design"
        ]
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 48,
        "title": "merge backward and optimizer",
        "body": "```c++\r\nvoid BackwardAndUpdate(VariableHandle loss, Optimizer& opt) {\r\n\tget_global_tape().Backward(loss);\r\n\topt.Step();\r\n}\r\n\r\nclass Optimizer {\r\npublic:\r\n\tvirtual void Step() = 0;\r\n\tvirtual void Update(ParameterHandle param) = 0;\r\n};\r\n\r\nclass SGD : public Optimizer {\r\npublic:\r\n\tvirtual void Step() override {\r\n\t\tfor (auto& w : AllParameters()) {\r\n\t\t\tUpdate(w);\r\n\t\t}\r\n\t}\r\n};\r\n\r\nclass Adam : public Optimizer {\r\npublic:\r\n\tvirtual void Step() override {\r\n\t\tUpdateBeta();\r\n\t\tfor (auto& w : AllParameters()) {\r\n\t\t\tUpdate(w);\r\n\t\t}\r\n\t}\r\n};\r\n```",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "kexinzhao",
        "created_at": "2018-07-03T01:50:40+00:00",
        "updated_at": "2018-07-03T22:49:23+00:00",
        "closed_at": "2018-07-03T22:49:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 32,
        "title": "Enable OpMaker default attribute",
        "body": "To create an operator, we currently have to list all attribute. This is very painful.\r\n\r\nhttps://github.com/PaddlePaddle/tape/blob/b9370a0fb9096ef8cbd56b9da14ead0b5c346c4d/src/function.h#L175-L181\r\n\r\nWe have `SetDefault` in `OpMaker`. There should be a way to avoid this.",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-25T19:07:07+00:00",
        "updated_at": "2018-06-25T21:15:37+00:00",
        "closed_at": "2018-06-25T21:15:37+00:00",
        "comments_count": [
            "tonyyang-svail"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 46,
        "title": "save/load model",
        "body": "Model is code.\r\n```c++\r\nLinear linear0;  // contains W0, B1\r\nLinear linear1;  // contains W2, B3\r\n\r\n// saving tensors /tmp/W0.pd, /tmp/B1.pd, /tmp/W2.pd and /tmp/B3.pd\r\nGlobalParameterCollection().SaveParameters(\"/tmp/\"); \r\n\r\nParameterCollection PC;  // Another parameter collection\r\nPC.LoadFromFile(\"tmp\")  // Load all the tensor files to memory\r\n\r\nanother_linear0 = Linear(PC.LookUp(linear0.ParameterNames())); // deepcopy of linear0\r\nanother_linear1 = Linear(PC.LookUp(linear1.ParameterNames())); // deepcopy of linear1\r\n```",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "kexinzhao",
        "created_at": "2018-06-29T23:25:54+00:00",
        "updated_at": "2018-07-04T20:38:10+00:00",
        "closed_at": "2018-07-04T20:38:10+00:00",
        "comments_count": [],
        "labels": [
            "design"
        ]
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 56,
        "title": "the problem of MPI allreduce when the forward code involves control flow",
        "body": "The AllReduce call on each device requires special care.\r\n\r\n1. Gradient calculation might be out of order\r\n```c++\r\nif (cond) {\r\n    y = w2 * (w1 * x);  // backward: w2_grad generated first, then w1_grad\r\n} else {\r\n    y = w1 * (w2 * x);  // backward: w1_grad generated first, then w2_grad\r\n}\r\n```\r\n2. Gradient might be missing\r\n```c++\r\nif (cond) {\r\n    y = w2 * x;  // backward only generates w2_grad\r\n} else {\r\n    y = w1 * (w2 * x);  // backward generates w1_grad, w2_Grad\r\n}\r\n```\r\n\r\n",
        "state": "open",
        "user": "tonyyang-svail",
        "closed_by": null,
        "created_at": "2018-07-04T22:04:49+00:00",
        "updated_at": "2018-07-04T22:04:49+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 41,
        "title": "Use lambda to reuse forward code",
        "body": "While using tape, we can define the forward pass as a lambda function so that it could be used in both training and testing.\r\n```c++\r\nauto forward = [&](VariableHandle input) -> VariableHandle {\r\n    // a complicated forward function\r\n    return linear3(linear2(linear1(input)));\r\n};\r\n\r\n// training\r\n{\r\n    reset_global_tape();\r\n    auto pred = forward(training_data);\r\n    auto loss = cross_entropy(pred, training_label)\r\n    ...\r\n}\r\n\r\n// testing\r\n{\r\n    reset_global_tape();\r\n    auto pred = forward(testing_data);\r\n    auto loss = cross_entropy(pred, testing_label)\r\n    auto acc = accuracy(pred, testing_label)\r\n}\r\n```",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "tonyyang-svail",
        "created_at": "2018-06-28T17:51:59+00:00",
        "updated_at": "2018-07-02T20:21:53+00:00",
        "closed_at": "2018-07-02T20:21:53+00:00",
        "comments_count": [
            "tonyyang-svail"
        ],
        "labels": [
            "design"
        ]
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 58,
        "title": "Can we make Tape independent of protobuf?",
        "body": "",
        "state": "open",
        "user": "tonyyang-svail",
        "closed_by": null,
        "created_at": "2018-07-10T04:09:35+00:00",
        "updated_at": "2018-07-10T04:09:35+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 55,
        "title": "Benchmark on VGG",
        "body": "Goal: evaluate the cost of constructing a tape in every iteration.\r\n\r\nBefore benchmark, make sure the model can be trained to converge.\r\n\r\nMetrics:\r\n- Thoughput (pics/sec), under different batch sizes, different devices.\r\n    - average over 1000 iterations\r\n\r\n\r\nModel configuration [[fluid](https://github.com/PaddlePaddle/Paddle/blob/develop/benchmark/fluid/models/vgg.py), tape]\r\n",
        "state": "closed",
        "user": "tonyyang-svail",
        "closed_by": "kexinzhao",
        "created_at": "2018-07-04T21:33:39+00:00",
        "updated_at": "2018-07-18T20:02:18+00:00",
        "closed_at": "2018-07-18T20:02:18+00:00",
        "comments_count": [
            "kexinzhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 59,
        "title": "MKLDNN causing segment fault",
        "body": "The reason for segment fault is found.\r\n\r\nWhen we are using the following cmake flag:\r\n`-DWITH_GPU=ON -DWITH_TESTING=ON -DCUDA_ARCH_NAME=Auto`\r\n\r\nSegment fault will happen on server machine with Titan X GPU. \r\nHowever, everything is fine on the local develop machine with Titan Z.\r\n\r\ngdb debugging traces the error in shared_ptr or other random code when we modify the code, which does not make sense.\r\n\r\nBy comparing the different cmake flags on the local machine and the server machine, we found that segment fault is caused by including and building the MKLDNN library. \r\n\r\nThe CPU in the local machine does not support AVX2, so the USE_MKLDNN flag is off for local machine. In comparison, server machine CPU support AVX2, so USE_MKLDNN is on. The following cmake scripts are the reason for this to happen.\r\nhttps://github.com/kexinzhao/Paddle/blob/732390392df139bedb3cb80c4143a1cb4b4a4191/CMakeLists.txt#L125-L133\r\n\r\nBy specifically setting this flag to off on the server machine, the `test_cifar` runs without segmentation fault, which verified my suspicion\r\n\r\n",
        "state": "closed",
        "user": "kexinzhao",
        "closed_by": "wanglei828",
        "created_at": "2018-07-11T23:19:38+00:00",
        "updated_at": "2018-07-12T20:41:39+00:00",
        "closed_at": "2018-07-12T20:41:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/tape",
        "number": 60,
        "title": "Make sure that Tape and Paddle are compiled using the same optimization flag",
        "body": "The default cmake optimization is as follows:\r\n\r\nCMAKE_CXX_FLAGS is None\r\nCMAKE_CXX_FLAGS_DEBUG is -g\r\nCMAKE_CXX_FLAGS_RELEASE is -O3 -DNDEBUG\r\nCMAKE_CXX_FLAGS_RELWITHDEBINFO is -O2 -g -DNDEBUG\r\nCMAKE_CXX_FLAGS_MINSIZEREL is -Os -DNDEBUG\r\n\r\nIf we don't specify `-DCMAKE_BUILD_TYPE` in cmake:\r\nThe `CMAKE_CXX_FLAGS` is used, if we specify `cmake .. -DCMAKE_BUILD_TYPE=Debug`, the g++ flags will be CMAKE_CXX_FLAGS plus CMAKE_CXX_FLAGS_DEBUG, which will be `-g`.\r\n\r\nCurrently, tape/paddle/CMakeLists.txt set the build type to RELWITHDEBINFO, if we don't specify one in our cmake command. \r\nhttps://github.com/kexinzhao/Paddle/blob/732390392df139bedb3cb80c4143a1cb4b4a4191/CMakeLists.txt#L67-L72\r\n\r\nMoreover, tape/paddle/CMakeLists.txt also override the cmake default RELWITHDEBINFO flag to be as follows:\r\n`set(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3 -g -DNDEBUG\")`\r\n\r\nThis is a normal variable which is only effective inside tape/paddle/CMakeLists.txt but will not affect the parallel tape/src/CMakeLists.txt. \r\nSo the CMAKE_CXX_FLAGS_RELWITHDEBINFO in tape/src/CMakelists.txt is still the default `-O2 -g -DNDEBUG`.\r\n\r\nSo the src folder is compiled with `-O2` while paddle folder is compiled with `-O3` flag if we don't specify our CMake_Build_Type. (This is because CMAKE_BUILD_TYPE is set to be cache variable in tape/paddle/CMakeLists.txt which will also be effective in tape/src/CMakelists.txt).\r\n\r\nWe need to either specify `CMakeBuildType` to `release`, or change the `CMAKE_CXX_FLAGS_RELWITHDEBINFO` to be a cache variable to be also effective in src folder.\r\n\r\nBut refactoring cmake files is a must do in the near future.\r\n\r\n\r\n",
        "state": "closed",
        "user": "kexinzhao",
        "closed_by": "wanglei828",
        "created_at": "2018-07-12T00:11:27+00:00",
        "updated_at": "2018-07-12T20:41:39+00:00",
        "closed_at": "2018-07-12T20:41:39+00:00",
        "comments_count": [],
        "labels": []
    }
]