[
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 4
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 5,
        "title": "新模型/新硬件需求收集 (Requirement collection for new model/hardware)",
        "body": "如果你希望FastDeploy支持新的AI模型/新硬件，或与FastDeploy共建生态，欢迎随时联系我们 （If you want FastDeploy to support new AI models/new hardware, or build an ecosystem with FastDeploy, please feel free to contact us）\r\n\r\n1. 硬件型号 (hardware)：例如: 晶晨A311D (Example:Amlogic A311D)\r\n\r\n2. 系统 (OS)：例如：Linux (Example:Linux)\r\n\r\n3. 网络名称 (model name)：例如：PP-PicoDet (Example:PP-PicoDet)\r\n\r\n4. 模型链接（model link）： 例如：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet (Example:https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet)\r\n\r\n5. 详细描述（Detailed Description）：例如：希望将PP-PicoDet-M模型用到自己车辆检测场景中实现服务化部署，但是对算法前后处理不理解，部署困难。（Example: I hope to use the PP-PicoDet-M model in my own vehicle detection scene to achieve service deployment, but I do not understand the pre- and post-processing of the algorithm, and the deployment is difficult.）\r\n\r\n6. 联系方式（contact info）：leiqing_ucas@163.com",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "leiqing1",
        "created_at": "2022-06-30T07:19:28+00:00",
        "updated_at": "2022-06-30T08:10:34+00:00",
        "closed_at": "2022-06-30T08:10:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 9,
        "title": "边缘侧部署Serving",
        "body": "反馈一个bug,文档中写的是./easyedge_serving ./RES可以启动http服务\n![1addb70e0f0a8ba8.jpg](https://user-images.githubusercontent.com/61459740/176999462-bbe2a4d4-4ece-464e-b242-1e492bb73684.jpg)\n但实际需要在RES后面添加一个参数，任意一个数字\n![mmexport1656762023211.jpg](https://user-images.githubusercontent.com/61459740/176999506-d3cc8eff-e6b9-41e9-a9d5-e7a6ac13de68.jpg)\n![mmexport1656762018367.jpg](https://user-images.githubusercontent.com/61459740/176999511-eaeeb8c7-dc9b-4739-8351-9f7f53b39b64.jpg)\n这里模型采用的是PP-YOLO的物体检测\n![mmexport1656762015495.jpg](https://user-images.githubusercontent.com/61459740/176999522-58385c24-3479-4433-a423-7b908036e908.jpg)\n",
        "state": "closed",
        "user": "Sqhttwl",
        "closed_by": "jiangjiajun",
        "created_at": "2022-07-02T11:49:40+00:00",
        "updated_at": "2022-11-15T06:23:21+00:00",
        "closed_at": "2022-11-15T06:23:21+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": [
            "Documentation"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 65,
        "title": "Jetson部署OCR",
        "body": "请问有在Jetson上部署OCR的Demo嘛",
        "state": "closed",
        "user": "ccLemonTree",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-03T06:33:56+00:00",
        "updated_at": "2024-02-06T04:26:14+00:00",
        "closed_at": "2024-02-06T04:26:14+00:00",
        "comments_count": [
            "leiqing1",
            "ccLemonTree",
            "AI-Mart",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 7,
        "title": "Requirement collection for new model/hardware",
        "body": "If you want FastDeploy to support new AI models/new hardware, or build an ecosystem with FastDeploy, please feel free to contact us：\r\n1. Hardware: Example:Amlogic A311D\r\n2. OS: Example:Linux\r\n3. Model name: Example:PP-PicoDet\r\n4. Model link: Example:https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet\r\n5. Detailed Description: Example: I hope to use the PP-PicoDet-M model in my own vehicle detection scene to achieve service deployment, but I do not understand the pre- and post-processing of the algorithm, and the deployment is difficult.\r\n6. Contact info:leiqing@baidu.com",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "jiangjiajun",
        "created_at": "2022-06-30T08:14:52+00:00",
        "updated_at": "2024-02-06T04:18:55+00:00",
        "closed_at": "2024-02-06T04:18:55+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 57,
        "title": "cmake失败",
        "body": "模型如下\r\n![image](https://user-images.githubusercontent.com/54087448/181667247-54b036fd-72c3-4a5e-b0d3-05ee7480b85a.png)\r\n\r\ncmake报错如下\r\n![image](https://user-images.githubusercontent.com/54087448/181667209-1ad56baf-2156-4076-ba3f-85bd931de755.png)\r\n",
        "state": "closed",
        "user": "BuLuoPiaoYu",
        "closed_by": "jiangjiajun",
        "created_at": "2022-07-29T02:03:06+00:00",
        "updated_at": "2024-02-06T04:26:15+00:00",
        "closed_at": "2024-02-06T04:26:15+00:00",
        "comments_count": [
            "leiqing1",
            "BuLuoPiaoYu",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 24,
        "title": "USA Team seeking english support",
        "body": "Hello my friends -- we are a NYC based team interested to explore using the PaddlePaddle framework. Do you know any english-speaking members of the PaddlePaddle community that we may connect with?\r\n\r\nThank you\r\nZev ",
        "state": "closed",
        "user": "zevr10",
        "closed_by": "jiangjiajun",
        "created_at": "2022-07-18T16:14:03+00:00",
        "updated_at": "2024-02-06T04:26:16+00:00",
        "closed_at": "2024-02-06T04:26:15+00:00",
        "comments_count": [
            "leiqing1",
            "leiqing1",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2,
        "title": "和paddlex有什么区别？",
        "body": "大概看了一下，功能各方面好像和paddlex有很多重复，而且貌似paddle系列的很多东西都有类似重复的东西",
        "state": "closed",
        "user": "gxwyxajh88",
        "closed_by": "ZeyuChen",
        "created_at": "2022-06-28T01:41:22+00:00",
        "updated_at": "2024-10-10T02:54:19+00:00",
        "closed_at": "2022-07-02T15:48:47+00:00",
        "comments_count": [
            "jiangjiajun",
            "wanghaisheng",
            "ZeyuChen",
            "Pointvar"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 6,
        "title": "FastDeploy模型、硬件支持计划",
        "body": "## 模型 （CV、NLP、Speech、Cross-modal）\r\n- [ ] PP-Tracking(Doing)\r\n- [ ] Robust Video Matting(Doing)\r\n\r\n## 平台& 硬件 （云、边、端）\r\n- [ ] Android ARM CPU\r\n- - [ ] Picodet(Doing)\r\n- - [ ] PaddleClas(Doing)\r\n- [ ] RKNN2\r\n- - [ ] YOLOv5(Doing)\r\n\r\n\r\n如果你希望FastDeploy支持新的AI模型/新硬件，或与FastDeploy共建生态，欢迎提供如下信息我们：\r\n1. 硬件型号：例如: 晶晨A311D \r\n2. 操作系统：例如：Linux \r\n3. 网络名称：例如：PP-PicoDet \r\n4. 模型链接：例如：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet \r\n5. 详细描述：例如：希望将PP-PicoDet-M模型用到自己车辆检测场景中实现服务化部署，但是对算法前后处理不理解，部署困难。\r\n6. 联系方式：leiqing@baidu.com",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-06-30T08:13:32+00:00",
        "updated_at": "2025-02-25T06:45:08+00:00",
        "closed_at": "2025-02-25T06:45:08+00:00",
        "comments_count": [
            "datalee",
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun",
            "dachengai",
            "jiangjiajun",
            "datalee",
            "KRuok",
            "jiangjiajun",
            "wzy123456wzy",
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun",
            "acgcc12",
            "jiangjiajun",
            "monkeycc",
            "dwdcth",
            "trefoil0219",
            "liutingxi",
            "jiangjiajun",
            "wang-xinyu",
            "trefoil0219",
            "wang-xinyu",
            "qiulongquan",
            "trefoil0219",
            "Amy234543",
            "sunzhaoyang",
            "apexg",
            "leiqing1",
            "cumthxy",
            "bigbeef",
            "WilliamQf-AI",
            "qiulongquan",
            "121786404",
            "SunYanCN",
            "ijiami-01",
            "monkeycc",
            "nefusmzj",
            "jiangjiajun",
            "lrp123456",
            "nine-city",
            "monkeycc",
            "weida008",
            "lqsweet"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 78,
        "title": "Just for image testing, ignore this issue",
        "body": null,
        "state": "closed",
        "user": "jiangjiajun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-08T09:14:05+00:00",
        "updated_at": "2025-06-29T18:07:29+00:00",
        "closed_at": "2022-08-08T09:14:08+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun",
            "Jiang-Jia-Jun",
            "Jiang-Jia-Jun",
            "Jiang-Jia-Jun",
            "nepeplwu",
            "nepeplwu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 135,
        "title": "这个不会用啊",
        "body": "不会用啊  还是和lite.ai.toolkit一样要先自己编译吗?\r\n话说好不容易学会lite.ai.toolkit的编译  难受",
        "state": "closed",
        "user": "l7518597",
        "closed_by": "l7518597",
        "created_at": "2022-08-19T09:14:09+00:00",
        "updated_at": "2022-08-19T10:19:13+00:00",
        "closed_at": "2022-08-19T09:22:54+00:00",
        "comments_count": [
            "DefTruth",
            "l7518597",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 143,
        "title": "抠图的readme是不是写错？",
        "body": "\r\n你好，\r\n      我这边测试matting抠图cpu方案时，按照readme.txt 操作，明明下载的是gpu的依赖包，为啥安装下时候指定cpu的路径？\r\n![企业微信截图_54c40950-aeeb-402d-918c-97c323686c9f](https://user-images.githubusercontent.com/66000167/185962382-164550e6-56fd-4e87-a90d-9cc51be63c69.png)\r\n",
        "state": "closed",
        "user": "wsx958191",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-22T15:42:33+00:00",
        "updated_at": "2024-02-06T04:26:13+00:00",
        "closed_at": "2024-02-06T04:26:13+00:00",
        "comments_count": [
            "jiangjiajun",
            "wsx958191",
            "jiangjiajun",
            "wsx958191",
            "DefTruth",
            "wsx958191",
            "DefTruth",
            "wsx958191",
            "jiangjiajun"
        ],
        "labels": [
            "Documentation"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 138,
        "title": "【Supported】Yolov5 classification",
        "body": "Hi,\r\n\r\nIs it possible to support https://github.com/ultralytics/yolov5/pull/8956 models?\r\n\r\nThanks",
        "state": "closed",
        "user": "sctrueew",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-20T10:28:28+00:00",
        "updated_at": "2022-11-25T11:32:23+00:00",
        "closed_at": "2022-11-25T11:32:23+00:00",
        "comments_count": [
            "DefTruth",
            "sctrueew",
            "DefTruth",
            "sctrueew",
            "DefTruth",
            "sctrueew",
            "DefTruth",
            "sctrueew"
        ],
        "labels": [
            "Enhancement",
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 150,
        "title": "不同平台编译问题咨询",
        "body": "你好，\r\n      我这边在搭建环境过程中，看见好多third_party都是需要直接下载使用的，不知道是否可行在不同平台自己拿源码编译，有没有变异文档类似资料？\r\n",
        "state": "closed",
        "user": "wsx958191",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-24T03:27:21+00:00",
        "updated_at": "2024-02-06T04:26:11+00:00",
        "closed_at": "2024-02-06T04:26:11+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "wsx958191",
            "DefTruth",
            "DefTruth",
            "wsx958191",
            "DefTruth",
            "wsx958191",
            "DefTruth",
            "wsx958191",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Windows x64",
            "Linux x86"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 147,
        "title": "怎么cmake？编译文档",
        "body": "怎么cmake？可以像lite.ai.tookit一样出一套流程吗",
        "state": "closed",
        "user": "l7518597",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-23T08:42:18+00:00",
        "updated_at": "2024-02-06T04:26:12+00:00",
        "closed_at": "2024-02-06T04:26:12+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "l7518597",
            "jiangjiajun",
            "l7518597",
            "l7518597",
            "l7518597",
            "jiangjiajun",
            "l7518597",
            "DefTruth",
            "l7518597",
            "l7518597",
            "DefTruth",
            "l7518597",
            "DefTruth",
            "DefTruth",
            "l7518597",
            "DefTruth",
            "DefTruth",
            "leiqing1",
            "l7518597",
            "DefTruth",
            "l7518597",
            "l7518597",
            "jiangjiajun"
        ],
        "labels": [
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 165,
        "title": "Vs2022 社区版Matting 测试失败",
        "body": "你好：\r\n     我们这边在2022 win10平台测试matting时候，发现在模型加载的时候crash了，dll依赖都是我们本地编译的的，没有使用预编译的库，在linux平台测试正常，麻烦后续关注下？谢谢\r\n\r\n<img width=\"722\" alt=\"企业微信截图_16615875006549\" src=\"https://user-images.githubusercontent.com/66000167/187021359-78335fb7-a83b-48b8-a482-b746d3ba8d6c.png\">\r\n<img width=\"668\" alt=\"企业微信截图_16615877091412\" src=\"https://user-images.githubusercontent.com/66000167/187021372-7773c054-4f43-4189-b4a0-f1c4aeb15e54.png\">\r\n<img width=\"515\" alt=\"企业微信截图_16615877499198\" src=\"https://user-images.githubusercontent.com/66000167/187021386-787cb0fc-ff3d-4690-ac39-8674957faa5f.png\">\r\n<img width=\"515\" alt=\"企业微信截图_16615877499198\" src=\"https://user-images.githubusercontent.com/66000167/187021388-70da663c-0b60-4d4b-abed-0724998b9f15.png\">\r\n\r\n\r\n",
        "state": "closed",
        "user": "wsx958191",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-27T08:09:26+00:00",
        "updated_at": "2024-02-06T04:26:10+00:00",
        "closed_at": "2024-02-06T04:26:10+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "wsx958191",
            "jiangjiajun"
        ],
        "labels": [
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 154,
        "title": "PaddleX导出的模型,怎么办",
        "body": "PaddleX导出的模型,怎么办\r\n\r\n配置文件 只有\r\nmodel.yml\r\npipeline.yml",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-08-24T20:59:54+00:00",
        "updated_at": "2022-09-16T01:53:06+00:00",
        "closed_at": "2022-09-16T01:53:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 168,
        "title": "多进程任务报错",
        "body": "fastdeploy人脸识别python部署，采用多进程进行人脸特征提取及人脸比对，实现通过主进程开启摄像头，获取帧，裁剪人脸roi存到Queue中，识别进程从Queue中获取人脸数据最后报错。\r\n核心代码如下：\r\n<img width=\"624\" alt=\"image\" src=\"https://user-images.githubusercontent.com/44053467/187147584-67e0fd59-ed80-4a98-87f2-f978dcbe4055.png\">\r\n报错代码：\r\n[ERROR] csrc/fastdeploy/pybind/main.cc(94)::NumpyDataTypeToOpenCvType\tNumpyDataTypeToOpenCvType() only support int32/int8/uint8/float32 now.\r\n\r\n\r\nps：目前fastdeploy python不熟不支持多线程么，我尝试多线程处理，速度并没有提升。是不是pybind进行耗时处理的方法绑定中没释放gil？\r\n",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-29T07:31:42+00:00",
        "updated_at": "2024-02-06T04:26:09+00:00",
        "closed_at": "2024-02-06T04:26:09+00:00",
        "comments_count": [
            "DefTruth",
            "ChaoII",
            "jiangjiajun",
            "ChaoII",
            "DefTruth",
            "ChaoII",
            "DefTruth",
            "ChaoII",
            "DefTruth",
            "ChaoII",
            "AI-Mart",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 180,
        "title": "python 3.10 for windows 64bit的版本的whl安装包,谢谢!",
        "body": "不知道合适方便编译一个 python 3.10 for windows 64bit的版本的whl安装包,谢谢!",
        "state": "closed",
        "user": "blueicesir",
        "closed_by": "jiangjiajun",
        "created_at": "2022-08-31T03:09:52+00:00",
        "updated_at": "2024-02-06T04:26:08+00:00",
        "closed_at": "2024-02-06T04:26:08+00:00",
        "comments_count": [
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Windows x64",
            "Python:3.10"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 199,
        "title": "ocr的demo编译后代码报错",
        "body": "![image](https://user-images.githubusercontent.com/39193398/188798450-8a382d68-459d-40ab-8164-1098a86636c3.png)\r\nocr的PPOCRSystemv3编译完成后打开出现上述错误,其他demo比如classification可以正常Release生成infer_demo.exe,是否完成ocr的支持?",
        "state": "closed",
        "user": "kc-w",
        "closed_by": "kc-w",
        "created_at": "2022-09-07T05:54:01+00:00",
        "updated_at": "2022-09-07T09:14:28+00:00",
        "closed_at": "2022-09-07T09:14:28+00:00",
        "comments_count": [
            "jiangjiajun",
            "kc-w",
            "kc-w",
            "jiangjiajun",
            "kc-w"
        ],
        "labels": [
            "OCR",
            "C++"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 203,
        "title": "The FastDeploy didn't compile with GPU, will force to use CPU.",
        "body": "[WARNING] csrc/fastdeploy/fastdeploy_runtime.cc(141)::fastdeploy::RuntimeOption::UseGpu The FastDeploy didn't compile with GPU, will force to use CPU.\r\n",
        "state": "closed",
        "user": "NicoSaronAI",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-08T07:15:54+00:00",
        "updated_at": "2024-02-06T04:26:07+00:00",
        "closed_at": "2024-02-06T04:26:07+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 212,
        "title": "请问pr文档如何避免ci",
        "body": null,
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "ChaoII",
        "created_at": "2022-09-09T00:55:54+00:00",
        "updated_at": "2022-09-09T07:25:10+00:00",
        "closed_at": "2022-09-09T07:25:10+00:00",
        "comments_count": [
            "jiangjiajun",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 206,
        "title": "undefined reference to `cv::imwrite",
        "body": "尝试跑C++ 推理示例 报错\r\ngcc demo.cpp \\\r\n-I./FastDeploy/build/fastdeploy-0.0.3/include/ \\\r\n-I./FastDeploy/build/fastdeploy-0.0.3/third_libs/install/opencv/include/ \\\r\n-I./FastDeploy/build/third_libs/install/opencv/include/ \\\r\n-lfastdeploy \\\r\n-lopencv_core \\\r\n-lstdc++ \\\r\n-ldl \\\r\n-o demo.so \\\r\n\r\n/usr/bin/ld: /tmp/ccxAZsgp.o: in function `main':\r\ndemo.cpp:(.text+0x1b1): undefined reference to `cv::imread(cv::String const&, int)'\r\n/usr/bin/ld: demo.cpp:(.text+0x2a0): undefined reference to `cv::imwrite(cv::String const&, cv::_InputArray const&, std::vector<int, std::allocator<int> > const&)'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [Makefile:2: demo.so] Error 1",
        "state": "closed",
        "user": "xwang365",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-08T08:40:41+00:00",
        "updated_at": "2024-02-06T04:26:06+00:00",
        "closed_at": "2024-02-06T04:26:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "xwang365",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 213,
        "title": "能否提供C#的DEMO和教程",
        "body": "能否提供C#的DEMO和教程",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-09-09T07:24:27+00:00",
        "updated_at": "2022-09-15T08:23:43+00:00",
        "closed_at": "2022-09-15T08:23:43+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 215,
        "title": "视觉模型部署没有实例分割吗",
        "body": "视觉模型部署没有 实例分割 吗",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-09T07:36:14+00:00",
        "updated_at": "2024-02-06T04:26:05+00:00",
        "closed_at": "2024-02-06T04:26:05+00:00",
        "comments_count": [
            "DefTruth",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 216,
        "title": "facedet和faceid咨询",
        "body": "1、能否检测人脸的姿态角\r\n2、能否做一个python和c++的检测和识别串起来的案例，比如输入一张图片，图片里面有多个人脸，能分别输出每个人脸的的坐标和每个人脸的向量，做一个整体的案例最大化的压缩预测的时间",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-09T10:25:48+00:00",
        "updated_at": "2024-02-06T04:26:04+00:00",
        "closed_at": "2024-02-06T04:26:04+00:00",
        "comments_count": [
            "DefTruth",
            "AI-Mart",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 222,
        "title": "在VS中使用C++接口报错",
        "body": "**新手求助**\r\n配置：win+VS 2019\r\n按照教程，C++SDK成功运行后想通过C++接口来使用\r\n(SDK参照Windows C++SDK教程实现)\r\n使用教程中的代码(在Debug模式下)：\r\n![image](https://user-images.githubusercontent.com/111327813/190044725-8182cb9c-ff48-404a-8a0a-05884a590c4a.png)\r\n\r\n新建的项目没有配置目录，配置信息如下：\r\nVC++包含目录\r\n![image](https://user-images.githubusercontent.com/111327813/190044908-ec908580-9f2e-4fd3-ac8e-073459808f9b.png)\r\n\r\nVC++库目录\r\n![image](https://user-images.githubusercontent.com/111327813/190045017-12605909-5d1c-412b-9d33-403a6bcd074d.png)\r\n\r\nC/C++常规、附加包含目录\r\n![image](https://user-images.githubusercontent.com/111327813/190045097-c91a9c76-69f8-454c-8f47-ac9b4032bf2f.png)\r\n\r\n配置可以找到fastdeploy、opencv等源文件，运行后报错\r\n**错误\tLNK1104\t无法打开文件“C:\\Users\\lihao.qin\\paddle\\FastDeploy\\build\\fastdeploy-win-x64-gpu-0.2.0\\lib.obj”**\r\n\r\n![image](https://user-images.githubusercontent.com/111327813/190045336-ef5ea553-6dda-411a-b587-45af0140531e.png)\r\n\r\n查看预编译库中没有错误信息中提到的文件，请问该如何解决这个问题？谢谢！\r\n\r\n=================================================================================\r\n更换为Release模式，出现新的报错信息。\r\n![image](https://user-images.githubusercontent.com/111327813/190055822-d7933dd7-2adc-43a1-9b58-d21826632746.png)\r\n",
        "state": "closed",
        "user": "qinlihaoWork",
        "closed_by": "qinlihaoWork",
        "created_at": "2022-09-14T02:35:21+00:00",
        "updated_at": "2022-09-22T09:15:52+00:00",
        "closed_at": "2022-09-22T09:15:52+00:00",
        "comments_count": [
            "DefTruth",
            "qinlihaoWork"
        ],
        "labels": [
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 228,
        "title": "C++ SDK在linux服务端部署，支持服务化部署吗？",
        "body": "C++ SDK在linux服务端部署，支持服务化部署吗，服务端用C++服务化部署，客户端用python写，服务端和客户端通过rpc通信，这样能否支持？",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-14T13:15:34+00:00",
        "updated_at": "2024-02-06T04:26:03+00:00",
        "closed_at": "2024-02-06T04:26:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "AI-Mart",
            "AI-Mart",
            "jiangjiajun",
            "heliqi",
            "heliqi",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 229,
        "title": "Compile with OpenVINO",
        "body": "I am trying to run the \\examples\\runtime\\python\\infer_onnx_openvino.py on Windows 10\r\n\r\nAnd I get the folowwing error:\r\n[ERROR] csrc/fastdeploy/fastdeploy_runtime.cc(193)::fastdeploy::RuntimeOption::UseOpenVINOBackend       The FastDeploy didn't compile with OpenVINO.\r\n\r\nHow do I compile with OpenVINO backend?",
        "state": "closed",
        "user": "n3ry7",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-14T14:19:14+00:00",
        "updated_at": "2024-02-06T04:26:02+00:00",
        "closed_at": "2024-02-06T04:26:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "n3ry7",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 230,
        "title": "Python如何解决模型加密问题",
        "body": "Python如何解决模型加密问题\r\n\r\n训练出来的模型 不想给别人用\r\n\r\npython可以加密成pyd文件  或者用nuitka打包编译\r\n\r\n但是模型怎么办",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-14T15:20:03+00:00",
        "updated_at": "2024-02-06T04:26:01+00:00",
        "closed_at": "2024-02-06T04:26:01+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 231,
        "title": "有支持C#部署的打算吗",
        "body": "有支持C#部署的打算吗",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-09-14T23:14:25+00:00",
        "updated_at": "2024-02-13T06:40:01+00:00",
        "closed_at": "2024-02-13T06:40:01+00:00",
        "comments_count": [
            "ZeyuChen",
            "monkeycc",
            "1316540491",
            "dogpandacat",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 237,
        "title": "让用户自己选择模型，vision应该一怎么判断",
        "body": "让用户自己选择模型的文件夹\r\n\r\n那么我应该怎么判断模型种类\r\n\r\n然后判断 逻辑处理 对应的\r\n```\r\nvision.classification.PaddleClasModel(model_file, params_file, config_file,)\r\nvision.detection.YOLOv3(model_file, params_file, config_file,)\r\nvision.detection.PPYOLO(model_file, params_file, config_file)\r\nvision.detection.FasterRCNN(model_file, params_file, config_file)\r\nvision.detection.MaskRCNN(model_file, params_file, config_file)\r\n```\r\n\r\n是否增加一个判断模型种类的函数",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-09-15T08:13:36+00:00",
        "updated_at": "2022-09-16T01:53:23+00:00",
        "closed_at": "2022-09-16T01:53:23+00:00",
        "comments_count": [
            "ChaoII",
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 235,
        "title": "GPU编译报错 subprocess.CalledProcessError: Command '['D:\\\\Anaconda3\\\\Library\\\\bin\\\\cmake.exe', '--build', '.', '--config', 'Release', '--', '/maxcpucount:20']' returned non-zero exit status 1.",
        "body": "```\r\nset \"CUDA_DIRECTORY\"=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\"\r\nset WITH_GPU=ON\r\nset ENABLE_ORT_BACKEND=ON\r\nset ENABLE_TRT_BACKEND=OFF\r\nset ENABLE_PADDLE_BACKEND=ON\r\nset ENABLE_VISION=ON\r\nset ENABLE_VISION_VISUALIZE=ON\r\n\r\n(fastdeploy) PS D:\\FastDeploy\\python> python setup.py build\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\nCMake Warning (dev) at CMakeLists.txt:15 (PROJECT):\r\n  Policy CMP0048 is not set: project() command manages VERSION variables.\r\n  Run \"cmake --help-policy CMP0048\" for policy details.  Use the cmake_policy\r\n  command to set the policy and suppress this warning.\r\n\r\n  The following variable(s) would be set to empty:\r\n\r\n    CMAKE_PROJECT_VERSION\r\n    CMAKE_PROJECT_VERSION_MAJOR\r\n    CMAKE_PROJECT_VERSION_MINOR\r\n    CMAKE_PROJECT_VERSION_PATCH\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.22000.\r\nDecompress file D:/FastDeploy/python/.setuptools-cmake-build/opencv-win-x64-3.4.16.zip ...\r\n-- OpenCV ARCH: x64\r\n-- OpenCV RUNTIME: vc15\r\n-- OpenCV STATIC: OFF\r\n-- Found OpenCV 3.4.16 in D:/FastDeploy/python/.setuptools-cmake-build/third_libs/install/opencv-win-x64-3.4.16/build/x64/vc15/lib\r\n-- You might need to add D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_libs\\install\\opencv-win-x64-3.4.16\\build\\x64\\vc15\\bin to your PATH to be able to run your applications.\r\nFASTERTOKENIZER_COMPILE_LIB = D:/FastDeploy/python/.setuptools-cmake-build/third_libs/install/faster_tokenizer/lib/core_tokenizers.lib\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.19.6\r\n--   CMake command             : D:/Anaconda3/Library/bin/cmake.exe\r\n--   System                    : Windows\r\n--   C++ compiler              : D:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC/14.29.30133/bin/Hostx64/x64/cl.exe\r\n--   C++ compiler version      : 19.29.30146.0\r\n--   CXX flags                 : /DWIN32 /D_WINDOWS /W3 /GR /EHsc\r\n--   Build type                : Release\r\n--   Compile definitions       : YAML_CPP_DLL;FASTDEPLOY_LIB;EIGEN_STRONG_INLINE=inline;ENABLE_PADDLE_FRONTEND;ENABLE_ORT_BACKEND;ENABLE_VISION;ENABLE_VISION_VISUALIZE;ENABLE_TEXT\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : C:/Program Files/fastdeploy\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.2.1\r\n--   Paddle2ONNX version       : 1.0.1rc\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_DEBUG              : OFF\r\n--   ENABLE_VISION_VISUALIZE   : ON\r\n--   Python executable         : D:\\Anaconda3\\envs\\fastdeploy\\python.exe\r\n--   Python includes           : D:\\Anaconda3\\envs\\fastdeploy\\include\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: D:/FastDeploy/python/.setuptools-cmake-build\r\n用于 .NET Framework 的 Microsoft (R) 生成引擎版本 16.11.2+f32259642\r\n版权所有(C) Microsoft Corporation。保留所有权利。\r\n\r\n  Checking File Globs\r\n  Checking Build System\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  yaml-cpp.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\Release\\yaml-cpp.dll\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  fastdeploy.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy.dll\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n    正在创建库 D:/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.lib 和对象 D:/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.exp\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“ppseg_pybind.obj”(函数“\"public: virtual void * __cdecl pybind11::reference_cast_error::`vector deleting destructor'(unsigned int)\"\r\n(??_Ereference_cast_error@pybind11@@UEAAPEAXI@Z)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“segmentation_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“vision_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“visualize_pybind.obj”(函数“\"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _\r\nobject *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“ppmatting_pybind.obj”(函数“\"protected: struct std::pair<struct std::_List_node<struct std::pair<class std::type_index const ,class\r\nstd::vector<bool (__cdecl*)(struct _object *,void * &),class std::allocator<bool (__cdecl*)(struct _object *,void * &)> > >,void *> *,bool> __cdecl std::_Hash<class std::_Umap_traits<clas\r\ns std::type_index,class std::vector<bool (__cdecl*)(struct _object *,void * &),class std::allocator<bool (__cdecl*)(struct _object *,void * &)> >,class std::_Uhash_compare<class std::type\r\n_index,struct pybind11::detail::type_hash,struct pybind11::detail::type_equal_to>,class std::allocator<struct std::pair<class std::type_index const ,class std::vector<bool (__cdecl*)(stru\r\nct _object *,void * &),class std::allocator<bool (__cdecl*)(struct _object *,void * &)> > > >,0> >::_Try_emplace<class std::type_index const &>(class std::type_index const &)\" (??$_Try_em\r\nplace@AEBVtype_index@std@@$$V@?$_Hash@V?$_Umap_traits@Vtype_index@std@@V?$vector@P6A_NPEAU_object@@AEAPEAX@ZV?$allocator@P6A_NPEAU_object@@AEAPEAX@Z@std@@@2@V?$_Uhash_compare@Vtype_index@\r\nstd@@Utype_hash@detail@pybind11@@Utype_equal_to@45@@2@V?$allocator@U?$pair@$$CBVtype_index@std@@V?$vector@P6A_NPEAU_object@@AEAPEAX@ZV?$allocator@P6A_NPEAU_object@@AEAPEAX@Z@std@@@2@@std@\r\n@@2@$0A@@std@@@std@@IEAA?AU?$pair@PEAU?$_List_node@U?$pair@$$CBVtype_index@std@@V?$vector@P6A_NPEAU_object@@AEAPEAX@ZV?$allocator@P6A_NPEAU_object@@AEAPEAX@Z@std@@@2@@std@@PEAX@std@@_N@1@\r\nAEBVtype_index@1@@Z)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“ocr_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“ocrmodel_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“ocrsys_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“vpl_pybind.obj”(函数“\"public: static struct pybind11::detail::npy_api & __cdecl pybind11::detail::npy_api::get(void)\" (?get@npy_api\r\n@detail@pybind11@@SAAEAU123@XZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“faceid_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“modnet_pybind.obj”(函数“\"private: class pybind11::object & __cdecl pybind11::detail::accessor<struct pybind11::detail::accessor_pol\r\nicies::generic_item>::get_cache(void)const \" (?get_cache@?$accessor@Ugeneric_item@accessor_policies@detail@pybind11@@@detail@pybind11@@AEBAAEAVobject@3@XZ)”中)导入 [D:\\FastDeploy\\pytho\r\nn\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“matting_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“arcface_pybind.obj”(函数“\"public: static struct pybind11::detail::npy_api & __cdecl pybind11::detail::npy_api::get(void)\" (?get@npy\r\n_api@detail@pybind11@@SAAEAU123@XZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“cosface_pybind.obj”(函数“\"public: static struct pybind11::detail::npy_api & __cdecl pybind11::detail::npy_api::get(void)\" (?get@npy\r\n_api@detail@pybind11@@SAAEAU123@XZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“insightface_rec_pybind.obj”(函数“\"public: static struct pybind11::detail::npy_api & __cdecl pybind11::detail::npy_api::get(void)\" (\r\n?get@npy_api@detail@pybind11@@SAAEAU123@XZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“partial_fc_pybind.obj”(函数“\"public: static struct pybind11::detail::npy_api & __cdecl pybind11::detail::npy_api::get(void)\" (?get@\r\nnpy_api@detail@pybind11@@SAAEAU123@XZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4217: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“ libcpmt.lib(cout.obj)”中定义)已由“scrfd_pybind.obj”(函数“\"public: void __cdecl pybind11::detail::instance::deallocate_layout(void)\" (?deallocate_layout@instance@deta\r\nil@pybind11@@QEAAXXZ)”中)导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nLINK : warning LNK4286: 符号“??1?$basic_ios@DU?$char_traits@D@std@@@std@@UEAA@XZ (public: virtual __cdecl std::basic_ios<char,struct std::char_traits<char> >::~basic_ios<char,struct std::ch\r\nar_traits<char> >(void))”(在“libcpmt.lib(cout.obj)”中定义)已由“ultraface_pybind.obj”导入 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\n\r\n............\r\n\r\n\r\nmain.obj : error LNK2019: 无法解析的外部符号 \"__declspec(dllimport) public: class std::basic_ostream<char,struct std::char_traits<char> > & __cdecl std::basic_ostream<char,struct std::char_traits<\r\nchar> >::operator<<(int)\" (__imp_??6?$basic_ostream@DU?$char_traits@D@std@@@std@@QEAAAEAV01@H@Z)，函数 \"public: class fastdeploy::FDLogger & __cdecl fastdeploy::FDLogger::operator<<<int>(int\r\n const &)\" (??$?6H@FDLogger@fastdeploy@@QEAAAEAV01@AEBH@Z) 中引用了该符号 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy_main.cp38-win_amd64.pyd : fatal error LNK1120: 8 个无法解析的外部命令 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastd\r\neploy_main.vcxproj]\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 387, in <module>\r\n    setuptools.setup(\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\__init__.py\", line 87, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 973, in run_commands\r\n    self.run_command(cmd)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 992, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 132, in run\r\n    self.run_command(cmd_name)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 992, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 249, in run\r\n    self.run_command('cmake_build')\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 992, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 243, in run\r\n    subprocess.check_call(build_args)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['D:\\\\Anaconda3\\\\Library\\\\bin\\\\cmake.exe', '--build', '.', '--config', 'Release', '--', '/maxcpucount:20']' returned non-zero exit status 1.\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-15T06:28:22+00:00",
        "updated_at": "2024-02-06T04:26:00+00:00",
        "closed_at": "2024-02-06T04:26:00+00:00",
        "comments_count": [
            "DefTruth",
            "monkeycc",
            "joey12300",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 241,
        "title": "cmake 编译，第三方库冗余、冲突的问题",
        "body": "我在自己编译FastDeploy时，发现很多third_party都是直接下载使用的，感觉这好像是贵司的传统？下载的预编译库还带了个OpenCV3和onnxruntime的动态链接库，然而我自己的环境已经安装了特定版本的OpenCV4，onnxruntime，yaml-cpp等，我却无法链接这些已经存在的库，很容易出现多版本、静动态库混用的问题。所以能否增加使用系统已安装库的编译选项？",
        "state": "closed",
        "user": "LiuPeiqiCN",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-16T02:23:21+00:00",
        "updated_at": "2024-02-06T04:25:59+00:00",
        "closed_at": "2024-02-06T04:25:59+00:00",
        "comments_count": [
            "DefTruth",
            "LiuPeiqiCN",
            "jiangjiajun",
            "LiuPeiqiCN",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 246,
        "title": "这个源码能在cuda10.2+cudnn7.6.5上进行编译吗",
        "body": "我在windows10 x64用VS2019+cuda10.2+cudnn7.6.5编译总出错",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-16T13:22:31+00:00",
        "updated_at": "2024-02-06T04:25:57+00:00",
        "closed_at": "2024-02-06T04:25:57+00:00",
        "comments_count": [
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 242,
        "title": "RuntimeError: tensor type 0 is not supported",
        "body": "按照教程替换模型与demo图片，如上述报错，如何修改输入呢？\r\n模型是ppyoloe\r\n输入是2k图",
        "state": "closed",
        "user": "ZhangLe-fighting",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-16T04:44:55+00:00",
        "updated_at": "2024-02-06T04:25:58+00:00",
        "closed_at": "2024-02-06T04:25:58+00:00",
        "comments_count": [
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "DefTruth",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "ZhangLe-fighting",
            "jiangjiajun",
            "ZhangLe-fighting",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 252,
        "title": "无法关闭运行日志",
        "body": "使用`option.DisablePaddleLogInfo();`无法关闭`[INFO]`日志。\r\n比如加载模型时：\r\n```bash\r\n[INFO] fastdeploy/fastdeploy_runtime.cc(265)::Init      Runtime initialized with Backend::ORT in device Device::CPU.\r\n[INFO] fastdeploy/fastdeploy_runtime.cc(278)::Init      Runtime initialized with Backend::PDINFER in device Device::CPU.\r\n```",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-19T03:19:16+00:00",
        "updated_at": "2024-02-06T04:25:56+00:00",
        "closed_at": "2024-02-06T04:25:56+00:00",
        "comments_count": [
            "lifw555",
            "lifw555",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 269,
        "title": "TensorRT cache file loading error on Windows",
        "body": "![image](https://user-images.githubusercontent.com/19339784/191708183-e55e5d99-b464-4c30-9644-27d39b3d0fb1.png)\r\n",
        "state": "closed",
        "user": "jiangjiajun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-22T09:15:13+00:00",
        "updated_at": "2024-02-06T04:25:55+00:00",
        "closed_at": "2024-02-06T04:25:55+00:00",
        "comments_count": [
            "DefTruth",
            "steven-vvv",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Windows x64",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 307,
        "title": "RuntimeError: FastDeploy initalized failed!",
        "body": "硬件：Mac m1\r\n版本：fastdeploy-python  0.2.1\r\n安装方式：pip install numpy opencv-python fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n运行demo： python infer.py --model Unet_cityscapes_without_argmax_infer --image cityscapes_demo.png --device cpu\r\n报错：\r\n\r\n> $  python infer.py --model Unet_cityscapes_without_argmax_infer --image cityscapes_demo.png --device cpu\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda3/envs/py39/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 154, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: dlopen(/opt/miniconda3/envs/py39/lib/python3.9/site-packages/fastdeploy/libs/fastdeploy_main.cpython-39-darwin.so, 0x0002): Library not loaded: '@rpath/libfastdeploy.0.2.1.dylib'\r\n  Referenced from: '/opt/miniconda3/envs/py39/lib/python3.9/site-packages/fastdeploy/libs/fastdeploy_main.cpython-39-darwin.so'\r\n  Reason: tried: '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/paddle2onnx/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/opencv/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/faster_tokenizer/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/include/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/include/paddle2onnx/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/lib/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/include/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/DWARF/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/DWARF/DWARF/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/tokenizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/normalizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/core/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/postprocessors/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/utils/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/models/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/pretokenizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/decoders/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/glog/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/gflags/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/re2/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/nlohmann/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/lib/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/lib/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv/opencv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/superres/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/shape/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/flann/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/hal/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/cuda/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/cuda/detail/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/utils/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/autogenerated/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/detail/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/stitching/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/stitching/detail/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgproc/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgproc/hal/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/objdetect/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/ml/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgcodecs/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/dnn/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/dnn/utils/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/videoio/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/highgui/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/features2d/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/features2d/hal/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/videostab/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/photo/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/calib3d/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/pkgconfig/python2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/python-2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/python-2.7/python-2.7/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/paddle2onnx/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/opencv/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/third_libs/install/faster_tokenizer/lib/libfastdeploy.0.2.1.dylib' (no such file), '/Users/qiuyanjun/Desktop/dev/fastdeploy_build/FastDeploy/python/.setuptools-cmake-build/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/include/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/include/paddle2onnx/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/paddle2onnx/lib/paddle2onnx/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/include/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/DWARF/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/onnxruntime/lib/libonnxruntime.1.12.0.dylib.dSYM/Contents/Resources/DWARF/DWARF/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/tokenizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/normalizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/core/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/postprocessors/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/utils/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/models/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/pretokenizers/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/include/faster_tokenizer/decoders/decoders/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/glog/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/gflags/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/re2/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/include/nlohmann/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/third_party/lib/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/faster_tokenizer/lib/nlohmann/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv/opencv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/superres/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/shape/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/flann/calib3d/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/hal/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/cuda/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/cuda/detail/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/utils/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/opencl/runtime/autogenerated/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/core/detail/autogenerated/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/stitching/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/stitching/detail/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgproc/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgproc/hal/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/objdetect/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/ml/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/imgcodecs/detail/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/dnn/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/dnn/utils/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/videoio/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/highgui/utils/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/features2d/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/features2d/hal/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/videostab/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/photo/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/include/opencv2/calib3d/hal/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/pkgconfig/python2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/python-2.7/libfastdeploy.0.2.1.dylib' (no such file), 'loader_path/$loader_path/third_libs/opencv/lib/python2.7/site-packages/cv2/python-2.7/python-2.7/libfastdeploy.0.2.1.dylib' (no such file), '/opt/miniconda3/envs/py39/bin/../lib/libfastdeploy.0.2.1.dylib' (no such file), '/opt/miniconda3/envs/py39/bin/../lib/libfastdeploy.0.2.1.dylib' (no such file), '/usr/local/lib/libfastdeploy.0.2.1.dylib' (no such file), '/usr/lib/libfastdeploy.0.2.1.dylib' (no such file)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/netwu/workspace/PaddlePaddle/FastDeploy/examples/vision/segmentation/paddleseg/python/infer.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"/opt/miniconda3/envs/py39/lib/python3.9/site-packages/fastdeploy/__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (Frontend, Backend, FDDataType, TensorInfo, Device,\r\n  File \"/opt/miniconda3/envs/py39/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 156, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!",
        "state": "closed",
        "user": "netwu",
        "closed_by": "jiangjiajun",
        "created_at": "2022-09-29T09:33:24+00:00",
        "updated_at": "2024-02-06T04:25:54+00:00",
        "closed_at": "2024-02-06T04:25:54+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "JackonLiu",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 310,
        "title": "issues from wechat, about paddle serving",
        "body": "hi, 这里记录了一些 在微信群里的问题, 主要涉及到 FastDeploy 和 paddleserving 两个库相关的问题, 可能放在这里更合适一点, 可以让更多的人看到, 希望可以解答一下, 感谢🙏\r\n\r\n- 问一下，fastdeploy 跟 paddle serving 区别是什么;\r\n\r\n- paddle serving 的开发似乎不太活跃了, 请问长期来看还会继续支持吗? 还是完全用 fastdeploy  替代?\r\n\r\n- 它(指FastDeploy)比 padddle serving 优势在哪里，方便，安全，速度 还是互补的关系",
        "state": "closed",
        "user": "gglin001",
        "closed_by": "gglin001",
        "created_at": "2022-09-30T03:52:07+00:00",
        "updated_at": "2022-10-09T13:38:59+00:00",
        "closed_at": "2022-10-09T13:38:59+00:00",
        "comments_count": [
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 318,
        "title": "Quantization model deploy on GPU",
        "body": "I have below error. model downloaded from your exported ones:\r\n\r\nwith cache or without cache enabled same error.\r\n\r\n> > python3 infer.py --model yolov7-w6-end2end-ort-nms.onnx   --image /data/Videos/hiv00131/hiv00131/00000480.png  --device gpu --use_trt True\r\n> [INFO] fastdeploy/backends/tensorrt/trt_backend.cc(540)::CreateTrtEngineFromOnnx\tDetect serialized TensorRT Engine file in yolotrt.cache, will load it directly.\r\n> [INFO] fastdeploy/backends/tensorrt/trt_backend.cc(106)::LoadTrtCache\tBuild TensorRT Engine from cache file: yolotrt.cache with shape range information as below,\r\n> [INFO] fastdeploy/backends/tensorrt/trt_backend.cc(109)::LoadTrtCache\tInput name: images, shape=[1, 3, 640, 640], min=[1, 3, 640, 640], max=[1, 3, 640, 640]\r\n> \r\n> [INFO] fastdeploy/fastdeploy_runtime.cc(270)::Init\tRuntime initialized with Backend::TRT in device Device::GPU.\r\n> [ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(384)::AllocateOutputsBuffer\tCannot find output: num_dets of tensorrt network from the original model.\r\n> Aborted (core dumped)",
        "state": "closed",
        "user": "MyraBaba",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-03T21:16:01+00:00",
        "updated_at": "2024-02-06T04:25:53+00:00",
        "closed_at": "2024-02-06T04:25:53+00:00",
        "comments_count": [
            "DefTruth",
            "MyraBaba",
            "jiangjiajun",
            "MyraBaba",
            "jiangjiajun",
            "MyraBaba",
            "jiangjiajun",
            "yunyaoXYY",
            "MyraBaba",
            "yunyaoXYY",
            "MyraBaba",
            "MyraBaba",
            "MyraBaba",
            "yunyaoXYY",
            "MyraBaba",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 321,
        "title": "二次开发，使用FastDeploy作为依赖库，封装dll后，很多接口都没法使用问题，麻烦跟踪排查一下。",
        "body": "- 环境：\r\n```\r\nwindwos10 VS2019 3060 6G笔记本显卡\r\n```\r\n- 问题：\r\n调用ppyoloe，yolov7和maskrcnn,ocr，均是在example下正常，但是封装dll后均报错\r\nyolov7和ppyoloe是模型无法生成trt文件，maskrcnn是，cpu gpu和trt都崩溃，cpu和gpu模式初始化成功，predict崩溃;\r\n以ppyoloe为例：\r\n封装dll代码：\r\n\r\n- .h文件： \r\n\r\n```C++\r\n__declspec(dllexport) int __stdcall fastdeploy_model_trt_convert(const char* _modeldir, int _trtmodeltype = 1, int _gpu_id = 0);\r\n```\r\n\r\n- .cpp文件 \r\n```C++\r\n#include \"fastdeploy/vision.h\"\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\n#ifdef WIN32\r\n#define ACCESS(fileName,accessMode) _access(fileName,accessMode)\r\n#else\r\n#define ACCESS(fileName,accessMode) access(fileName,accessMode)\r\n#endif\r\n\r\n__declspec(dllexport) int __stdcall fastdeploy_model_trt_convert(const char* _modeldir, int _trtmodeltype /*= 1*/, int _gpu_id /*= 0*/)\r\n{\r\n\tint gpuid = 0;\r\n\tif (_gpu_id < 0)\r\n\t{\r\n\t\tgpuid = 0;\r\n\t}\r\n\telse {\r\n\t\tgpuid = _gpu_id;\r\n\t}\r\n\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu(gpuid);\r\n\toption.UseTrtBackend();\r\n\r\n\tauto model_file = (std::string)_modeldir + sep + \"model.pdmodel\";\r\n\tauto params_file = (std::string)_modeldir + sep + \"model.pdiparams\";\r\n\tauto config_file = (std::string)_modeldir + sep + \"infer_cfg.yml\";\r\n\r\n\tstd::string  trtpath_file = \"\";\r\n\tif (_trtmodeltype == 1)\r\n\t{\r\n\t\ttrtpath_file = (std::string)_modeldir + sep + \"deploy_fp16.trt\";\r\n\t\toption.EnableTrtFP16();\r\n\t}\r\n\telse if (_trtmodeltype == 0) {\r\n\t\ttrtpath_file = (std::string)_modeldir + sep + \"deploy_fp32.trt\";\r\n\t}\r\n\telse {\r\n\t\ttrtpath_file = (std::string)_modeldir + sep + \"deploy_fp8.trt\";\r\n\t}\r\n\r\n\toption.SetTrtCacheFile(trtpath_file);\r\n\tauto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file, config_file, option);\r\n\r\n\tif (!model.Initialized()) {\r\n\t\treturn -2;\r\n\t}\r\n\r\n\tif (ACCESS(trtpath_file.c_str(),0) == -1)\r\n\t{\r\n\t\treturn -3;\r\n\t}\r\n\r\n\treturn 0;\r\n}\r\n```\r\n\r\n控制台调用程序：\r\n\r\n```C++\r\nint main()\r\n{\r\n    std::string modeldir = \".\\\\ppyoloe\";\r\n    fastdeploy_model_trt_convert(modeldir.c_str());\r\n    std::cout << \"Hello World!\\n\";\r\n}\r\n```\r\n\r\n接口设置了SetTrtCacheFile(trtpath_file);，并且模型初始化成功了fastdeploy::vision::detection::PPYOLOE和model.Initialized()均返回正常，但是没有生成trt文件：\r\n```C++\r\nif (ACCESS(trtpath_file.c_str(),0) == -1)\r\n\t{\r\n\t\treturn -3;\r\n\t}\r\n```\r\n返回了-3\r\n\r\n",
        "state": "closed",
        "user": "xinsuinizhuan",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-06T13:04:17+00:00",
        "updated_at": "2024-02-06T04:25:52+00:00",
        "closed_at": "2024-02-06T04:25:52+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 322,
        "title": "自训练PP_LiteSeg模型，模型导出后，使用FD推理，基本没效果。",
        "body": "使用PP_LiteSeg训练模型：\r\n训练命令：\r\npython export.py  --config configs/quick_start/pp_liteseg_optic_disc_512x512_1k.yml --model_path output/iter_10000/model.pdparams --save_dir output --input_shape 1 3 512 512\r\n\r\n训练完后，使用python测试效果：\r\n![图片](https://user-images.githubusercontent.com/40679769/194497348-c34e5c75-42c9-4647-9dd7-eaff09cf7506.png)\r\n效果还可以。\r\n\r\n模型导出：\r\npython export.py  --config configs/quick_start/pp_liteseg_optic_disc_512x512_1k.yml --model_path output/iter_10000/model.pdparams --save_dir output --input_shape 1 3 512 512\r\n其中，yaml参数文件为：\r\nDeploy:\r\n  input_shape:\r\n  - 1\r\n  - 3\r\n  - 512\r\n  - 512\r\n  model: model.pdmodel\r\n  output_dtype: int32\r\n  output_op: argmax\r\n  params: model.pdiparams\r\n  transforms:\r\n  - type: Normalize\r\n\r\n然后使用FD推理，cpu，效果：\r\n![图片](https://user-images.githubusercontent.com/40679769/194497638-3fa9b3ec-c089-4ff8-9bc8-d27c89c543c9.png)\r\n\r\n\r\n但是使用https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/segmentation/paddleseg下的[PP-LiteSeg-T(STDC1)-cityscapes](https://bj.bcebos.com/paddlehub/fastdeploy/PP_LiteSeg_T_STDC1_cityscapes_without_argmax_infer.tgz)提供的模型推理，正常，其中yaml文件为：\r\nDeploy:\r\n  input_shape:\r\n  - -1\r\n  - 3\r\n  - -1\r\n  - -1\r\n  model: model.pdmodel\r\n  output_dtype: float32\r\n  output_op: none\r\n  params: model.pdiparams\r\n  transforms:\r\n  - type: Normalize\r\n",
        "state": "closed",
        "user": "xinsuinizhuan",
        "closed_by": "felixhjh",
        "created_at": "2022-10-07T07:32:51+00:00",
        "updated_at": "2022-10-09T02:20:33+00:00",
        "closed_at": "2022-10-09T02:20:33+00:00",
        "comments_count": [
            "jiangjiajun",
            "felixhjh",
            "leiqing1",
            "felixhjh",
            "felixhjh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 323,
        "title": "OCR部署，初始化判断问题",
        "body": "![image](https://user-images.githubusercontent.com/19339784/194505773-2e8cbc95-5156-4bdb-a0b0-c2f895a5d6fb.png)\r\n\r\n此外，OCR中模型的预处理需说明清楚。 如若模型由PaddleOCR导出，或下载，需对哪几个模型做怎样的处理操作\r\n \r\n@yunyaoXYY ",
        "state": "closed",
        "user": "jiangjiajun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-07T08:13:12+00:00",
        "updated_at": "2022-10-09T10:56:53+00:00",
        "closed_at": "2022-10-09T10:56:53+00:00",
        "comments_count": [
            "yunyaoXYY"
        ],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 336
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 325,
        "title": "想问下关于openvino的核显推理加速大概什么时候添加支持?",
        "body": null,
        "state": "closed",
        "user": "kc-w",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-08T05:43:57+00:00",
        "updated_at": "2024-02-06T04:25:51+00:00",
        "closed_at": "2024-02-06T04:25:51+00:00",
        "comments_count": [
            "jiangjiajun",
            "kc-w",
            "jiangjiajun",
            "kc-w",
            "jiangjiajun",
            "kc-w",
            "jiangjiajun",
            "joey12300",
            "kc-w",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 329,
        "title": "Will Text Detection Using PP-OCR Be Made Available for Server Side Deployment?",
        "body": "I was hoping to do a light deployment of text detection on a Linux server using FastDeploy in Python.\r\n\r\nHowever, PP-OCR and later versions aren't listed in the supported model list for server side deployment.\r\n\r\nI have tried to import it in any case along the lines of the tutorial for PPYOLOE but not sure how to pass the required det_model and cls_model parameters. I presume there isn't a way of doing this and this is just a stub?\r\n\r\nWondering if PP-OCR may be made available for server side deployment at some stage?",
        "state": "closed",
        "user": "rattling",
        "closed_by": "rattling",
        "created_at": "2022-10-08T10:15:37+00:00",
        "updated_at": "2022-10-09T08:45:04+00:00",
        "closed_at": "2022-10-08T18:47:17+00:00",
        "comments_count": [
            "yunyaoXYY",
            "rattling",
            "yunyaoXYY",
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 344,
        "title": "新增代码编译后，链接时出现问题",
        "body": "我新增了rknpu的代码，在编译环节已经加入一起编译了\r\n<img width=\"1512\" alt=\"image\" src=\"https://user-images.githubusercontent.com/58363586/195056680-0d034cf9-459a-41cb-849e-f8d7b6e32a64.png\">\r\n\r\n但是当编译完成后，我的测试文件引用fastdeploy库时却出现未定义的情况\r\n<img width=\"1512\" alt=\"image\" src=\"https://user-images.githubusercontent.com/58363586/195056894-b91aaa1b-7094-4523-a9c3-b187dcc6556b.png\">\r\n\r\nfastdeploy::RKNPU2Backend::GetSDKAndDeviceVersion()是在rknpu2_backend.cc中定义的，并且在rknpu2_backend.h中已经声明了\r\n<img width=\"836\" alt=\"image\" src=\"https://user-images.githubusercontent.com/58363586/195057277-03226b13-2b55-4c7a-a145-f42a8dbc9135.png\">\r\n",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-10-11T09:48:02+00:00",
        "updated_at": "2022-10-12T04:00:00+00:00",
        "closed_at": "2022-10-12T04:00:00+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 330,
        "title": "新模型咨询",
        "body": "请问一下，FD后期是否有对paddledetection 危险行为识别等pipeline部署的支持，还有paddlecls中的物品识别？目前有需求做异常行为识别，人/车辆属性分析等等。谢谢",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-08T12:42:36+00:00",
        "updated_at": "2022-10-08T13:52:42+00:00",
        "closed_at": "2022-10-08T13:52:26+00:00",
        "comments_count": [
            "jiangjiajun",
            "ChaoII",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 356,
        "title": "{\"code\":\"NoSuchKey\",\"message\":\"The specified key does not exist.\",\"requestId\":\"1e32ec72-7aed-48f4-b466-c2fcc5fb158e\"}",
        "body": "Please using git, rather than BCE....",
        "state": "closed",
        "user": "lucasjinreal",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-12T12:12:34+00:00",
        "updated_at": "2024-02-06T04:25:50+00:00",
        "closed_at": "2024-02-06T04:25:50+00:00",
        "comments_count": [
            "jiangjiajun",
            "lucasjinreal",
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 358,
        "title": "3D Object Detection什么时候可以支持",
        "body": "请问3D Object Detection计划什么时候可以支持",
        "state": "closed",
        "user": "A1exy",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-13T01:24:59+00:00",
        "updated_at": "2024-02-06T04:25:49+00:00",
        "closed_at": "2024-02-06T04:25:49+00:00",
        "comments_count": [
            "jiangjiajun",
            "A1exy",
            "A1exy",
            "jiangjiajun"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 360,
        "title": "serving部署，设置tensorrt推理引擎无效",
        "body": "镜像是paddlepaddle/fastdeploy:0.3.0-gpu-cuda11.4-trt8.4-21.10\r\n\r\n![image](https://user-images.githubusercontent.com/37280580/195551974-5ea80b55-1619-49ad-a2b5-1f1ce08a8b0a.png)\r\n![image](https://user-images.githubusercontent.com/37280580/195552332-e3d9dc85-7d7e-463f-b048-29e6411e1edb.png)\r\n",
        "state": "closed",
        "user": "richjjj",
        "closed_by": "richjjj",
        "created_at": "2022-10-13T09:00:42+00:00",
        "updated_at": "2022-10-16T14:11:49+00:00",
        "closed_at": "2022-10-16T14:11:49+00:00",
        "comments_count": [
            "heliqi",
            "richjjj",
            "heliqi",
            "richjjj"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 361,
        "title": "【已支持】RobustVideoMatting是否支持",
        "body": "Matting模型部署C++ SDK没看到对RobustVideoMatting的支持呐",
        "state": "closed",
        "user": "publicforlijun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-13T09:23:18+00:00",
        "updated_at": "2024-02-06T04:25:48+00:00",
        "closed_at": "2024-02-06T04:25:48+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 362,
        "title": "Windows GPU环境下使用，提示cudnn64_8.dll异常",
        "body": "![Untitled](https://user-images.githubusercontent.com/37092245/195559567-64fdb5a8-3535-48c6-a6ea-1586ed62aa85.png)\r\n",
        "state": "closed",
        "user": "publicforlijun",
        "closed_by": "publicforlijun",
        "created_at": "2022-10-13T09:30:11+00:00",
        "updated_at": "2025-06-09T07:35:19+00:00",
        "closed_at": "2022-10-14T02:44:08+00:00",
        "comments_count": [
            "publicforlijun",
            "publicforlijun",
            "jiangjiajun",
            "DefTruth",
            "publicforlijun",
            "DefTruth",
            "publicforlijun",
            "publicforlijun",
            "publicforlijun",
            "DefTruth",
            "YourHotPotato",
            "Object1009"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 379,
        "title": "MemoryError",
        "body": "Hi I get this error with batchsize = 32 after 10 epoch\r\ncreating index...\r\nindex created!\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 134, in <module>\r\n    model.train()\r\n  File \"train.py\", line 126, in train\r\n    mAP05 = self.evaluation.compute_map(self.val_dataloader, self.model)\r\n  File \"C:\\final\\dataset\\FastestDet-main\\utils\\evaluation.py\", line 98, in compute_map\r\n    mAP05 = self.coco_evaluate(gts, pts)\r\n  File \"C:\\final\\dataset\\FastestDet-main\\utils\\evaluation.py\", line 45, in coco_evaluate\r\n    coco_pred.dataset[\"images\"].append({\"id\": i})\r\nMemoryError",
        "state": "closed",
        "user": "centurions",
        "closed_by": "centurions",
        "created_at": "2022-10-16T18:22:51+00:00",
        "updated_at": "2022-10-16T18:23:05+00:00",
        "closed_at": "2022-10-16T18:23:05+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 369,
        "title": "希望能够尽快适配好PP-ShiTu。",
        "body": "目前pp-shitu的端侧部署很困难，尤其是跨平台，希望FastDeploy适配好pp-shitu的端侧部署，尤其是Windows上的部署。",
        "state": "closed",
        "user": "yingeo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-14T09:00:15+00:00",
        "updated_at": "2024-12-03T06:42:47+00:00",
        "closed_at": "2024-12-03T06:42:47+00:00",
        "comments_count": [
            "ChaoII",
            "ziqi-jin",
            "ziqi-jin",
            "yingeo",
            "ziqi-jin",
            "thomas-yanxin",
            "LL020202",
            "yingeo",
            "monkeycc",
            "yingeo",
            "monkeycc",
            "shao77622",
            "shao77622",
            "thomas-yanxin",
            "jiangjiajun",
            "Hongyuan-Liu",
            "thomas-yanxin",
            "wenzhu2018"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 373,
        "title": "有计划支持Multi-object tracking PP-Tracking吗？",
        "body": "对应PaddleDetection：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/mot",
        "state": "closed",
        "user": "jerryandjune",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-16T04:36:22+00:00",
        "updated_at": "2024-02-06T04:25:47+00:00",
        "closed_at": "2024-02-06T04:25:47+00:00",
        "comments_count": [
            "jiangjiajun",
            "ChaoII",
            "jerryandjune",
            "jiangjiajun"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 383,
        "title": "IS that one GPU can only support one tensorRT ENGINE inference? ",
        "body": "IS that one GPU can only support one tensorRT ENGINE inference? \r\n![9](https://user-images.githubusercontent.com/59491993/196087005-668819f8-aa80-4947-8517-d01f3b49a27f.jpg)\r\n![9](https://user-images.githubusercontent.com/59491993/196087036-72b5ae81-3231-45cb-9801-7621751845e8.jpg)\r\nIS that the Deployment would automatically find the Most optimal resource of one GPU?\r\n\r\nWhen I opened one .exe using the tensorRT engine to infer, the GPU memory usage is about 40%. Accordingly, it should be OK for the GPU to run 2.exe parallelly. However, it can only inferences serially as the time doubled.\r\n\r\nWhy can't two kernels run in parallel?\r\nNow, the parallel of HtoD data transmission and kernel computing has been implemented, but the parallel of two kernel computing  hasn't been realized. \r\nSo the reason is that the GPU resources used are not enough?\r\nIs there a problem with my method?\r\nIs there a sample that can be used for reference?\r\nMuch respect and Thanks!\r\n![9](https://user-images.githubusercontent.com/59491993/196087072-3fea198b-1bc7-48e6-a7b1-17a1ad3abdd4.jpg)\r\n",
        "state": "closed",
        "user": "brilliant-soilder",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-17T04:06:01+00:00",
        "updated_at": "2024-02-06T04:25:46+00:00",
        "closed_at": "2024-02-06T04:25:46+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 385,
        "title": "OpenCV(4.3.0) Error: Assertion failed (inv_scale_x > 0) in resize",
        "body": "2022-10-17 18:06:46.732 25736-25934/com.package.test E/cv::error(): OpenCV(4.3.0) Error: Assertion failed (inv_scale_x > 0) in resize, file /build/master_pack-android/opencv/modules/imgproc/src/resize.cpp, line 3932\r\n    \r\n    --------- beginning of crash\r\n2022-10-17 18:06:46.732 25736-25934/com.package.test A/libc: /usr/local/google/buildbot/src/android/ndk-release-r20/external/libcxx/../../external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion \"terminating with uncaught exception of type cv::Exception: OpenCV(4.3.0) /build/master_pack-android/opencv/modules/imgproc/src/resize.cpp:3932: error: (-215:Assertion failed) inv_scale_x > 0 in function 'resize'\r\n    \" failed\r\n2022-10-17 18:06:46.760 25736-25934/com.package.test A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 25934 (ModelTestActivi), pid 25736 (.pda.mdrh.smart)\r\n2022-10-17 18:06:46.874 25977-25977/? E/IDM-BleGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.875 25977-25977/? E/IDM-BtClassicGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.875 25977-25977/? E/IDM-BtGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.876 25977-25977/? E/IDM-BleGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.876 25977-25977/? E/IDM-BtClassicGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.876 25977-25977/? E/IDM-BtGovernor: BtGovernor died. need to init\r\n2022-10-17 18:06:46.877 25977-25977/? E/IDM-BleGovernor: BLE is not enabled\r\n2022-10-17 18:06:46.878 25977-25977/? E/IDM-BtClassicGovernor: BLE is not enabled\r\n2022-10-17 18:06:46.878 25977-25977/? E/IDM-BtGovernor: BLE is not enabled\r\n2022-10-17 18:06:47.028 25977-25977/? E/IDM-IotLocalControlSerivceImpl: IotLocalControlSerivceImpl IDM_LOCALSERVICE_UUIDffff3ffc-09da-4bed-9652-f507366fcfc5\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: Build fingerprint: 'xiaomi/whyred/whyred:9/PKQ1.180904.001/V12.0.3.0.PEICNXM:user/release-keys'\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: Revision: '0'\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: ABI: 'arm'\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: pid: 25736, tid: 25934, name: ModelTestActivi  >>> com.package.test <<<\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG: Abort message: '/usr/local/google/buildbot/src/android/ndk-release-r20/external/libcxx/../../external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion \"terminating with uncaught exception of type cv::Exception: OpenCV(4.3.0) /build/master_pack-android/opencv/modules/imgproc/src/resize.cpp:3932: error: (-215:Assertion failed) inv_scale_x > 0 in function 'resize'\r\n    \" failed'\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG:     r0  00000000  r1  0000654e  r2  00000006  r3  00000008\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG:     r4  00006488  r5  0000654e  r6  caf7b4ac  r7  0000010c\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG:     r8  c6da6375  r9  ffffff29  r10 caf7cd20  r11 caf7cc68\r\n2022-10-17 18:06:47.029 26075-26075/? A/DEBUG:     ip  eb2b23bc  sp  caf7b498  lr  eb21c7f9  pc  eb21401e\r\n2022-10-17 18:06:47.030 25977-25977/? E/IDMJni: jbyte_array Length is zero\r\n2022-10-17 18:06:47.034 25977-25977/? E/IDMJni: jbyte_array Length is zero\r\n2022-10-17 18:06:47.089 25977-26074/? E/IDM-UwbController:  getUwbController Error e :java.lang.NoClassDefFoundError: Failed resolution of: Lcom/nxp/uwb/UwbAdapter;\r\n2022-10-17 18:06:47.178 26075-26075/? A/DEBUG: backtrace:\r\n2022-10-17 18:06:47.178 26075-26075/? A/DEBUG:     #00 pc 0001d01e  /system/lib/libc.so (abort+58)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #01 pc 0001d28f  /system/lib/libc.so (__assert2+22)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #02 pc 00064fed  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libc++_shared.so\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #03 pc 00065165  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libc++_shared.so\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #04 pc 0006cb79  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libc++_shared.so\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #05 pc 0006c527  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libc++_shared.so\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #06 pc 0006c4ef  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libc++_shared.so (__cxa_throw+74)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #07 pc 00a66e61  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libedge-infer.so (cv::error(cv::Exception const&)+264)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #08 pc 00a66a1f  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libedge-infer.so (cv::error(int, std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char>> const&, char const*, char const*, int)+86)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #09 pc 00b314f5  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libedge-infer.so (cv::resize(cv::_InputArray const&, cv::_OutputArray const&, cv::Size_<int>, double, double, int)+680)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #10 pc 003c7527  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libedge-infer.so (easyedge::preprocess_image(cv::Mat&, easyedge::ImagePreprocess&, int, std::__ndk1::vector<float, std::__ndk1::allocator<float>>&, int&, int&, int&, int&)+302)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #11 pc 0038fd07  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/lib/arm/libedge-infer.so (Java_com_baidu_ai_edge_core_infer_InferLiteJni_predictNew+230)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #12 pc 0004059d  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.odex (offset 0x3a000) (com.baidu.ai.edge.core.infer.InferLiteJni.predictNew+156)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #13 pc 0040e775  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #14 pc 003e817b  /system/lib/libart.so (art_quick_invoke_static_stub+222)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #15 pc 000a1427  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+154)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #16 pc 001e6615  /system/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+236)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #17 pc 001e110f  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+814)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #18 pc 003e3cb3  /system/lib/libart.so (MterpInvokeStatic+130)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #19 pc 00401614  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #20 pc 0041ae3a  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.baidu.ai.edge.core.infer.InferManager.a+4)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #21 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #22 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #23 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #24 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #25 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #26 pc 0041852a  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.baidu.ai.edge.core.base.BaseManager.a+86)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #27 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #28 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #29 pc 001e1f33  /system/lib/libart.so (bool art::interpreter::DoCall<true, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+586)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #30 pc 003e3f11  /system/lib/libart.so (MterpInvokeVirtualRange+424)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #31 pc 00401794  /system/lib/libart.so (ExecuteMterpImpl+14996)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #32 pc 00418812  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.baidu.ai.edge.core.base.BaseManager.b+18)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #33 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #34 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #35 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #36 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.179 26075-26075/? A/DEBUG:     #37 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #38 pc 004188d4  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.baidu.ai.edge.core.base.BaseManager.detect+2)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #39 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #40 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #41 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #42 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #43 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #44 pc 00f8262e  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.sgcc.pda.lib_easyedge.service.BaseEdgeWrapper.runByModelType+182)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #45 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #46 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #47 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #48 pc 003e3b65  /system/lib/libart.so (MterpInvokeDirect+196)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #49 pc 00401594  /system/lib/libart.so (ExecuteMterpImpl+14484)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #50 pc 00f8252e  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.sgcc.pda.lib_easyedge.service.BaseEdgeWrapper.run+58)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #51 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #52 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #53 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #54 pc 003e38a7  /system/lib/libart.so (MterpInvokeInterface+1010)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #55 pc 00401694  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #56 pc 00f56956  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.sgcc.pda.mdrh.imagerecognition.handler.rec.BaseRec.rec+176)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #57 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #58 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #59 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #60 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #61 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #62 pc 00f5618c  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.sgcc.pda.mdrh.imagerecognition.handler.OfflineRec.rec+70)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #63 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #64 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #65 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #66 pc 003e3cb3  /system/lib/libart.so (MterpInvokeStatic+130)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #67 pc 00401614  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #68 pc 00f56260  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.sgcc.pda.mdrh.imagerecognition.handler.OfflineRec.rec+28)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #69 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #70 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #71 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #72 pc 003e3cb3  /system/lib/libart.so (MterpInvokeStatic+130)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #73 pc 00401614  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #74 pc 00a5cede  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.busi.TestBusi.execOffline+106)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #75 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.180 26075-26075/? A/DEBUG:     #76 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #77 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #78 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #79 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #80 pc 00a5ca3c  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.busi.TestBusi.execOfflineForList)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #81 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #82 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #83 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #84 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #85 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #86 pc 00a6e0b2  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.ui.activity.test.ModelTestActivity.execOffline+26)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #87 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #88 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #89 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #90 pc 003e3b65  /system/lib/libart.so (MterpInvokeDirect+196)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #91 pc 00401594  /system/lib/libart.so (ExecuteMterpImpl+14484)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #92 pc 00a6e080  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.ui.activity.test.ModelTestActivity.access$000)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #93 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #94 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #95 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #96 pc 003e3cb3  /system/lib/libart.so (MterpInvokeStatic+130)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #97 pc 00401614  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #98 pc 00a6dc7e  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.ui.activity.test.ModelTestActivity$21.onWork+8)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #99 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #100 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #101 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #102 pc 003e38a7  /system/lib/libart.so (MterpInvokeInterface+1010)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #103 pc 00401694  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #104 pc 00a6c32c  /data/app/com.package.test-SJUAQUAnu0lTWBVeXG3tAQ==/oat/arm/base.vdex (com.package.test.ui.activity.test.BasePhotoActivity$1.handleMessage+70)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #105 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #106 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #107 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #108 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #109 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #110 pc 00bd3982  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+42)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #111 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #112 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #113 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #114 pc 003e2cd7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n2022-10-17 18:06:47.181 26075-26075/? A/DEBUG:     #115 pc 00401494  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #116 pc 00be6530  /system/framework/boot-framework.vdex (android.os.Looper.loop+414)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #117 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #118 pc 001c9f71  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+152)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #119 pc 001e10f7  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+790)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #120 pc 003e3cb3  /system/lib/libart.so (MterpInvokeStatic+130)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #121 pc 00401614  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #122 pc 00bd3308  /system/framework/boot-framework.vdex (android.os.HandlerThread.run+56)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #123 pc 001c588b  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3971342586+378)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #124 pc 001c9eb7  /system/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+82)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #125 pc 003d67d9  /system/lib/libart.so (artQuickToInterpreterBridge+880)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #126 pc 00412cff  /system/lib/libart.so (art_quick_to_interpreter_bridge+30)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #127 pc 0040e775  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #128 pc 003e8079  /system/lib/libart.so (art_quick_invoke_stub+224)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #129 pc 000a1415  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+136)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #130 pc 003488ed  /system/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+52)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #131 pc 00349645  /system/lib/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+320)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #132 pc 0036a5e7  /system/lib/libart.so (art::Thread::CreateCallback(void*)+866)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #133 pc 00071851  /system/lib/libc.so (__pthread_start(void*)+22)\r\n2022-10-17 18:06:47.182 26075-26075/? A/DEBUG:     #134 pc 0001e045  /system/lib/libc.so (__start_thread+24)\r\n2022-10-17 18:06:47.459 26105-26105/? A/linker: CANNOT LINK EXECUTABLE \"/system/bin/dpmd\": \"/system/lib64/libdpmframework.so\" is 32-bit instead of 64-bit\r\n2022-10-17 18:06:47.560 25977-26094/? E/IDM-BonjourGovernor: stopAdvertising: exit since not in advertising\r\n2022-10-17 18:06:48.232 26075-26075/? E/crash_dump32: cannot open libmiuindbg.so: No such file or directory\r\n2022-10-17 18:06:48.237 1160-1160/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_01\r\n2022-10-17 18:06:48.276 2077-2158/? E/InputDispatcher: channel '24c836b com.package.test/com.package.test.ui.activity.test.ModelTestActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n2022-10-17 18:06:48.276 2077-2158/? E/InputDispatcher: channel '522b62c com.package.test/com.package.test.ui.activity.test.ModelTestActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n2022-10-17 18:06:48.276 2077-2158/? E/InputDispatcher: channel 'c8014a9 com.package.test/com.package.test.ui.activity.MainActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n2022-10-17 18:06:48.330 2077-3142/? E/ActivityTrigger: activityResumeTrigger: not whiteListedcom.miui.home/com.miui.home.launcher.Launcher/421133058",
        "state": "closed",
        "user": "chriswack",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-17T10:26:57+00:00",
        "updated_at": "2024-02-06T04:25:45+00:00",
        "closed_at": "2024-02-06T04:25:45+00:00",
        "comments_count": [
            "DefTruth",
            "chriswack",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 388,
        "title": "二次开发封装为DLL异常,麻烦看一下什么原因呐",
        "body": "![Untitled](https://user-images.githubusercontent.com/37092245/196318046-535d7515-96cc-49fe-b7f5-a44caa95973d.png)\r\n",
        "state": "closed",
        "user": "publicforlijun",
        "closed_by": "publicforlijun",
        "created_at": "2022-10-18T02:02:42+00:00",
        "updated_at": "2022-10-18T02:03:50+00:00",
        "closed_at": "2022-10-18T02:03:50+00:00",
        "comments_count": [
            "publicforlijun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 390,
        "title": "yolov7 姿态",
        "body": "请问近期会有计划部署yolov7 的人体姿态识别吗？",
        "state": "closed",
        "user": "JustinBili",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-18T05:28:08+00:00",
        "updated_at": "2024-02-06T04:25:44+00:00",
        "closed_at": "2024-02-06T04:25:44+00:00",
        "comments_count": [
            "DefTruth",
            "JustinBili",
            "jiangjiajun"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 404,
        "title": "目标板子是armhf架构，onnxruntime1.12.0版本编译不出onnxruntime_cxx_api.h等文件，需要依赖armhf对应的libonnxruntime.so",
        "body": "请教一下，我想在RV1109机子上部署FastDeploy（双核armv7，均属于armhf），看到目前只支持arm64和x86的。\r\n用交叉编译的方式可以编译到100%，但是最后会链接相关so，而下载来了so都是x86的。\r\n\r\n目前测试环境是x86的虚拟机ubuntu18，因为编译速度较快。\r\n\r\n我已经在板子上编译出paddle2onnx.so了，但是用microsoft/onnxruntime仓库编译不出这些文件：\r\nthird_libs/install/onnxruntime$ tree\r\n.\r\n├── include\r\n│   ├── cpu_provider_factory.h\r\n│   ├── onnxruntime_c_api.h\r\n│   ├── onnxruntime_cxx_api.h\r\n│   ├── onnxruntime_cxx_inline.h\r\n│   ├── onnxruntime_run_options_config_keys.h\r\n│   ├── onnxruntime_session_options_config_keys.h\r\n│   └── provider_options.h\r\n└── lib\r\n    ├── libonnxruntime.so -> libonnxruntime.so.1.12.0\r\n    └── libonnxruntime.so.1.12.0\r\n\r\n2 directories, 9 files\r\n\r\n获得的是这些：\r\n~/work/fastdeploy/onnxruntime/install$ tree\r\n.\r\n├── bin\r\n│   └── onnx_test_runner\r\n├── include\r\n│   └── onnxruntime\r\n│       └── core\r\n│           ├── common\r\n│           │   ├── basic_types.h\r\n│           │   ├── code_location.h\r\n│           │   ├── common.h\r\n│           │   ├── const_pointer_container.h\r\n│           │   ├── denormal.h\r\n│           │   ├── eigen_common_wrapper.h\r\n│           │   ├── exceptions.h\r\n│           │   ├── gsl_suppress.h\r\n│           │   ├── inlined_containers_fwd.h\r\n│           │   ├── inlined_containers.h\r\n│           │   ├── logging\r\n│           │   │   ├── capture.h\r\n│           │   │   ├── isink.h\r\n│           │   │   ├── logging.h\r\n│           │   │   ├── macros.h\r\n│           │   │   └── severity.h\r\n│           │   ├── make_string.h\r\n│           │   ├── optional.h\r\n│           │   ├── parse_string.h\r\n│           │   ├── profiler_common.h\r\n│           │   ├── span_utils.h\r\n│           │   ├── spin_pause.h\r\n│           │   ├── status.h\r\n│           │   └── string_helper.h\r\n│           ├── framework\r\n│           │   ├── allocator.h\r\n│           │   ├── alloc_kind.h\r\n│           │   ├── buffer_deleter.h\r\n│           │   ├── customregistry.h\r\n│           │   ├── data_types.h\r\n│           │   ├── data_types_internal.h\r\n│           │   ├── endian.h\r\n│           │   ├── execution_provider.h\r\n│           │   ├── fence.h\r\n│           │   ├── float16.h\r\n│           │   ├── framework_common.h\r\n│           │   ├── func_api.h\r\n│           │   ├── kernel_def_builder.h\r\n│           │   ├── kernel_registry.h\r\n│           │   ├── op_kernel_context.h\r\n│           │   ├── op_kernel.h\r\n│           │   ├── op_kernel_info.h\r\n│           │   ├── op_node_proto_helper.h\r\n│           │   ├── ortdevice.h\r\n│           │   ├── ortmemoryinfo.h\r\n│           │   ├── ort_value.h\r\n│           │   ├── provider_options.h\r\n│           │   ├── provider_options_utils.h\r\n│           │   ├── provider_shutdown.h\r\n│           │   ├── run_options.h\r\n│           │   ├── sparse_tensor.h\r\n│           │   ├── tensor.h\r\n│           │   ├── tensor_shape.h\r\n│           │   └── to_tensor_proto_element_type.h\r\n│           ├── graph\r\n│           │   ├── basic_types.h\r\n│           │   ├── constants.h\r\n│           │   ├── function.h\r\n│           │   ├── graph.h\r\n│           │   ├── graph_nodes.h\r\n│           │   ├── graph_viewer.h\r\n│           │   ├── indexed_sub_graph.h\r\n│           │   ├── node_arg.h\r\n│           │   └── schema_registry.h\r\n│           ├── optimizer\r\n│           │   ├── graph_transformer_config.h\r\n│           │   ├── graph_transformer.h\r\n│           │   ├── graph_transformer_level.h\r\n│           │   ├── graph_transformer_utils.h\r\n│           │   ├── rewrite_rule.h\r\n│           │   └── rule_based_graph_transformer.h\r\n│           ├── providers\r\n│           │   └── cpu\r\n│           │       └── cpu_provider_factory.h\r\n│           └── session\r\n│               ├── environment.h\r\n│               ├── experimental_onnxruntime_cxx_api.h\r\n│               ├── experimental_onnxruntime_cxx_inline.h\r\n│               ├── onnxruntime_c_api.h\r\n│               ├── onnxruntime_cxx_api.h\r\n│               ├── onnxruntime_cxx_inline.h\r\n│               ├── onnxruntime_run_options_config_keys.h\r\n│               ├── onnxruntime_session_options_config_keys.h\r\n│               └── snippets.dox\r\n└── lib\r\n    ├── libonnxruntime_common.a\r\n    ├── libonnxruntime_flatbuffers.a\r\n    ├── libonnxruntime_framework.a\r\n    ├── libonnxruntime_graph.a\r\n    ├── libonnxruntime_mlas.a\r\n    ├── libonnxruntime_optimizer.a\r\n    ├── libonnxruntime_providers.a\r\n    ├── libonnxruntime_providers_shared.so\r\n    ├── libonnxruntime_session.a\r\n    ├── libonnxruntime_util.a\r\n    └── pkgconfig\r\n        └── libonnxruntime.pc\r\n\r\n14 directories, 89 files\r\n\r\n我已经根据FastDeploy 0.3.0用onnxruntime 1.12.0，把onnxruntime的仓库切换到rel-1.12.0版本了。\r\n请教一下，onnxruntime相关库和文件的具体编译方法是什么？谢谢。\r\n仔细查看目录，原来后者已经生成了所需的头文件，还差so，不知道我可否通过改名，再编译？",
        "state": "closed",
        "user": "xxJian",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-20T10:48:21+00:00",
        "updated_at": "2024-02-06T04:25:42+00:00",
        "closed_at": "2024-02-06T04:25:41+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "leiqing1",
            "xxJian",
            "xxJian",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 398,
        "title": "Python CPU版本推理时报错：RuntimeError: FastDeploy initalized failed!",
        "body": "运行环境：Win10 64位，Python3.7.5，fastdeploy-python==0.3.0\r\n安装fastdeploy-python版本如下：\r\nhttps://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n[fastdeploy_python-0.3.0-cp37-cp37m-win_amd64.whl](https://bj.bcebos.com/fastdeploy/release/wheels/fastdeploy_python-0.3.0-cp37-cp37m-win_amd64.whl)\r\n——————————————————————————————\r\n运行代码(官方给的Demo Code)：\r\n`import cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\")\r\nim = cv2.imread(\"000000014439.jpg\")\r\nresult = model.predict(im.copy())\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n`\r\n——————————————————————————————\r\n报错信息：\r\nTraceback (most recent call last):\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 159, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed: 找不到指定的程序。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\360MoveData\\Users\\xxx\\Desktop\\FastDeploy_Paddle\\FastDeploy.py\", line 2, in <module>\r\n    import fastdeploy.vision as vision\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (ModelFormat, Backend, FDDataType, TensorInfo, Device,\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 161, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n——————————————————————————————\r\n安装对应包的版本如下：\r\nRequirement already satisfied: tqdm in d:\\python37\\lib\\site-packages (from fastdeploy-python==0.3.0) (4.63.0)\r\nRequirement already satisfied: wheel in d:\\python37\\lib\\site-packages (from fastdeploy-python==0.3.0) (0.37.1)\r\nRequirement already satisfied: requests in d:\\python37\\lib\\site-packages (from fastdeploy-python==0.3.0) (2.27.1)\r\nRequirement already satisfied: opencv-python in d:\\python37\\lib\\site-packages (from fastdeploy-python==0.3.0) (4.1.2.30)\r\nRequirement already satisfied: numpy in d:\\python37\\lib\\site-packages (from fastdeploy-python==0.3.0) (1.21.5)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in d:\\python37\\lib\\site-packages (from requests->fastdeploy-python==0.3.0) (2.0.12)\r\nRequirement already satisfied: idna<4,>=2.5 in d:\\python37\\lib\\site-packages (from requests->fastdeploy-python==0.3.0) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in d:\\python37\\lib\\site-packages (from requests->fastdeploy-python==0.3.0) (2021.10.8)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python37\\lib\\site-packages (from requests->fastdeploy-python==0.3.0) (1.26.8)\r\nRequirement already satisfied: colorama in d:\\python37\\lib\\site-packages (from tqdm->fastdeploy-python==0.3.0) (0.4.5)\r\n——————————————————————————————\r\n请问是否还有其他依赖项需要安装？\r\n\r\n",
        "state": "closed",
        "user": "stq054188",
        "closed_by": "stq054188",
        "created_at": "2022-10-19T10:32:16+00:00",
        "updated_at": "2022-10-19T12:28:46+00:00",
        "closed_at": "2022-10-19T12:28:46+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "stq054188",
            "DefTruth",
            "stq054188"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 407,
        "title": "windows10上开发，采用genymotion模拟器，用的easyedge arm sdk，调用sdk报错。",
        "body": "错误信息：\r\n],nativeLibraryDirectories=[/data/app/xxx.xxx-xxxx-Zr9QUg==/lib/x86, /system/lib, /system/vendor/lib]]] couldn't find \"libedge-infer.so\" .安装了Genymotion-ARM-Translation_for_9.0也不行",
        "state": "closed",
        "user": "vsharer",
        "closed_by": "vsharer",
        "created_at": "2022-10-20T14:16:45+00:00",
        "updated_at": "2022-10-23T05:03:34+00:00",
        "closed_at": "2022-10-23T05:03:34+00:00",
        "comments_count": [
            "vsharer",
            "jiangjiajun",
            "vsharer",
            "DefTruth",
            "vsharer"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 391,
        "title": "matting 是否支持背景全透明",
        "body": "我想要matting后的图片背景全部透明.\r\n![Untitled](https://user-images.githubusercontent.com/37092245/196347590-0436a6ad-3cdd-47de-bf50-51a3303798dd.png)\r\n\r\n尝试了使用一张256X256全透明的png图片,替换读取图片默认IMREAD_COLOR参数为IMREAD_UNCHANGED,在SwapBackgroundMatting的时候会有内存报错.",
        "state": "closed",
        "user": "publicforlijun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-18T05:59:03+00:00",
        "updated_at": "2024-02-06T04:25:43+00:00",
        "closed_at": "2024-02-06T04:25:43+00:00",
        "comments_count": [
            "DefTruth",
            "publicforlijun",
            "jiangjiajun",
            "evehal",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 414,
        "title": "使用android ocr sdk(就是这个EasyEdge-Android-paddle-ocr-arm-SDK），如何配置模型Infer的相关参数，有没有说明文档？",
        "body": "例如在paddle ocr里，我们通常这样配置：= PaddleOCR(use_angle_cls=True, lang=\"ch\",det_db_box_thresh =0.1, det_db_score_mode='slow') 。在android ocr sdk里，应该如何设置这些参数呢？（我想对应的初始化实例代码应该是：InferManager manager = new InferManager(context, config, serialNum)。",
        "state": "closed",
        "user": "vsharer",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-22T14:19:58+00:00",
        "updated_at": "2024-02-06T04:25:41+00:00",
        "closed_at": "2024-02-06T04:25:40+00:00",
        "comments_count": [
            "leiqing1",
            "vsharer",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 419,
        "title": "web_demo ,npm install 为啥找不到包",
        "body": "![image](https://user-images.githubusercontent.com/57437076/197441491-fa510999-1829-4473-a1fd-911c65f14d77.png)\r\n",
        "state": "closed",
        "user": "kkkkkkxser",
        "closed_by": "chenqianhe",
        "created_at": "2022-10-24T03:13:23+00:00",
        "updated_at": "2023-01-10T14:39:47+00:00",
        "closed_at": "2023-01-10T14:39:47+00:00",
        "comments_count": [
            "kkkkkkxser",
            "peterwangfeng",
            "chenqianhe",
            "chenqianhe"
        ],
        "labels": [
            "js"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 434,
        "title": "有支持PPYOLOE+",
        "body": "请问现在能支持PPYOLOE+吗",
        "state": "closed",
        "user": "jo-dean",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-26T02:16:02+00:00",
        "updated_at": "2024-02-06T04:25:40+00:00",
        "closed_at": "2024-02-06T04:25:39+00:00",
        "comments_count": [
            "jiangjiajun",
            "jo-dean",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 436,
        "title": "使用use_trt_backend()报错",
        "body": "主要代码：\r\noption = fd.RuntimeOption()\r\noption.use_gpu()\r\noption.use_trt_backend()\r\nstart = time.time()\r\nprint(\"time:\",start)\r\n\r\nmodel = vision.detection.YOLOv3(\"assets/model.pdmodel\",\r\n                                 \"assets/model.pdiparams\",\r\n                                 \"assets/infer_cfg.yml\",option)\r\n\r\n报错：\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(573)::CreateTrtEngineFromOnnx       Failed to parse ONNX model by TensorRT.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(265)::InitFromOnnx  Failed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime.cc(633)::CreateTrtBackend    Load model from Paddle failed while initliazing TrtBackend.\r\n已放弃 (核心已转储)",
        "state": "closed",
        "user": "fanyuanxiang",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-26T06:04:46+00:00",
        "updated_at": "2024-02-06T04:25:39+00:00",
        "closed_at": "2024-02-06T04:25:38+00:00",
        "comments_count": [
            "jiangjiajun",
            "fanyuanxiang",
            "fanyuanxiang",
            "jiangjiajun",
            "fanyuanxiang",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 442,
        "title": "doc error in benchmark/readme.md",
        "body": "[文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/benchmark/README.md) 提供的案例有小错误，漏写了`tgz`后缀\r\n原文：\r\n```\r\nwget https://bj.bcebos.com/paddlehub/fastdeploy/MobileNetV1_x0_25_infer.tgz\r\ntar -xvf MobileNetV1_x0_25_infer\r\n```\r\n应该为：\r\n```\r\nwget https://bj.bcebos.com/paddlehub/fastdeploy/MobileNetV1_x0_25_infer.tgz\r\ntar -xvf MobileNetV1_x0_25_infer.tgz\r\n```",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-27T01:16:28+00:00",
        "updated_at": "2024-02-06T04:25:38+00:00",
        "closed_at": "2024-02-06T04:25:37+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 446,
        "title": "可支持推理结果、绘制分开执行吗？？",
        "body": "fast deploy 可以支持推理和可视化结果分开吗；预测得到的DetectionResult 结果 ( result = model.predict(im.copy()) ) .可以支持序列化吗？ 然后在反序列化结果执行 vis_im = vision.vis_detection(im, result, score_threshold=0.5) 得到绘制检测对象的图片？\r\n\r\n\r\n使用pickle序列化会出现错误\r\nTypeError: can't pickle fastdeploy.libs.fastdeploy_main.vision.DetectionResult objects",
        "state": "closed",
        "user": "fanyuanxiang",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-27T08:32:03+00:00",
        "updated_at": "2024-02-06T04:25:37+00:00",
        "closed_at": "2024-02-06T04:25:37+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 457,
        "title": "python api 怎么修改推理结果的现况颜色",
        "body": "如题",
        "state": "closed",
        "user": "jiangbowen",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-29T14:52:29+00:00",
        "updated_at": "2024-02-06T04:25:36+00:00",
        "closed_at": "2024-02-06T04:25:36+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 452,
        "title": "PaddleSeg Python部署示例：推理完成后，不输出可视化结果",
        "body": "Win10 64位，Python3.8，fastdeploy-python==0.4.0，按照下面链接的步骤做实例分割推理，不输出可视化结果\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/segmentation/paddleseg/python\r\n——————————————————————————————————\r\n测试模型：Unet_cityscapes_without_argmax_infer\r\n运行指令：python infer.py --model Unet_cityscapes_without_argmax_infer --image 0.jpg --device cpu\r\n使用图片：\r\n![image](https://user-images.githubusercontent.com/15775782/198524232-b0e42bcd-a555-4975-81cb-0551fd3dbdb2.png)\r\n\r\n——————————————————————————————————\r\n报错信息：\r\nresult shape is: [480 640]\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 56, in <module>\r\n    vis_im = fd.vision.visualize.vis_segmentation(im, result)\r\n  File \"D:\\Python38\\lib\\site-packages\\fastdeploy\\vision\\visualize\\__init__.py\", line 35, in vis_segmentation\r\n    return C.vision.vis_segmentation(im_data, seg_result)\r\nTypeError: vis_segmentation(): incompatible function arguments. The following argument types are supported:\r\n    1. (arg0: numpy.ndarray, arg1: fastdeploy.libs.fastdeploy_main.vision.SegmentationResult, arg2: float) -> numpy.ndarray\r\n——————————————————————————————————\r\n    \r\n",
        "state": "closed",
        "user": "stq054188",
        "closed_by": "stq054188",
        "created_at": "2022-10-28T07:02:48+00:00",
        "updated_at": "2024-05-28T01:07:19+00:00",
        "closed_at": "2022-10-31T08:02:53+00:00",
        "comments_count": [
            "felixhjh",
            "felixhjh",
            "stq054188",
            "felixhjh",
            "stq054188",
            "zhouweic36"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 465,
        "title": "Yolov5-seg",
        "body": "Hi, thanks for your work. Is it possible to support yolov5-seg?\r\n\r\nThanks in advance",
        "state": "closed",
        "user": "sctrueew",
        "closed_by": "jiangjiajun",
        "created_at": "2022-10-31T17:24:27+00:00",
        "updated_at": "2024-02-06T04:25:35+00:00",
        "closed_at": "2024-02-06T04:25:35+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 466,
        "title": "paddle_inference.cmake 版本更新需要同步更新 paddle inference IPU安装包",
        "body": "[commit](https://github.com/PaddlePaddle/FastDeploy/commit/f2747133d62f55b1b418dbc4e2f72089a28d2189) 更新了paddle_inference的版本名称，但是上传的IPU安装包后缀是`2.4-dev1`, 需要麻烦fastdeploy的同事同步更新下云端IPU安装包命名，否则会下载失败。谢谢。",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-01T00:49:26+00:00",
        "updated_at": "2022-11-01T02:14:03+00:00",
        "closed_at": "2022-11-01T02:14:02+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 477,
        "title": "无法下载 FastDeploy macOS arm64 预编译库",
        "body": "[fastdeploy-osx-arm64-0.3.0.tgz](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-osx-arm64-0.4.0.tgz)",
        "state": "closed",
        "user": "albyho",
        "closed_by": "albyho",
        "created_at": "2022-11-01T15:34:23+00:00",
        "updated_at": "2022-11-09T05:16:28+00:00",
        "closed_at": "2022-11-09T05:16:28+00:00",
        "comments_count": [
            "jiangjiajun",
            "albyho",
            "jiangjiajun",
            "albyho"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 480,
        "title": "easyedge ocr的sdk,是否有文档？",
        "body": "api接口说明，以及ocr模型相关的参数有哪些 ，在哪里配置？这些是否提供说明？例如det_db_box_thresh =0.3,det_db_score_mode='slow', det_limit_type='min'，这些参数在服务版的ocr里是可以配置的，但在easyedge里如何配置？",
        "state": "closed",
        "user": "vsharer",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-02T08:05:45+00:00",
        "updated_at": "2024-02-06T04:25:33+00:00",
        "closed_at": "2024-02-06T04:25:33+00:00",
        "comments_count": [
            "leiqing1",
            "leiqing1",
            "vsharer",
            "leiqing1",
            "vsharer",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 483,
        "title": "如何将代码集成到我自己开发的app中呢？",
        "body": "如何将代码集成到我自己开发的app中呢，我想在自己的app中调用模型",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-02T10:43:12+00:00",
        "updated_at": "2024-02-06T04:25:32+00:00",
        "closed_at": "2024-02-06T04:25:32+00:00",
        "comments_count": [
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 501,
        "title": "paddleocr训练出来的三个模型，能否部署到同一个web服务上？",
        "body": "用paddleocr训练出来的三个模型，模型1为验证码识别，模型2为身份证件识别，模型3为动物识别\r\n能否多个模型用开启一个web服务（即只用一个端口8866），通过url路径参数区分模型？\r\n这种场景，怎么使用FastDeploy部署哈？\r\n",
        "state": "closed",
        "user": "waizao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-04T11:32:11+00:00",
        "updated_at": "2024-10-29T06:43:34+00:00",
        "closed_at": "2024-10-29T06:43:33+00:00",
        "comments_count": [
            "magicleo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 496,
        "title": "非常期待FastDeploy Runtime 的C++推理示例",
        "body": "非常期待FastDeploy Runtime 的C++推理示例\r\n",
        "state": "closed",
        "user": "hengji200",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-04T04:18:31+00:00",
        "updated_at": "2024-02-06T04:25:31+00:00",
        "closed_at": "2024-02-06T04:25:31+00:00",
        "comments_count": [
            "wjj19950828",
            "hengji200",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 498,
        "title": "easyedge我只下载到ocr sdk的编译过的包，请问其中的开源java代码在哪里有，能否提供一下链接，谢谢。",
        "body": null,
        "state": "closed",
        "user": "vsharer",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-04T07:02:43+00:00",
        "updated_at": "2024-02-06T04:25:30+00:00",
        "closed_at": "2024-02-06T04:25:30+00:00",
        "comments_count": [
            "vsharer",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 516,
        "title": "Error when using video ",
        "body": "Hi\r\nThanks for sharing this project. \r\nI want to use yolov5-face detection in **c++ on raspberry pi**. \r\nwhen I use it for **image**, everything is ok. but when I use it for **video**, I get the below error. \r\n\r\n```\r\n[INFO] fastdeploy/runtime.cc(481)::Init\tRuntime initialized with Backend::ORT in Device::CPU.\r\nError opening video stream or file\r\nterminate called after throwing an instance of 'cv::Exception'\r\n  what():  OpenCV(3.4.14) /jiangjiajun/opencv-3.4.14/modules/highgui/src/window.cpp:652: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\r\n\r\nAborted (core dumped)\r\n```\r\nI am working on raspberry pi with **ubuntu 22.04 64bit** and my Opencv version is **4.3.0** and  Opencv is built from source.\r\nI built Opencv by [link](https://qengineering.eu/install-opencv-4.3-on-raspberry-64-os.html).  \r\nI upgraded Opencv version to **4.5.5** but I got the above error, again.",
        "state": "closed",
        "user": "saeedkhanehgir",
        "closed_by": "saeedkhanehgir",
        "created_at": "2022-11-07T10:25:33+00:00",
        "updated_at": "2023-03-08T06:21:36+00:00",
        "closed_at": "2022-11-08T07:15:12+00:00",
        "comments_count": [
            "jiangjiajun",
            "saeedkhanehgir",
            "jiangjiajun",
            "saeedkhanehgir",
            "liuyingtao941212"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 519,
        "title": "RKNPU2 import issue",
        "body": "FastDeploy在设备无关的代码上import了设备相关的依赖`RKNPU2`:\r\n\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/f95654b7ef818781899f19f7d56f90b67f0f3b9d/python/fastdeploy/runtime.py#L19\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/f95654b7ef818781899f19f7d56f90b67f0f3b9d/python/fastdeploy/__init__.py#L22\r\n\r\n如果源码编译时，没有enable rknpu2,  `import fastdeploy`会报错：\r\n```\r\n(base) λ sgjur-pod006-1 python → λ git develop* → python infer.py --model ResNet50_vd_infer --image ILSVRC2012_val_00000010.jpeg --device cpu --topk 1\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"/root/miniconda3/lib/python3.7/site-packages/fastdeploy/__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\nImportError: cannot import name 'rknpu2' from 'fastdeploy.c_lib_wrap' (/root/miniconda3/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py)\r\n```\r\n\r\n建议将RKNPU包一层不要直接import进来。\r\n\r\n另外， `ModelFormat`被import了两次，应该也是个小问题：\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/f95654b7ef818781899f19f7d56f90b67f0f3b9d/python/fastdeploy/__init__.py#L20\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/f95654b7ef818781899f19f7d56f90b67f0f3b9d/python/fastdeploy/__init__.py#L28",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-08T03:28:20+00:00",
        "updated_at": "2022-11-08T07:02:47+00:00",
        "closed_at": "2022-11-08T07:02:47+00:00",
        "comments_count": [
            "czr-gc",
            "czr-gc",
            "czr-gc",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 521,
        "title": "编译后打开编译代码，引入都报错了",
        "body": "git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON\r\nmake -j12\r\nmake install\r\n\r\n按照以上执行后打开compiled_fastdeploy_sdk里的代码\r\n<img width=\"1475\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6427629/200478980-0d34cd8c-1e1f-4fa2-9dad-0c7b603cb6ed.png\">\r\n引入都报错的",
        "state": "closed",
        "user": "chfeizy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-08T04:59:40+00:00",
        "updated_at": "2024-03-19T06:40:00+00:00",
        "closed_at": "2024-03-19T06:40:00+00:00",
        "comments_count": [
            "chfeizy",
            "jiangjiajun",
            "chfeizy",
            "jiangjiajun",
            "chfeizy",
            "chfeizy",
            "chfeizy",
            "chfeizy",
            "wuhongsheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 527,
        "title": "paddlex inference_model 模型可以支持使用fastDeploy推理、部署吗 ？",
        "body": "paddlex v1.3.0\r\n\r\n├── model.yml\r\n├── __model__\r\n└──  __params__\r\n        \r\n==============\r\npaddlex v2.1.0\r\n\r\n├── model.pdiparams\r\n├── model.pdiparams.info\r\n├── model.pdmodel\r\n├── model.yml\r\n└── pipeline.yml\r\n",
        "state": "closed",
        "user": "fanyuanxiang",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-08T08:25:07+00:00",
        "updated_at": "2024-02-06T04:25:29+00:00",
        "closed_at": "2024-02-06T04:25:29+00:00",
        "comments_count": [
            "jiangjiajun",
            "fanyuanxiang",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 542,
        "title": "🔥🔥🔥【FastDeploy 部署三日直播课】11月09日～11月11日🔥🔥🔥",
        "body": "- 🔥 **2022.11.09 20:30～21:30，《覆盖云边端全场景，150+热门模型快速部署》。微信扫码报名**\r\n- 🔥 **2022.11.10 20:30～21:30，《瑞芯微、晶晨、恩智浦等10+AI硬件部署，直达产业落地》。微信扫码报名**\r\n- 🔥 **2022.11.10 19:00～20:00，《10+热门模型在RK3588、RK3568部署实战》。微信扫码报名**\r\n <div align=\"center\">\r\n  <img src=\"https://user-images.githubusercontent.com/54695910/200145290-d5565d18-6707-4a0b-a9af-85fd36d35d13.jpg\" width = \"120\" height = \"120\" />\r\n  </div>",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-09T08:18:42+00:00",
        "updated_at": "2022-11-15T12:22:01+00:00",
        "closed_at": "2022-11-15T12:22:01+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 535,
        "title": "using ppyolo_tiny in c++ with onnxruntime backend",
        "body": "Hi \r\nThanks for sharing this project. \r\nI trained ppyolo_tiny with[ this repo](https://github.com/PaddlePaddle/PaddleDetection).\r\nthen I converted trained model to ONNX with this [link](https://github-com.translate.goog/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/EXPORT_ONNX_MODEL.md?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en)\r\nin during converting model to inference model and ONNX model  I set **TestReader.inputs_def.image_shape=[3,416,416]**\r\n.\r\nafter that, I built FastDeploy with the modified bellow scripts.\r\n[ppyolo.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/9961246/ppyolo.cc.txt)\r\n[ppyolo.h.txt](https://github.com/PaddlePaddle/FastDeploy/files/9961249/ppyolo.h.txt)\r\n[ppyoloe.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/9961251/ppyoloe.cc.txt)\r\n[ppyoloe.h.txt](https://github.com/PaddlePaddle/FastDeploy/files/9961252/ppyoloe.h.txt)\r\n\r\n\r\n[infer_ppyolo.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/9961259/infer_ppyolo.cc.txt)\r\ninfer_ppyolo.cc compiled without problem but when I used it, I got the below error. \r\n\r\n```\r\n[ERROR] fastdeploy/vision/detection/ppdet/ppyoloe.cc(68)::BuildPreprocessPipelineFromConfig\tFailed to load yaml file , maybe you should check this file.\r\n[ERROR] fastdeploy/vision/detection/ppdet/ppyolo.cc(37)::Initialize\tFailed to build preprocess pipeline from configuration file.\r\nFailed to initialize.\r\n```\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "saeedkhanehgir",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-08T12:56:09+00:00",
        "updated_at": "2024-02-06T04:25:28+00:00",
        "closed_at": "2024-02-06T04:25:28+00:00",
        "comments_count": [
            "jiangjiajun",
            "saeedkhanehgir",
            "jiangjiajun",
            "jiangjiajun",
            "saeedkhanehgir",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 545,
        "title": "RaspberryPi",
        "body": "Hello!\r\nDoes FastDeploy supports RaspberryPi 3 ?\r\n",
        "state": "closed",
        "user": "sushidelivery",
        "closed_by": "sushidelivery",
        "created_at": "2022-11-09T08:40:47+00:00",
        "updated_at": "2022-11-09T09:36:27+00:00",
        "closed_at": "2022-11-09T09:36:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 536,
        "title": "v0.5.0 RobustVideoMatting demo 的 void MattingResult::Resize(int size) 报错",
        "body": "``` c++\r\nvoid MattingResult::Resize(int size) {\r\n  alpha.resize(size);\r\n  if (contain_foreground) {\r\n    FDASSERT((shape.size() == 3),\r\n             \"Please initial shape (h,w,c) before call Resize.\");\r\n    int c = static_cast<int>(shape[3]);\r\n    foreground.resize(size * c);\r\n  }\r\n}\r\n```\r\n\r\n`shape.size()`的值为 2 。\r\n\r\n----\r\n\r\n- 测试环境：macOS arm64\r\n- 编译 FastDeploy 的 cmake 命令：\r\n``` sh\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF \\\r\n         -DENABLE_OPENVINO_BACKEND=OFF \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON\r\n```\r\n- 测试命令：`./infer_demo rvm_mobilenetv3_fp32.onnx matting_input.jpg matting_bgr.jpg 0`",
        "state": "closed",
        "user": "albyho",
        "closed_by": "albyho",
        "created_at": "2022-11-08T13:41:34+00:00",
        "updated_at": "2022-11-08T14:30:10+00:00",
        "closed_at": "2022-11-08T14:30:09+00:00",
        "comments_count": [
            "albyho"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 537,
        "title": "PP-OCRv3识别中文字符乱码",
        "body": "运行examples\\vision\\ocr\\PP-OCRv3\\cpp中的infer.cc代码，可以检测到字符具体位置，但是输出的中文字符内容是乱码，数字正常。模型、字典都确认过没问题。",
        "state": "closed",
        "user": "sunyuchen1977",
        "closed_by": "sunyuchen1977",
        "created_at": "2022-11-09T02:24:15+00:00",
        "updated_at": "2023-01-13T06:54:20+00:00",
        "closed_at": "2022-11-11T05:42:18+00:00",
        "comments_count": [
            "yunyaoXYY",
            "sunyuchen1977",
            "yunyaoXYY",
            "sunyuchen1977",
            "GreenAvocado92"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 543,
        "title": "Typo in requirement.txt",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/30412cb59995d191bc63a22bd23e1bc6db2311e0/examples/text/ernie-3.0/python/requirements.txt#L1\r\n\r\n应该是：`faster_tokenizer`",
        "state": "closed",
        "user": "czr-gc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-09T08:28:29+00:00",
        "updated_at": "2022-11-09T09:24:06+00:00",
        "closed_at": "2022-11-09T09:24:06+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 551
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 549,
        "title": "您好，我在使用您们给的yolov7end2end_trt案例时，遇到问题CudaCast only support input INT64, output INT32。",
        "body": "您好，我在使用您们给的yolov7end2end_trt案例时，遇到问题CudaCast only support input INT64, output INT32。\r\n我的环境是windows，fastdeploy-gpu-python0.5,0，python3.8.5，cuda11.6，cudnn11.6\r\n使用的命令是python infer.py --model yolov7-end2end-trt-nms.onnx --image 001.jpg --device gpu --use_trt True\r\n报错信息是：\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> D:\\CUDA\\development\\bin\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(486)::fastdeploy::TrtBackend::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(572)::fastdeploy::TrtBackend::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/runtime.cc(475)::fastdeploy::Runtime::Init    Runtime initialized with Backend::TRT in Device::GPU.\r\n[ERROR] fastdeploy/function/cuda_cast.cu(41)::fastdeploy::CudaCast      CudaCast only support input INT64, output INT32.\r\n非常感谢!!\r\n",
        "state": "closed",
        "user": "TWK2022",
        "closed_by": "wang-xinyu",
        "created_at": "2022-11-10T02:51:28+00:00",
        "updated_at": "2022-11-10T09:09:13+00:00",
        "closed_at": "2022-11-10T09:09:13+00:00",
        "comments_count": [
            "wang-xinyu",
            "TWK2022",
            "wang-xinyu",
            "TWK2022",
            "wang-xinyu",
            "TWK2022",
            "wang-xinyu",
            "TWK2022"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 550,
        "title": "请问在jetson 上能运行benchmark?",
        "body": "当运行时报错\r\n[ERROR] fastdeploy/runtime.cc(319)::EnablePaddleToTrt   While using TrtBackend with EnablePaddleToTrt, require the FastDeploy is compiled with Paddle Inference Backend, please rebuild your FastDeploy.\r\n重新编译\r\nCMake Error at cmake/paddle_inference.cmake:67 (message):\r\n  Paddle Backend doesn't support linux aarch64 now.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:212 (include)\r\n发下使用trt backend 很慢 在jetson上查看jtop 发现gpu占用很低 set_trt_input_shape与set_trt_cache_file均已经设置 是我设置的问题吗？应该怎样解决",
        "state": "closed",
        "user": "tomjeans",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-10T03:22:57+00:00",
        "updated_at": "2024-02-06T04:25:27+00:00",
        "closed_at": "2024-02-06T04:25:27+00:00",
        "comments_count": [
            "wjj19950828",
            "tomjeans",
            "wjj19950828",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 553,
        "title": "PPYOLO不支持TRT后端",
        "body": "使用命令运行`python infer_ppyolo.py --model_dir ppyolo_mbv3_small_coco --image 000000014439.jpg --device gpu --use_trt  True`\r\n发现\r\n`PaddleDetection/PPYOLO is not supported with backend Backend::TRT.`\r\n\r\n而且cpu推理时间比gpu推理时间快\r\n",
        "state": "closed",
        "user": "jo-dean",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-10T07:01:54+00:00",
        "updated_at": "2024-02-06T04:25:26+00:00",
        "closed_at": "2024-02-06T04:25:26+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 559,
        "title": "Dynamic Batch Inference",
        "body": "I have exported my yolov7 model with dynamic axes as tensorrt.Engine runs fine on single batch but I want to be able to make inference with multiple images.How can I achive this ? How should I feed my images to the model.predict method ? I have tried using ```model.predict(np.array([im.copy(),im.copy()]))```  but I get errors.\r\nAny help would be much appreciated.",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-10T08:52:54+00:00",
        "updated_at": "2024-06-18T06:41:06+00:00",
        "closed_at": "2024-06-18T06:41:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "heliqi",
            "wjj19950828",
            "UygarUsta99",
            "wjj19950828",
            "miknyko",
            "UygarUsta99",
            "wjj19950828",
            "wjj19950828",
            "TWK2022",
            "wjj19950828",
            "TWK2022",
            "wjj19950828",
            "heliqi",
            "TWK2022",
            "UygarUsta99",
            "jiangjiajun",
            "UygarUsta99",
            "clveryang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 589,
        "title": "[RobustVideoMatting]libc++abi: terminating with uncaught exception of type std::length_error: vector",
        "body": "FastDeploy 版本：预编译版 0.6.0\r\nOS：macOS 12.6 (Arm64)\r\n语言：C++\r\n模型：rvm_mobilenetv3_fp32.onnx\r\n推理方式：CPU\r\n\r\n编译运行 infer_demo，并使用固定的参数(CpuInfer) 。测试发现，经常出现这个异常(并不是每次)。\r\n请问大致会是什么原因？\r\n",
        "state": "closed",
        "user": "albyho",
        "closed_by": "albyho",
        "created_at": "2022-11-14T13:49:14+00:00",
        "updated_at": "2022-11-15T09:20:59+00:00",
        "closed_at": "2022-11-15T09:20:58+00:00",
        "comments_count": [
            "albyho",
            "wjj19950828",
            "albyho"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 561,
        "title": "Due to the unsupported operators, the conversion is aborted.",
        "body": "/home/aistudio/work/FastDeploy/examples/vision/detection/paddledetection/quantize/python\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[Paddle2ONNX] [Info] The Paddle model is a quantized model. \r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including fake_channel_wise_dequantize_max_abs,fake_quantize_moving_average_abs_max,masked_select,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.\r\n\r\n\r\n 在CPU上使用ONNX Runtime推理量化模型\r\n!python infer_ppyoloe.py --model /home/aistudio/work/PaddleDetection/deploy/auto_compression/output --image=/home/aistudio/work/data_coco/test/2022-09-201700_85.jpg --device cpu --backend ort\r\n出现以上报错，请问这个是什么问题呢？",
        "state": "closed",
        "user": "weijingsu",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-10T10:47:59+00:00",
        "updated_at": "2024-02-06T04:25:25+00:00",
        "closed_at": "2024-02-06T04:25:25+00:00",
        "comments_count": [
            "jiangjiajun",
            "weijingsu",
            "yunyaoXYY",
            "jiangjiajun",
            "weijingsu",
            "yunyaoXYY",
            "weijingsu",
            "weijingsu",
            "jiangjiajun",
            "weijingsu",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 595,
        "title": "RKNPU2目前已知存在问题合集",
        "body": "12-08\r\n* 没有分类模型的支持\r\n* RKYOLO系列模型后处理需要优化\r\n* model_format修改为根据模型后缀自动判断",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-11-15T08:00:57+00:00",
        "updated_at": "2022-12-21T03:14:23+00:00",
        "closed_at": "2022-12-21T03:14:23+00:00",
        "comments_count": [
            "Lebhoryi",
            "Zheng-Bicheng",
            "Eric0039"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 596,
        "title": "FastDeploy部署模型时运行出错",
        "body": "FastDeploy部署模型时运行出错，提示未加载kernelbase.pdb",
        "state": "closed",
        "user": "colinchou2020",
        "closed_by": "heliqi",
        "created_at": "2022-11-15T08:37:03+00:00",
        "updated_at": "2024-02-27T08:05:29+00:00",
        "closed_at": "2024-02-27T08:05:29+00:00",
        "comments_count": [
            "jiangjiajun",
            "chenqianhe",
            "colinchou2020",
            "chenqianhe",
            "colinchou2020",
            "jiangjiajun",
            "colinchou2020",
            "colinchou2020",
            "colinchou2020",
            "colinchou2020",
            "jiangjiajun",
            "colinchou2020",
            "colinchou2020",
            "jiangjiajun",
            "qinlihaoWork",
            "jiangjiajun",
            "qinlihaoWork",
            "geoyee",
            "franciszq",
            "franciszq",
            "franciszq"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 597,
        "title": "Web Demo替换模型失败",
        "body": "按照教程（https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/application/js/WebDemo.md ）来，使用 npm run dev 后成功运行demo，OCR中也可以正常识别。但在模型替换时，按照步骤3给的例子：\r\nawait ocr.init();\r\n修改为\r\nawait ocr.init({modelPath: \"https://js-models.bj.bcebos.com/PaddleOCR/PP-OCRv3/ch_PP-OCRv3_det_infer_js_960/model.json\"});\r\n替换模型后，再次 npm run dev 后，页面就一直停留在“正在加载模型”了。\r\n\r\n检查网页元素，正常时det和rec的model.json都读取过来了；替换模型后，发现只有rec的model.json文件读取过来了。\r\n\r\n是哪里还需要操作么，还是我的操作不对？",
        "state": "closed",
        "user": "dium6i",
        "closed_by": "chenqianhe",
        "created_at": "2022-11-15T09:15:37+00:00",
        "updated_at": "2025-03-24T12:51:51+00:00",
        "closed_at": "2022-11-29T03:33:22+00:00",
        "comments_count": [
            "chenqianhe",
            "chenqianhe",
            "dium6i",
            "chenqianhe",
            "chenqianhe",
            "dium6i",
            "chenqianhe",
            "dium6i",
            "LDOUBLEV",
            "dium6i",
            "chenqianhe",
            "dium6i",
            "chenqianhe",
            "dium6i",
            "yinqinggong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 601,
        "title": "FastDeploy与官方的easyedge的demo，同样模型性能有明显差距",
        "body": "同一部手机，FastDeploy的性能非常低，不知道是不是优化没做好",
        "state": "closed",
        "user": "denonzhu",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-15T13:01:11+00:00",
        "updated_at": "2024-02-06T04:25:24+00:00",
        "closed_at": "2024-02-06T04:25:24+00:00",
        "comments_count": [
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "leiqing1",
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "jiangjiajun"
        ],
        "labels": [
            "Performance"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 602,
        "title": "咨询：PP-yoloe-r 这个模型能用fastdeploy部署在安卓上吗",
        "body": "PP-YOLOE-R是一个高效的单阶段Anchor-free旋转框检测模型https://github.com/PaddlePaddle/PaddleDetection/blob/develop/configs/rotate/ppyoloe_r/README.md",
        "state": "closed",
        "user": "denonzhu",
        "closed_by": "denonzhu",
        "created_at": "2022-11-16T00:53:13+00:00",
        "updated_at": "2022-11-16T03:15:38+00:00",
        "closed_at": "2022-11-16T03:15:30+00:00",
        "comments_count": [
            "jiangjiajun",
            "denonzhu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 603,
        "title": "使用TRT对gpu有要求吗？除了版本",
        "body": "采用Nvidia  T600\r\n\r\nWed Nov 16 10:32:04 2022\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.04       Driver Version: 530.04       CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA T600        WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 38%   45C    P5    N/A /  41W |   3652MiB /  4096MiB |     21%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n按要求配置python的运行环境 0.6.0，后端使用trt,提示如下错误，似乎是内存或参数错误。\r\n请问有什么需要注意的吗？\r\n\r\n[WARN][Paddle2ONNX] [multiclass_nms3: multiclass_nms3_0.tmp_1] Paramter nms_top_k:10000 is exceed limit in TensorRT BatchedNMS plugin, will force to 4096.\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(616)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx     Cannot build engine right now, because there's dynamic input shape exists, list as below,\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(620)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx     Input 0: TensorInfo(name: image, shape: [-1, 3, 640, 640], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(620)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx     Input 1: TensorInfo(name: scale_factor, shape: [-1, 2], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(622)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx     FastDeploy will build the engine while inference with input data, and will also collect the input shape range information. You should be noticed that FastDeploy will rebuild the engine while new input shape is out of the collected shape range, this may bring some time consuming problem, refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/runtime.cc(487)::fastdeploy::Runtime::Init    Runtime initialized with Backend::TRT in Device::GPU.\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] input name: image, shape: [1, 3, 640, 640], The shape range before: min_shape=[-1, 3, 640, 640], max_shape=[-1, 3, 640, 640].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 640, 640], max_shape=[1, 3, 640, 640].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] input name: scale_factor, shape: [1, 2], The shape range before: min_shape=[-1, 2], max_shape=[-1, 2].  [WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] The updated shape range now: min_shape=[1, 2], max_shape=[1, 2].[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(278)::fastdeploy::TrtBackend::Infer       TensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(466)::fastdeploy::TrtBackend::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::fastdeploy::FDTrtLogger::log  1: [virtualMemoryBuffer.cpp::nvinfer1::StdVirtualMemoryBufferImpl::resizePhysical::131] Error Code 1: Cuda Driver (invalid argument)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::fastdeploy::FDTrtLogger::log  2: [globWriter.cpp::nvinfer1::builder::HybridGlobWriter::HybridGlobWriter::383] Error Code 2: OutOfMemory (no further information)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::fastdeploy::FDTrtLogger::log  2: [builder.cpp::nvinfer1::builder::Builder::buildSerializedNetwork::636] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(529)::fastdeploy::TrtBackend::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(369)::fastdeploy::TrtBackend::SetInputs     TRTBackend SetInputs not find name:image",
        "state": "closed",
        "user": "GuoZhiyong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-16T02:34:22+00:00",
        "updated_at": "2024-05-14T06:42:06+00:00",
        "closed_at": "2024-05-14T06:42:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "GuoZhiyong",
            "jiangjiajun",
            "GeT-RiGhTTT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 604,
        "title": "Runtime initialized with Backend::PDINFER in Device::CPU ,为什么是显示CPU",
        "body": "[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[INFO] fastdeploy/runtime.cc(498)::fastdeploy::Runtime::Init    Runtime initialized with Backend::PDINFER in Device::CPU.\r\n\r\n\r\n不是应该GPU检测吗\r\n这里怎么显示 Device::CPU.\r\n代码中应该加入什么  设置为GPU检测的\r\n\r\n安装是\r\n`pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html`\r\n没有编译安装",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-11-16T02:51:59+00:00",
        "updated_at": "2022-11-18T13:50:32+00:00",
        "closed_at": "2022-11-18T13:50:31+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 606,
        "title": "Segmentation fault",
        "body": "platform：wsl2 Ubuntu20.0.4\r\npython3.9\r\nonnx==1.12.0\r\n安装方式\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nyolov5-5.0\r\n导出方式\r\npython export.py --weights runs/train/exp33/weights/best.pt  --include onnx\r\n运行命令\r\n root@computer:/mnt/c/Users/Administrator/projects/FastDeploy-develop/examples/vision/detection/yolov5/python# python infer.py --model best.onnx --image 3-3.png --device cpu\r\n\r\n出现错误\r\n`\r\n[INFO] fastdeploy/runtime.cc(513)::Init Runtime initialized with Backend::OPENVINO in Device::CPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\nSegmentation fault\r\n`\r\n\r\n",
        "state": "closed",
        "user": "JackonLiu",
        "closed_by": "JackonLiu",
        "created_at": "2022-11-16T03:58:30+00:00",
        "updated_at": "2022-11-16T05:44:49+00:00",
        "closed_at": "2022-11-16T05:44:49+00:00",
        "comments_count": [
            "JackonLiu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 607,
        "title": "测试了rvm和scrfd都比lite.ai.toolkit里的慢",
        "body": "rvm在这里用时大概 660ms到750ms\r\nlite.ai.toolkit 大概 115ms到230ms\r\n\r\nscrfd这里大概11ms到30ms\r\nlite.ai.toolkit 大概 8ms到20ms\r\n\r\n环境是MacBook Pro 2.6 GHz 六核Intel Core i7",
        "state": "closed",
        "user": "chfeizy",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-16T05:22:25+00:00",
        "updated_at": "2024-02-06T04:25:23+00:00",
        "closed_at": "2024-02-06T04:25:23+00:00",
        "comments_count": [
            "DefTruth",
            "chfeizy",
            "DefTruth",
            "chfeizy",
            "DefTruth",
            "wjj19950828",
            "chfeizy",
            "wjj19950828",
            "ziqi-jin",
            "chfeizy",
            "chfeizy",
            "wjj19950828",
            "chfeizy",
            "wjj19950828",
            "chfeizy",
            "jiangjiajun"
        ],
        "labels": [
            "Performance"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 610,
        "title": "vision.vis_detection 可以自定义颜色吗",
        "body": "vision.vis_detection 可以自定义颜色吗\r\n\r\n比如我要全部统一红色",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-16T08:05:58+00:00",
        "updated_at": "2024-02-06T04:25:22+00:00",
        "closed_at": "2024-02-06T04:25:22+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 612,
        "title": "如何获取 result 里面的参数",
        "body": "```\r\n    result = model.predict(img_array)\r\n    print(\"result\",result[0])\r\n```\r\n\r\n[xmin, ymin, xmax, ymax, score, label_id]\r\n想要单独获取其中的参数\r\n\r\n比如 score\r\nresult[4]\r\n\r\n但是报错\r\n\r\n不知道怎么样获取",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-11-16T08:29:22+00:00",
        "updated_at": "2022-11-16T09:25:33+00:00",
        "closed_at": "2022-11-16T09:25:33+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 621,
        "title": "OCR 使用tensorrt 推理报错 ",
        "body": "python infer.py --det_model ./ch_PP-OCRv3_det_infer --cls_model ./ch_ppocr_mobile_v2.0_cls_infer --rec_model ./ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 22.jpg --device gpu --backend trt\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(466)::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   4: [graphShapeAnalyzer.cpp::analyzeShapes::1294] Error Code 4: Miscellaneous (IElementWiseLayer p2o.Add.62: broadcast dimensions must be conformable)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   2: [builder.cpp::buildSerializedNetwork::636] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(529)::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(636)::CreateTrtEngineFromOnnx       Failed to build tensorrt engine.\r\n[INFO] fastdeploy/runtime.cc(487)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(466)::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(552)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/runtime.cc(487)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(466)::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(552)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/runtime.cc(487)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update     [New Shape Out of Range] input name: x, shape: [1, 3, 544, 608], The shape range before: min_shape=[1, 3, 48, 10], max_shape=[1, 3, 48, 2304].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update     [New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 48, 10], max_shape=[1, 3, 544, 2304].\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(278)::Infer       TensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(466)::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   4: [shapeCompiler.cpp::evaluateShapeChecks::911] Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: condition '==' violated. 4 != 3. p2o.Add.62: dimensions not compatible for elementwise)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   2: [builder.cpp::buildSerializedNetwork::636] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(529)::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(369)::SetInputs     TRTBackend SetInputs not find name:x\r\n已放弃 (核心已转储)",
        "state": "closed",
        "user": "forword-1234",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-17T09:18:37+00:00",
        "updated_at": "2024-05-14T06:42:07+00:00",
        "closed_at": "2024-05-14T06:42:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "yunyaoXYY",
            "forword-1234",
            "forword-1234",
            "yunyaoXYY",
            "forword-1234",
            "yunyaoXYY",
            "forword-1234",
            "namemzy",
            "yunyaoXYY",
            "namemzy",
            "zhangzhidaSunny",
            "zhangzhidaSunny",
            "taojishou"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 626,
        "title": "[RobustVideoMatting]example 中 Infer.cc 调用 fastdeploy::vision::SwapBackground 方法应该使用 res.foreground 而非原图进行 blend",
        "body": "如题",
        "state": "closed",
        "user": "albyho",
        "closed_by": "albyho",
        "created_at": "2022-11-17T16:56:34+00:00",
        "updated_at": "2022-11-22T07:04:32+00:00",
        "closed_at": "2022-11-22T07:04:32+00:00",
        "comments_count": [
            "DefTruth",
            "albyho"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 629,
        "title": "vision.detection.MaskRCNN 如何进行裁剪图片",
        "body": "想要把识别出来的  裁剪图片出来\r\n\r\n```\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/api/vision_results/segmentation_result.md\r\n\r\nfastdeploy.vision.SegmentationResult\r\n\r\nlabel_map(list of int): 成员变量，表示单张图片每个像素点的分割类别\r\nscore_map(list of float): 成员变量，与label_map一一对应的所预测的分割类别概率值(当导出模型时指定--output_op argmax)或者经过softmax归一化化后的概率值(当导出模型时指定--output_op softmax或者导出模型时指定--output_op none同时模型初始化的时候设置模型[类成员属性](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/segmentation/paddleseg/python)apply_softmax=true)\r\nshape(list of int): 成员变量，表示输出图片的shape，为H*W\r\n```\r\n\r\nPaddleX中 我看有可以解析mask的\r\n但是fastdeploy 不知道怎么实现\r\n想要把识别出来的  裁剪图片出来\r\n\r\n或者不能用语义分割来裁剪\r\n要用实例分割?",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-18T03:24:49+00:00",
        "updated_at": "2024-07-16T06:40:51+00:00",
        "closed_at": "2024-07-16T06:40:51+00:00",
        "comments_count": [
            "guoyunqingyue",
            "nefusmzj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 634,
        "title": "AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals'",
        "body": "```\r\n python tools/export_model.py -c configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.yml --output_dir=F:/00Biaozhu/OK_weights  -o weights=F:/00Biaozhu/outputFG/mask_rcnn_r50_fpn_1x_coco/10.pdparams\r\n\r\n\r\n\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\r\n  'nearest': Image.NEAREST,\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\r\n  'bilinear': Image.BILINEAR,\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\r\n  'bicubic': Image.BICUBIC,\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\r\n  'box': Image.BOX,\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\r\n  'lanczos': Image.LANCZOS,\r\nD:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\vision\\transforms\\functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\r\n  'hamming': Image.HAMMING\r\n[11/18 21:27:18] ppdet.utils.checkpoint INFO: Finish loading model weights: F:/00Biaozhu/Guan20001008/01_CP/outputFG/mask_rcnn_r50_fpn_1x_coco/model_final.pdparams\r\nloading annotations into memory...\r\nDone (t=0.01s)\r\ncreating index...\r\nindex created!\r\n[11/18 21:27:18] ppdet.engine INFO: Export inference config file to F:/00Biaozhu/Guan20001008/01_CP/outputFG/OK_weights\\mask_rcnn_r50_fpn_1x_coco\\infer_cfg.yml\r\nTraceback (most recent call last):\r\n  File \"tools/export_model.py\", line 108, in <module>\r\n    main()\r\n  File \"tools/export_model.py\", line 104, in main\r\n    run(FLAGS, cfg)\r\n  File \"tools/export_model.py\", line 73, in run\r\n    trainer.export(FLAGS.output_dir)\r\n  File \"D:\\0SDXX\\PaddleDetection\\ppdet\\engine\\trainer.py\", line 1060, in export\r\n    save_dir)\r\n  File \"D:\\0SDXX\\PaddleDetection\\ppdet\\engine\\trainer.py\", line 1019, in _get_infer_cfg_and_input_spec\r\n    input_spec, static_model.forward.main_program,\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 563, in main_program\r\n    concrete_program = self.concrete_program\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 479, in concrete_program\r\n    return self.concrete_program_specify_input_spec(input_spec=None)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 517, in concrete_program_specify_input_spec\r\n    *desired_input_spec)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 427, in get_concrete_program\r\n    concrete_program, partial_program_layer = self._program_cache[cache_key]\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 723, in __getitem__\r\n    self._caches[item] = self._build_once(item)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 714, in _build_once\r\n    **cache_key.kwargs)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\decorator.py\", line 232, in fun\r\n    return caller(func, *(extras + args), **kw)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\wrapped_decorator.py\", line 25, in __impl__\r\n    return wrapped_func(*args, **kwargs)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\base.py\", line 51, in __impl__\r\n    return func(*args, **kwargs)\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 668, in from_func_spec\r\n    error_data.raise_new_exception()\r\n  File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\error.py\", line 336, in raise_new_exception\r\n    six.exec_(\"raise new_exception from None\")\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: In transformed code:\r\n\r\n    File \"D:\\0SDXX\\PaddleDetection\\ppdet\\modeling\\architectures\\mask_rcnn.py\", line 133, in get_pred\r\n        bbox_pred, bbox_num, mask_pred = self._forward()\r\n    File \"D:\\0SDXX\\PaddleDetection\\ppdet\\modeling\\architectures\\mask_rcnn.py\", line 102, in _forward\r\n        else:\r\n    File \"D:\\0SDXX\\PaddleDetection\\ppdet\\modeling\\proposal_generator\\rpn_head.py\", line 141, in forward\r\n        rois, rois_num = self._gen_proposal(scores, deltas, anchors, inputs)\r\n    File \"D:\\0SDXX\\PaddleDetection\\ppdet\\modeling\\proposal_generator\\rpn_head.py\", line 198, in _gen_proposal\r\n        # Generate proposals for each level and each batch.\r\n        # Discard batch-computing to avoid sorting bbox cross different batches.\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        for i in range(batch_size):\r\n            rpn_rois_list = []\r\n\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\convert_call_func.py\", line 258, in convert_call\r\n        converted_call = convert_to_static(call_func)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 140, in convert_to_static\r\n        static_func = _FUNCTION_CACHE.convert_with_cache(function)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 77, in convert_with_cache\r\n        static_func = self._convert(func)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\program_translator.py\", line 115, in _convert\r\n        root_wrapper = self._dygraph_to_static.get_static_ast(root)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\ast_transformer.py\", line 58, in get_static_ast\r\n        self.static_analysis_visitor = StaticAnalysisVisitor(root)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 207, in __init__\r\n        self.run(ast_root)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 215, in run\r\n        self.dfs_visit(ast_root)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 241, in dfs_visit\r\n        func_type = self.dfs_visit(child)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 244, in dfs_visit\r\n        self.dfs_visit(child)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 244, in dfs_visit\r\n        self.dfs_visit(child)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 244, in dfs_visit\r\n        self.dfs_visit(child)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 247, in dfs_visit\r\n        cur_wrapper.node_var_type = self._get_node_var_type(cur_wrapper)\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\static_analysis.py\", line 359, in _get_node_var_type\r\n        if is_dygraph_api(node):\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\utils.py\", line 201, in is_dygraph_api\r\n        if is_api_in_module(node, DYGRAPH_TO_STATIC_MODULE_PREFIX):\r\n    File \"D:\\anaconda31114\\envs\\PaddleDetection\\lib\\site-packages\\paddle\\fluid\\dygraph\\dygraph_to_static\\utils.py\", line 193, in is_api_in_module\r\n        module_prefix))\r\n    File \"<string>\", line 1, in <module>\r\n\r\n\r\n    AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals'\r\n```\r\n\r\n除了改数据集\r\n其他的都没动",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-11-18T13:29:23+00:00",
        "updated_at": "2022-11-18T13:29:48+00:00",
        "closed_at": "2022-11-18T13:29:48+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 643,
        "title": "Python API可视化增加标签展示",
        "body": "platform：windows10\r\npython3.8\r\nonnx==1.12.0\r\n安装方式\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nyolov5-5.0\r\n导出方式\r\n`python export.py --weights runs/train/exp33/weights/best.pt --include onnx`\r\n\r\n       result = model.predict(im.copy())\r\n        print(result)\r\n        print(frame_index)\r\n        t3 = time.time()\r\n        print('detect frames: %.4f frames/s' % (1 / (t3 - t2)))\r\n        # 预测结果可视化\r\n        label_ids = result.label_ids\r\n        if label_ids:\r\n            for i in range(len(label_ids)):\r\n                if label_ids[i] == 0:\r\n                    label_ids[i] = 'hat'\r\n                else:\r\n                    label_ids[i] = 'person'\r\n        result.label_ids = label_ids\r\n        if result.boxes:\r\n            vis_im = fd.vision.vis_detection(im, result,score_threshold=0.7)\r\n\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\r\n[INFO] fastdeploy/runtime.cc(528)::fastdeploy::Runtime::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n(1440, 2560, 3)\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n219.655457,190.753174, 591.998047, 611.119873, 0.911301, 0\r\n\r\n0\r\ndetect frames: 2.4705 frames/s\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2020.1\\plugins\\python\\helpers\\pydev\\pydevd.py\", line 1438, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2020.1\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/Administrator/projects/FastDeploy-develop/examples/vision/detection/yolov5/python/infer_video.py\", line 68, in <module>\r\n    result.label_ids = label_ids\r\nTypeError: (): incompatible function arguments. The following argument types are supported:\r\n    1. (self: fastdeploy.libs.fastdeploy_main.vision.DetectionResult, arg0: List[int]) -> None\r\n\r\nInvoked with: DetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n219.655457,190.753174, 591.998047, 611.119873, 0.911301, 0\r\n, ['hat']\r\n\r\n我知道错误的原因是不能直接换成字符串，但是希望之后能增加字符串的方式，不然很多场景检测出来的图片不能用！！！\r\n\r\n",
        "state": "closed",
        "user": "JackonLiu",
        "closed_by": "JackonLiu",
        "created_at": "2022-11-21T06:28:42+00:00",
        "updated_at": "2022-11-28T12:35:52+00:00",
        "closed_at": "2022-11-21T07:37:38+00:00",
        "comments_count": [
            "jiangjiajun",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 650,
        "title": "[模型支持] 期望能够支持paddleOCR 中的版面分析模型",
        "body": "Hardware: x86 CPU \r\nOS: Linux\r\nModel name: picodet_lcnet_x1_0_fgd_layout\r\nModel link: [picodet_lcnet_x1_0_fgd_layout](https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppstructure/docs/models_list.md)\r\nDetailed Description: Example: 还没发现本框架有支持版面分析模型，有这方面需求。能否以picodet_lcnet_x1_0_fgd_layout模型为例，提供一个版面分析模型deploy.\r\n",
        "state": "closed",
        "user": "wmpscc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-21T13:49:14+00:00",
        "updated_at": "2024-02-06T04:25:21+00:00",
        "closed_at": "2024-02-06T04:25:21+00:00",
        "comments_count": [
            "jiangjiajun",
            "wmpscc",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 637,
        "title": "[RobustVideoMatting]推理速度比官方慢的问题",
        "body": "环境：\r\nOS: macOS 12.6 (Apple M1)\r\n测试集：仅一张 1440*960 的 jpg 人像图片\r\n\r\n----\r\n\r\nFastDeploy C++ infer.cc 代码修改：\r\n``` C++\r\n  for(auto i = 0; i < 100; i++)\r\n  {\r\n    clock_t start,end;\r\n    start = clock();\r\n\r\n    if (!model.Predict(&im, &res)) {\r\n      std::cerr << \"Failed to predict.\" << std::endl;\r\n      return;\r\n    }\r\n\r\n    end = clock();\r\n    std::cout << \"time = \" << double(end - start) / CLOCKS_PER_SEC << std::endl;\r\n  }\r\n\r\n```\r\n\r\n测试脚本：\r\n\r\n``` sh\r\n./infer_demo rvm_mobilenetv3_fp32.onnx 1.jpg bg_1440x960.jpg 0\r\n```\r\n\r\n模型：rvm_mobilenetv3_fp32.onnx\r\nCPU 使用率：250% 左右\r\n耗时：400ms 以上\r\n\r\n----\r\n\r\nRobustVideoMatting 官方 inference.py 代码修改：\r\n\r\n``` python\r\nimport time\r\nfor x in range(100):\r\n    start = time.perf_counter()\r\n    fgr, pha, *rec = model(src, *rec, downsample_ratio)\r\n    end = time.perf_counter()\r\n    print('time = {:.4f}s'.format(end-start))\r\n```\r\n\r\n测试脚本：\r\n\r\n``` sh\r\npython inference.py \\\r\n    --variant mobilenetv3 \\\r\n    --checkpoint \"rvm_mobilenetv3.pth\" \\\r\n    --device cpu \\\r\n    --downsample-ratio 0.25 \\\r\n    --input-source \"input\" \\\r\n    --output-type \"png_sequence\" \\\r\n    --output-composition \"composition\" \\\r\n    --output-alpha \"alpha\" \\\r\n    --output-foreground \"foreground\"\r\n```\r\n\r\n模型：rvm_mobilenetv3.pth\r\nCPU 使用率：250% 左右（用 FastDeploy 差不多）\r\n耗时: 180 ~ 200ms\r\n\r\n> input 目录只存放了一张图片 1440*960  的人像图片。\r\n",
        "state": "closed",
        "user": "albyho",
        "closed_by": "albyho",
        "created_at": "2022-11-20T08:47:52+00:00",
        "updated_at": "2022-11-22T04:03:27+00:00",
        "closed_at": "2022-11-22T04:03:27+00:00",
        "comments_count": [
            "wjj19950828",
            "albyho"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 664,
        "title": "fd安卓部署为啥不能支持paddlite 导出的nb模型，这样性能会不会更好点",
        "body": null,
        "state": "closed",
        "user": "denonzhu",
        "closed_by": "denonzhu",
        "created_at": "2022-11-22T11:48:46+00:00",
        "updated_at": "2022-11-23T01:05:18+00:00",
        "closed_at": "2022-11-23T01:05:18+00:00",
        "comments_count": [
            "jiangjiajun",
            "denonzhu",
            "jiangjiajun",
            "denonzhu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 666,
        "title": "deploying ppocr_v2 with only recognition model",
        "body": "Hi \r\nThanks for sharing this project.\r\nI want to change **ppocr_v2** to get **only the recognition** model (for conditions where we have another detection model such as Yolo and I would like to use only the recognition ppocr model). I am working on the **0.7.0** release. I use this [instruction](https://github.com/PaddlePaddle/FastDeploy/blob/release/0.7.0/docs/en/faq/develop_a_new_model.md) for this work.\r\nAccording to this instruction, I modified  **ppocr_v2.h** and **ppocr_v2.cc**  to **ppocr_v4.h** and **ppocr_v4.cc** ( arbitary name) \r\nI put these two file in **`.../FastDeploy/fastdeploy/vision/ocr`** address. then I add **`#include \"fastdeploy/vision/ocr/ppocr/ppocr_v4.h\"`** to **vision.h**. then I changed **ppocr_pybind.cc** file in **`.../FastDeploy/fastdeploy/vision/ocr/ppocr`** address and **ocr_pybind.cc** file in **`.../fastdeploy/vision/ocr`**.\r\nMy changed and modified files:\r\n\r\n[ppocr_v4.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/10067489/ppocr_v4.cc.txt)\r\n[ppocr_v4.h.txt](https://github.com/PaddlePaddle/FastDeploy/files/10067495/ppocr_v4.h.txt)\r\n[ppocr_pybind.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/10067498/ppocr_pybind.cc.txt)\r\n[ocr_pybind.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/10067499/ocr_pybind.cc.txt)\r\n\r\n\r\n\r\nMy infer.cc and sample image are:\r\n\r\n[infer.cc.txt](https://github.com/PaddlePaddle/FastDeploy/files/10067434/infer.cc.txt)\r\n\r\n![1](https://user-images.githubusercontent.com/65589645/203317006-601c4b72-c729-4f8e-80d8-d413b4256b73.jpg)\r\n\r\nwhen I want to compile infer.cc I get the bellow error :\r\n![Screenshot from 2022-11-22 16-15-10](https://user-images.githubusercontent.com/65589645/203317397-4291bc21-4a57-490f-9206-24fafe4a6fea.png)\r\n\r\n",
        "state": "closed",
        "user": "saeedkhanehgir",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-22T12:46:05+00:00",
        "updated_at": "2024-02-06T04:25:20+00:00",
        "closed_at": "2024-02-06T04:25:20+00:00",
        "comments_count": [
            "jiangjiajun",
            "saeedkhanehgir",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 662,
        "title": "picodet 训练的模型安卓报错，用的是configs/picodet/picodet_s_416_coco_lcnet.yml",
        "body": "<img width=\"1319\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5187679/203303091-7f212853-84dd-46bd-a7a9-851ca56c0281.png\">\r\n",
        "state": "closed",
        "user": "denonzhu",
        "closed_by": "denonzhu",
        "created_at": "2022-11-22T11:29:47+00:00",
        "updated_at": "2022-11-24T13:17:19+00:00",
        "closed_at": "2022-11-24T13:13:51+00:00",
        "comments_count": [
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "denonzhu",
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "DefTruth",
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "denonzhu",
            "denonzhu",
            "DefTruth",
            "DefTruth",
            "denonzhu",
            "DefTruth",
            "denonzhu",
            "denonzhu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 669,
        "title": "windows/Ubuntu C++ SDK 标签ID不一致的问题",
        "body": "platform：windows10\r\npython3.8\r\nonnx==1.12.0\r\n安装方式\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nyolov5-5.0\r\n导出方式\r\npython export.py --weights runs/train/exp33/weights/best.pt --include onnx\r\n\r\nwindows C++ SDK 编译方式是参考这个文档：use_sdk_on_windows.md\r\n`\r\nE:\\program\\fastdeploy-win-x64-gpu-0.2.1\\examples\\vision\\detection\\yolov7\\cpp\\build\\Release>infer.exe models/cable.onnx photo/2.png 0  # CPU\r\n[INFO] fastdeploy/fastdeploy_runtime.cc(283)::fastdeploy::Runtime::Init Runtime initialized with Backend::OPENVINO in device Device::CPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n0.000000,0.000000, 0.000000, 0.000000, 1.548668, 44\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n`\r\n\r\n我的onnx标签是从0-4的，但是这里居然 出现了44，请问这是什么原因？",
        "state": "closed",
        "user": "JackonLiu",
        "closed_by": "JackonLiu",
        "created_at": "2022-11-23T02:07:46+00:00",
        "updated_at": "2023-01-04T10:13:12+00:00",
        "closed_at": "2023-01-04T10:12:14+00:00",
        "comments_count": [
            "wjj19950828",
            "JackonLiu",
            "wjj19950828",
            "JackonLiu",
            "JackonLiu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 670,
        "title": "FastDeploy部署PaddleDetection模型",
        "body": "您好，请问如何利用FastDeploy部署PaddleDetection训练出的yolov5模型？  我想把.pdmodel和.pdiparams转化成onnx格式，但是调用paddle2onnx出现了如下报错。\r\n![1669169408897](https://user-images.githubusercontent.com/61613537/203456960-ca3df049-74d0-4cab-a0d6-8ad2b052e78d.jpg)\r\n",
        "state": "closed",
        "user": "Abris-087",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-23T02:12:40+00:00",
        "updated_at": "2024-02-06T04:25:19+00:00",
        "closed_at": "2024-02-06T04:25:19+00:00",
        "comments_count": [
            "jiangjiajun",
            "Abris-087",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 672,
        "title": "ERROR: No matching distribution found for fastdeploy-gpu-python",
        "body": "pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nLooking in links: https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nERROR: Could not find a version that satisfies the requirement fastdeploy-gpu-python (from versions: none)\r\nERROR: No matching distribution found for fastdeploy-gpu-python",
        "state": "closed",
        "user": "athmoon",
        "closed_by": "athmoon",
        "created_at": "2022-11-23T03:18:05+00:00",
        "updated_at": "2022-11-23T05:52:17+00:00",
        "closed_at": "2022-11-23T04:05:12+00:00",
        "comments_count": [
            "athmoon",
            "jiangjiajun",
            "athmoon",
            "athmoon"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 673,
        "title": "./infer_demo: error while loading shared libraries: libpaddle2onnx.so.1.0.1: cannot open shared object file: No such file or directory",
        "body": "platform：wsl2 Ubuntu20.0.4\r\npython3.9\r\nonnx==1.12.0\r\n安装方式\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n\r\n步骤参考文档：FastDeploy-develop\\examples\\vision\\detection\\yolov7\\cpp\\README.md\r\n\r\n步骤一切顺利，但是推理时报错\r\n`(base) root@computer:/mnt/c/Users/Administrator/projects/FastDeploy-develop/fastdeploy-linux-x64-0.6.0/examples/vision/detection/yolov7/cpp/build# ./infer_demo yolov7.onnx 000000014439.jpg 2\r\n./infer_demo: error while loading shared libraries: libpaddle2onnx.so.1.0.1: cannot open shared object file: No such file or directory\r\n`\r\n\r\n",
        "state": "closed",
        "user": "JackonLiu",
        "closed_by": "JackonLiu",
        "created_at": "2022-11-23T03:51:38+00:00",
        "updated_at": "2023-01-04T07:18:23+00:00",
        "closed_at": "2023-01-04T07:18:23+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "jiangjiajun",
            "JackonLiu",
            "JackonLiu",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 680,
        "title": "FP32、FP16、INT8精度相关",
        "body": "请问FastDeploy的PP_OCR_v3是否支持FP32、FP16、INT8的推理部署？另外多batch推理如何开启？",
        "state": "closed",
        "user": "namemzy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-23T10:07:23+00:00",
        "updated_at": "2025-01-28T06:40:13+00:00",
        "closed_at": "2025-01-28T06:40:13+00:00",
        "comments_count": [
            "yunyaoXYY",
            "namemzy",
            "yunyaoXYY",
            "namemzy",
            "namemzy",
            "txy00001"
        ],
        "labels": [
            "OCR",
            "Model Precision"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 684,
        "title": "运行错误",
        "body": "linux(ubuntu 18.04)编译成功后，运行测试 报错 Illegal instruction (core dumped),是因为cpu 指令不支持吗？",
        "state": "closed",
        "user": "huyuejingling",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-23T13:26:54+00:00",
        "updated_at": "2024-02-06T04:25:18+00:00",
        "closed_at": "2024-02-06T04:25:18+00:00",
        "comments_count": [
            "jiangjiajun",
            "huyuejingling",
            "jiangjiajun",
            "huyuejingling",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Linux x86",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 685,
        "title": "Runtime运行结果和onnxruntime、pytorch不一致",
        "body": "通过pytorch训练导出的onnx模型，pytorch和onnxruntimer推理结果一致；但是和fastdeploy的输出结果不一致。\r\n为了验证，使用了infer_onnx_onnxruntime.py改造下，固定输入（原有示例是随机输入），发现和onnxruntime的也不一致。\r\nimport fastdeploy as fd\r\nfrom fastdeploy import ModelFormat\r\nimport numpy as np\r\nimport cv2\r\n\r\n\r\nmodel_url = \"https://bj.bcebos.com/fastdeploy/models/mobilenetv2.onnx\"\r\nfd.download(model_url, path=\"./temp\")\r\n\r\ndef prev_proc(img:np.ndarray):\r\n\r\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = img.astype(np.float32)   # 不做颜色转换\r\n    img = cv2.resize(img, (224, 224))\r\n    img /= 255.0\r\n    img = np.transpose(img, (2,0,1))\r\n    img = np.expand_dims(img, axis=0)\r\n    # img = torch.from_numpy(img).to(device, non_blocking=True)\r\n\r\n    return img\r\n\r\n\r\noption = fd.RuntimeOption()\r\noption.set_model_path(\"./temp/mobilenetv2.onnx\", model_format=ModelFormat.ONNX)\r\n\r\n\r\noption.use_ort_backend()\r\noption.use_gpu()\r\n\r\n\r\nruntime = fd.Runtime(option)\r\n\r\n\r\ninput_name = runtime.get_input_info(0).name\r\noutput_name = runtime.get_output_info(0).name\r\nprint(' input_name:', input_name, runtime.get_input_info(0).shape, runtime.get_input_info(0).dtype)\r\nprint('output_name:', output_name)\r\n\r\nfname = 'imgs/__roi.jpg'\r\nimg = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)\r\nimg = prev_proc(img)\r\n\r\n\r\n\r\nresults = runtime.infer({\r\n    input_name: img\r\n})\r\n\r\nprint(results[0][0][:10])\r\n\r\n# 以上是 官方的runtime推理版本，下面是onnxruntime推理版本：\r\n\r\nimport onnxruntime as ort\r\nimport numpy as np\r\nimport cv2\r\n\r\ndef prev_proc(img:np.ndarray):\r\n\r\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = img.astype(np.float32)   # 不做颜色转换\r\n    img = cv2.resize(img, (224, 224))\r\n    img /= 255.0\r\n    img = np.transpose(img, (2,0,1))\r\n    img = np.expand_dims(img, axis=0)\r\n\r\n    return img\r\n\r\nfname = 'imgs/__roi.jpg'\r\nimg = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)\r\nimg = prev_proc(img)\r\n\r\nonnx_file = \"./temp/mobilenetv2.onnx\"\r\nort_sess = ort.InferenceSession(onnx_file, providers=ort.get_available_providers())\r\nort_inputs = {ort_sess.get_inputs()[0].name: img}\r\nresults = ort_sess.run(None, ort_inputs)\r\n\r\nprint(results[0][0][:10])\r\n",
        "state": "closed",
        "user": "yz2yz",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-24T01:45:29+00:00",
        "updated_at": "2022-11-24T10:57:03+00:00",
        "closed_at": "2022-11-24T10:57:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "yz2yz",
            "jiangjiajun",
            "yz2yz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 690,
        "title": "是否支持windows32位编译？",
        "body": "是否支持windows32位编译？",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-24T07:02:31+00:00",
        "updated_at": "2024-04-30T06:42:41+00:00",
        "closed_at": "2024-04-30T06:42:41+00:00",
        "comments_count": [
            "DefTruth",
            "wzy123456wzy",
            "hhxdestiny"
        ],
        "labels": [
            "Enhancement",
            "Build",
            "Windows x86"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 686,
        "title": "希望c#也能用fastdeploy部署",
        "body": "c#广泛用于工业应用场景，尤其是制造业。反而，C++的应用在这一块是很少的，请求有C#调用的方法",
        "state": "closed",
        "user": "ZXH109030",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-24T01:47:23+00:00",
        "updated_at": "2024-02-06T04:25:17+00:00",
        "closed_at": "2024-02-06T04:25:17+00:00",
        "comments_count": [
            "albyho",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 694,
        "title": "0.8.0版本ocrv3的predict方法直接程序退出",
        "body": "0.8.0版本ocrv3的predict方法直接程序退出，0.7.0版本没有这个问题。测试脚本如下，使用是paddleocr的公开模型，48行执行异常退出，没有报错信息：\r\n\r\nimport os\r\nos.environ['CUDA_MODULE_LOADING'] = 'LAZY'\r\n\r\nimport fastdeploy as fd\r\nfrom fastdeploy import ModelFormat\r\nimport cv2\r\nimport numpy as np\r\nimport os\r\nimport psutil\r\n\r\ndet_model_file  = 'weights/ch_PP-OCRv3_det_infer/inference.pdmodel'\r\ndet_params_file = 'weights/ch_PP-OCRv3_det_infer/inference.pdiparams'\r\nrec_model_file  = 'weights/ch_PP-OCRv3_rec_infer/inference.pdmodel'\r\nrec_params_file = 'weights/ch_PP-OCRv3_rec_infer/inference.pdiparams'\r\n\r\noption_det = fd.RuntimeOption()\r\noption_det.use_gpu()\r\noption_det.use_trt_backend() # TensorRT\r\noption_det.set_trt_cache_file('weights/ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det.trt')\r\noption_det.set_trt_input_shape(tensor_name='x', min_shape=[1,3,224,224], opt_shape=[1,3,896,896], max_shape=[1,3,3584,3584])\r\noption_det.set_trt_max_workspace_size(1 << 28) # 256M 1GB\r\n\r\ndet_model = fd.vision.ocr.DBDetector(\r\n    model_file=det_model_file, \r\n    params_file=det_params_file, \r\n    runtime_option=option_det, model_format=ModelFormat.PADDLE)\r\nprint('det_model:', det_model)\r\n\r\noption_rec = fd.RuntimeOption()\r\noption_rec.use_gpu()\r\noption_rec.use_trt_backend() # TensorRT\r\noption_rec.set_trt_cache_file('weights/ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec.trt')\r\noption_rec.set_trt_input_shape('x', (1,3,48,8), (1,3,48,320), (1,3,48,1792))\r\noption_rec.set_trt_max_workspace_size(1 << 28) # 256M 1GB\r\nrec_model = fd.vision.ocr.Recognizer(\r\n    model_file=rec_model_file, \r\n    params_file=rec_params_file, \r\n    label_path='weights/ch_PP-OCRv3_rec_infer/ppocr_keys_v1.txt',\r\n    runtime_option=option_rec, model_format=ModelFormat.PADDLE)\r\nprint('rec_model:', rec_model)\r\n\r\nmodel = fd.vision.ocr.PPOCRv3(det_model=det_model, cls_model=None, rec_model=rec_model)\r\nprint('model:', model)\r\n\r\n\r\nfname = 'imgs/KN511816.jpg'\r\nimg = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)\r\nresult = model.predict(img)\r\nprint(type(result))\r\nprint(result)\r\n\r\nmem = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\r\nprint(f'mem:{mem:.4f}GB')\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "yz2yz",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-24T12:03:55+00:00",
        "updated_at": "2024-02-06T04:25:16+00:00",
        "closed_at": "2024-02-06T04:25:16+00:00",
        "comments_count": [
            "yz2yz",
            "yunyaoXYY",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "OCR",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 697,
        "title": "有考虑提供rust绑定吗？",
        "body": null,
        "state": "closed",
        "user": "yuanyan3060",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-24T14:42:08+00:00",
        "updated_at": "2024-02-06T04:25:15+00:00",
        "closed_at": "2024-02-06T04:25:15+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 698,
        "title": "Garbage in RPATH of opencv shared libraries",
        "body": "检查 [`fastdeploy-linux-x64-0.8.0.tgz`](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-linux-x64-0.8.0.tgz) 中动态库的 RPATH, 发现存在如 `/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64` 的内容, 部署时这个目录不一定存在, 应该没有作用.\r\n暂不清楚其他版本是否有此情况\r\n\r\n```shell\r\n./third_libs/install/opencv/lib64/libopencv_core.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_dnn.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_shape.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_highgui.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_features2d.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_stitching.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_calib3d.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_photo.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_imgcodecs.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_imgproc.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_objdetect.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_ml.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_videostab.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_videoio.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_flann.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_video.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n./third_libs/install/opencv/lib64/libopencv_superres.so: RPATH=/jiangjiajun/opencv-3.4.16/build/installed_opencv_share/lib64\r\n```\r\n",
        "state": "closed",
        "user": "horror-proton",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-24T14:50:56+00:00",
        "updated_at": "2024-02-06T04:25:14+00:00",
        "closed_at": "2024-02-06T04:25:14+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 699,
        "title": "能否提供模型加密的接口？",
        "body": "模型在交付给客户的时候，没有加密手段，很难落地在客户环境上，只能自己公司环境内使用，没有办法满足客户私有云环境的需求。",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-25T02:16:00+00:00",
        "updated_at": "2024-02-20T06:43:05+00:00",
        "closed_at": "2024-02-20T06:43:05+00:00",
        "comments_count": [
            "denonzhu",
            "wzy123456wzy",
            "jiangjiajun",
            "wzy123456wzy",
            "felixhjh",
            "felixhjh"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 707,
        "title": "ppyoloe模型 多显卡环境使用trt加速，使用第二张显卡出错。Builder was created on device different than current device.",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 0.7.0 自己编译的 修改了opencv的版本\r\n- 系统平台: Windows x64(Windows10)\r\n- 硬件： CUDA 11.2 CUDNN 8.3 N卡2060 12G和2070ti 11G\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\n![7a5ef5ba6d9cae71e4452f9ed748f26](https://user-images.githubusercontent.com/54255051/204067192-ace5fbd7-6a6f-4afe-986f-1f851a985498.png)\r\n\r\n![9047c34aaf4cc1949276577a7a83641](https://user-images.githubusercontent.com/54255051/204067193-4fafa2f5-df5c-482a-a0e1-5a0200f8c421.png)\r\n![4dfb7e781cfb7f0f7b613d8b44f0850](https://user-images.githubusercontent.com/54255051/204067196-b8c810e4-9410-4607-be7c-e8706133dc96.png)\r\n",
        "state": "closed",
        "user": "yyj2013-two",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-26T01:24:52+00:00",
        "updated_at": "2024-02-06T04:25:13+00:00",
        "closed_at": "2024-02-06T04:25:13+00:00",
        "comments_count": [
            "jiangjiajun",
            "yyj2013-two",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Question",
            "Windows x64",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 711,
        "title": "[Backend] PaddleDetection PicoDet demo 开启paddle_trt出现double free or corruption (!prev)",
        "body": "## 环境\r\n\r\n- FastDeploy版本： Develop\r\n- 系统平台: Linux x64(Ubuntu 18.04)\r\n- 硬件：T4， CUDA 11.7 CUDNN 8.4\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\n\r\nPaddleDetection demo，在跑paddle_trt后端时会报```double free or corruption (!prev)```, 如下图, paddle_gpu/paddle_cpu无问题\r\n<img width=\"958\" alt=\"image\" src=\"https://user-images.githubusercontent.com/19977378/204131093-c78f7357-47d6-4a8f-a967-7fa1a428dabf.png\">\r\n\r\n复现步骤：\r\n以PicoDet为例，在GpuInfer中，option配置如下：\r\n```c++\r\nauto option = fastdeploy::RuntimeOption();\r\noption.UseGpu();\r\noption.UseTrtBackend();\r\noption.EnablePaddleToTrt();\r\n\r\nauto model = fastdeploy::vision::detection::PicoDet(model_file, params_file,\r\n                                                    config_file, option);\r\n```\r\n\r\n编译\r\n```shell\r\nmkdir build && cd build\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-x64-x.x.x\r\nmake -j\r\n./infer_picodet_demo ppdet_model/picodet_l_320_coco_lcnet/ 000000014439.jpg 1\r\n```\r\n\r\n",
        "state": "closed",
        "user": "wjj19950828",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-27T10:49:29+00:00",
        "updated_at": "2024-02-06T04:25:12+00:00",
        "closed_at": "2024-02-06T04:25:12+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "GPU",
            "Paddle TRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 738,
        "title": "NameError: name 'ModelFormat' is not defined ",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3060  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.7\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n```\r\nimport fastdeploy\r\n\r\nmodel = fastdeploy.vision.classification.YOLOv5Cls(r\"D:\\AHONG_Space\\AHONG\\mytools\\best.onnx\", params_file=None, runtime_option=None, model_format=ModelFormat.ONNX)\r\n\r\nmodel.predict(r'D:\\yolov5-master_v7.0\\cash_fenlei\\val\\cash_you\\ALN_cash_cash_2_3.jpg', topk=4)\r\n```\r\n\r\nbug:\r\n```\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\nTraceback (most recent call last):\r\n  File \"d:/AHONG_Space/AHONG/mytools/Fastdeply_yolov5_cla.py\", line 20, in <module>\r\n    model = fastdeploy.vision.classification.YOLOv5Cls(r\"D:\\AHONG_Space\\AHONG\\mytools\\best.onnx\", params_file=None, runtime_option=None, model_format=ModelFormat.ONNX)\r\nNameError: name 'ModelFormat' is not defined\r\n```",
        "state": "closed",
        "user": "HiaHong",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-29T07:37:21+00:00",
        "updated_at": "2024-02-06T04:25:11+00:00",
        "closed_at": "2024-02-06T04:25:11+00:00",
        "comments_count": [
            "jiangjiajun",
            "HiaHong",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 740,
        "title": "关于使用FastDeploy部署YOLOE模型的一些疑问",
        "body": "请问我想在Jeston nano上部署PP-YOLOE模型，使用FastDeploy和Paddle-Lite哪个比较合适\r\n",
        "state": "closed",
        "user": "Dekadenc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-29T08:39:47+00:00",
        "updated_at": "2024-02-06T04:25:10+00:00",
        "closed_at": "2024-02-06T04:25:10+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Documentation",
            "Question",
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 742,
        "title": "以openvino为后端运行ppocrv3cpu利用率低",
        "body": "以openvino为后端运行ppocrv3 cpu利用率低\r\n命令：\r\ninfer.py --det_model model/ch_PP-OCRv3_det_infer --cls_model model/ch_ppocr_mobile_v2.0_cls_infer --rec_model model/ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu --backend openvino\r\n\r\n[INFO] fastdeploy/backends/openvino/ov_backend.cc(161)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime.cc(532)::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n[INFO] fastdeploy/backends/openvino/ov_backend.cc(161)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime.cc(532)::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n[INFO] fastdeploy/backends/openvino/ov_backend.cc(161)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime.cc(532)::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n运行100次，平均:0.35195813784999996\r\n\r\n![1669717005(1)](https://user-images.githubusercontent.com/23567603/204502672-4298740d-916a-48f0-b94d-bc2d5f4d1e32.png)\r\n\r\n时间有点久，cpu利用率一直只用1个核，是哪里设置没有设置对吗？\r\n",
        "state": "closed",
        "user": "intjun",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-29T10:18:38+00:00",
        "updated_at": "2024-04-02T06:40:00+00:00",
        "closed_at": "2024-04-02T06:40:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "guobayang"
        ],
        "labels": [
            "Bug",
            "OCR",
            "OpenVINO"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 743,
        "title": "使用c++版本的PPOCRV3时，cpu和GPU能正常进行推理，但是基于tensorrt推理出错",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-linux-x64-gpu-0.8.0.tgz\r\n- 系统平台: Linux x64(Ubuntu 18.04) \r\n- 硬件： 说明具体硬件型号，如 Nvidia GPU 3090， CUDA 11.6 CUDNN 8.3\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(623)::CreateTrtEngineFromOnnx        Detect serialized TensorRT Engine file in ./ch_PP-OCRv3_det_infer/det_trt_cache.trt, will load it directly.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache   Build TensorRT Engine from cache file: ./ch_PP-OCRv3_det_infer/det_trt_cache.trt with shape range information as below,\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache   Input name: x, shape=[-1, 3, -1, -1], min=[1, 3, 50, 50], max=[1, 3, 1536, 1536]\r\n\r\n[INFO] fastdeploy/runtime.cc(506)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(623)::CreateTrtEngineFromOnnx        Detect serialized TensorRT Engine file in ./ch_ppocr_mobile_v2.0_cls_infer/cls_trt_cache.trt, will load it directly.\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update     [New Shape Out of Range] input name: x, shape: [8, 3, 48, 1024], The shape range before: min_shape=[1, 3, 48, 10], max_shape=[1, 3, 48, 1024].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update     [New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 48, 10], max_shape=[8, 3, 48, 1024].\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache   Build TensorRT Engine from cache file: ./ch_ppocr_mobile_v2.0_cls_infer/cls_trt_cache.trt with shape range information as below,\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache   Input name: x, shape=[-1, 3, -1, -1], min=[1, 3, 48, 10], max=[8, 3, 48, 1024]\r\n\r\n[INFO] fastdeploy/runtime.cc(506)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(623)::CreateTrtEngineFromOnnx        Detect serialized TensorRT Engine file in ./ch_PP-OCRv3_rec_infer/rec_trt_cache.trt, will load it directly.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache   Build TensorRT Engine from cache file: ./ch_PP-OCRv3_rec_infer/rec_trt_cache.trt with shape range information as below,\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache   Input name: x, shape=[-1, 3, 48, -1], min=[1, 3, 48, 10], max=[1, 3, 48, 2304]\r\n\r\n[INFO] fastdeploy/runtime.cc(506)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update     [New Shape Out of Range] input name: x, shape: [4, 3, 48, 584], The shape range before: min_shape=[1, 3, 48, 10], max_shape=[1, 3, 48, 2304].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update     [New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 48, 10], max_shape=[4, 3, 48, 2304].\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(291)::Infer       TensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(479)::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   10: [optimizer.cpp::computeCosts::2011] Error Code 10: Internal Error (Could not find any implementation for node {ForeignNode[linear_13.w_0 + (Unnamed Layer* 321) [Shuffle]...p2o.Reshape.23 + p2o.Transpose.7]}.)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   2: [builder.cpp::buildSerializedNetwork::609] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(542)::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\nSegmentation fault (core dumped)\r\nMakefile:89: recipe for target 'run' failed\r\nmake: *** [run] Error 139\r\n",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-11-29T12:31:10+00:00",
        "updated_at": "2024-05-21T06:40:41+00:00",
        "closed_at": "2024-05-21T06:40:41+00:00",
        "comments_count": [
            "yunyaoXYY",
            "jasper-cell",
            "yunyaoXYY",
            "jasper-cell",
            "catofyuanyuan"
        ],
        "labels": [
            "Question",
            "Linux x86",
            "OCR",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 750,
        "title": "md文档",
        "body": "\r\n\r\n## 在fastdeply/docs/cn/quick_start/models/cpp.md中第三步准备CmakeList.txt和第四步中的目录结构不太明白\r\n\r\n3. 准备CMakeList.txt\r\nFastDeploy中包含多个依赖库，直接采用g++或编译器编译较为繁杂，推荐使用cmake进行编译配置。示例配置如下，\r\n\r\n假设下载或准备的FastDeploy C++ SDK在/Paddle/Download目录下，且目录名为fastdeploy_cpp_sdk，在开发者的项目中只需添加如下代码，即可引入FASTDEPLOY_INCS和FASTDEPLOY_LIBS两个变量，分别表示依赖的头文件和库文件\r\n\r\ninclude(/Paddle/Download/fastdeploy_cpp_sdk/FastDeploy.cmake)\r\nPROJECT(infer_demo C CXX)\r\nCMAKE_MINIMUM_REQUIRED (VERSION 3.10)\r\n\r\ninclude(/Path/to/fastdeploy_cpp_sdk/FastDeploy.cmake)\r\n\r\ninclude_directories(${FASTDEPLOY_INCS})\r\n\r\nadd_executable(infer_demo ${PROJECT_SOURCE_DIR}/infer_demo.cc)\r\ntarget_link_libraries(infer_demo ${FASTDEPLOY_LIBS})\r\n4. 编译可执行程序\r\n假设当前目录已经准备好infer_demo.cc和CMakeLists.txt两个文件，目录结构如下所示，即可进行编译\r\n\r\n这里目录结构文档中没有显示",
        "state": "closed",
        "user": "GeT-RiGhTTT",
        "closed_by": "GeT-RiGhTTT",
        "created_at": "2022-11-30T02:14:53+00:00",
        "updated_at": "2022-12-05T02:59:09+00:00",
        "closed_at": "2022-12-05T02:59:09+00:00",
        "comments_count": [
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "GeT-RiGhTTT",
            "jiangjiajun"
        ],
        "labels": [
            "Documentation",
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 756,
        "title": "【已支持】jetson什么时候可以支持fastdeploy gpu版本或者tensorrt版本？",
        "body": "如题，fastdeploy目前只能在jetson上使用cpu，什么时候可以支持gpu",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "jiangjiajun",
        "created_at": "2022-11-30T07:02:18+00:00",
        "updated_at": "2024-02-06T04:25:09+00:00",
        "closed_at": "2024-02-06T04:25:09+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement",
            "TensorRT",
            "Jetson",
            "GPU"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 758,
        "title": "ARM开发板使用cpu部署Fastdeploy遇到的问题？",
        "body": "## 环境\r\n- FastDeploy版本： 没有适用的python包。\r\n- 系统平台: Ubuntu 2004\r\n- 硬件： Khadas VIM3\r\n- 编译语言： Python(3.8）\r\n\r\n## 问题描述\r\n在wheel列表中找不到适用于arrch的whl文件，也就代表着没法安装环境，但是很奇怪的是能用pip 直接安装fastdeploy-python包，但是版本过低。\r\n![image](https://user-images.githubusercontent.com/74920047/204745456-d72da433-1878-49d8-87b5-d727ae0ef9af.png)\r\nzhe这是我下载的manylinux那个whl文件。\r\n![image](https://user-images.githubusercontent.com/74920047/204745544-cc727405-d817-4ded-af54-f3203c22a2e6.png)\r\n\r\n",
        "state": "closed",
        "user": "Hyperpepe",
        "closed_by": "Hyperpepe",
        "created_at": "2022-11-30T08:28:33+00:00",
        "updated_at": "2022-11-30T08:41:36+00:00",
        "closed_at": "2022-11-30T08:41:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "Hyperpepe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 759,
        "title": "在windows平台使用Ninja编译失败",
        "body": "## 环境\r\n\r\n- FastDeploy版本： release/1.0.0\r\n- 系统平台: Windows x64(Windows10)\r\n- 硬件： github actions\r\n- 编译语言： C++ \r\n\r\n```\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.0\r\n--   CMake command             : C:/Program Files/CMake/bin/cmake.exe\r\n--   System                    : Windows\r\n--   C++ compiler              : C:/Program Files/Microsoft Visual Studio/2022/Enterprise/VC/Tools/MSVC/14.33.31629/bin/HostX64/x64/cl.exe\r\n--   C++ compiler version      : 19.33.31631.0\r\n--   CXX flags                 : /DWIN32 /D_WINDOWS /W3 /GR /EHsc\r\n--   Build type                : Release\r\n--   Compile definitions       : YAML_CPP_DLL;FASTDEPLOY_LIB;EIGEN_STRONG_INLINE=inline;ENABLE_ORT_BACKEND;ENABLE_VISION;ENABLE_VISION_VISUALIZE;ENABLE_PADDLE_FRONTEND\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : D:/a/build-librarys/build-librarys/build/fastdeploy\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   Paddle2ONNX version       : 1.0.4rc0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_DEBUG              : \r\n--   ENABLE_VISION_VISUALIZE   : \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: D:/a/build-librarys/build-librarys/FastDeploy/build\r\n```\r\n\r\n## 问题描述\r\n尝试使用github actions 搭建CI，但在windows平台编译出错，日志显示是最后`copy_yaml_library`时出现错误\r\n```\r\n[190/196] Linking CXX executable third_party\\yaml-cpp\\util\\sandbox.exe\r\n[191/196] Linking CXX executable third_party\\yaml-cpp\\util\\parse.exe\r\n[192/196] Linking CXX shared library fastdeploy.dll\r\n[193/196] cmd.exe /C \"cd /D D:\\a\\build-librarys\\build-librarys\\FastDeploy\\build && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E copy_directory D:/a/build-librarys/build-librarys/FastDeploy/third_party/yaml-cpp/include D:/a/build-librarys/build-librarys/FastDeploy/build/third_libs/install/yaml-cpp/include\"\r\n[194/196] cmd.exe /C \"cd /D D:\\a\\build-librarys\\build-librarys\\FastDeploy\\build && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E copy_directory D:/a/build-librarys/build-librarys/FastDeploy/build/third_party/yaml-cpp/Release D:/a/build-librarys/build-librarys/FastDeploy/build/third_libs/install/yaml-cpp/lib\"\r\nFAILED: CMakeFiles/copy_yaml_library D:/a/build-librarys/build-librarys/FastDeploy/build/CMakeFiles/copy_yaml_library \r\ncmd.exe /C \"cd /D D:\\a\\build-librarys\\build-librarys\\FastDeploy\\build && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E copy_directory D:/a/build-librarys/build-librarys/FastDeploy/build/third_party/yaml-cpp/Release D:/a/build-librarys/build-librarys/FastDeploy/build/third_libs/install/yaml-cpp/lib\"\r\nError copying directory from \"D:/a/build-librarys/build-librarys/FastDeploy/build/third_party/yaml-cpp/Release\" to \"D:/a/build-librarys/build-librarys/FastDeploy/build/third_libs/install/yaml-cpp/lib\".\r\n[195/196] Linking CXX executable third_party\\yaml-cpp\\util\\read.exe\r\nninja: build stopped: subcommand failed.\r\n```\r\nhttps://github.com/aa889788/build-librarys/actions/runs/3582170039/jobs/6026077544\r\n",
        "state": "closed",
        "user": "aa889788",
        "closed_by": "aa889788",
        "created_at": "2022-11-30T10:23:05+00:00",
        "updated_at": "2022-12-05T11:23:44+00:00",
        "closed_at": "2022-12-05T11:23:44+00:00",
        "comments_count": [
            "jiangjiajun",
            "aa889788",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement",
            "Windows x64",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 768,
        "title": "Windows 运行超分(SR)推理错误",
        "body": "## 环境\r\n- FastDeploy版本： fastdeploy-win-x64-1.0.0\r\n- 系统平台: Windows x64(Windows10)\r\n- 硬件： CPU  Intel Core  i7-8700\r\n- 编译语言： C++\r\n- 模型: PP-MSVSR_reds_x4\r\n\r\n## 问题描述\r\n\r\n**模型加载成功，但执行推理异常**\r\n\r\n- 测试代码\r\n```c++\r\n    auto model = fastdeploy::vision::sr::PPMSVSR(model_file, params_file);\r\n    if (model.Initialized())\r\n    {\r\n        cv::Mat im = cv::imread(\"0000.png\");\r\n        cv::cvtColor(im, im, cv::COLOR_RGBA2BGR);\r\n\r\n        std::vector<cv::Mat> imgs;\r\n        imgs.push_back(im);\r\n\r\n        std::vector<cv::Mat> results;\r\n        if (model.Predict(imgs, results) && results.size() > 0)\r\n        {\r\n            cv::imwrite(\"result0.jpg\", results[0]);\r\n        }\r\n    }\r\n```\r\n\r\n- 输出: \r\n```bash\r\n[INFO] fastdeploy/runtime.cc(517)::fastdeploy::Runtime::Init    Runtime initialized with Backend::PDINFER in Device::CPU.\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\nNot support stack backtrace yet.\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: Input(X) and Input(Grid) dimension[0] should be equal, but received X dimension[0](16) != Grid dimension[0](1)\r\n  [Hint: Expected grid_dims[0] == x_dims[0], but received grid_dims[0]:1 != x_dims[0]:16.] (at ..\\paddle\\phi\\infermeta\\binary.cc:1399)\r\n```\r\n",
        "state": "closed",
        "user": "LGP-GH",
        "closed_by": "LGP-GH",
        "created_at": "2022-12-01T03:57:17+00:00",
        "updated_at": "2022-12-01T06:00:42+00:00",
        "closed_at": "2022-12-01T06:00:42+00:00",
        "comments_count": [
            "ChaoII",
            "LGP-GH",
            "ChaoII",
            "LGP-GH"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 763,
        "title": "Fastdeploy 1.0.0 Docker container GPU crashing ",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.0\r\nOS Platform: Linux x64 \r\nHardware:  Nvidia T4\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\n\r\nI am trying to run PP-OCR V3 example via docker serving using following image:\r\n\r\n`docker pull paddlepaddle/fastdeploy:1.0.0-gpu-cuda11.4-trt8.4-21.10`\r\n\r\nBut on inference triton server is crashing with following traceback:\r\n\r\n```\r\nSignal (11) received.\r\n 0# 0x000055A10674A8A9 in fastdeployserver\r\n 1# 0x00007F46B8676210 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# fastdeploy::AdaptivePool2dKernel::CpuAdaptivePool(std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, float const*, float*) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 3# fastdeploy::AdaptivePool2dKernel::Compute(OrtKernelContext*) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 4# Ort::CustomOpBase<fastdeploy::AdaptivePool2dOp, fastdeploy::AdaptivePool2dKernel>::CustomOpBase()::{lambda(void*, OrtKernelContext*)#8}::operator()(void*, OrtKernelContext*) const in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 5# Ort::CustomOpBase<fastdeploy::AdaptivePool2dOp, fastdeploy::AdaptivePool2dKernel>::CustomOpBase()::{lambda(void*, OrtKernelContext*)#8}::_FUN(void*, OrtKernelContext*) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 6# 0x00007F464319CE67 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n 7# 0x00007F4643995154 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n 8# 0x00007F4643977ADA in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n 9# 0x00007F464397AFC1 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n10# 0x00007F46431CED46 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n11# 0x00007F46431CF0A8 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n12# 0x00007F464315E260 in /opt/fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0\r\n13# Ort::Session::Run(Ort::RunOptions const&, Ort::IoBinding const&) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n14# fastdeploy::OrtBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n15# fastdeploy::Runtime::Infer() in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n16# 0x00007F467E1A2134 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n17# 0x00007F467E1A5B96 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n18# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n19# 0x00007F46B920283A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n20# 0x00007F46B920304D in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n21# 0x00007F46B90B7801 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n22# 0x00007F46B91FCDC7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n23# 0x00007F46B8A64DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n24# 0x00007F46B8EE2609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n25# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n```",
        "state": "closed",
        "user": "akansal1",
        "closed_by": "heliqi",
        "created_at": "2022-11-30T13:40:20+00:00",
        "updated_at": "2023-01-30T08:16:13+00:00",
        "closed_at": "2023-01-30T08:16:13+00:00",
        "comments_count": [
            "HexToString"
        ],
        "labels": [
            "Bug",
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 772,
        "title": "paddleDetection 导出的yolov3 训练模型，在fastdeploy中使用范例预测报错",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 编译安装的0.8.0\r\n- 系统平台: Linux x64(Ubuntu 18.04)\r\n- 硬件： 说明具体硬件型号，CPU\r\n- 编译语言： Python3.8\r\n\r\n## 问题描述\r\n在paddledetection中通过 configs/yolov3/yolov3_darknet53_270e_voc.yml 训练的模型，训练完成后通过 tools/infer.py 测试没有问题，\r\n后通过 tools/export_model.py 导出静态模型。导出命令如下:\r\ncd ~/work/PaddleDetection/ && python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_voc.yml --output_dir=./inference_model\r\n\r\n后将导出的静态模型部署到fastdeploy 环境，使用example中的代码进行测试，报错。\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/release/1.0/examples/vision/detection/paddledetection/python/infer_yolov3.py\r\n\r\n具体报错内容如下：\r\n![5fd60932a61a021547099e06f51c204](https://user-images.githubusercontent.com/25760739/205075789-0f50d711-002a-45ab-9b6c-77a12ac9c3f6.png)\r\n\r\n模型是使用的是下面的开源项目预测的\r\nhttps://aistudio.baidu.com/aistudio/projectdetail/2025555?channelType=0&channel=0",
        "state": "closed",
        "user": "formero009",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-01T14:19:50+00:00",
        "updated_at": "2024-02-06T04:25:08+00:00",
        "closed_at": "2024-02-06T04:25:08+00:00",
        "comments_count": [
            "formero009",
            "jiangjiajun",
            "formero009",
            "jiangjiajun",
            "formero009",
            "jiangjiajun",
            "formero009",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Object Detection",
            "Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 774,
        "title": "Fastdeploy OCR model accuracy less than PPOCR model",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.0\r\nOS Platform: Linux x64 \r\nHardware:  Nvidia T4\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\n\r\nResults of fastdeploy PPOCRV3 models are less accurate that inference from PPOCR library runtime. I am not able to identify the reason and if that is the expected result.",
        "state": "closed",
        "user": "akansal1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-01T18:59:04+00:00",
        "updated_at": "2024-11-26T06:40:44+00:00",
        "closed_at": "2024-11-26T06:40:44+00:00",
        "comments_count": [
            "yunyaoXYY",
            "akansal1",
            "yunyaoXYY",
            "akansal1",
            "yunyaoXYY",
            "yunyaoXYY",
            "huangjun11"
        ],
        "labels": [
            "OCR",
            "Model Precision",
            "Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 773,
        "title": "jetson上编译报错 CMake Error: Problem with archive_read_open_file(): Unrecognized archive format",
        "body": "## 环境\r\n\r\n- FastDeploy版本： FastDeploy-develop\r\n- 系统平台: Linux x64(Ubuntu 18.04)\r\n- 硬件： jetson\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\n[ 18%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n[ 18%] Performing download step (download, verify and extract) for 'extern_paddle2onnx'\r\n[ 18%] Built target yaml-cpp\r\n-- File already exists but no hash specified (use URL_HASH):\r\n  file='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n-- File already exists but no hash specified (use URL_HASH):\r\n  file='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.4rc0.tgz'\r\nOld file will be removed and new file downloaded from URL.\r\nOld file will be removed and new file downloaded from URL.\r\n-- Downloading...\r\n   dst='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.4rc0.tgz'\r\n   timeout='none'\r\n-- Downloading...\r\n   dst='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle2onnx-linux-aarch64-1.0.4rc0.tgz'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-aarch64-1.12.0.tgz'\r\nScanning dependencies of target yaml-cpp-read\r\nScanning dependencies of target yaml-cpp-parse\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-parse.dir/parse.cpp.o\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-read.dir/read.cpp.o\r\n-- Downloading... done\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.4rc0.tgz'\r\n-- extracting...\r\n     src='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n     dst='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/extern_paddle2onnx'\r\n     dst='/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- extracting... [tar xfz]\r\nCMake Error: Problem with archive_read_open_file(): Unrecognized archive format\r\nCMake Error: Problem extracting tar: /home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.4rc0.tgz\r\nCMake Error: Problem with archive_read_open_file(): Unrecognized archive format\r\nCMake Error: Problem extracting tar: /home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz\r\n-- extracting... [error clean up]\r\nCMake Error at extern_paddle2onnx-stamp/extract-extern_paddle2onnx.cmake:33 (message):\r\n  error: extract of\r\n  '/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.4rc0.tgz'\r\n  failed\r\n\r\n\r\n-- extracting... [error clean up]\r\nCMake Error at extern_onnxruntime-stamp/extract-extern_onnxruntime.cmake:33 (message):\r\n  error: extract of\r\n  '/home/lcfc/fastdeploy_fast/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n  failed\r\n\r\n\r\nCMakeFiles/extern_paddle2onnx.dir/build.make:89: recipe for target 'third_libs/paddle2onnx/src/extern_paddle2onnx-stamp/extern_paddle2onnx-download' failed\r\nmake[2]: *** [third_libs/paddle2onnx/src/extern_paddle2onnx-stamp/extern_paddle2onnx-download] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/extern_paddle2onnx.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_paddle2onnx.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\nCMakeFiles/extern_onnxruntime.dir/build.make:89: recipe for target 'third_libs/onnxruntime/src/extern_onnxruntime-stamp/extern_onnxruntime-download' failed\r\nmake[2]: *** [third_libs/onnxruntime/src/extern_onnxruntime-stamp/extern_onnxruntime-download] Error 1\r\nCMakeFiles/Makefile2:104: recipe for target 'CMakeFiles/extern_onnxruntime.dir/all' failed\r\nmake[1]: *** [CMakeFiles/extern_onnxruntime.dir/all] Error 2\r\n[ 18%] Linking CXX executable parse\r\n[ 18%] Linking CXX executable read\r\n[ 18%] Built target yaml-cpp-read\r\n[ 18%] Built target yaml-cpp-parse\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\n",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-01T15:07:52+00:00",
        "updated_at": "2024-07-09T06:40:35+00:00",
        "closed_at": "2024-07-09T06:40:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jasper-cell",
            "jasper-cell",
            "jiangjiajun",
            "kewuyu",
            "kewuyu"
        ],
        "labels": [
            "Question",
            "Jetson",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 776,
        "title": "部署ocr服务端 报错",
        "body": "1.按照[FastDeploy/examples/vision/ocr/PP-OCRv3/serving/](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCRv3/serving)中的教程，进行服务端部署，使用了cpu版本的docker\r\n2.在执行[启动服务端](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCRv3/serving#13-%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%9C%A8docker%E5%86%85)时：fastdeployserver --model-repository=/ocr_serving/models，报错error: creating server: Internal - failed to load all models\r\n3.模型文件已经下载并按照教程解压等操作\r\n4.部分日志如下：\r\nE1202 06:13:21.330826 189 model_repository_manager.cc:1395] failed to load model 'cls_pp': at least one version must be available under the version policy of model 'cls_pp'\r\nE1202 06:13:21.330861 189 model_repository_manager.cc:1395] failed to load model 'pp_ocr': at least one version must be available under the version policy of model 'pp_ocr'\r\nE1202 06:13:21.330882 189 model_repository_manager.cc:1395] failed to load model 'rec_pp': at least one version must be available under the version policy of model 'rec_pp'\r\n...\r\nI1202 06:13:21.331038 189 server.cc:592]\r\n+-----------------+---------+----------------------------------------+\r\n| Model           | Version | Status                                 |\r\n+-----------------+---------+----------------------------------------+\r\n| cls_postprocess | 1       | READY                                  |\r\n| cls_pp          | -       | Not loaded: No model version was found |\r\n| cls_runtime     | 1       | READY                                  |\r\n| det_postprocess | 1       | READY                                  |\r\n| det_preprocess  | 1       | READY                                  |\r\n| det_runtime     | 1       | READY                                  |\r\n| pp_ocr          | -       | Not loaded: No model version was found |\r\n| rec_postprocess | 1       | READY                                  |\r\n| rec_pp          | -       | Not loaded: No model version was found |\r\n| rec_runtime     | 1       | READY                                  |\r\n+-----------------+---------+----------------------------------------+\r\n\r\nCleaning up...\r\nI1202 06:13:22.375757 189 model_repository_manager.cc:1166] successfully unloaded 'det_postprocess' version 1\r\nI1202 06:13:22.376097 189 model_repository_manager.cc:1166] successfully unloaded 'cls_postprocess' version 1\r\nI1202 06:13:22.379952 189 model_repository_manager.cc:1166] successfully unloaded 'det_preprocess' version 1\r\nI1202 06:13:22.381652 189 model_repository_manager.cc:1166] successfully unloaded 'rec_postprocess' version 1\r\nI1202 06:13:23.331767 189 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n\r\n",
        "state": "closed",
        "user": "Gmgge",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-02T06:21:19+00:00",
        "updated_at": "2022-12-02T08:24:44+00:00",
        "closed_at": "2022-12-02T08:24:44+00:00",
        "comments_count": [
            "Gmgge",
            "HexToString",
            "HexToString",
            "Gmgge",
            "Gmgge",
            "HexToString",
            "HexToString",
            "HexToString",
            "Gmgge"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 777,
        "title": "请问目前安卓端的目标检测模型只支持PicoDet模型吗？能否提供其他模型？",
        "body": "请问目前安卓端的目标检测模型只支持PicoDet模型吗？PicoDet是否只支持picodet_l_320_coco_lcnet,支持其他的PioDet模型吗？比如picodet_l_640_coco_lcnet.yml，picodet_l_960_coco_lcnet.yml",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-02T08:53:47+00:00",
        "updated_at": "2024-04-02T06:51:31+00:00",
        "closed_at": "2024-04-02T06:40:01+00:00",
        "comments_count": [
            "jiangjiajun",
            "DefTruth",
            "wzy123456wzy",
            "leiqing1",
            "vamoslc",
            "vamoslc"
        ],
        "labels": [
            "Question",
            "Object Detection",
            "Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 778,
        "title": "PaddleClas模型，Python部署，config_file文件找不到！",
        "body": "`fd.vision.classification.PaddleClasModel(model_file, params_file, config_file, runtime_option=None, model_format=ModelFormat.PADDLE)\r\n`\r\n其中model_file, params_file为训练模型导出的Paddle inference文件\r\n但是\r\nconfig_file 是什么文件\r\nconfig_file就不是训练模型导出的文件？那是什么？\r\n\r\n```\r\nPaddleClas 导出模型\r\ninference.pdiparams\r\ninference.pdiparams.info\r\ninference.pdmodel\r\n```\r\n\r\n\r\nconfig_file 文件是什么\r\n并且在哪里找出来",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-12-02T11:26:57+00:00",
        "updated_at": "2022-12-02T11:40:55+00:00",
        "closed_at": "2022-12-02T11:40:54+00:00",
        "comments_count": [
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 779,
        "title": "fastdeploy::OrtBackend::Infer      Failed to Infer: Got invalid dimensions for input: x for the following indicesFailed to inference by runtime.",
        "body": "## 环境\r\n\r\n- FastDeploy版本： fastdeploy-gpu-python 0.6.0 pip\r\n- 系统平台: Windows x64(Windows11) \r\n- 硬件： cuda11.6 cudnn8.4\r\n- 编译语言： Python(3.7）\r\n\r\n## 问题描述\r\n```\r\nFastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime.cc(481)::fastdeploy::Runtime::Init    Runtime initialized with Backend::ORT in Device::CPU.\r\n[ERROR] fastdeploy/backends/ort/ort_backend.cc(229)::fastdeploy::OrtBackend::Infer      Failed to Infer: Got invalid dimensions for input: x for the following indices\r\n index: 2 Got: 224 Expected: 512\r\n index: 3 Got: 224 Expected: 512\r\n Please fix either the inputs or the model.\r\n[ERROR] fastdeploy/vision/classification/ppcls/model.cc(70)::fastdeploy::vision::classification::PaddleClasModel::BatchPredict  Failed to inference by runtime.\r\nClassifyResult(\r\nlabel_ids:\r\nscores:\r\n)\r\n```\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2022-12-02T11:43:54+00:00",
        "updated_at": "2022-12-02T12:08:02+00:00",
        "closed_at": "2022-12-02T12:08:02+00:00",
        "comments_count": [
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 780,
        "title": "Runtime initialized with Backend::ORT in Device::  CPU 和 GPU 有问题",
        "body": "## 环境\r\n\r\nFastDeploy版本： fastdeploy-gpu-python 0.6.0 pip\r\n系统平台: Windows x64(Windows11)\r\n硬件： cuda11.6 cudnn8.4\r\n编译语言： Python(3.7）\r\n问题描述\r\n\r\n## 问题描述\r\n\r\n\r\n```\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\")\r\nim = cv2.imread(\"000000014439.jpg\")\r\nresult = model.predict(im.copy())\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n\r\n```\r\n显示 Runtime initialized with Backend::ORT in Device::  CPU\r\n\r\n-----------------------\r\n\r\n```\r\nimport cv2\r\nimport fastdeploy as fd\r\n\r\ndef build_option():\r\n    option = fd.RuntimeOption()\r\n\r\n    # if args.device.lower() == \"gpu\":\r\n    option.use_gpu()\r\n\r\n    # if args.use_trt:\r\n    #     option.use_trt_backend()\r\n    return option\r\n\r\nruntime_option = build_option()\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\", runtime_option=runtime_option)\r\nim = cv2.imread(\"000000014439.jpg\")\r\nresult = model.predict(im.copy())\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n\r\n```\r\n显示 Runtime initialized with Backend::ORT in Device::  GPU\r\n\r\n-----\r\n\r\n发现 用 fastdeploy.vision as vision 只会显示 CPU\r\n需要用import fastdeploy as fd  才会显示GPU\r\n\r\n这两个 有什么区别的",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-02T12:22:14+00:00",
        "updated_at": "2024-02-06T04:25:07+00:00",
        "closed_at": "2024-02-06T04:25:07+00:00",
        "comments_count": [
            "jiangjiajun",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Question",
            "RuntimeOption"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 781,
        "title": "模型量化压缩",
        "body": "请问下把其他模型转换为padddle模型之后用fastdeploy做部署时，还需要提前使用ACT做量化压缩吗？还是说内部已经自动做了量化和压缩。\r\n比如把yolov5模型用xpaddle转换为paddle模型部署模型时。需要自己使用act做量化压缩吗？\r\n",
        "state": "closed",
        "user": "xiaotailang",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-02T12:24:09+00:00",
        "updated_at": "2024-02-06T04:25:06+00:00",
        "closed_at": "2024-02-06T04:25:06+00:00",
        "comments_count": [
            "yunyaoXYY",
            "xiaotailang",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "jiangjiajun"
        ],
        "labels": [
            "Question",
            "Quantize"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 787,
        "title": "Arch Linux下无法导入包管理器安装的依赖",
        "body": "## 环境\r\n\r\n- FastDeploy版本： r1349.d74e1209\r\n- 系统平台: Linux x64(Arch Linux rolling)\r\n- 硬件： Intel i5-8265U\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\nArch Linux 编译时无法引入包管理器安装的依赖\r\nPKGBUILD:\r\n```\r\npkgname=fastdeploy-git\r\npkgrel=1\r\npkgver=r1349.d74e1209\r\npkgdesc=\"An Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge.\"\r\nurl=\"https://github.com/PaddlePaddle/FastDeploy\"\r\nprovides=(fastdeploy)\r\nlicense=('Apache2')\r\narch=('x86_64')\r\nsource=(\"FastDeploy::git+https://github.com/PaddlePaddle/FastDeploy.git\")\r\nsha512sums=('SKIP')\r\ndepends=()\r\nmakedepends=(\r\n    gcc \\\r\n    cmake \r\n)\r\n\r\npkgver() {\r\n  cd ${srcdir}/FastDeploy\r\n  printf \"r%s.%s\" \"$(git rev-list --count HEAD)\" \"$(git rev-parse --short HEAD)\"\r\n}\r\nbuild(){\r\n    cd ${srcdir}/FastDeploy\r\n    cmake -S . -B build \\\r\n        -DENABLE_ORT_BACKEND=ON \\\r\n        -DENABLE_PADDLE_BACKEND=ON \\\r\n        -DENABLE_OPENVINO_BACKEND=ON \\\r\n        -DCMAKE_INSTALL_PREFIX=/usr \\\r\n        -DENABLE_VISION=ON \\\r\n        -DENABLE_TEXT=ON \\\r\n        -DOPENCV_DIRECTORY=/usr/include/opencv4 \\\r\n        -DOPENVINO_DIRECTORY=/opt/intel/openvino\r\n    cmake --build build\r\n}\r\npackage(){\r\n    cd ${srcdir}/FastDeploy\r\n    DESTDIR=${pkgdir} cmake --install build\r\n}\r\n```\r\n`makepkg`编译时有如下报错\r\n```\r\n~/fastdeploy-git > makepkg \r\n==> 正在创建软件包：fastdeploy-git r1349.d74e1209-1 (2022年12月04日 星期日 13时59分29秒)\r\n==> 正在检查运行时依赖关系...\r\n==> 正在检查编译时依赖关系==> 获取源代码...\r\n  -> 正在克隆 FastDeploy git 仓库...\r\n克隆到纯仓库 '/home/kevin/fastdeploy-git/FastDeploy'...\r\nremote: Enumerating objects: 45611, done.\r\nremote: Counting objects: 100% (898/898), done.\r\nremote: Compressing objects: 100% (402/402), done.\r\nremote: Total 45611 (delta 478), reused 830 (delta 439), pack-reused 44713\r\n接收对象中: 100% (45611/45611), 35.87 MiB | 8.18 MiB/s, 完成.\r\n处理 delta 中: 100% (26783/26783), 完成.\r\n==> 正在验证 source 文件，使用sha512sums...\r\n    FastDeploy ... 已跳过==> 正在释放源码...\r\n  -> 正在建立 FastDeploy git 仓库的拷贝...\r\n正克隆到 'FastDeploy'...\r\n完成。==> 正在开始 pkgver()...\r\n==> 正在开始 build()...\r\n-- The C compiler identification is GNU 12.2.0\r\n-- The CXX compiler identification is GNU 12.2.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/lib/ccache/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/lib/ccache/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-x86_64.tar.gz to /home/kevin/fastdeploy-git/src/FastDeploy/build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 17% complete]\r\n-- [download 22% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 43% complete]\r\n-- [download 48% complete]\r\n-- [download 53% complete]\r\n-- [download 58% complete]\r\n-- [download 64% complete]\r\n-- [download 69% complete]\r\n-- [download 74% complete]\r\n-- [download 79% complete]\r\n-- [download 84% complete]\r\n-- [download 90% complete]\r\n-- [download 95% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/kevin/fastdeploy-git/src/FastDeploy/build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/install/onnxruntime\r\nCMake Warning (dev) at /usr/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:98 (ExternalProject_Add)\r\n  CMakeLists.txt:206 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at /usr/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/paddle_inference.cmake:82 (ExternalProject_Add)\r\n  CMakeLists.txt:224 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Use the openvino lib specified by user. The OpenVINO path: /opt/intel/openvino\r\nOPENVINO_LIBS = /opt/intel/openvino/runtime/lib/intel64/libopenvino.so;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/include/opencv4\r\n-- Found OpenCV: /usr (found version \"4.6.0\") \r\nFASTTOKENIZER_COMPILE_LIB = /home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.so\r\nCMake Warning (dev) at /usr/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/fast_tokenizer.cmake:80 (ExternalProject_Add)\r\n  CMakeLists.txt:438 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at /usr/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/paddle2onnx.cmake:67 (ExternalProject_Add)\r\n  CMakeLists.txt:443 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.1\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/lib/ccache/bin/c++\r\n--   C++ compiler version      : 12.2.0\r\n--   CXX flags                 : -Wno-format\r\n--   Build type                : \r\n--   Compile definitions       : FASTDEPLOY_LIB;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;ENABLE_OPENVINO_BACKEND;ENABLE_VISION;ENABLE_VISION_VISUALIZE;ENABLE_TEXT;ENABLE_PADDLE_FRONTEND\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   Paddle2ONNX version       : 1.0.4rc0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : 2.4-dev3\r\n--   OpenVINO version          : \r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_DEBUG              : \r\n--   ENABLE_VISION_VISUALIZE   : \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/kevin/fastdeploy-git/src/FastDeploy/build\r\n[  1%] Creating directories for 'extern_onnxruntime'\r\n[  1%] Creating directories for 'extern_fast_tokenizer'\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Creating directories for 'extern_paddle_inference'\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  3%] Creating directories for 'extern_paddle2onnx'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_fast_tokenizer'\r\n[  5%] Performing download step (download, verify and extract) for 'extern_paddle2onnx'\r\n[  5%] Performing download step (download, verify and extract) for 'extern_paddle_inference'\r\n-- Downloading...\r\n   dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-linux-x64-1.12.0.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-x64-1.12.0.tgz'\r\n-- Downloading...\r\n   dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-linux-x64-1.0.4rc0.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Downloading...\r\n   dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle_inference/src/paddle_inference-linux-x64-2.4-dev3.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle2onnx-linux-x64-1.0.4rc0.tgz'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle_inference-linux-x64-2.4-dev3.tgz'\r\n-- Downloading...\r\n   dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/fast_tokenizer/src/fast_tokenizer-linux-x64-1.0.0.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddlenlp/fast_tokenizer/fast_tokenizer-linux-x64-1.0.0.tgz'\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 17%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 17%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 18%] Linking CXX static library libyaml-cpp.a\r\n[ 18%] Built target yaml-cpp\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-parse.dir/parse.cpp.o\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-sandbox.dir/sandbox.cpp.o\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-read.dir/read.cpp.o\r\n[ 19%] Linking CXX executable read\r\n[ 20%] Linking CXX executable parse\r\n[ 21%] Linking CXX executable sandbox\r\n[ 21%] Built target yaml-cpp-sandbox\r\n[ 21%] Built target yaml-cpp-read\r\n[ 21%] Built target yaml-cpp-parse\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-linux-x64-1.12.0.tgz'\r\n     dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-linux-x64-1.0.4rc0.tgz'\r\n     dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle2onnx/src/extern_paddle2onnx'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 22%] No update step for 'extern_onnxruntime'\r\n[ 22%] No update step for 'extern_paddle2onnx'\r\n[ 22%] No patch step for 'extern_onnxruntime'\r\n[ 23%] No patch step for 'extern_paddle2onnx'\r\n[ 23%] No configure step for 'extern_onnxruntime'\r\n[ 23%] No configure step for 'extern_paddle2onnx'\r\n[ 23%] No build step for 'extern_onnxruntime'\r\n[ 24%] No build step for 'extern_paddle2onnx'\r\n[ 24%] Performing install step for 'extern_onnxruntime'\r\n[ 24%] Performing install step for 'extern_paddle2onnx'\r\n[ 25%] Completed 'extern_onnxruntime'\r\n[ 25%] Completed 'extern_paddle2onnx'\r\n[ 25%] Built target extern_onnxruntime\r\n[ 25%] Built target extern_paddle2onnx\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/fast_tokenizer/src/fast_tokenizer-linux-x64-1.0.0.tgz'\r\n     dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/fast_tokenizer/src/extern_fast_tokenizer'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 25%] No update step for 'extern_fast_tokenizer'\r\n[ 26%] No patch step for 'extern_fast_tokenizer'\r\n[ 27%] No configure step for 'extern_fast_tokenizer'\r\n[ 27%] No build step for 'extern_fast_tokenizer'\r\n[ 28%] Performing install step for 'extern_fast_tokenizer'\r\n[ 28%] Completed 'extern_fast_tokenizer'\r\n[ 28%] Built target extern_fast_tokenizer\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle_inference/src/paddle_inference-linux-x64-2.4-dev3.tgz'\r\n     dst='/home/kevin/fastdeploy-git/src/FastDeploy/build/third_libs/paddle_inference/src/extern_paddle_inference'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 29%] No update step for 'extern_paddle_inference'\r\n[ 29%] No patch step for 'extern_paddle_inference'\r\n[ 30%] No configure step for 'extern_paddle_inference'\r\n[ 30%] No build step for 'extern_paddle_inference'\r\n[ 31%] Performing install step for 'extern_paddle_inference'\r\n[ 32%] Completed 'extern_paddle_inference'\r\n[ 32%] Built target extern_paddle_inference\r\n[ 33%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_tensor.cc.o\r\n[ 33%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/common/multiclass_nms.cc.o\r\n[ 33%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/fastdeploy_model.cc.o\r\n[ 33%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/allocate.cc.o\r\n[ 34%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/clip.cc.o\r\n[ 34%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cast.cc.o\r\n[ 35%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_type.cc.o\r\n[ 36%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/concat.cc.o\r\n[ 36%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cumprod.cc.o\r\n[ 36%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/eigen.cc.o\r\n[ 37%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/elementwise.cc.o\r\n[ 37%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/full.cc.o\r\n[ 38%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gather_scatter_along_axis.cc.o\r\n[ 38%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gaussian_random.cc.o\r\n[ 38%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/linspace.cc.o\r\n[ 39%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/isfinite.cc.o\r\n[ 40%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/math.cc.o\r\n[ 40%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/pad.cc.o\r\n[ 40%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/quantile.cc.o\r\n[ 41%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/reduce.cc.o\r\n[ 41%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/slice.cc.o\r\n[ 42%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/softmax.cc.o\r\n[ 43%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/sort.cc.o\r\n[ 43%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/split.cc.o\r\n[ 43%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/tile.cc.o\r\n[ 44%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/transpose.cc.o\r\n[ 44%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime.cc.o\r\n[ 45%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/utils/utils.cc.o\r\n[ 45%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/ort/ops/adaptive_pool2d.cc.o\r\n[ 45%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/ort/ops/multiclass_nms.cc.o\r\n[ 46%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/ort/ort_backend.cc.o\r\n[ 46%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/ort/utils.cc.o\r\n[ 47%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/paddle/paddle_backend.cc.o\r\n[ 47%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/paddle/util.cc.o\r\n[ 48%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/backends/openvino/ov_backend.cc.o\r\nIn file included from /home/kevin/fastdeploy-git/src/FastDeploy/fastdeploy/runtime.cc:37:\r\n/home/kevin/fastdeploy-git/src/FastDeploy/./fastdeploy/backends/openvino/ov_backend.h:24:10: 致命错误：openvino/openvino.hpp：没有那个文件或目录   24 | #include \"openvino/openvino.hpp\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~\r\n编译中断。make[2]: *** [CMakeFiles/fastdeploy.dir/build.make:440：CMakeFiles/fastdeploy.dir/fastdeploy/runtime.cc.o] 错误 1\r\nmake[2]: *** 正在等待未完成的任务....\r\n[ 48%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/vision/classification/contrib/resnet.cc.o\r\nIn file included from /home/kevin/fastdeploy-git/src/FastDeploy/fastdeploy/backends/openvino/ov_backend.cc:15:\r\n/home/kevin/fastdeploy-git/src/FastDeploy/./fastdeploy/backends/openvino/ov_backend.h:24:10: 致命错误：openvino/openvino.hpp：没有那个文件或目录   24 | #include \"openvino/openvino.hpp\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~\r\n编译中断。make[2]: *** [CMakeFiles/fastdeploy.dir/build.make:552：CMakeFiles/fastdeploy.dir/fastdeploy/backends/openvino/ov_backend.cc.o] 错误 1\r\nmake[1]: *** [CMakeFiles/Makefile2:284：CMakeFiles/fastdeploy.dir/all] 错误 2\r\nmake: *** [Makefile:136：all] 错误 2\r\n==> 错误： 在 build() 中发生一个错误。    正在放弃...\r\n\r\n```",
        "state": "closed",
        "user": "Seele-Vollerei32",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-04T06:03:32+00:00",
        "updated_at": "2024-02-06T04:25:05+00:00",
        "closed_at": "2024-02-06T04:25:05+00:00",
        "comments_count": [
            "Seele-Vollerei32",
            "jiangjiajun",
            "horror-proton",
            "horror-proton",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Question",
            "Build",
            "Linux x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 782,
        "title": "PP-OCRv3 Rec 模型，FastDeploy 的准确率相较 PaddleOCR 有一定幅度的下降",
        "body": "## 环境\r\n\r\n- FastDeploy版本： develop 自行编译，尝试过 release/0.8, release/1.0, 目前使用的是 commit 4fd333f, 均稳定复现\r\n- 系统平台: Windows 11\r\n- 硬件： CPU\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\n\r\n仅使用识别模型，不使用检测。使用同一个模型（ch_PP-OCRv3_rec_infer），fastdeploy 在相比原本的 [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) 库识别效果下降了不少（PaddleOCR 的 [接口](https://github.com/MaaAssistantArknights/PaddleOCR) 是我们自己封装的，不过应该没啥影响），很多原来可以识别出来的图，现在识别错了，或者干脆没有识别结果了。\r\n\r\n一开始尝试的是 FastDeploy ONNX Runtime，发现识别率下降了不少，以为是模型转换带来的精度损失。故又使用了 FastDeploy Paddle Inference，识别结果 text 与 Ort 相同，score 差异微乎其微。但这些原本的 PaddleOCR 库都是可以正确识别且得分很高的。怀疑是不是 FastDeploy 在预处理的时候损失了精度或者弄错了什么？\r\n\r\n（注：我的环境等应该是正常的，大部分情况识别还是 OK 的，只是少部分图片一直不对）\r\n\r\n## 测试用例\r\n\r\n测试图片：\r\n![10k](https://user-images.githubusercontent.com/18511905/205291635-efa93d33-cc0b-4ec7-892c-ed556bea1d3d.png)\r\n\r\n\r\nPaddleOCR，识别正确且得分很高：\r\n![image](https://user-images.githubusercontent.com/18511905/205291879-22679294-6ec0-40a0-986d-d78ca2353aaa.png)\r\n\r\nFastDeploy ONNX Runtime，无识别结果，得分 0：\r\n![image](https://user-images.githubusercontent.com/18511905/205294193-f4bf545b-9843-4223-9520-92effc79016e.png)\r\n\r\nFastDeploy Paddle Inference，无识别结果，得分 0：\r\n![image](https://user-images.githubusercontent.com/18511905/205293736-a647fe31-e423-4c4c-ae7d-269076686522.png)\r\n\r\nFastDeploy 测试代码：\r\n```c++\r\n#include <opencv2/opencv.hpp>\r\n#include <fastdeploy/vision.h>\r\n\r\nint main()\r\n{\r\n\tfastdeploy::RuntimeOption option;\r\n\t//option.UseOrtBackend();\r\n\toption.UsePaddleInferBackend();\r\n\r\n\tfastdeploy::vision::ocr::Recognizer rec(\r\n\t\t\"PaddleOCR/rec/inference.pdmodel\",\r\n\t\t\"PaddleOCR/rec/inference.pdiparams\",\r\n\t\t\"PaddleOCR/ppocr_keys_v1.txt\",\r\n\t\toption);\r\n\r\n\tif (!rec.Initialized()) {\r\n\t\treturn -1;\r\n\t}\r\n\r\n\tcv::Mat image = cv::imread(\"10k.png\");\r\n\r\n\tstd::vector<std::string> texts;\r\n\tstd::vector<float> scores;\r\n\trec.BatchPredict({ image }, &texts, &scores);\r\n\r\n\tif (texts.empty() || scores.empty()) {\r\n\t\tstd::cerr << \"empty\" << std::endl;\r\n\t}\r\n\telse {\r\n\t\tstd::cout << \"text:\" << texts.front() << \", score:\" << scores.front() << std::endl;\r\n\t}\r\n\r\n\treturn 0;\r\n}\r\n```\r\n\r\n## 题外话\r\n\r\n`release/0.8` 的时候还是有 `Recognizer::Predict` 的，支持传入单张图片，现在咋只有 `Recognizer::BatchPredict` 了（只能传入一组图片）？嘛虽然也没什么影响，感觉 det 都有 rec 却删了这个挺怪的_(:з」∠)_\r\n\r\n## 再题外话\r\n\r\n看各位在一些 issue 下回复问为啥用 ORT 而不是 PDInfer，我想说 PDInfer 吃内存实在是吃的太厉害的，加载同一个模型，检测 + 识别 720p 同一张图片，PDInfer 后端要占用 700M 左右内存，且再识别又会吃的更多，我也不知道是 cache 还是内存泄漏。ORT 只占用 300M 不到，而且速度能快一倍以上。不过这个应该不是 FastDeploy 的问题，PaddleOCR 那边也是这样\r\n\r\n## 再再题外话\r\n\r\n这个库实在是太方便了，感谢各位！\r\n",
        "state": "closed",
        "user": "MistEO",
        "closed_by": "MistEO",
        "created_at": "2022-12-02T12:44:07+00:00",
        "updated_at": "2022-12-21T07:57:45+00:00",
        "closed_at": "2022-12-21T07:53:21+00:00",
        "comments_count": [
            "MistEO",
            "yunyaoXYY",
            "MistEO",
            "MistEO",
            "yunyaoXYY",
            "yunyaoXYY",
            "MistEO"
        ],
        "labels": [
            "OCR",
            "Model Precision"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 784,
        "title": "yolov7模型转换成paddlepaddle模型出错",
        "body": "## Environment\r\n\r\nOS Platform:  Windows x64 \r\nHardware: CPU\r\nProgram Language: e.g. Python 3.9\r\n\r\nfastdeploy-tools   0.0.1\r\nx2paddle           1.3.9\r\npaddlepaddle       2.4.0\r\n\r\n## Problem description\r\n说明：转换的onnx模型，是从yolov7的pt模型转换过来的，使用的方法，在git的yolov7 repo的根目录下执行：\r\n```\r\npython export.py --grid --dynamic --weights weight/yolov7-finetune.pt\r\n```\r\n然后，再执行如下命令，报错信息如下：\r\n```\r\n(x2paddle) E:\\git_workspace\\yolov7>fastdeploy convert --framework onnx --model yolov7-finetune.onnx --save_dir pd_model\r\nTraceback (most recent call last):\r\n  File \"D:\\anaconda3\\envs\\x2paddle\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"D:\\anaconda3\\envs\\x2paddle\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\anaconda3\\envs\\x2paddle\\Scripts\\fastdeploy.exe\\__main__.py\", line 7, in <module>\r\n  File \"D:\\anaconda3\\envs\\x2paddle\\lib\\site-packages\\common_tools\\common_tools.py\", line 109, in main\r\n    onnx2paddle(\r\nTypeError: onnx2paddle() got an unexpected keyword argument 'input_shape_dict'\r\n```",
        "state": "closed",
        "user": "lifw555",
        "closed_by": "lifw555",
        "created_at": "2022-12-02T14:56:06+00:00",
        "updated_at": "2023-02-09T13:35:16+00:00",
        "closed_at": "2023-02-09T13:35:16+00:00",
        "comments_count": [
            "wjj19950828",
            "lifw555",
            "lifw555",
            "lifw555",
            "Sun-Happy-YKX",
            "lifw555"
        ],
        "labels": [
            "Bug",
            "Object Detection",
            "X2Paddle"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 789,
        "title": "FastDeploy加载PaddleSeg导出模型的推理结果与Paddleseg推理结果不一致",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 1.0.0 cpu版本\r\n- 系统平台: Windows x64(Windows10)\r\n- 硬件：  Cpu I5-12400F\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\nfd 加载PaddleSeg导出的模型，推理结果和 使用PaddleSeg本身推理模块推理出来的结果不一致。\r\n模型是语义分割模型，分割类别有0,1,2。\r\nfd推理的结果会在类别2的边缘增加类别1。实际标注数据类别2的边缘不存在类别1，PaddleSeg推理的结果正常。\r\n下图是，屏蔽的类别2的情况，明显看到fd推理结果在类别2边缘增加了类别1的轮廓\r\n\r\n![1](https://user-images.githubusercontent.com/63178095/205495113-fb1d2be5-079d-4bf0-a949-c4888d43654e.jpg)\r\n\r\n",
        "state": "closed",
        "user": "FjjRichard",
        "closed_by": "FjjRichard",
        "created_at": "2022-12-04T14:05:19+00:00",
        "updated_at": "2022-12-06T11:37:10+00:00",
        "closed_at": "2022-12-06T11:37:10+00:00",
        "comments_count": [
            "felixhjh",
            "FjjRichard",
            "felixhjh",
            "FjjRichard",
            "felixhjh",
            "FjjRichard"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 793,
        "title": "视频文件读取失败",
        "body": "这是.cc文件\r\n![cc](https://user-images.githubusercontent.com/58615953/205540483-b303e3eb-5fdc-443c-930d-e1ff809bc324.png)\r\n下面是报错情况\r\n![ccc](https://user-images.githubusercontent.com/58615953/205540549-d1005c4f-5a84-4629-9f49-7441c72f3be0.png)\r\n\r\n",
        "state": "closed",
        "user": "GeT-RiGhTTT",
        "closed_by": "GeT-RiGhTTT",
        "created_at": "2022-12-05T03:04:32+00:00",
        "updated_at": "2022-12-06T11:49:30+00:00",
        "closed_at": "2022-12-06T11:49:30+00:00",
        "comments_count": [
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 798,
        "title": "Load model from ONNX failed while initliazing TrtBackend - Yolov5",
        "body": "## Environment\r\n\r\nFastDeploy version:  latest code in develop branch\r\nOS Platform: jetson\r\nHardware: e.g. jetson nano\r\nProgram Language: e.g. c plus plus\r\n\r\n## Problem description\r\nUsing jetson I receive the below error with yolov5 .\r\n\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(637)::CreateTrtEngineFromOnnx\tFailed to parse ONNX model by TensorRT.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(263)::InitFromOnnx\tFailed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime.cc(764)::CreateTrtBackend\tLoad model from ONNX failed while initliazing TrtBackend.\r\nAborted (core dumped)\r\n\r\nI built the library according to build on jetson docs.\r\n\r\n",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "UygarUsta99",
        "created_at": "2022-12-05T09:03:15+00:00",
        "updated_at": "2022-12-05T14:32:34+00:00",
        "closed_at": "2022-12-05T14:32:34+00:00",
        "comments_count": [
            "jiangjiajun",
            "UygarUsta99",
            "UygarUsta99",
            "UygarUsta99"
        ],
        "labels": [
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 809,
        "title": "Error in Build FastDeploy Library on Nvidia Jetson Platform",
        "body": "## Environment\r\n\r\nFastDeploy version: the latest code in develop branch\r\nOS Platform: Jetson AGX Xavier\r\nProgram Language: e.g. Python 3.6\r\n\r\n## Problem description\r\nPython build success with only onnxruntime on CPU. When running with the trt backend, libonnxruntime.so is not found.\r\n",
        "state": "closed",
        "user": "OwenHuaKargo",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-06T22:17:39+00:00",
        "updated_at": "2024-02-06T04:25:04+00:00",
        "closed_at": "2024-02-06T04:25:04+00:00",
        "comments_count": [
            "OwenHuaKargo",
            "jiangjiajun",
            "OwenHuaKargo",
            "jiangjiajun",
            "OwenHuaKargo",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 811,
        "title": "这个框架Anomaly.Paddle能不能推理的",
        "body": "`https://github.com/ultranity/Anomaly.Paddle`\r\n\r\n我看模型也是 pdparams",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-07T02:13:58+00:00",
        "updated_at": "2024-02-06T04:25:03+00:00",
        "closed_at": "2024-02-06T04:25:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 812,
        "title": "python打包exe运行时找不到dll",
        "body": "## 环境\r\nwin10 cuda11.8 python3.8 yolov7 gpu版本\r\n\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\r\nTraceback (most recent call last):\r\n  File \"fastdeploy\\c_lib_wrap.py\", line 164, in <module>\r\nImportError: DLL load failed while importing fastdeploy_main: 动态链接库(DLL)初始化例程失败。\r\n",
        "state": "closed",
        "user": "maokeuncle",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-07T03:19:43+00:00",
        "updated_at": "2024-05-07T06:40:33+00:00",
        "closed_at": "2024-05-07T06:40:33+00:00",
        "comments_count": [
            "jiangjiajun",
            "LiJiaming5558",
            "weiweijeff",
            "zachary-zheng",
            "WilliamQf-AI",
            "zachary-zheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 819,
        "title": "建议：人脸对其，建议对齐后，输出仿射变换后人脸",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0或自行编译的develop代码（附上自行编译的方式，及cmake时print的编译选项截图）\r\n- 系统平台: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 硬件： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\n- 附上详细的问题日志有助于工程师快速定位分析\r\n- 性能问题，描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- 模型部署出错\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供自己的代码使用方式或自己的模型，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "xinsuinizhuan",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-07T07:00:33+00:00",
        "updated_at": "2024-02-06T04:25:01+00:00",
        "closed_at": "2024-02-06T04:25:01+00:00",
        "comments_count": [
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement",
            "Question",
            "FaceAlign"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 818,
        "title": "windows cmake gui下编译后，出现问题反馈",
        "body": "windows10 VS2019 cmake gui编译后：\r\n1、vision_facealign三个Vsion工程和headpos工程报错，链接错误，说是_PathApend函数链接错误，需要在链接库里面增加shlwapi.lib\r\n2、编译后在bin/release下只生成了exe，未将所有需要的库拷贝到release下，需要手动拷贝。\r\n",
        "state": "closed",
        "user": "xinsuinizhuan",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-07T06:58:27+00:00",
        "updated_at": "2024-02-06T04:25:02+00:00",
        "closed_at": "2024-02-06T04:25:02+00:00",
        "comments_count": [
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Windows x64",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 820,
        "title": "有人流量出入口统计模型的部署案例吗",
        "body": "有人流量出入口统计模型的部署案例吗",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-07T07:02:32+00:00",
        "updated_at": "2024-02-06T04:25:00+00:00",
        "closed_at": "2024-02-06T04:25:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "MontaEllis",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 822,
        "title": "onnx inference weird result",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: Windows x64\r\nHardware: cpu\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n\r\n\r\nHi, \r\nI got the latest FastDeploy SDK and installed successfully, and I am facing some issues with inference results.\r\nTesting 2 models (same classes, all the same parameters) just with different datasets(added more data) trained and converted to onnx, but one inferences fine, the other one draws boxes and label incorrectly.\r\nBoth onnx works fine when I do onnxruntim inference in python code.\r\n\r\ntest model : 2 Yolox tiny model converted to onnx (both opset 11)\r\ninference yolox exe command:\r\n$ ./vision_detection_yolox_infer.exe yolox_tiny_20221206_v5.onnx ./imgs/5.jpg 0\r\n\r\nWhen I look at the Netron, I don't see any differences at all.\r\nCould you please let me know what might be the reason for these weird results?\r\n\r\n\r\n\r\nThank you!",
        "state": "closed",
        "user": "gedance",
        "closed_by": "gedance",
        "created_at": "2022-12-07T08:50:05+00:00",
        "updated_at": "2022-12-09T00:02:54+00:00",
        "closed_at": "2022-12-09T00:00:32+00:00",
        "comments_count": [
            "jiangjiajun",
            "gedance",
            "gedance"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 828,
        "title": "FastDeploy/docs/cn/faq/custom_opencv.md",
        "body": "\r\n能读取视频，但是帧为空\r\n![111](https://user-images.githubusercontent.com/58615953/206196194-95c834fa-7440-4f20-8c8b-98d103dd5655.png)\r\n![1111](https://user-images.githubusercontent.com/58615953/206196287-846a96f0-42b5-4f06-8fa3-870529793c73.png)\r\n",
        "state": "closed",
        "user": "GeT-RiGhTTT",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-07T13:49:44+00:00",
        "updated_at": "2024-02-06T04:24:59+00:00",
        "closed_at": "2024-02-06T04:24:59+00:00",
        "comments_count": [
            "GeT-RiGhTTT",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 843,
        "title": "benchmark_ppocr.py 未能创建模型",
        "body": "  环境：windows 10 64位\r\n  git clone的最新的FastDeploy代码。\r\n 进入FastDeploy\\benchmark目录，执行下面的命令行\r\n(paddle_env) D:\\paddle\\FastDeploy\\benchmark>.\\benchmark_ppocr.py --model D:\\paddle\\models\\quat_OCRv3 --det_model det --rec_model rec    --cls_model cls --image d:\\openvino\\pic_car --cpu_num_thread 24 --iter_num 300 --backend ort  --rec_label_file D:\\paddle\\models\\quat\\imglist.txt\r\n\r\n发现在在调用到 fd.vision.ocr.DBDetector这函数时，Python程序就退出了\r\n![6494914c74c8335a3d990fbae5ef36a](https://user-images.githubusercontent.com/77605321/206650458-cc1a0851-6a2c-4c38-81ef-e00f41621a53.jpg)\r\n\r\n说明：\r\n1。 D:\\paddle\\models\\quat_OCRv3\\det和D:\\paddle\\models\\quat_OCRv3\\rec目录下的模型文件来自 PaddleOCR开源大礼包\\OCR场景应用模型库\\车牌识别\\蓝绿黄牌识别\\CCPD中带quant的模型。\r\n2。backend 参数，换作paddle也是出错\r\n\r\n",
        "state": "closed",
        "user": "dchld",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-09T07:39:24+00:00",
        "updated_at": "2024-02-06T04:24:58+00:00",
        "closed_at": "2024-02-06T04:24:58+00:00",
        "comments_count": [
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 848,
        "title": " 官方环境 error while loading shared libraries: libopencv_video.so.3.4:",
        "body": "## 环境\r\n- 官方环境：docker pull paddlepaddle/fastdeploy:1.0.0-gpu-cuda11.4-trt8.4-21.10\r\n\r\n## 问题描述\r\n\r\n参照https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/sr/ppmsvsr/cpp说明进行执行出错：\r\n预编译使用：[fastdeploy-linux-x64-1.0.1.tgz](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-linux-x64-1.0.1.tgz)\r\n\r\n命令：./infer_demo PP-MSVSR_reds_x4 vsr_src.mp4 2 2\r\n错误：./infer_demo: error while loading shared libraries: libopencv_video.so.3.4: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "zdyshine",
        "closed_by": "zdyshine",
        "created_at": "2022-12-09T12:50:13+00:00",
        "updated_at": "2022-12-13T08:17:47+00:00",
        "closed_at": "2022-12-13T08:17:47+00:00",
        "comments_count": [
            "jiangjiajun",
            "zdyshine"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 844,
        "title": "imgasize设置问题",
        "body": "\r\n之前使用0.6版本的时候遇到需要修改图片尺寸 ，infer.cc里面加上model.size_={640,640};但是1.0版本里面这样加就不对了 ，应该怎么去设置呢  谢谢",
        "state": "closed",
        "user": "sl00001",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-09T08:36:10+00:00",
        "updated_at": "2024-02-06T04:24:57+00:00",
        "closed_at": "2024-02-06T04:24:56+00:00",
        "comments_count": [
            "jiangjiajun",
            "sl00001",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 847,
        "title": "Fail to convert multiclass_nms3 /The 'nms_eta' must be in close range [,1.0]. Got:1.1",
        "body": "训练自己数据部署出现问题，但PaddleDetection模型可以正常导出使用，使用FastDeploy推理出现如下问题：\r\n\r\n本地运行：\r\n![3b0855da55b4cdd883f6c8ca4449dcd](https://user-images.githubusercontent.com/44935269/206703490-ffd50ef1-2b71-49b4-8e43-be987f627f29.png)\r\n\r\nAIStudio 运行：\r\n![2ae35c97d9160506e2cf5ab7275cf89](https://user-images.githubusercontent.com/44935269/206703523-d199f3f5-e293-4367-93be-902bd3791c31.jpg)\r\n",
        "state": "closed",
        "user": "LiQiang0307",
        "closed_by": "LiQiang0307",
        "created_at": "2022-12-09T12:35:10+00:00",
        "updated_at": "2022-12-10T02:20:30+00:00",
        "closed_at": "2022-12-10T02:20:30+00:00",
        "comments_count": [
            "jiangjiajun",
            "LiQiang0307",
            "jiangjiajun",
            "LiQiang0307"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 850,
        "title": "C++部署编译出错",
        "body": "## CPU-C++部署编译\r\n\r\n- 系统平台: \r\n![111](https://user-images.githubusercontent.com/58615953/206852022-5e6b5198-6006-45c4-9fa3-f8c031b0483c.png)\r\n\r\n## 问题描述\r\n- 按照cpp.md编译出错 ，错误如下\r\n![1111](https://user-images.githubusercontent.com/58615953/206852062-cb22d837-bd16-4b48-a113-36aa0aac2dd6.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "GeT-RiGhTTT",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-10T11:10:21+00:00",
        "updated_at": "2024-02-06T04:24:55+00:00",
        "closed_at": "2024-02-06T04:24:55+00:00",
        "comments_count": [
            "jiangjiajun",
            "GeT-RiGhTTT",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "GeT-RiGhTTT",
            "GeT-RiGhTTT",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 849,
        "title": "Web Demo OCR无法检测和识别文本",
        "body": "按照教程https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/application/js/WebDemo.md 来部署web demo。进行OCR功能的测试。\r\n\r\n> 运行环境： win 10 + google chrome 107.0.5304.107 + node v18.12.1 + npm 8.19.2\r\n\r\n在模型加载阶段好像可以下载好det和rec模型（状态码都为200）\r\n\r\n![download_models](https://user-images.githubusercontent.com/12782418/206849465-1b1b0985-f91f-4ef2-8498-93d977cd3ae7.png)\r\n\r\n但在点击识别后，发现没有识别结果：\r\n![rec_result](https://user-images.githubusercontent.com/12782418/206849552-78af7140-e32d-4036-b378-a6e8c302284a.png)\r\n\r\n",
        "state": "closed",
        "user": "wujushan",
        "closed_by": "wujushan",
        "created_at": "2022-12-10T10:20:53+00:00",
        "updated_at": "2022-12-12T09:20:39+00:00",
        "closed_at": "2022-12-12T09:20:39+00:00",
        "comments_count": [
            "chenqianhe",
            "wujushan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 853,
        "title": "windows下c++ sdk部署paddledetection的yolov3",
        "body": "用paddledetection训练yolov3，部署按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows.md\r\nvs2019可以编译成功，但是运行时出现问题，\r\n![image](https://user-images.githubusercontent.com/31379701/206899956-72393c27-16a2-4f0f-8941-344b45d5c2b4.png)\r\n用官方的部署模型也无法运行，猜测可能是依赖库有些不兼容？",
        "state": "closed",
        "user": "DreamMaker777",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-11T11:03:46+00:00",
        "updated_at": "2024-02-06T04:24:54+00:00",
        "closed_at": "2024-02-06T04:24:54+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "DreamMaker777",
            "DreamMaker777",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 858,
        "title": "部署OCR时，是否可以只选择识别模型？",
        "body": "部署OCR时，是否可以只选择识别模型？不加载检测与分类模型，有的场景是只识别单行小图片，没有使用检测模型的必要，而且有时候检测单行文件会为空，不如直接进行识别。",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "wzy123456wzy",
        "created_at": "2022-12-12T07:30:42+00:00",
        "updated_at": "2022-12-12T07:37:24+00:00",
        "closed_at": "2022-12-12T07:37:06+00:00",
        "comments_count": [
            "wzy123456wzy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 869,
        "title": "fatal error: onnxruntime_cxx_api.h: No such file or directory",
        "body": "## 环境\r\nFastDeploy版本： \r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\n\r\n如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DORT_DIRECTORY=/workspace/onnxruntime-1.13.1 \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\nonnxruntime-1.13.1是从这里下载的: https://github.com/microsoft/onnxruntime/releases\r\n- 系统平台:\r\ndocker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04\r\n- 硬件： 3090\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\ncmake之后\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.1\r\n--   CMake command             : /usr/local/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   Build type                :\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;ENABLE_ORT_BACKEND;WITH_GPU;ENABLE_PADDLE_FRONTEND\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /workspace/FastDeploy/build/compiled_fastdeploy_sdk\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.0.0\r\n--   Paddle2ONNX version       : 1.0.5\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   WITH_GPU                  : ON\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              :\r\n--   BUILD_CUDA_SRC            : ON\r\n--   ENABLE_VISION             : OFF\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_DEBUG              :\r\n--   ENABLE_VISION_VISUALIZE   :\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /workspace/FastDeploy/build\r\n\r\n之后执行make -j12出错:\r\n\r\n/workspace/FastDeploy/./fastdeploy/backends/ort/ort_backend.h:23:10: fatal error: onnxruntime_cxx_api.h: No such file or directory\r\n   23 | #include \"onnxruntime_cxx_api.h\"  // NOLINT\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:440: CMakeFiles/fastdeploy.dir/fastdeploy/runtime.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[1]: *** [CMakeFiles/Makefile2:128: CMakeFiles/fastdeploy.dir/all] Error 2\r\nmake: *** [Makefile:136: all] Error 2\r\n\r\n我是想使用docker进行部署，仅使用ORT推理",
        "state": "closed",
        "user": "zdyshine",
        "closed_by": "zdyshine",
        "created_at": "2022-12-13T08:23:42+00:00",
        "updated_at": "2022-12-14T00:30:00+00:00",
        "closed_at": "2022-12-14T00:30:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "zdyshine",
            "zdyshine",
            "jiangjiajun",
            "zdyshine",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 865,
        "title": "yolov5 serving 部署有问题",
        "body": "## 环境\r\n- 镜像版本是 paddlepaddle/fastdeploy:1.0.0-gpu-cuda11.4-trt8.4-21.10\r\n\r\n## 问题描述\r\n- 出错代码 https://github.com/PaddlePaddle/FastDeploy/blob/7415552cb2d7001fa0c5d69a12b6a1181395a989/examples/vision/detection/yolov5/serving/models/preprocess/1/model.py#L72\r\n- 当前版本下预处理代码有变化，没有preprocess方法。postprocess同理\r\n",
        "state": "closed",
        "user": "richjjj",
        "closed_by": "richjjj",
        "created_at": "2022-12-13T00:45:50+00:00",
        "updated_at": "2022-12-14T03:06:44+00:00",
        "closed_at": "2022-12-14T03:06:44+00:00",
        "comments_count": [
            "wjj19950828",
            "richjjj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 870,
        "title": "error while loading shared libraries: libfastdeploy.so.0.0.0: cannot open shared object file: No such file or directory",
        "body": "## 环境\r\n\r\n- FastDeploy版本：git clone https://github.com/PaddlePaddle/FastDeploy.git\r\n- 系统平台: Linux firefly 4.19.219  aarch64 aarch64 aarch64 GNU/Linux\r\n- 硬件： AIO-RK3568J\r\n- 编译语言： C++\r\n\r\n## 编译流程\r\n按照下面链接中的编译c++ SDK执行\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rknpu2.md\r\ncd FastDeploy\r\nfirefly@firefly:~/FastDeploy/build$ cmake .. -DENABLE_ORT_BACKEND=ON -DENABLE_RKNPU2_BACKEND=ON -DENABLE_VISION=ON -DRKNN2_TARGET_SOC=RK356X -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.3 \r\nfirefly@firefly:~/FastDeploy/build$ make -j8\r\nfirefly@firefly:~/FastDeploy/build$ make install\r\n上述编译流程均成功\r\n\r\n\r\n## 问题描述\r\n- 模型部署出错\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rknpu2/cpp\r\n生成基本目录，编译均成功,在运行例程中出现错误提示:error while loading shared libraries: libfastdeploy.so.0.0.0: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-13T08:46:07+00:00",
        "updated_at": "2022-12-14T02:26:48+00:00",
        "closed_at": "2022-12-14T02:26:48+00:00",
        "comments_count": [
            "jiangjiajun",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 872,
        "title": "vehicle_attribute_model 模型报错",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-windows-cpu-1.0.0\r\n- 系统平台:  Windows x64(Windows10)\r\n- 编译语言： Python(3.10）\r\n\r\n## 问题描述\r\n代码：\r\n```python\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\n# 使用的这个模型  \r\n# https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip\r\n\r\nbase = 'some_path/vehicle_attribute_model'\r\nmodel = vision.classification.PPLCNet(f\"{base}/model.pdmodel\",\r\n                                 f\"{base}/model.pdiparams\",\r\n                                 f\"{base}/infer_cfg.yml\")\r\n\r\n# 图片 https://raw.githubusercontent.com/PaddlePaddle/PaddleClas/release/2.4/deploy/images/PULC/vehicle_attribute/0002_c002_00030670_0.jpg\r\n\r\nim = cv2.imread(f\"{base}/00che.jpeg\")\r\nresult = model.predict(im.copy())\r\nprint(result)\r\n\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n```\r\n\r\n报错如下：\r\n\r\n```\r\nfastdeploy\\vision\\classification\\ppcls\\__init__.py\", line 97, in predict\r\n    return self._model.predict(im)\r\nRuntimeError: [ PARAMETER_MISMATCH ] Failed to set input blob with precision: U8, if CNNNetwork input blob precision is: FP32\r\n```\r\n\r\n\r\n",
        "state": "closed",
        "user": "dwdcth",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-13T09:06:58+00:00",
        "updated_at": "2024-02-06T04:24:52+00:00",
        "closed_at": "2024-02-06T04:24:52+00:00",
        "comments_count": [
            "jiangjiajun",
            "dwdcth",
            "jiangjiajun",
            "dwdcth",
            "dwdcth",
            "baiyu7z",
            "jiangjiajun",
            "dwdcth",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 873,
        "title": "【12月部署月-直播课来啦】FastDeploy联合十大硬件公司，《产业级AI 模型部署全攻略》，12月12日～12月30日",
        "body": "![12月部署月](https://user-images.githubusercontent.com/54695910/207280316-f95d56eb-0054-4ae7-848d-0056bf3332a1.jpg)\r\n",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-13T09:32:58+00:00",
        "updated_at": "2024-02-06T04:24:51+00:00",
        "closed_at": "2024-02-06T04:24:51+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 877,
        "title": "C++GPU 编译错误 cuda",
        "body": "hi,我通过下载预编译文件fastdeploy-linux-x64-gpu-1.0.1。在\r\nFastDeploy/build 工作目录下执行\r\n$ cmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-x64-gpu-1.0.1\r\n\r\n我的环境：\r\ncuda 11.6\r\n\r\n出现了cuda相关错误：\r\n-- Check for working CUDA compiler: /usr/bin/nvcc\r\n-- Check for working CUDA compiler: /usr/bin/nvcc - broken\r\nCMake Error at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.22/Modules/CMakeTestCUDACompiler.cmake:56 (message):\r\n  The CUDA compiler\r\n\r\n    \"/usr/bin/nvcc\"\r\n\r\n  is not able to compile a simple test program.\r\n\r\n  It fails with the following output:\r\n\r\n    Change Dir: /data/LX/FastDeploy/build/CMakeFiles/CMakeTmp\r\n    \r\n    Run Build Command(s):/usr/bin/ninja cmTC_29932 && [1/2] Building CUDA object CMakeFiles/cmTC_29932.dir/main.cu.o\r\n    FAILED: CMakeFiles/cmTC_29932.dir/main.cu.o \r\n    /usr/bin/nvcc      -c /data/LX/FastDeploy/build/CMakeFiles/CMakeTmp/main.cu -o CMakeFiles/cmTC_29932.dir/main.cu.o\r\n    ptxas fatal   : Value 'sm_30' is not defined for option 'gpu-name'\r\n    ninja: build stopped: subcommand failed.\r\n\r\n",
        "state": "closed",
        "user": "827346462",
        "closed_by": "wang-xinyu",
        "created_at": "2022-12-14T03:02:19+00:00",
        "updated_at": "2022-12-14T06:45:57+00:00",
        "closed_at": "2022-12-14T06:45:57+00:00",
        "comments_count": [
            "827346462",
            "wang-xinyu",
            "wang-xinyu",
            "827346462",
            "827346462",
            "827346462"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 883,
        "title": "C++,GPU编译opencv报错",
        "body": "FastDeploy版本：\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\n\r\n如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON\r\n-DWITH_GPU=ON\r\n-DORT_DIRECTORY=/workspace/onnxruntime-1.13.1\r\n-DCUDA_DIRECTORY=/usr/local/cuda\r\n-DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk\r\nonnxruntime-1.13.1是从这里下载的: https://github.com/microsoft/onnxruntime/releases\r\n\r\n系统平台:\r\ndocker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04\r\n硬件： 3090\r\n编译语言： C++\r\n\r\n# 昨天尝试编译没有opencv的版本成功了，今天准备加上opencv，进行如下操作：\r\n\r\n从网页https://opencv.org/releases/下载OpenCV – 4.5.5的Sources代码，然后执行：\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DORT_DIRECTORY=/workspace/onnxruntime-linux-x64-gpu-1.12.0 \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/workspace/opencv-4.5.5\r\n出错如下：\r\nCMake Error at cmake/opencv.cmake:142 (find_package):\r\n  Could not find a package configuration file provided by \"OpenCV\" with any\r\n  of the following names:\r\n\r\n    OpenCVConfig.cmake\r\n    opencv-config.cmake\r\n\r\n  Add the installation prefix of \"OpenCV\" to CMAKE_PREFIX_PATH or set\r\n  \"OpenCV_DIR\" to a directory containing one of the above files.  If \"OpenCV\"\r\n  provides a separate development package or SDK, be sure it has been\r\n  installed.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:420 (include)\r\n\r\n尝试locate OpenCVConfig.cmake和find / OpenCVConfig.cmake都没有找到该文件\r\n请问是我opencv代码下载错了吗？该怎么操作。\r\n",
        "state": "closed",
        "user": "zdyshine",
        "closed_by": "zdyshine",
        "created_at": "2022-12-14T09:54:52+00:00",
        "updated_at": "2022-12-16T01:40:22+00:00",
        "closed_at": "2022-12-16T01:40:22+00:00",
        "comments_count": [
            "jiangjiajun",
            "zdyshine",
            "jiangjiajun",
            "zdyshine",
            "jiangjiajun",
            "zdyshine",
            "jiangjiajun",
            "zdyshine",
            "jiangjiajun",
            "zdyshine",
            "zdyshine",
            "zdyshine",
            "jiangjiajun",
            "zdyshine"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 881,
        "title": "paddleclas配置文件出错，导致fastdeploy无法识别",
        "body": "使用paddleclas，基于 MobileNetV3_large_x1_0_ssld完成训练，效果很好\r\n移植到fast deploy，使用paddleclas的inference_cls.yml作为配置文件，识别出错",
        "state": "closed",
        "user": "l976308589",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-14T07:32:43+00:00",
        "updated_at": "2024-02-06T04:24:50+00:00",
        "closed_at": "2024-02-06T04:24:50+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 888
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 889
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 884,
        "title": "libonnxruntime.so.1.12.0 no such file",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.1\r\nOS Platform: Linux x86\r\nHardware: Nvidia GPU 3090  CUDA 11.6 CUDNN 8.4\r\nProgram Language: Python 3.6\r\n\r\n## Problem description\r\n我使用预编译包C++ SDK推理 examples/vision/detection/paddledetection/cpp 内的文件\r\n\r\n按照example 编译\r\n编译安装成功\r\n\r\n运行infer_ppyoloe_demo出现libonnxruntime.so.1.12.0 错误\r\n./infer_ppyoloe_demo: error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory\r\n\r\n但项目内存在：./fastdeploy-linux-x64-gpu-1.0.1/third_libs/install/onnxruntime/lib/libonnxruntime.so.1.12.0",
        "state": "closed",
        "user": "827346462",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-14T10:23:58+00:00",
        "updated_at": "2024-02-06T04:24:49+00:00",
        "closed_at": "2024-02-06T04:24:49+00:00",
        "comments_count": [
            "827346462",
            "jiangjiajun",
            "827346462",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 887,
        "title": "yolov5做nms时，有一个agnostic参数，确定是否在类间做nms，但fastdeploy的predict函数只有nms的iou阈值设置，不知道这个是在哪块设置呢？",
        "body": null,
        "state": "closed",
        "user": "liuyiche",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-15T03:55:46+00:00",
        "updated_at": "2024-02-06T04:24:48+00:00",
        "closed_at": "2024-02-06T04:24:48+00:00",
        "comments_count": [
            "wjj19950828",
            "liuyiche",
            "liuyiche",
            "wjj19950828",
            "liuyiche",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 891,
        "title": "PPOCRV3服务化部署,切换TRT后端报错",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 官方docker的GPU版本fastdeploy-linux-gpu-1.0.0\r\n- 系统平台: Linux x64(Ubuntu 18.04) \r\n- 硬件： NVIDIA V100\r\n- 编译语言： C++ / Python3.8\r\n\r\n## 问题描述\r\n- 参考PP-OCR服务化部署示例，在docker中部署PPOCRV3的服务化版本，切换到TensorRT引擎，参数如下：\r\noptimization {\r\n  execution_accelerators {\r\n    gpu_execution_accelerator : [\r\n      {\r\n        name : \"tensorrt\"\r\n        # 使用TensorRT的FP16推理,其他可选项为: trt_fp32、trt_int8\r\n        parameters { key: \"precision\" value: \"trt_fp16\" }\r\n      }\r\n    ]\r\n  }\r\n}\r\n发送预测请求后，报如下错误：\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update\t[New Shape Out of Range] input name: x, shape: [1, 3, 960, 736], The shape range before: min_shape=[-1, 3, -1, -1], max_shape=[-1, 3, -1, -1].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update\t[New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 960, 736], max_shape=[1, 3, 960, 736].\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(296)::Infer\tTensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(495)::BuildTrtEngine\t[TrtBackend] Use FP16 to inference.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine\tStart to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log\t2: [pluginV2DynamicExtRunner.cpp::execute::115] Error Code 2: Internal Error (Assertion status == kSTATUS_SUCCESS failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log\t2: [builder.cpp::buildSerializedNetwork::636] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(563)::BuildTrtEngine\tFailed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(394)::SetInputs\tTRTBackend SetInputs not find name:x\r\nSignal (6) received.\r\n 0# 0x0000558EC5DF48A9 in fastdeployserver\r\n 1# 0x00007EFC6FF46210 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# gsignal in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# abort in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 4# fastdeploy::TrtBackend::SetInputs(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> > const&) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 5# fastdeploy::TrtBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 6# fastdeploy::Runtime::Infer() in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.0\r\n 7# 0x00007EFC50020134 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n 8# 0x00007EFC50023B96 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n 9# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n10# 0x00007EFC70AD283A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n11# 0x00007EFC70AD304D in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n12# 0x00007EFC70987801 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n13# 0x00007EFC70ACCDC7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n14# 0x00007EFC70334DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n15# 0x00007EFC707B2609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n16# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\n",
        "state": "closed",
        "user": "wxz5459",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-15T10:57:29+00:00",
        "updated_at": "2024-02-06T04:24:47+00:00",
        "closed_at": "2024-02-06T04:24:47+00:00",
        "comments_count": [
            "jiangjiajun",
            "wxz5459",
            "jiangjiajun",
            "wxz5459",
            "HexToString",
            "wxz5459",
            "yunyaoXYY",
            "wxz5459",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 895,
        "title": "请问 infer 函数支持拼图吗？",
        "body": "参考这个案例，会遇到需要大图切图再训练的情况，paddledetection 的infer支持拼图预测\r\n\r\n!python tools/infer.py -c configs/smalldet/ppyoloe_crn_l_80e_sliced_visdrone_640_025.yml -o weights=output/ppyoloe_crn_l_80e_sliced_visdrone_640_025/best_model.pdparams --infer_img=../data/test/1__H2_817171_IO-NIO198M_210121A0050-1-1.jpg --draw_threshold=0.25 --slice_infer --slice_size 640 640 --overlap_ratio 0.25 0.25 --combine_method=nms --match_threshold=0.6 --match_metric=iou --save_results=True\r\n\r\nhttps://aistudio.baidu.com/aistudio/projectdetail/5122094?contributionType=1\r\n\r\n如果用这个案例训练的模型，导出后在fastdeploy预测，可以支持infer_img是原图，但是先切图再预测，再拼图，再传回拼图的预测结果吗？",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-15T13:57:57+00:00",
        "updated_at": "2024-02-06T04:24:46+00:00",
        "closed_at": "2024-02-06T04:24:46+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 900,
        "title": "请问Vectornet部署支持吗？",
        "body": "*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- FastDeploy版本： fastdeploy-linux-gpu-0.8.0\r\n- 系统平台: Linux x64(Ubuntu 20.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 硬件： 说明具体硬件型号，如 Nvidia GPU RTX3060 CUDA 11.4 CUDNN 8.3\r\n- 编译语言： C++ \r\n\r\n## 问题描述\r\n请问支持vectornet部署吗？\r\n\r\n*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n",
        "state": "closed",
        "user": "tanjatang",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-16T08:38:20+00:00",
        "updated_at": "2024-02-06T04:24:45+00:00",
        "closed_at": "2024-02-06T04:24:45+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 901,
        "title": "集成新模型出錯",
        "body": "FastDeploy版本：\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\n系统平台:\r\ndocker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04\r\n增加模型流程:\r\n参考https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/develop_a_new_model.md#test\r\n\r\n1.新增文件夹\r\nmkdir FastDeploy/fastdeploy/vision/repaire\r\nmkdir FastDeploy/fastdeploy/vision/repaire/denoise\r\n2.新增文件\r\nvim FastDeploy/fastdeploy/vision/repaire/denoise/denoise.h\r\nvim FastDeploy/fastdeploy/vision/repaire/denoise/denoise.cc\r\n并对应修改内部处理逻辑\r\n3.在vision.h文件中加入新增模型文件\r\nvim FastDeploy/fastdeploy/vision.h\r\n#ifdef ENABLE_VISION\r\n#include \"fastdeploy/vision/repaire/denoise/denoise.h\"\r\n#endif\r\n\r\n4.编译\r\ncmake .. -DENABLE_ORT_BACKEND=ON          -DWITH_GPU=ON          -DCUDA_DIRECTORY=/usr/local/cuda          -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy_sdk          -DENABLE_VISION=ON\r\nmake -j8\r\nmake install\r\n\r\n5.测试\r\nvim FastDeploy/examples/test_repaire/test.cc\r\n\r\nauto model = fastdeploy::vision::repaire::DENOISE(model_file, option);\r\n  if (!model.Initialized()) {\r\n    std::cerr << \"Failed to initialize.\" << std::endl;\r\n    return;\r\n  }\r\n\r\n执行代码：\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=/workspace/FastDeploy/build/compiled_fastdeploy_sdk\r\nmake\r\n报错：\r\nroot@6647dfdea427:/workspace/FastDeploy/examples/test_repaire/build# make\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/test.cc.o\r\n/workspace/FastDeploy/examples/test_repaire/test.cc: In function ‘int main(int, char**)’:\r\n/workspace/FastDeploy/examples/test_repaire/test.cc:19:36: error: ‘fastdeploy::vision::repaire’ has not been declared\r\n   19 |   auto model = fastdeploy::vision::repaire::DENOISE(model_file, option);\r\n      |                                    ^~~~~~~\r\n/workspace/FastDeploy/examples/test_repaire/test.cc:22:5: error: return-statement with no value, in function returning ‘int’ [-fpermissive]\r\n   22 |     return;\r\n      |     ^~~~~~\r\n/workspace/FastDeploy/examples/test_repaire/test.cc:28:22: error: ‘runtime_option’ was not declared in this scope\r\n   28 |   if (!runtime->Init(runtime_option)) {\r\n      |                      ^~~~~~~~~~~~~~\r\nmake[2]: *** [CMakeFiles/infer_demo.dir/build.make:76: CMakeFiles/infer_demo.dir/test.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/infer_demo.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2\r\n\r\n请问改怎么处理\r\n\r\n\r\n",
        "state": "closed",
        "user": "zdyshine",
        "closed_by": "zdyshine",
        "created_at": "2022-12-17T03:43:11+00:00",
        "updated_at": "2022-12-17T05:14:23+00:00",
        "closed_at": "2022-12-17T05:14:23+00:00",
        "comments_count": [
            "zdyshine"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 896,
        "title": "最后的执行程序如何得到一个.so动态库文件",
        "body": "感谢开源和指导，最后成功make到可执行二进制文件./runtime_demo。想请问下，如果想得到一个.so动态库文件，该如何操作？",
        "state": "closed",
        "user": "zdyshine",
        "closed_by": "zdyshine",
        "created_at": "2022-12-16T01:49:22+00:00",
        "updated_at": "2022-12-19T05:39:04+00:00",
        "closed_at": "2022-12-19T05:39:04+00:00",
        "comments_count": [
            "jiangjiajun",
            "zdyshine"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 902,
        "title": "yolov5推理速度比torch原生慢一些",
        "body": "*********************************************\r\n完整脚本和测试图片，网盘：\r\n链接：https://pan.baidu.com/s/1wK9psssA7mL71bof4RJeSQ \r\n提取码：7a3p \r\n*********************************************\r\n\r\n## 环境\r\n\r\n- FastDeploy版本： fastdeploy_python-1.0.1-cp38-cp38-win_amd64.whl\r\n- 系统平台:Windows x64(Windows10)\r\n- 硬件： 如 Nvidia GPU 3070TI， CUDA 11.6 CUDNN 8.5\r\n- 编译语言： Python 3.8\r\n\r\n## 问题描述\r\n针对相同的yolov5导出的onnx模型，先预加载运行一次，然后再运行10次计算平均时间，\r\ntorch的推理时间约为0.0132秒，fastdeploy的推理时间约为0.023秒。请问下，该如何配置优化。\r\n\r\n## fastdeploy测试脚本：\r\n'''\r\n车牌对象识别\r\n'''\r\nimport os\r\nimport cv2\r\nimport psutil\r\nfrom PIL import Image\r\nimport fastdeploy as fd\r\nfrom common import is_en_fp16, set_en_fp16, get_gpuid, get_most_idle_gpu, plot_one_box, merge_iou\r\nimport numpy as np\r\nimport time\r\n\r\nresnet_model = None\r\nconf_thres = 0.3\r\niou_thres = 0.3\r\nweights = 'weights/plates.onnx'\r\ntrtfile = 'weights/plates.trt'\r\nnames   = ['plate']\r\nsel_color = (12,16,255)\r\n\r\n\r\ndef load_model():\r\n    global resnet_model, names, colors, imgsz\r\n    if resnet_model:\r\n        return\r\n\r\n    # 模型推理的配置信息\r\n    option = fd.RuntimeOption()\r\n\r\n    # 切换使用CPU/GPU\r\n    gpuid = get_gpuid()\r\n    if gpuid == -1:\r\n        gpuid = get_most_idle_gpu()\r\n    print('chk_plates use_gpuid:'+str(gpuid))\r\n    option.use_gpu(gpuid)   # 使用GPU\r\n\r\n    # 切换不同后端\r\n    option.use_trt_backend() # TensorRT\r\n    option.set_trt_cache_file(trtfile)\r\n    if is_en_fp16():\r\n        option.enable_trt_fp16()\r\n    # option.set_trt_input_shape('images', (1, 3, 640, 640), (1, 3, 1280, 1280), (1, 3, 1280, 1280))\r\n    # option.set_trt_max_workspace_size(1 << 28) # 256M 1GB\r\n\r\n    # \r\n    model_file = weights\r\n    params_file = ''\r\n    model = fd.vision.detection.YOLOv5(model_file, params_file, option)\r\n    model.preprocessor.size = [1280, 1280]\r\n    resnet_model = model\r\n\r\n\r\ndef get_plate_rects(img0:np.ndarray, draw=False):\r\n    '''\r\n        得到里程表的边框，一个图片最多一个\r\n    '''\r\n    if isinstance(img0, Image.Image):\r\n        img0 = np.array(img0)\r\n\r\n    load_model()\r\n\r\n    objs = []\r\n    result = resnet_model.predict(img0)\r\n    for cls, conf, box in zip(result.label_ids, result.scores, result.boxes):\r\n        # print('res:', cls, conf, box)\r\n        a = (box[3] - box[1]) * (box[2] - box[0])\r\n        objs.append( (a, cls, box, conf) )\r\n\r\n    objs = sorted(objs, key=lambda t: t[0], reverse=True)\r\n    objs = merge_iou(objs)\r\n    # if draw:\r\n    #     img0 = fd.vision.visualize.vis_detection(img0, result, score_threshold=conf_thres)\r\n    if draw:\r\n        for a, icls, r, conf in objs:\r\n            label = f'{names[icls]} {conf:.2f}'\r\n            plot_one_box(r, img0, label=label, color=sel_color, line_thickness=2)\r\n            \r\n    return objs, img0\r\n\r\n\r\nif __name__ == '__main__':\r\n    set_en_fp16(True)\r\n\r\n    fname = 'imgs/b5125c60-4dcd-4cdf-998a-d418591e041a.jpg'\r\n    # fname = 'imgs/123456.jpg'\r\n    # fname = 'imgs/fd55869c-5f4d-4bf5-a3dc-b4bde317370f.jpg'\r\n    img0 = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)   #BGR\r\n    print('img0.shape:', img0.shape)\r\n    objs, img2 = get_plate_rects(img0, True)\r\n    # objs = get_plate_rects_old(img0, True)\r\n    # print('img0.shape:', img0.shape)\r\n    # print('objs:', objs)\r\n\r\n    cnt = 10\r\n    t1 = time.time()\r\n    for i in range(cnt):\r\n        get_plate_rects(img0)\r\n    t2 = time.time()\r\n    print('avg:', (t2-t1)/cnt, 'total:', (t2-t1))\r\n\r\n    mem = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\r\n    print(f'mem:{mem:.4f}GB')\r\n\r\n\r\n## torch测试脚本：\r\n\r\n'''\r\n车牌对象识别\r\n'''\r\nimport os\r\nimport cv2\r\nimport torch\r\nimport random\r\nimport numpy as np\r\nfrom models.experimental import attempt_load\r\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\r\nfrom utils.dataloaders import letterbox\r\nimport time\r\nfrom utils.plots import plot_one_box\r\nimport psutil\r\n\r\nresnet_model = None\r\nconf_thres = 0.3\r\niou_thres = 0.3\r\n# stride  = 32\r\ndohalf  = False\r\naugment = False\r\n# imgsz   = 640\r\nstride  = 64\r\nimgsz   = 1280\r\nweights = 'weights/plates.pt'\r\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nnames   = []\r\ncolors  = []\r\n\r\ndef load_model():\r\n    global resnet_model, names, colors, imgsz\r\n    if resnet_model:\r\n        return\r\n\r\n    # Load model\r\n    resnet_model = attempt_load(weights, device=device)  # load FP32 model\r\n    resnet_model.eval()\r\n    gs = max(int(resnet_model.stride.max()), 32)  # grid size (max stride)\r\n    imgsz = check_img_size(imgsz, s=gs)  # check img_size\r\n    # print(f'load model:{time.time()-t1}  imgsz:{imgsz}')\r\n\r\n    names = {k: v for k, v in enumerate(resnet_model.names if hasattr(resnet_model, 'names') else resnet_model.module.names)}\r\n    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\r\n\r\n\r\ndef get_img(img0:np.ndarray):\r\n    img = letterbox(img0, imgsz, stride=stride)[0]\r\n\r\n    # Convert\r\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\r\n    img = np.ascontiguousarray(img)\r\n    img = torch.from_numpy(img).to(device, non_blocking=True)\r\n    if dohalf:\r\n        img = img.half()\r\n    else:\r\n        img = img.float()\r\n    img /= 255.0  # 0 - 255 to 0.0 - 1.0\r\n    if img.ndimension() == 3:\r\n        img = img.unsqueeze(0)\r\n\r\n    # 原始图片，处理后图片\r\n    return img0, img\r\n\r\n\r\ndef get_plate_rects(img0:np.ndarray, draw=False):\r\n    load_model()\r\n    # img0 = np.array(org_img)\r\n    img0, img = get_img(img0)\r\n    # print('img0:',  img0.shape)\r\n    # print(' img:',  img.shape)\r\n\r\n    objs = []\r\n    with torch.no_grad():\r\n        pred, _ = resnet_model(img, augment=augment)  # inference and training outputs\r\n\r\n        out = non_max_suppression(pred, conf_thres=conf_thres, iou_thres=iou_thres, labels=None, multi_label=False)\r\n\r\n        for i, det in enumerate(out):  # detections per image\r\n            if len(det):\r\n                # Rescale boxes from img_size to im0 size\r\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\r\n\r\n                # Write results\r\n                for *xyxy, conf, cls in reversed(det):\r\n                    icls = int(cls)\r\n                    label = f'{names[icls]} {conf:.2f}'\r\n                    # print(f'cls:{icls}  label:{label}')\r\n\r\n                    r = [int(i.cpu().item()) for i in xyxy]\r\n                    a = (r[3]-r[1]) * (r[2]-r[0])\r\n                    objs.append( (a, icls, r, float(conf)) )\r\n                    if draw:\r\n                        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=1)\r\n    \r\n    # objs = sorted(objs, key=lambda t: t[3], reverse=True)\r\n    objs = sorted(objs, key=lambda t: t[3], reverse=True)\r\n    return objs\r\n\r\n\r\nif __name__ == '__main__':\r\n    fname = 'imgs/b5125c60-4dcd-4cdf-998a-d418591e041a.jpg'\r\n    # fname = 'imgs/123456.jpg'\r\n    # fname = 'imgs/fd55869c-5f4d-4bf5-a3dc-b4bde317370f.jpg'\r\n    img0 = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)   #BGR\r\n    print('img0.shape:', img0.shape)\r\n    objs = get_plate_rects(img0, True)\r\n\r\n    cnt = 10\r\n    t1 = time.time()\r\n    for i in range(cnt):\r\n        get_plate_rects(img0)\r\n    t2 = time.time()\r\n    print('avg:', (t2-t1)/cnt, 'total:', (t2-t1))\r\n\r\n    mem = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\r\n    print(f'mem:{mem:.4f}GB')\r\n\r\n\r\n",
        "state": "open",
        "user": "yz2yz",
        "closed_by": null,
        "created_at": "2022-12-17T04:12:47+00:00",
        "updated_at": "2023-06-21T13:44:06+00:00",
        "closed_at": null,
        "comments_count": [
            "wjj19950828",
            "yz2yz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 904,
        "title": "使用FastDeploy进行OCR识别结果与PaddleOCR推理结果不一样",
        "body": "## 完整脚本和测试图片，网盘：\r\n链接：https://pan.baidu.com/s/1M88ybh4tqWIMvtguZWBUYg \r\n提取码：gcez \r\n\r\n## 环境\r\nFastDeploy版本： fastdeploy_python-1.0.1-cp38-cp38-win_amd64.whl\r\n系统平台:Windows x64(Windows10)\r\n硬件： 如 Nvidia GPU 3070TI， CUDA 11.6 CUDNN 8.5\r\n编译语言： Python 3.8\r\n\r\n## 问题描述\r\n使用相同的权重，使用PaddleOCR推理能够正确识别车牌；\r\n而通过fastdeploy识别错误。\r\n\r\n## fastdeploy测试脚本：\r\n\r\nimport fastdeploy as fd\r\nfrom fastdeploy import ModelFormat\r\nimport cv2\r\nimport numpy as np\r\nimport time\r\n\r\ng_ocr1 = None\r\ndrop_score = 0.1\r\n\r\nclass Ocr1Trt:\r\n    def __init__(self, enable_f16=False):\r\n\r\n        self.det_model_file  = 'weights/chepai_det_serv_infer/inference.pdmodel'\r\n        self.det_params_file = 'weights/chepai_det_serv_infer/inference.pdiparams'\r\n        self.det_trt_file    = 'weights/chepai_det_serv_infer/chepai_det_serv_infer.trt'\r\n        self.rec_model_file  = 'weights/chepai_rec_infer/inference.pdmodel'\r\n        self.rec_params_file = 'weights/chepai_rec_infer/inference.pdiparams'\r\n        self.rec_trt_file    = 'weights/chepai_rec_infer/chepai_rec_infer.trt'\r\n\r\n        option_det = fd.RuntimeOption()\r\n        gpuid = 0\r\n        option_det.use_gpu(gpuid)\r\n        option_det.use_trt_backend() # TensorRT\r\n        if enable_f16:\r\n            option_det.enable_trt_fp16()\r\n        option_det.set_trt_cache_file(self.det_trt_file)\r\n        option_det.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 960, 960], [1, 3, 1920, 1920])\r\n\r\n        t1 = time.time()\r\n        self.det_model = fd.vision.ocr.DBDetector(\r\n            model_file=self.det_model_file, \r\n            params_file=self.det_params_file, \r\n            runtime_option=option_det, model_format=ModelFormat.PADDLE)\r\n        self.det_model.preprocessor.max_side_len = 960  # 960  # 1600\r\n        print('ocr1 det_model:', self.det_model, (time.time() - t1))\r\n\r\n        option_rec = fd.RuntimeOption()\r\n        print('ocr_1_rec use_gpuid:'+str(gpuid))\r\n        option_rec.use_gpu(gpuid)\r\n        option_rec.use_trt_backend() # TensorRT\r\n        if enable_f16:\r\n            option_rec.enable_trt_fp16()\r\n        option_rec.set_trt_cache_file(self.rec_trt_file)\r\n        option_rec.set_trt_input_shape(\"x\", [1, 3, 48, 10], [10, 3, 48, 320],  [128, 3, 48, 2304])\r\n        t1 = time.time()\r\n        self.rec_model = fd.vision.ocr.Recognizer(\r\n            model_file=self.rec_model_file, \r\n            params_file=self.rec_params_file, \r\n            label_path='weights/dict/ppocr_keys_v1.txt',\r\n            runtime_option=option_rec, model_format=ModelFormat.PADDLE)\r\n        print('ocr1 rec_model:', self.rec_model, (time.time() - t1))\r\n\r\n        self.g_model = fd.vision.ocr.PPOCRv3(det_model=self.det_model, cls_model=None, rec_model=self.rec_model)\r\n        print('ocr1_model:', self.g_model)\r\n\r\n\r\n    def ocr(self, img:np.ndarray, cls=False):\r\n        result = self.g_model.predict(img)\r\n        reses = []\r\n        step = 2\r\n        for conf,txt,box in zip(result.rec_scores, result.text, result.boxes):\r\n            if conf < drop_score:\r\n                continue\r\n            box = [box[i:i+step] for i in range(0,len(box),step)]\r\n            reses.append((box, (txt, conf)))\r\n        return reses\r\n\r\n\r\ndef get_ocr1():\r\n    global g_ocr1\r\n    if g_ocr1:\r\n        return g_ocr1\r\n    \r\n    g_ocr1 = Ocr1Trt(enable_f16=True)\r\n    return g_ocr1\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    fname = 'imgs/__truck.jpg'\r\n    # fname = 'imgs/__crop1.jpg'\r\n    img1 = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)\r\n\r\n    result1 = get_ocr1().ocr(img1, cls=False)\r\n    print('result1:', result1)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## PaddleOCR测试脚本：\r\nimport cv2\r\nimport numpy as np\r\nfrom paddleocr import PaddleOCR\r\n\r\ng_ocr1 = None\r\n\r\n\r\ndef get_ocr1():\r\n    global g_ocr1\r\n    if g_ocr1:\r\n        return g_ocr1\r\n    \r\n    g_ocr1 = PaddleOCR(use_angle_cls=False, lang=\"ch\", drop_score=0.1,\r\n        det_model_dir='weights/chepai_det_serv_infer',\r\n        rec_model_dir='weights/chepai_rec_infer',\r\n        det_limit_side_len=960)\r\n\r\n    return g_ocr1\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    fname = 'imgs/__truck.jpg'\r\n    # fname = 'imgs/__crop1.jpg'        # 检测出为None\r\n    img1 = cv2.imdecode(np.fromfile(fname, dtype=np.uint8), cv2.IMREAD_COLOR)   #BGR\r\n\r\n    result1 = get_ocr1().ocr(img1, cls=False)\r\n    print('result1:', result1)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "yz2yz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-12-17T09:46:34+00:00",
        "updated_at": "2024-04-23T06:40:50+00:00",
        "closed_at": "2024-04-23T06:40:50+00:00",
        "comments_count": [
            "yunyaoXYY",
            "yz2yz",
            "yunyaoXYY",
            "yz2yz",
            "yunyaoXYY",
            "maobaoqi",
            "yunyaoXYY",
            "maobaoqi",
            "yunyaoXYY",
            "laugh12321"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 906,
        "title": "Windows10下C++编译出错",
        "body": "## 环境\r\n\r\n- FastDeploy版本 : fastdeploy-release-1.0.1\r\n- 编译命令: cmake .. -G \"Visual Studio 16 2019\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON -DWITH_GPU=ON -DTRT_DIRECTORY=\"F:TensorRT-8.4.1.5\" -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\" -DCMAKE_INSTALL_PREFIX=\"F:\\Paddle\\compiled_fastdeploy\"\r\n- 系统平台: Windows x64(Windows10) \r\n- 硬件： Nvidia GPU 3060， CUDA 11.6 CUDNN 8.4\r\n- 编译语言： C++\r\n\r\n![QQ图片20221218200935](https://user-images.githubusercontent.com/109460192/208297611-e9121828-58ea-4529-8b95-66c96ec61cad.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "GoooHi",
        "closed_by": "wang-xinyu",
        "created_at": "2022-12-18T12:10:37+00:00",
        "updated_at": "2022-12-20T02:43:58+00:00",
        "closed_at": "2022-12-20T02:43:58+00:00",
        "comments_count": [
            "wang-xinyu",
            "GoooHi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 908,
        "title": "每次在预测之前，都要初始化一下模型，导入模型文件，这个过程很慢。能不能程序初始化只导入一次",
        "body": "## 环境\r\n\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-windows-cpu-1.0\r\n- 如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 系统平台: Windows x64(Windows10)\r\n- 硬件： Cpu\r\n- 编译语言： C++ \r\n\r\n## 问题描述\r\n以OCRv3为例：\r\n\tauto det_model = fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, option);\r\n\tauto cls_model = fastdeploy::vision::ocr::Classifier(cls_model_file, cls_params_file, option);\r\n\tauto rec_model = fastdeploy::vision::ocr::Recognizer(rec_model_file, rec_params_file, rec_label_file, option);\r\n\r\n\tauto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);\r\n每次在PPOCRv3预测之前必须对以上3个模型进行初始化，否则会崩。我的本意是：在程序初始化时只初始化一次这些模型文件，预测时，直接调用PPOCRv3即可。\r\n\r\n",
        "state": "closed",
        "user": "gllm126",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-18T15:59:52+00:00",
        "updated_at": "2024-02-06T04:24:44+00:00",
        "closed_at": "2024-02-06T04:24:44+00:00",
        "comments_count": [
            "yz2yz",
            "jiangjiajun",
            "gllm126",
            "gllm126",
            "jiangjiajun",
            "szufafa",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 917,
        "title": "Any plans on supporting yolov5 instance segmentation ?",
        "body": "As stated in the title,do you have any plans on supporting yolov5 or yolov7 instance segmentation models ?",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "UygarUsta99",
        "created_at": "2022-12-19T14:11:48+00:00",
        "updated_at": "2022-12-20T07:40:26+00:00",
        "closed_at": "2022-12-20T07:40:26+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 910,
        "title": "多个线程调用同一实例化的model对象产生的错误",
        "body": "## 环境\r\n- FastDeploy版本：FastDeploy-win-x64-gpu-0.4.0\r\n- 系统平台：Windows x64(Windows10)\r\n- 硬件： Nvidia GPU 2060 6G， CUDA 10.2，CUDNN 8.2.1\r\n- 编译语言： C++ \r\n\r\n## 问题描述\r\n使用如下代码构建的一个，PPYOLOE的检测器对象\r\n```cpp\r\nvision::detection::PPYOLOE model(模型路径, 模型参数类路径, 推理配置路径, RuntimeOption对象);\r\n```\r\nRuntimeOption对象的设置\r\n```cpp\r\nRuntimeOption option;\r\noption.UseGpu();\r\noption.UseTrtBackend();\r\n//张量维度固定\r\noption.SetTrtInputShape(\"scale_factor\", ScaleFactorMin, ScaleFactorOpt, ScaleFactorMax);\r\noption.SetTrtInputShape(\"image\", ImageMin, ImageOpt, Image_Max);\r\n//设置TensoRT缓存文件\r\noption.SetTrtCacheFile(CachePath);\r\n```\r\n多个线程共同调用PPYOLOE的检测器对象`model`\r\n在运行一段时间后会时不时的出现如下错误\r\n```cpp\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(229)::fastdeploy::FDTrtLogger::log                       \r\n1: [convolutionRunner.cpp::nvinfer1::rt::cuda::CudnnConvolutionRunner::executeConv::465] Error Code 1: Cudnn (CUDNN_STATUS_BAD_PARAM)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(310)::fastdeploy::TrtBackend::Infer\r\nFailed to Infer with TensorRT.\r\n[ERROR] fastdeploy/vision/detection/ppdet/ppyoloe.cc(272)::fastdeploy::vision::detection::PPYOLOE::Predict\r\nFailed to inference while using model:PaddleDetection/PPYOLOE.\r\n```\r\n使用单个线程的时候，不会出现如上错误。",
        "state": "closed",
        "user": "KunMengcode",
        "closed_by": "KunMengcode",
        "created_at": "2022-12-19T06:02:13+00:00",
        "updated_at": "2022-12-19T07:22:00+00:00",
        "closed_at": "2022-12-19T07:06:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "KunMengcode"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 923,
        "title": "vs2019跑Release模式可以跑通，Debug模式就会报错",
        "body": "## 环境\r\n\r\n- FastDeploy版本： fastdeploy-win-x64-gpu-1.0.1\r\n- 系统平台:  Windows x64(Windows10) \r\n- 硬件： Nvidia GPU 2080TI， CUDA 11.2 CUDNN 8.3\r\n- 编译语言： C++ \r\n\r\n## 问题描述\r\n- Release模式可以跑通，Debug模式就会报错，有包含opencv的debug库opencv_world3416d.dll\r\n![image](https://user-images.githubusercontent.com/59383646/208615541-edaa2fe4-c651-43fd-aa34-78de9813f9e1.png)\r\n",
        "state": "closed",
        "user": "alfheim1993",
        "closed_by": "alfheim1993",
        "created_at": "2022-12-20T08:08:13+00:00",
        "updated_at": "2022-12-20T08:23:06+00:00",
        "closed_at": "2022-12-20T08:23:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "alfheim1993"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 925,
        "title": "[ERROR] Paddle2ONNX: Only support weight with lod_level = 0.",
        "body": "\r\n## 环境\r\n\r\n- FastDeploy版本： jetson自行编译\r\n- 系统平台: jetpack 4.6.1   \r\n- 硬件： jetson nano 4GB\r\n- 编译语言： C++\r\n\r\n## 问题描述\r\n- 模型部署出错\r\n- - 使用的是paddledetection2.4导出的picodet-m-320模型，类别数为2，导出的时候没有去掉NMS，就是很淳朴的导出。然后准备用trt推理，就报下面的错了。\r\n\r\n*********************************************\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[ERROR] Paddle2ONNX: Only support weight with lod_level = 0.\r\nAborted (core dumped)\r\n\r\n",
        "state": "closed",
        "user": "zhuqiang00099",
        "closed_by": "zhuqiang00099",
        "created_at": "2022-12-20T11:41:33+00:00",
        "updated_at": "2023-09-12T07:34:11+00:00",
        "closed_at": "2022-12-20T11:45:55+00:00",
        "comments_count": [
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 932,
        "title": "RK356X/RK3588使用中碰到的问题合集",
        "body": "## 简介\r\n\r\n在使用FastDeploy的过程中大家可能会碰到很多的问题，这个issues用来记录已经解决的共性问题，方便大家查阅。",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-21T03:17:49+00:00",
        "updated_at": "2023-11-27T06:40:19+00:00",
        "closed_at": "2023-01-13T02:28:18+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "yutao007",
            "Zheng-Bicheng",
            "zhouweic36",
            "zhouweic36"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 933,
        "title": "使用onnxruntime推理和fastdeploy推理同一个onnx模型，结果出现比较大的偏差",
        "body": "\r\n## 环境\r\n\r\n- FastDeploy版本： fastdeploy-gpu-python              0.5.0\r\n- 系统平台:Windows x64(Windows11) \r\n- 硬件： rtx3060ti\r\n- 编译语言： Python3.9.7\r\n\r\n## 问题描述\r\n使用相同数据经过相同的处理方式，输入到onnexruntime和fastdeploy，最终结果出来有非常大的偏差，onnxruntime的结果没问题，但是fastdeploy基本就是不可用的结果\r\n![2](https://user-images.githubusercontent.com/38773954/208836080-787218a1-1d60-4bb4-bc0a-d91b9498171c.png)\r\n这张是onnxruntime推理的结果\r\n![1](https://user-images.githubusercontent.com/38773954/208836139-c757e1fe-5174-4cd8-82f2-3679cb1bc113.png)\r\n这张是fastdeploy的结果\r\n\r\n代码、模型和数据在这里\r\n链接：https://pan.baidu.com/s/1YjpzENl_OdsfHof0qzZABw\r\n提取码：v3lo\r\n",
        "state": "closed",
        "user": "xxjordan",
        "closed_by": "xxjordan",
        "created_at": "2022-12-21T06:25:23+00:00",
        "updated_at": "2022-12-22T01:34:29+00:00",
        "closed_at": "2022-12-22T01:34:29+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "xxjordan",
            "Zheng-Bicheng",
            "xxjordan",
            "jiangjiajun",
            "xxjordan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 934,
        "title": "使用PPOCRv3样例进行trt推理时报错",
        "body": " 环境：可正常使用fastdeploy1.0.1（使用目标检测任务都没问题）\r\n问题：在使用PPOCRv3样例https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCRv3/python时，发现用ort时能够运行，但用trt时会报错。使用的代码和命令都是样例中的\r\n报错信息如下：\r\n![797bd98f4b8e3ff9c95043a894d3a2a](https://user-images.githubusercontent.com/109967047/208843929-18b11b7e-2c4c-4fbe-b824-27100973455c.png)\r\n谢谢！",
        "state": "closed",
        "user": "TWK2022",
        "closed_by": "yunyaoXYY",
        "created_at": "2022-12-21T07:27:26+00:00",
        "updated_at": "2022-12-21T07:48:33+00:00",
        "closed_at": "2022-12-21T07:48:33+00:00",
        "comments_count": [
            "TWK2022"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 936,
        "title": "预编译库没有CMakeLists.txt",
        "body": "预编译库没有CMakeLists.txt 应该怎么处理？ CMake-gui 也没用\r\n![image](https://user-images.githubusercontent.com/16428236/208854147-28e6a5a8-ebb4-490c-b10d-ba3318d06cbc.png)\r\n![image](https://user-images.githubusercontent.com/16428236/208854233-576f57ae-01b8-4c23-b4da-497e15b38d75.png)\r\n",
        "state": "closed",
        "user": "shengzhe8688",
        "closed_by": "shengzhe8688",
        "created_at": "2022-12-21T08:16:38+00:00",
        "updated_at": "2022-12-22T08:38:38+00:00",
        "closed_at": "2022-12-22T08:38:38+00:00",
        "comments_count": [
            "shengzhe8688",
            "shengzhe8688"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 937,
        "title": "rknn example里面infer_picodet.cc用到 deploy.yaml 找不到",
        "body": "*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n\r\n## 环境\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 系统平台: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 硬件： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\n这个文件用onnx 推理用到了 deploy.yaml  文件找不到，运行报错\r\n[infer_picodet.cc](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/cpp/infer_picodet.cc)\r\ninfer_cfg.yml 是有的，把代码里面 deploy.yaml  路径换成 infer_cfg.yml  也是直接报错的\r\n请问去哪能生成 deploy.yaml   ？\r\n",
        "state": "closed",
        "user": "pengwei1024",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-21T10:14:35+00:00",
        "updated_at": "2022-12-22T10:07:52+00:00",
        "closed_at": "2022-12-22T10:06:47+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "pengwei1024"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 942,
        "title": "能否支持下paddlespeech的语音识别功能",
        "body": "能否支持下paddlespeech的语音识别功能，要是能在端上部署那就更好了",
        "state": "closed",
        "user": "jackyzjk",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-22T02:21:15+00:00",
        "updated_at": "2024-02-06T04:24:43+00:00",
        "closed_at": "2024-02-06T04:24:43+00:00",
        "comments_count": [
            "leiqing1",
            "DefTruth",
            "jackyzjk",
            "jackyzjk",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 945,
        "title": "yolov5相关模型，需要开放一个参数，来控制letterbox里的auto参数",
        "body": "老师好，由于训练时默认开启了rectangular training，推理时如果不适用，会出现精度下降严重的问题；\r\n需要在fastdeploy的预处理里，letterbox处理过程中，提供auto参数设置的选项。\r\n同时，发现开启auto=True的请看下，由于计算量小很多，推理速度也会加快。\r\n\r\n目前我们测试了is_no_pad、is_mini_pad不起效果：\r\n    model = fd.vision.detection.YOLOv5(model_file, params_file, option)\r\n    model.preprocessor.size = [1280, 1280]\r\n    model.is_no_pad   = False\r\n    model.is_mini_pad = True\r\n    model.stride = 64\r\n\r\n谢谢！",
        "state": "closed",
        "user": "yz2yz",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-22T04:07:42+00:00",
        "updated_at": "2024-02-06T04:24:42+00:00",
        "closed_at": "2024-02-06T04:24:42+00:00",
        "comments_count": [
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 946,
        "title": "paddle-js-models/ocr 引用问题",
        "body": "## 环境\r\n\r\n Windows x64(Windows10) /VUE 2.6/EDEG/\r\n\r\n\r\n## 问题描述\r\nvue2环境中引用npm包组件\r\nimport * as ocr from '@paddle-js-models/ocr'\r\n提示下面的报错信息，是不支持vue2使用吗？\r\n![image](https://user-images.githubusercontent.com/98594539/209064281-3c8333fd-2ec5-45d7-b5af-97462feadfef.png)\r\n",
        "state": "closed",
        "user": "f13800138001",
        "closed_by": "f13800138001",
        "created_at": "2022-12-22T05:36:05+00:00",
        "updated_at": "2023-01-10T04:52:44+00:00",
        "closed_at": "2022-12-22T05:36:58+00:00",
        "comments_count": [
            "chrispavs"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 952,
        "title": "FastDeploy连续Infer同一张图100次,显存增长到4G,不释放问题。",
        "body": "*********************************************\r\n支持飞桨的用户和程序员们也很不容易,也是一个帮助飞桨成长的过程,希望官方的大佬们能够认真对待、客观回答、直面问题，给出有效的解决方案、不逃避问题、不搪塞用户、不辜负众多支持飞桨的码农们。\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- FastDeploy版本： fastdeploy-linux-gpu-1.0.1 使用预编译库文件\r\n- 测试程序：使用预编译库文件中的Demo,路径为：fastdeploy-linux-x64-gpu-1.0.1/examples/vision/ocr/PP-OCRv3/cpp/infer.cc\r\n- 系统平台: Linux x64(Ubuntu 18.04) \r\n- 硬件：  Nvidia GPU 3060TI CUDA 11.2 CUDNN  8.1.1.33\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\n- **说明：** 我们项目中之前使用PaddleOCR因存在显存不释放问题，没有解决方案的情况下，使用FastDeploy试水，没想到FastDeploy也存在同样的问题。\r\n- **具体问题：**\r\n  1.同一张图片在循环infer1000次的情况下，内存占用4G不进行释放。请问显存的管理机制和释放机制是什么？为什么不自动释放？如果不支持自动释放？如何手动释放？\r\n  2.fastDeploy是否支持多线程并行推理？支持的话Demo在哪儿或者如何实现？\r\n-  **使用代码：** fastdeploy-linux-x64-gpu-1.0.1/examples/vision/ocr/PP-OCRv3/cpp/infer.cc\r\n-  **测试方式：** 修改代码增加推理时间，在执行循环1000次推理，推理结束增加10s延时观察显存变化，并使用nvtop工具观察显存占用变化，发现推理完成、10s延时后内存并没有释放。\r\n-  **截图：** \r\n![image](https://user-images.githubusercontent.com/16428236/209087186-c61ecb74-c60c-4d59-b94f-1c1e33fd3af1.png)\r\n\r\n推理效果还是不错的,就是显存占用不释放。\r\n\r\n![image](https://user-images.githubusercontent.com/16428236/209087064-c05816ab-50fc-47b6-8bb4-44af9f7e8c5a.png)\r\n\r\n-  **使用代码:** \r\n ```\r\n  auto det_model = fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, det_option);\r\n  auto cls_model = fastdeploy::vision::ocr::Classifier(cls_model_file, cls_params_file, cls_option);\r\n  auto rec_model = fastdeploy::vision::ocr::Recognizer(rec_model_file, rec_params_file, rec_label_file, rec_option);\r\n\r\n  std::cout << \"fastdeploy::vision::ocr init create complete!\" << std::endl;\r\n\r\n\r\n  assert(det_model.Initialized());\r\n  assert(cls_model.Initialized());\r\n  assert(rec_model.Initialized());\r\n\r\n  // The classification model is optional, so the PP-OCR can also be connected in series as follows\r\n  // auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &rec_model);\r\n  std::cout << \"fastdeploy::pipeline::PPOCRv3  complete!\" << std::endl;\r\n  //auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);\r\n  auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &rec_model);\r\n\r\n  if(!ppocr_v3.Initialized()){\r\n    std::cerr << \"Failed to initialize PP-OCR.\" << std::endl;\r\n    return;\r\n  }\r\n\r\n  auto im = cv::imread(image_file);\r\n\r\n  fastdeploy::vision::OCRResult result;\r\n  int num=1000;\r\n  while (--num >0)\r\n  {\r\n    auto im_bak = im.clone();\r\n\r\n    std::cout << \"start detect!\" << std::endl;\r\n    auto start_time = std::chrono::system_clock::now();\r\n    result.Clear();\r\n    if (!ppocr_v3.Predict(&im, &result))\r\n    {\r\n      std::cerr << \"Failed to predict.\" << std::endl;\r\n      return;\r\n    }\r\n\r\n    std::cout << result.Str() << std::endl;\r\n    float dt1 = std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::system_clock::now() - start_time).count();\r\n    std::cout << \"get plateNumber total time:\" << dt1 << std::endl;\r\n  }\r\n  sleep(20);\r\n  // auto vis_im = fastdeploy::vision::VisOcr(im_bak, result);\r\n  // cv::imwrite(\"vis_result.jpg\", vis_im);\r\n  // std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n```\r\n\r\n- 附上详细的问题日志有助于工程师快速定位分析\r\n- 性能问题，描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- 模型部署出错\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供自己的代码使用方式或自己的模型，供工程师快速定位问题\r\n\r\n\r\n*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n",
        "state": "closed",
        "user": "shengzhe8688",
        "closed_by": "shengzhe8688",
        "created_at": "2022-12-22T08:10:32+00:00",
        "updated_at": "2023-06-27T09:10:47+00:00",
        "closed_at": "2022-12-22T08:37:45+00:00",
        "comments_count": [
            "leiqing1",
            "dzz10"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 947,
        "title": "使用ppocrv3进行cpp下的推理时，返回的result.text中的结果存在问题",
        "body": "## 环境\r\n\r\n- FastDeploy版本： fastdeploy-linux-gpu-0.8.0\r\n- 系统平台: Linux x64(Ubuntu 18.04) \r\n- 硬件：如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 编译语言： C++\r\n\r\n得到ppocrv3的推理结果后，该result.text是可以正常存储下来。\r\n但是无法输出到终端进行完整的显示，而且使用opencv绘制到图像上每个字符间出现问号？\r\n![image](https://user-images.githubusercontent.com/54393886/209069686-e9067eff-9050-4519-b2cb-278f497cf53a.png)\r\n请问如何才能正常对该结果字符串进行显示呢？",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-22T06:18:59+00:00",
        "updated_at": "2024-02-06T04:24:41+00:00",
        "closed_at": "2024-02-06T04:24:41+00:00",
        "comments_count": [
            "yunyaoXYY",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 956,
        "title": "我就想问多线程并行推理怎么做？",
        "body": "*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- FastDeploy版本： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 系统平台: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 硬件： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 编译语言： C++ / Python(3.7或3.8等）\r\n\r\n## 问题描述\r\n**我就想问多线程并行推理怎么做？**\r\n- 附上详细的问题日志有助于工程师快速定位分析\r\n- 性能问题，描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- 模型部署出错\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供自己的代码使用方式或自己的模型，供工程师快速定位问题\r\n\r\n\r\n*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n",
        "state": "closed",
        "user": "shengzhe8688",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-22T10:00:09+00:00",
        "updated_at": "2024-02-06T04:24:40+00:00",
        "closed_at": "2024-02-06T04:24:40+00:00",
        "comments_count": [
            "jiangjiajun",
            "GeT-RiGhTTT",
            "kFoodie",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 957,
        "title": "RKNN 不支持分类模型",
        "body": "*********************************************\r\n开源不易，工程师每天有大量研发工作，请直接按此issue模版进行提问\r\n\r\n这会大大减少工程师与你确认使用环境，编译过程中的基础信息时间\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- FastDeploy版本： 最新版本\r\n- 如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 系统平台: Linux x64(Ubuntu 18.04) \r\n- 硬件： 说明具体硬件型号，如 RKNN3588\r\n- 编译语言： C++ + Python 都想支持 \r\n\r\n## 问题描述\r\n- AssertionError: PaddleClasModel only support model format of ModelFormat.PADDLE now.\r\n- 先支持下常用的 PPLCNet、PPLCNetv2、MobileNetv3、PPHGNet 等\r\n",
        "state": "closed",
        "user": "pengwei1024",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-22T10:13:09+00:00",
        "updated_at": "2022-12-28T09:11:14+00:00",
        "closed_at": "2022-12-28T09:11:14+00:00",
        "comments_count": [
            "leiqing1",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 965,
        "title": "How to customize the dependency environment of FastDeploy",
        "body": "## 问题描述\r\nFastDeploy是一个很优秀的部署工具，在使用过程中对比之前的依赖管理，有以下疑问：\r\n1）之前的部署策略是依赖的后端用什么，依赖环境就装什么。比如单独装onnx。但是现在fastdeploy“支持CPU和Nvidia GPU的部署，默认集成Paddle Inference、ONNX Runtime、OpenVINO以及TensorRT推理后端，Vision视觉模型模块，Text文本NLP模型模块”，这在测试开发阶段很方便，但是实际部署后，是否存在依赖环境冗余？\r\n\r\n2）默认集成Paddle Inference、ONNX Runtime、OpenVINO以及TensorRT推理后端，Vision视觉模型模块，Text文本NLP模型模块的相关代码可以定制化裁剪吗？\r\n\r\n参考依赖说明\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/download_prebuilt_libraries.md#1\r\n\r\n![image](https://user-images.githubusercontent.com/9989714/209456445-7eec481e-28d8-488c-b570-2db4186e57f9.png)\r\n",
        "state": "closed",
        "user": "geekchen007",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-25T03:55:43+00:00",
        "updated_at": "2024-02-06T04:24:39+00:00",
        "closed_at": "2024-02-06T04:24:39+00:00",
        "comments_count": [
            "geekchen007",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 966,
        "title": "初始化用-dynamic导出的yolov5 onnx 出错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy windows 1.0.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1660ti， CUDA 11.4 CUDNN 8.2\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n想用batch input 来使用推理\r\n\r\n模型用的yolov5 v6.0 导出带-dynamic，\r\n模型格式如下：\r\n\r\n![微信图片_20221226112749](https://user-images.githubusercontent.com/11229566/209496159-481e5c08-9fba-4723-a91a-b4b8bf360a49.jpg)\r\n\r\n加载模型代码：\r\nauto option = fastdeploy::RuntimeOption();\r\noption.UseGpu();\r\noption.UseTrtBackend();\r\noption.EnableTrtFP16();\r\noption.SetTrtCacheFile(\"bin/trtcache_v5_dynamic_fp16\");\r\nauto model = fastdeploy::vision::detection::YOLOv5(model_file, \"\", option);\r\n在上面一句报错，\r\n\r\n报错信息如下\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(637)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx       Failed to parse ONNX model by TensorRT.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(263)::fastdeploy::TrtBackend::InitFromOnnx  Failed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime.cc(767)::fastdeploy::Runtime::CreateTrtBackend       Load model from ONNX failed while initliazing TrtBackend.\r\n\r\n",
        "state": "closed",
        "user": "yueyue0574",
        "closed_by": "yueyue0574",
        "created_at": "2022-12-26T03:28:51+00:00",
        "updated_at": "2023-01-14T23:41:23+00:00",
        "closed_at": "2023-01-14T23:41:23+00:00",
        "comments_count": [
            "wjj19950828"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 969,
        "title": "在rk3568板子上由于内存不足无法编译fastdeploy包，希望可以帮忙编译一个cp37m的，多谢",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： git上最新版本\r\n- 【编译命令】\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_RKNPU2_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport RKNN2_TARGET_SOC=RK3588\r\npython3 setup.py build\r\npython3 setup.py bdist_wheel\r\n- 【系统平台】: Linux aarch64(FriendlyElec 5.10.110)\r\n- 【硬件】： rk3568\r\n- 【编译语言】： Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n编译时卡住不动\r\n",
        "state": "closed",
        "user": "Rsndmmm",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-26T07:38:05+00:00",
        "updated_at": "2023-04-07T02:09:49+00:00",
        "closed_at": "2022-12-26T09:39:24+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "hjshd"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 971,
        "title": "请问是否支持paddle inference 的多线程并发部署和调用？",
        "body": "大神们好。我想问下，fastdeploy是否支持paddle  inference 的多线程并发部署和调用？\r\n想使用的项目：PaddleSpeech 中的 ASR 部署。希望能够支持多线程并发",
        "state": "closed",
        "user": "Tian14267",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-26T08:47:42+00:00",
        "updated_at": "2024-02-06T04:24:38+00:00",
        "closed_at": "2024-02-06T04:24:38+00:00",
        "comments_count": [
            "jiangjiajun",
            "Tian14267",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 974,
        "title": "使用fastdeploy运行paddle模型报错",
        "body": "大神好，我使用fastdeploy运行paddle模型时，\r\n```\r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including conditional_block,select_input,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.\r\nAborted\r\n```\r\n\r\n请问大神这个该怎么解决？\r\npaddle模型来源：https://github.com/yeyupiaoling/PPASR",
        "state": "closed",
        "user": "Tian14267",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-26T12:17:46+00:00",
        "updated_at": "2024-02-06T04:24:36+00:00",
        "closed_at": "2024-02-06T04:24:36+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 972,
        "title": "使用gRPC Python部署PPOCRv3推理服务, 推理报错",
        "body": "## 问题描述\r\n你好, 我们使用gRPC Python搭建PPOCRV3服务端的时候, 会遭遇`Segmentation Fault`\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： `fastdeploy-gpu-python==1.0.1`  `grpcio==1.51.1` \r\n- 【编译命令】wheel安装\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Nvidia T4\r\n- 【编译语言】： python==3.7\r\n\r\n## 问题日志\r\n\r\n1. 使用gRPC Python搭建OCR推理服务端, 当RPC方法中运行`fastdeploy.vision.ocr.PPOCRv3.predict(image)`时, 此时服务端进程会报`Segmentation Fault(Core dumped)`\r\n2. 尝试使用了不同的推理`backend`, 包括`openvino`, `onnx`, `cpu`, `gpu`, 都一定会出现\r\n3. 尝试单独启动模型, 测试没有问题, 只有当在gRPC方法中调用时, 会出现此问题\r\n4. 在此语句处替换成其他模型, 如`Yolov7`, 不会出现该错误, 推理正常完成.\r\n\r\n",
        "state": "closed",
        "user": "miknyko",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-26T09:23:30+00:00",
        "updated_at": "2024-02-06T04:24:37+00:00",
        "closed_at": "2024-02-06T04:24:37+00:00",
        "comments_count": [
            "jiangjiajun",
            "miknyko",
            "wang-xinyu",
            "miknyko",
            "wang-xinyu",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 979,
        "title": "是否有计划提供streamer python API？",
        "body": "目前看到这个获取RTSP视频流方法，只有C++方式，后续是否有计划提供python API方式？\r\n[streamer](https://github.com/PaddlePaddle/FastDeploy/tree/develop/streamer)",
        "state": "closed",
        "user": "jerryandjune",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-27T03:44:19+00:00",
        "updated_at": "2024-02-06T04:24:35+00:00",
        "closed_at": "2024-02-06T04:24:35+00:00",
        "comments_count": [
            "wang-xinyu",
            "jiangjiajun"
        ],
        "labels": [
            "streamer"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 980,
        "title": "请问如何串联ch_PP-OCRv3_det_infer和ch_ppocr_mobile_v2.0_rec_infer模型部署呢？",
        "body": null,
        "state": "closed",
        "user": "lilianjie111111",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-27T06:01:34+00:00",
        "updated_at": "2024-02-06T04:24:34+00:00",
        "closed_at": "2024-02-06T04:24:34+00:00",
        "comments_count": [
            "yunyaoXYY",
            "lilianjie111111",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 982,
        "title": "PPOCR-v3  demo运行报错",
        "body": "已解决",
        "state": "closed",
        "user": "lilianjie111111",
        "closed_by": "yunyaoXYY",
        "created_at": "2022-12-27T07:13:04+00:00",
        "updated_at": "2022-12-27T07:33:23+00:00",
        "closed_at": "2022-12-27T07:33:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 987,
        "title": "cpu 服务部署 如何运行容器启动容器",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [说明具体的版本，如fastdeploy-linux-gpu-0.8.0](fastdeploy:1.0.1-cpu-only-21.10)\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】：Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\nPaddleDetection 服务化部署示例\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/serving\r\n仅介绍了gpu的运行和启动容器，cpu咋整？\r\n\r\n(paddlecpu) D:\\pkg\\FastDeploy\\examples\\vision\\detection\\paddledetection\\serving>docker run -it --net=host --name fd_serving --shm-size=\"1g\"  -v /:/serving 928d09fd7108\r\n\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 21.10 (build <unknown>)\r\n\r\nCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\nfind: File system loop detected; '/usr/bin/X11' is part of the same file system loop as '/usr/bin'.\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use Docker with NVIDIA Container Toolkit to start this container; see\r\n   https://github.com/NVIDIA/nvidia-docker.\r\n\r\nroot@docker-desktop:/opt/tritonserver# fastdeployserver --model-repository=/serving/models\r\nI1227 08:52:41.713216 17 tritonserver.cc:1920]\r\n+----------------------------------+----------------------------------------------------------------------+\r\n| Option                           | Value                                                                |\r\n+----------------------------------+----------------------------------------------------------------------+\r\n| server_id                        | triton                                                               |\r\n| server_version                   | 2.15.0                                                               |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dep |\r\n|                                  | endents) schedule_policy model_configuration system_shared_memory cu |\r\n|                                  | da_shared_memory binary_tensor_data statistics                       |\r\n| model_repository_path[0]         | /serving/models                                                      |\r\n| model_control_mode               | MODE_NONE                                                            |\r\n| strict_model_config              | 1                                                                    |\r\n| rate_limit                       | OFF                                                                  |\r\n| pinned_memory_pool_byte_size     | 268435456                                                            |\r\n| response_cache_byte_size         | 0                                                                    |\r\n| min_supported_compute_capability | 0.0                                                                  |\r\n| strict_readiness                 | 1                                                                    |\r\n| exit_timeout                     | 30                                                                   |\r\n+----------------------------------+----------------------------------------------------------------------+\r\n\r\nI1227 08:52:41.713285 17 server.cc:249] **No server context available**. Exiting immediately.\r\nerror: creating server: Internal - failed to stat file /serving/models\r\nroot@docker-desktop:/opt/tritonserver#\r\n\r\n\r\n另请问：客户端请求是需要在conda新建一个python环境, 直接安装python -m pip install tritonclient[all]就可以吗？还是需要安装paddle, fastdeploy,各种？\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "heliqi",
        "created_at": "2022-12-27T09:22:25+00:00",
        "updated_at": "2023-02-09T06:28:48+00:00",
        "closed_at": "2023-01-10T09:37:12+00:00",
        "comments_count": [
            "heliqi",
            "cunjing56",
            "cunjing56",
            "cunjing56",
            "heliqi",
            "cunjing56",
            "heliqi",
            "heliqi",
            "cunjing56",
            "heliqi",
            "cunjing56",
            "heliqi",
            "heliqi",
            "cunjing56",
            "heliqi",
            "cunjing56",
            "cunjing56",
            "heliqi",
            "whtwhtw",
            "heliqi"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 989,
        "title": "解码模块video_decoder解不了rtmp",
        "body": "配置文件如下：\r\n\r\napp:\r\n  type: video_decoder\r\n  enable-perf-measurement: true\r\n  perf-measurement-interval-sec: 5\r\n\r\nnvurisrcbin:\r\n  uri: rtmp://localhost:55555/live/0.mp4\r\n  gpu-id: 0\r\n\r\nnvvideoconvert:\r\n  gpu-id: 0\r\n\r\ncapsfilter:\r\n  caps: video/x-raw,format=(string)BGR\r\n\r\nappsink:\r\n  sync: true\r\n  max-buffers: 60\r\n  drop: false\r\n\r\n运行结果如下：\r\n[INFO] /decode/FastDeploy/streamer/src/gstreamer/utils.cc(60)::CreatePipeline   Trying to launch pipeline: nvurisrcbin uri=rtmp://localhost:55555/live/0.mp4 gpu-id=0 ! nvvideoconvert gpu-id=0 ! capsfilter caps=video/x-raw,format=(string)BGR ! appsink sync=true max-buffers=60 drop=false \r\nappsink0\r\nappsink0\r\nRunning Asynchronous...\r\n一直停止不动。vlc播放器播放这个rtmp流没有问题。\r\n",
        "state": "closed",
        "user": "intjun",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-27T09:46:47+00:00",
        "updated_at": "2024-02-06T04:24:33+00:00",
        "closed_at": "2024-02-06T04:24:33+00:00",
        "comments_count": [
            "wang-xinyu",
            "lzh1998-jansen",
            "lzh1998-jansen",
            "intjun",
            "lzh1998-jansen",
            "intjun",
            "lzh1998-jansen",
            "wang-xinyu",
            "wang-xinyu",
            "intjun",
            "wang-xinyu",
            "intjun",
            "jiangjiajun"
        ],
        "labels": [
            "streamer"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 994,
        "title": "自定义配置 FastDeploy Streamer 的yaml配置文件指南",
        "body": "感谢作者的付出，我根据超链接跳到gstreamer的文档，https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html?gi-language=c#pipeline-examples，我本想像deepstream一样，配置输入输出rtsp流，显示出来，看看效果，但是那个gstreamer的文档，我没有看懂，怎么去配置 yaml文件，求讲解或者，多提供几个 yaml文件的demo",
        "state": "closed",
        "user": "lzh1998-jansen",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-28T01:13:17+00:00",
        "updated_at": "2024-02-06T04:24:31+00:00",
        "closed_at": "2024-02-06T04:24:31+00:00",
        "comments_count": [
            "wang-xinyu",
            "lzh1998-jansen",
            "wang-xinyu",
            "lzh1998-jansen",
            "lzh1998-jansen",
            "wang-xinyu",
            "lzh1998-jansen",
            "wang-xinyu",
            "lzh1998-jansen",
            "jiangjiajun"
        ],
        "labels": [
            "streamer"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 995,
        "title": "fastdeploy-gpu-python模块报错嘞",
        "body": "\r\n![image](https://user-images.githubusercontent.com/102579571/209754064-a095c819-581e-49a2-9455-d7fa6fcead2b.png)\r\n安装gpu版本并且使用gpu，trt进行推理时报错",
        "state": "closed",
        "user": "lilianjie111111",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-28T03:45:46+00:00",
        "updated_at": "2024-02-06T04:24:30+00:00",
        "closed_at": "2024-02-06T04:24:30+00:00",
        "comments_count": [
            "DefTruth",
            "lilianjie111111",
            "DefTruth",
            "lilianjie111111",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Windows x64",
            "GPU"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 996,
        "title": "React版本paddleocr识别的案例有吗",
        "body": "你好，咱们paddle-js是vue版本的，咱们有react版本的用法吗",
        "state": "closed",
        "user": "excxapp",
        "closed_by": "chenqianhe",
        "created_at": "2022-12-28T06:16:37+00:00",
        "updated_at": "2023-01-10T14:35:46+00:00",
        "closed_at": "2023-01-10T14:35:46+00:00",
        "comments_count": [
            "chenqianhe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 997,
        "title": "could not reshape a memory descriptor",
        "body": "大神们好。我在使用fastdeploy运行ASR的流式识别时，遇到了这个问题：\r\n```\r\nTraceback (most recent call last):\r\n  File \"infer_wav_1.py\", line 97, in <module>\r\n    real_time_predict_8k()\r\n  File \"infer_wav_1.py\", line 82, in real_time_predict_8k\r\n    result = predictor.predict_stream_fastdeploy(audio_data=data, use_pun=args.use_pun, is_itn=args.is_itn, is_end=d == b'', sample_rate_in=8000)\r\n  File \"/sdb/fffan/2_relevant_experiment/PPASR_u2_deploy_1227_new_server_static_fastdeploy/ppasr/predict_fastdeploy.py\", line 424, in predict_stream_fastdeploy\r\n    output_chunk_probs = self.predictor.predict_chunk_conformer_fastdeploy(x_chunk=one_x_chunk,\r\n  File \"/sdb/fffan/2_relevant_experiment/PPASR_u2_deploy_1227_new_server_static_fastdeploy/ppasr/infer_utils/inference_fastdeploy.py\", line 100, in predict_chunk_conformer_fastdeploy\r\n    output_data = self.runtime.infer({\r\n  File \"/root/anaconda3/envs/paddle_py38/lib/python3.8/site-packages/fastdeploy/runtime.py\", line 65, in infer\r\n    return self._runtime.infer(data)\r\nRuntimeError: could not reshape a memory descriptor\r\n```\r\n这个是在预测第二个人chunk的时候出现的。请问下这个是啥情况？",
        "state": "closed",
        "user": "Tian14267",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-28T06:36:40+00:00",
        "updated_at": "2024-02-06T04:24:29+00:00",
        "closed_at": "2024-02-06T04:24:29+00:00",
        "comments_count": [
            "Tian14267",
            "jiangjiajun",
            "Tian14267",
            "Tian14267",
            "joey12300",
            "Tian14267",
            "Tian14267",
            "joey12300",
            "Tian14267",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 998,
        "title": "Feature - Any plans on supporting RK3399 / RK3399Pro architectures ?",
        "body": "We have been working with rk3399 for some time now but model exporting/conversion is quite messy and undocumented.We would like to use fastdeploy for our applications.Are there any plans or a  roadmap for supporting RK3399 / RK3399Pro architectures ? Thanks for the great work.",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "UygarUsta99",
        "created_at": "2022-12-28T08:40:37+00:00",
        "updated_at": "2023-01-03T07:07:59+00:00",
        "closed_at": "2023-01-03T07:07:59+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "UygarUsta99"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1003,
        "title": "2G内存arm64在docker环境部署，只有部分模型能跑通，初步怀疑是内存问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python==1.0.1，fastdeploy-tools==0.0.1\r\n- 【编译命令】通过pip安装的cpu版本，pip install numpy opencv-python fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n- 【系统平台】: ARMV8，aarch64\r\n- 【硬件】： CPU: Amlogic S905 Cortex-A53，内存：1.795GiB\r\n- 【编译语言】：Python3.7\r\n- 【运行环境】：Docker，jupyter-lab\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【部分模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- 尝试了以下模型：\r\n- - ssd_mobilenet_v1_300_120e_voc  ×\r\n- - ppyoloe_crn_l_300e_coco   ×\r\n- - picodet_l_320_coco_lcnet   √\r\n- - yolox_s_300e_coco   √\r\n- - PP_LiteSeg_B_STDC2_cityscapes_with_argmax_infer  √\r\n- - PP_HumanSegV2_Lite_192x192_with_argmax_infer  √\r\n- - Portrait_PP_HumanSegV2_Lite_256x144_with_argmax_infer  √\r\n-  前两个模型资源不够，后面的能跑通\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n```\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel_dir = \"ppyoloe_crn_l_300e_coco \"\r\nsavefig_name = model_dir.split(\"_\")[0]\r\n# 若模型跑不通，都在以下代码无法通过\r\nmodel = vision.detection.PPYOLOE(f\"{model_dir}/model.pdmodel\",\r\n                                 f\"{model_dir}/model.pdiparams\",\r\n                                 f\"{model_dir}/infer_cfg.yml\")\r\n\r\nim = cv2.imread(\"000000014439.jpg\")\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(f\"vis_{savefig_name}.jpg\", vis_im)\r\n```\r\n![image](https://user-images.githubusercontent.com/36829977/209848791-d8575ce8-b709-4b25-b9cd-0006893e1e98.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "Vector-Cross",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-28T17:17:26+00:00",
        "updated_at": "2024-02-06T04:24:28+00:00",
        "closed_at": "2024-02-06T04:24:28+00:00",
        "comments_count": [
            "jiangjiajun",
            "Vector-Cross",
            "Vector-Cross",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1004,
        "title": "Jetson推理PP_PicoDet_V2_S_Pedestrian_192x192_infer时出错",
        "body": "## 环境\r\n\r\n- 【编译命令】遵循 https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/jetson.md， 去掉了Paddle Inference\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia Jetson Xavier NX\r\n- 【编译语言】： C++ / Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n基于https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/keypointdetection/det_keypoint_unite 中提供的python demo，成功运行了PP_PicoDet_V2_S_Pedestrian_320x320_infer+PP_TinyPose_256x192_infer，但切换为PP_PicoDet_V2_S_Pedestrian_192x192_infer+PP_TinyPose_128x96_infer时出错\r\n\r\n注：已将\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/keypointdetection/det_keypoint_unite#L39\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/keypointdetection/det_keypoint_unite#L55\r\n两处的size分别修改为1, 3, 192, 192和1, 3, 128, 96\r\n\r\n尝试了C++版本，遇到了同样的core\r\n\r\n下附日志：\r\n\r\n- python版本\r\n\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(586)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(588)::BuildTrtEngine Serialize TensorRTEngine to local file picodet_lite.trt.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(598)::BuildTrtEngine TensorRTEngine is serialized to local file picodet_lite.trt, we can load this model from the seralized engine directly next time.\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine Start to building TensorRT Engine...\r\n\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(586)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(588)::BuildTrtEngine Serialize TensorRTEngine to local file tinypose_lite.trt.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(598)::BuildTrtEngine TensorRTEngine is serialized to local file tinypose_lite.trt, we can load this model from the seralized engine directly next time.\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\nLoad video /home/nvidia/revai/sit_up/test_1.mp4\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(345)::Infer [ERROR] Error occurs while copy memory from GPU to CPU.\r\nAborted (core dumped)\r\n\r\n- C++版本\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(586)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(588)::BuildTrtEngine Serialize TensorRTEngine to local file ./picodet_lite.trt.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(598)::BuildTrtEngine TensorRTEngine is serialized to local file ./picodet_lite.trt, we can load this model from the seralized engine directly next time.\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(586)::BuildTrtEngine TensorRT Engine is built successfully.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(588)::BuildTrtEngine Serialize TensorRTEngine to local file ./tinypose_lite.trt.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(598)::BuildTrtEngine TensorRTEngine is serialized to local file ./tinypose_lite.trt, we can load this model from the seralized engine directly next time.\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\nLoad video\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(345)::Infer [ERROR] Error occurs while copy memory from GPU to CPU.\r\nAborted (core dumped)\r\n\r\n\r\n",
        "state": "closed",
        "user": "liutingxi",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-28T19:26:45+00:00",
        "updated_at": "2024-02-06T04:24:27+00:00",
        "closed_at": "2024-02-06T04:24:27+00:00",
        "comments_count": [
            "felixhjh",
            "jiangjiajun"
        ],
        "labels": [
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1005,
        "title": "RV1126跑demo出错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 开发版\r\n- 【编译命令】：根据交叉编译\r\n- 【系统平台】: linux 内核4.19.1111\r\n<img width=\"689\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2733701/209893178-8178ae38-170b-4834-9534-d5f144461c21.png\">\r\n\r\n`firefly@firefly:~/rv1126$ uname -a\r\nLinux firefly 4.19.111 #1 SMP PREEMPT Wed Aug 31 11:18:27 CST 2022 armv7l GNU/Linux\r\nfirefly@firefly:~/rv1126$ dmesg | grep Galcore\r\n[    7.016079] Galcore version 6.4.3.5.293908\r\n[    7.016097] Galcore options:\r\n[    7.026910] Galcore Info: ContiguousBase=0x3f956000 ContiguousSize=0x400000\r\n[    7.039457] Galcore Info: MMU mapped core 0 SRAM[0] hardware virtual address=0x400000 size=0x40000\r\n[    7.039508] Galcore Info: MMU mapped core 0 SRAM[1] hardware virtual address=0x440000 size=0x40000\r\n[ 2493.727331] Galcore version 6.4.6.5.351518\r\n[ 2493.727338] Galcore options:\r\n[ 2493.729993] Galcore Info: ContiguousBase=0x3f956000 ContiguousSize=0x400000\r\n[ 2493.735008] Galcore Info: MMU mapped core 0 SRAM[0] hardware virtual address=0x400000 size=0x40000\r\n[ 2493.735042] Galcore Info: MMU mapped core 0 SRAM[1] hardware virtual address=0x440000 size=0x40000\r\n`\r\n\r\n- 【硬件】： 瑞星微RV1226\r\n- 【编译语言】： C++ \r\n\r\n跑demo（https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rv1126/cpp），报错， 是我哪个环境除问题了？能不能给点提示\r\n<img width=\"1042\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2733701/209893200-b6d1b30d-2dec-4c96-85fe-9e89256837e8.png\">\r\n\r\n`[4 10/17 11:35:25.838 ...ite/lite/core/optimizer/mir/ssa_graph.cc:28 CheckBidirectionalConnection] node count 1646\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement feed host/any/any\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement feed host/any/any\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement calib arm/int8_t/NCHW\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement subgraph nnadapter/any/NCHW\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement calib arm/int8_t/NCHW\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement calib arm/int8_t/NCHW\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement calib arm/int8_t/NCHW\r\n[4 10/17 11:35:25.852 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement concat arm/any/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement calib arm/int8_t/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement split host/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement scale arm/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement elementwise_add arm/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement elementwise_add arm/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement concat arm/any/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement elementwise_mul arm/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement split host/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement concat arm/any/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement reshape2 host/any/any\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement elementwise_div arm/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement multiclass_nms3 host/float/NCHW\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement fetch host/any/any\r\n[4 10/17 11:35:25.853 ...e/optimizer/mir/generate_program_pass.cc:38 Apply] Statement fetch host/any/any\r\n[I 10/17 11:35:25.854 ...re/optimizer/mir/generate_program_pass.h:41 GenProgram] insts.size: 1\r\n[INFO] fastdeploy/runtime.cc(617)::Init Runtime initialized with Backend::LITE in Device::TIMVX.\r\n[4 10/17 11:35:26. 67 .../backends/nnadapter/nnadapter_wrapper.cc:60 Initialize] The NNAdapter library libnnadapter.so is loaded.\r\n[4 10/17 11:35:26. 67 .../backends/nnadapter/nnadapter_wrapper.cc:73 Initialize] NNAdapter_getVersion is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:74 Initialize] NNAdapter_getDeviceCount is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:75 Initialize] NNAdapterDevice_acquire is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:76 Initialize] NNAdapterDevice_release is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:77 Initialize] NNAdapterDevice_getName is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:78 Initialize] NNAdapterDevice_getVendor is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:79 Initialize] NNAdapterDevice_getType is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:80 Initialize] NNAdapterDevice_getVersion is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:81 Initialize] NNAdapterContext_create is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:82 Initialize] NNAdapterContext_destroy is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:83 Initialize] NNAdapterModel_create is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:84 Initialize] NNAdapterModel_destroy is loaded.\r\n[4 10/17 11:35:26. 68 .../backends/nnadapter/nnadapter_wrapper.cc:85 Initialize] NNAdapterModel_finish is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:86 Initialize] NNAdapterModel_addOperand is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:87 Initialize] NNAdapterModel_setOperandValue is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:88 Initialize] NNAdapterModel_getOperandType is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:89 Initialize] NNAdapterModel_addOperation is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:90 Initialize] NNAdapterModel_identifyInputsAndOutputs is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:91 Initialize] NNAdapterModel_getSupportedOperations is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:92 Initialize] NNAdapterCompilation_create is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:93 Initialize] NNAdapterCompilation_destroy is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:94 Initialize] NNAdapterCompilation_finish is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:95 Initialize] NNAdapterCompilation_queryInputsAndOutputs is loaded.\r\n[4 10/17 11:35:26. 69 .../backends/nnadapter/nnadapter_wrapper.cc:96 Initialize] NNAdapterExecution_create is loaded.\r\n[4 10/17 11:35:26. 70 .../backends/nnadapter/nnadapter_wrapper.cc:97 Initialize] NNAdapterExecution_destroy is loaded.\r\n[4 10/17 11:35:26. 70 .../backends/nnadapter/nnadapter_wrapper.cc:98 Initialize] NNAdapterExecution_setInput is loaded.\r\n[4 10/17 11:35:26. 70 .../backends/nnadapter/nnadapter_wrapper.cc:99 Initialize] NNAdapterExecution_setOutput is loaded.\r\n[4 10/17 11:35:26. 70 .../backends/nnadapter/nnadapter_wrapper.cc:100 Initialize] NNAdapterExecution_compute is loaded.\r\n[4 10/17 11:35:26. 70 .../backends/nnadapter/nnadapter_wrapper.cc:102 Initialize] Extract all of symbols from libnnadapter.so done.\r\n[F 10/17 11:35:26. 98 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'verisilicon_timvx' from libverisilicon_timvx.so, /home/firefly/rv1126/lib/libtim-vx.so: undefined symbol: vxTensorTableLookupLayer\r\nterminate called after throwing an instance of 'nnadapter::logging::Exception'\r\n  what():  NNAdapter C++ Exception: \r\n[F 10/17 11:35:26. 98 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'verisilicon_timvx' from libverisilicon_timvx.so, /home/firefly/rv1126/lib/libtim-vx.so: undefined symbol: vxTensorTableLookupLayer\r\n`\r\n\r\n\r\n",
        "state": "closed",
        "user": "ponycloud235",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-29T01:49:02+00:00",
        "updated_at": "2024-02-06T04:24:26+00:00",
        "closed_at": "2024-02-06T04:24:26+00:00",
        "comments_count": [
            "yeliang2258",
            "ponycloud235",
            "yeliang2258",
            "ponycloud235",
            "yeliang2258",
            "yeliang2258",
            "ponycloud235",
            "ponycloud235",
            "yeliang2258",
            "ponycloud235",
            "ponycloud235",
            "yeliang2258",
            "ponycloud235",
            "jiangjiajun"
        ],
        "labels": [
            "timvx"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1008,
        "title": "RKNUP(Python方式)部署中遇到问题:AssertionError: PicoDet model initialize failed.",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python      1.0.0 \r\n- 【系统平台】: Linux firefly 4.19.219   aarch64 GNU/Linux   (Ubuntu20.04.3)\r\n- 【硬件】： AIO-3568J开发板\r\n- 【编译语言】： Python(3.8.10）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 运行/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python目录下面的infer脚本文件提示下面错误:\r\n- 执行命令:\r\npython3 infer.py --model_file ./picodet_s_416_coco_lcnet/picodet_s_416_lcnet_rk3568.rknn --config_file ./picodet_s_416_coco_lcnet/infer_cfg.yml --image 000000014439.jpg                             \r\n-  错误提示:                      \r\n- - [INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[ERROR] fastdeploy/fastdeploy_model.cc(200)::CreateRKNPUBackend Cannot find an available npu backend to load this model.\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(23)::Initialize       Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 42, in <module>\r\n    model = fd.vision.detection.PicoDet(\r\n  File \"/home/firefly/.local/lib/python3.8/site-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 189, in __init__\r\n    assert self.initialized, \"PicoDet model initialize failed.\"\r\nAssertionError: PicoDet model initialize failed.\r\n-  备注1:picodet_s_416_coco_lcnet文件夹中的模型文件在c++方式部署时能够成功运行\r\n![image](https://user-images.githubusercontent.com/85120075/209896548-39b61ca7-2f74-49b5-b3d4-897a0558185f.png)",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2022-12-29T02:50:58+00:00",
        "updated_at": "2023-02-01T09:45:20+00:00",
        "closed_at": "2023-02-01T09:45:20+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1016,
        "title": "请问框架最后的识别结果得自己去解析吗？没有啥一键转list之类的功能吗？",
        "body": "![image](https://user-images.githubusercontent.com/102579571/209941595-66c5d0f5-520f-4a78-ad2d-9acb79fb07df.png)\r\n",
        "state": "closed",
        "user": "lilianjie111111",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-29T10:56:43+00:00",
        "updated_at": "2024-02-06T04:24:23+00:00",
        "closed_at": "2024-02-06T04:24:23+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1014,
        "title": "fastdeploy-linux-aarch64-1.0.1.tgz  无法下载",
        "body": "\r\n\r\ngit上[fastdeploy-linux-aarch64-1.0.1.tgz](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-linux-aarch64-1.0.1.tgz) 该地址无法下载。 \r\n\r\n{\"code\":\"NoSuchKey\",\"message\":\"The specified key does not exist.\",\"requestId\":\"a48d716a-93e9-4987-99df-34a0af8c49d2\"}",
        "state": "closed",
        "user": "snowperson01",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-29T08:23:55+00:00",
        "updated_at": "2024-02-06T04:24:24+00:00",
        "closed_at": "2024-02-06T04:24:24+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1020,
        "title": "OSError: (External) CUDNN error(14), CUDNN_STATUS_VERSION_MISMATCH.",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python         1.0.1\r\nfastdeploy-tools              0.0.1\r\naddle-bfloat                 0.1.7\r\npaddlepaddle                  2.4.1\r\npaddlepaddle-gpu              2.4.1\r\npaddleslim                    2.4.0\r\npandas                        1.3.2\r\npandocfilters                 1.4.3\r\nparl                          2.1.1\r\nparso                         0.8.2\r\ntorch                         1.5.0\r\ntorchvision                   0.6.0\r\ntornado                       6.1\r\n- 【编译命令】fastdeploy compress --config_path=/media/extend/FastDeploy/tools/common_tools/auto_compression/configs/detection/yolov5s_quant.yaml  --method='QAT' --save_dir='./yolov5s_qat_model/'\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1050TI， cuda_11.2.0_460.27.04_linux   cudnn-11.3-linux-x64-v8.2.0.53\r\n- 【编译语言】： Python3.7.6\r\n\r\nuantize': False, 'onnx_format': True, 'quant_post_first': False, 'scale_trainable': True, 'name': 'Distillation', 'loss': 'soft_label', 'node': [], 'alpha': 1.0, 'teacher_model_dir': './yolov5s.onnx', 'teacher_model_filename': 'model.pdmodel', 'teacher_params_filename': 'model.pdiparams'}\r\nAdding quant op with weight:|████████████████████████████████████████| 1360/1360\r\nAdding OutScale op:|███████████████████████████████████████████████████| 929/929\r\n2022-12-30 14:06:05,808-INFO: When a preprocess_func is used in quant_aware, Need to save a mapping table to match variable names in the convert phase.\r\n2022-12-30 14:06:05,808-INFO: The mapping table is saved as './mapping_table_for_saving_inference_model'.\r\nTraceback (most recent call last):\r\n  File \"/media/extend/anaconda3/bin/fastdeploy\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/common_tools/common_tools.py\", line 81, in main\r\n    auto_compress(args)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/common_tools/auto_compression/fd_auto_compress/fd_auto_compress.py\", line 127, in auto_compress\r\n    ac.compress()\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddleslim/auto_compression/compressor.py\", line 594, in compress\r\n    train_config)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddleslim/auto_compression/compressor.py\", line 776, in single_strategy_compress\r\n    train_program_info, test_program_info, strategy, train_config)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddleslim/auto_compression/compressor.py\", line 792, in _start_train\r\n    fetch_list=train_program_info.fetch_targets)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddle/fluid/executor.py\", line 1463, in run\r\n    six.reraise(*sys.exc_info())\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/six.py\", line 719, in reraise\r\n    raise value\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddle/fluid/executor.py\", line 1459, in run\r\n    return_merged=return_merged)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddle/fluid/executor.py\", line 1726, in _run_impl\r\n    return_merged=return_merged)\r\n  File \"/media/extend/anaconda3/lib/python3.7/site-packages/paddle/fluid/executor.py\", line 1254, in _run_parallel\r\n    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()\r\nOSError: (External) CUDNN error(14), CUDNN_STATUS_VERSION_MISMATCH. \r\n  [Hint: Please search for the error code(14) on website (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnStatus_t) to get Nvidia's official solution and advice about CUDNN Error.] (at /paddle/paddle/phi/kernels/gpudnn/pool_grad_kernel.cu:284)\r\n  [operator < pool2d_grad > error\r\n\r\n尝试换过cudnn-11.2-linux-x64-v8.1.1.33，仍然不能执行，并且报错找不到cudnn",
        "state": "closed",
        "user": "yutao007",
        "closed_by": "jiangjiajun",
        "created_at": "2022-12-30T06:25:35+00:00",
        "updated_at": "2024-02-06T04:24:22+00:00",
        "closed_at": "2024-02-06T04:24:22+00:00",
        "comments_count": [
            "yunyaoXYY",
            "yutao007",
            "yutao007",
            "yutao007",
            "yunyaoXYY",
            "yutao007",
            "yunyaoXYY",
            "yutao007",
            "yutao007",
            "yunyaoXYY",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1024,
        "title": "text的示例无法兼容windows平台运行",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.1\r\n- 【编译命令】\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 普通CPU环境\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- text目录下的示例无法兼容windows平台运行\r\n- 【模型跑不通】\r\n- - 示例 ernie-3.0，uie 在windows复现均有问题\r\n- - 出现运行循环卡顿，或异常退出\r\n\r\n- 【问题分析】\r\n- - 目前定位问题为Visual Studio 定义字符串为ANSI格式，而非Utf8字符串\r\n- - 但是预测模型的输入，默认为utf8字符串\r\n- - NLP底层引擎 core_tokenizers，fastdeploy 有字符串预处理部分，导致ANSI字符串进入后死循环\r\n\r\n- 【解决方法】\r\n定义函数\r\n```C++\r\n#include <windows.h>\r\nstd::string Uni2UTF8StrS(wchar_t *src) {\r\n#if 1\r\n  int nLen = WideCharToMultiByte(CP_UTF8, 0, src, -1, NULL, 0, NULL, NULL);\r\n  if (nLen <= 0)\r\n    return std::string(\"\");\r\n  char *pszDst = new char[nLen];\r\n  if (NULL == pszDst)\r\n    return std::string(\"\");\r\n  WideCharToMultiByte(CP_UTF8, 0, src, -1, pszDst, nLen, NULL, NULL);\r\n  pszDst[nLen - 1] = 0;\r\n  std::string strTemp(pszDst);\r\n  delete[] pszDst;\r\n  return strTemp;\r\n#endif\r\n#if 0\r\n\tstd::wstring_convert<std::codecvt_utf8_utf16<wchar_t>, wchar_t> cvt;\r\n\tcvt.to_bytes(std::wstring(pwszSrc));\r\n#endif\r\n};\r\n\r\nstd::string Uni2UTF8Str(const std::wstring &pwszSrc) {\r\n  LPCWSTR src = pwszSrc.c_str();\r\n  return Uni2UTF8StrS((wchar_t *)pwszSrc.c_str());\r\n}\r\n```\r\n\r\n将所以原始字符串定义类似如下：\r\n```C++\r\nstd::vector<std::string> texts_ds = {\"花呗收款额度限制\", \"花呗支持高铁票支付吗\"};\r\n```\r\n重新定义为：\r\n```C++\r\nstd::vector<std::string> texts_ds = {Uni2UTF8Str(L\"花呗收款额度限制\"), Uni2UTF8Str(L\"花呗支持高铁票支付吗\")};\r\n```\r\n有多处需要修改可解决上述问题，期待出windows编码兼容的版本。\r\n\r\n",
        "state": "closed",
        "user": "huangwm999",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-01T18:13:00+00:00",
        "updated_at": "2024-02-06T04:24:21+00:00",
        "closed_at": "2024-02-06T04:24:21+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "joey12300",
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Windows x64",
            "Text"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1025,
        "title": "TRT后端加速",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-develop\r\n- 【编译命令】cmake .. DENABLE_ORT_BACKEND=ON -DWITH_GPU=ON -DENABLE_TRT_BACKEND=ON -DTRT_DIRECTORY=PATH/TO/TENSORRT -DCUDA_DIRECTORY=/usr/local/cuda -DCMAKE_INSTALL_PREFIX=${pwd}/compiked_fastdeploy_sdk -DENABLE_VISION=ON -DOPENCV_DIRECTORY=PATH/TO/OPENCV\r\n- 【系统平台】: 银河麒麟桌面操作系统V10\r\n- 【硬件】： Nvidia GTX1060， CUDA 11.0 CUDNN 8.1.0\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【TRT后端加速】\r\n报如下错误\r\n![image](https://user-images.githubusercontent.com/58615953/210190535-ba78600d-9c05-41e1-9f4a-c09fb0ea3e12.png)\r\n\r\n",
        "state": "closed",
        "user": "GeT-RiGhTTT",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-02T01:59:01+00:00",
        "updated_at": "2024-02-06T04:24:20+00:00",
        "closed_at": "2024-02-06T04:24:20+00:00",
        "comments_count": [
            "jiangjiajun",
            "GeT-RiGhTTT",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1026,
        "title": "fastdeploy:x.y.z-gpu-cuda11.4-trt8.4-21.10",
        "body": "您好！请问如何把镜像的cuda11.4改为cuda11.2，如何重新编译这个镜像？\r\n",
        "state": "closed",
        "user": "intjun",
        "closed_by": "heliqi",
        "created_at": "2023-01-02T12:31:41+00:00",
        "updated_at": "2023-01-10T09:34:34+00:00",
        "closed_at": "2023-01-10T09:34:34+00:00",
        "comments_count": [
            "heliqi",
            "cailiang9",
            "cailiang9",
            "heliqi",
            "heliqi",
            "heliqi",
            "heliqi",
            "cailiang9",
            "heliqi"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1027,
        "title": "docs",
        "body": "# docs\r\nC++预测库（aarch64)下载链接无效\r\n![2d4731f245f0a2ef38ed4117afefefbe](https://user-images.githubusercontent.com/61459740/210234343-1d26cf0c-5a80-4d32-8425-184505dcb0c8.png)\r\n![a684137cc0ee710d3092f9f09ad9ae58](https://user-images.githubusercontent.com/61459740/210234369-a4974650-b748-4512-ba1e-e9477a2c1fbf.png)\r\n",
        "state": "closed",
        "user": "Sqhttwl",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-02T12:58:24+00:00",
        "updated_at": "2024-02-06T04:24:19+00:00",
        "closed_at": "2024-02-06T04:24:19+00:00",
        "comments_count": [
            "leiqing1",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1030,
        "title": "FastDeploy_examples_yolov7face_python_infer.py调用model.predict(im)报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.2\r\n- 【编译命令】用的example\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) \r\n- 【硬件】： CPU\r\n- 【编译语言】： Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.2/examples/vision/facedet/yolov7face/python/infer.py\r\n运行报错，提示model没有predict属性\r\n\r\n## 解决方案：\r\n需在Fastdeploy的python安装包文件中添加2行语句，请管理员合并到下一个版本里。\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.2/python/fastdeploy/vision/facedet/contrib/yolov7face.py\r\nclass YOLOv7Face(FastDeployModel)类下添加：\r\ndef predict(self, input_image):\r\n        return self._model.predict(input_image)\r\n",
        "state": "closed",
        "user": "i-passion",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-03T04:20:51+00:00",
        "updated_at": "2024-02-06T04:24:18+00:00",
        "closed_at": "2024-02-06T04:24:18+00:00",
        "comments_count": [
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1033,
        "title": "原生UIE性能到顶的情况下，FastDeploy能够带来多少的性能优化，希望提供测试报告附在github文档中",
        "body": "现在的情况是，多阶段推理，原生UIE fp16 use_faster=true 就已经到顶了\r\n\r\n希望能出一个测试报告，附在github文档的部署示例中，即在上述基础上，FastDeploy最大能够提高多少性能\r\n\r\n最好能做一个改造示例。因为从原生UIE迁移到FastDeploy部署，并且获得最大化的性能提升，里面还是有一些细节和注意事项的。\r\n\r\n文档里的说明内容更详细、性能提升结果可复现，有利于FastDeploy大规模推广、也方便我跟朋友们安利，祝好~\r\n\r\n类似这种效果：\r\n\r\n![image](https://user-images.githubusercontent.com/96770068/210310881-e187e83a-3a98-42a4-8548-992dff0dcb00.png)\r\n",
        "state": "closed",
        "user": "we-enjoy-today",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-03T06:05:48+00:00",
        "updated_at": "2024-02-06T04:24:17+00:00",
        "closed_at": "2024-02-06T04:24:17+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Text"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1035,
        "title": "怎么尽可能使用GPU",
        "body": "测试了rvm的trt加速，RTX2070显卡下1080P的输入图大概120ms，但是看了下gpu使用才4%左右，怎么尽可能的使用gpu，速度还能提升吗？",
        "state": "closed",
        "user": "JustinBili",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-03T06:47:59+00:00",
        "updated_at": "2024-02-06T04:24:16+00:00",
        "closed_at": "2024-02-06T04:24:16+00:00",
        "comments_count": [
            "jiangjiajun",
            "leiqing1",
            "JustinBili",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1036,
        "title": "希望能够开直播视频讲讲FastDeploy的设计思路、架构组成、先进点代码实现，并提供录播视频，链接放在github文档中",
        "body": "我和朋友都使用了FastDeploy，目前都有这方面的需求，讨论后决定发这个issue\r\n\r\n现有的介绍视频更多的是从产品使用的角度，但是为了更好的使用FastDeploy，希望能够更多的深入了解FastDeploy的架构设计、代码实现，以及后续的优化方向。",
        "state": "closed",
        "user": "we-enjoy-today",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-03T07:11:12+00:00",
        "updated_at": "2024-02-06T04:24:15+00:00",
        "closed_at": "2024-02-06T04:24:15+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1038,
        "title": "OCR中：det和cls模型可以正常编译为trt文件，但是rec模型编译时报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-release1.0.0\r\n- 【编译命令】cmake .. -DBUILD_ON_JETSON=ON \\\r\n         -DENABLE_VISION=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\n- 【系统平台】: jetson AGX\r\n- 【硬件】： jetpack4.6.1\r\n![image](https://user-images.githubusercontent.com/54393886/210316363-1758b806-1f2d-445e-83bd-b642703e6fec.png)\r\n\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n![image](https://user-images.githubusercontent.com/54393886/210316175-ecb1e658-4417-42d7-bb1a-fb7b36faf18b.png)\r\n\r\ndet和cls模型可以正常编译为trt文件，但是rec模型编译时报错",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-03T07:42:38+00:00",
        "updated_at": "2024-02-06T04:24:14+00:00",
        "closed_at": "2024-02-06T04:24:14+00:00",
        "comments_count": [
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jasper-cell",
            "jiangjiajun",
            "jasper-cell",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": [
            "OCR",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1047,
        "title": "'DetectionResult: [xmin, ymin, xmax, ymax, score, label_id 这种数据格式怎么解析成字典，或者list之类的数据格式",
        "body": "\r\n'DetectionResult: [xmin, ymin, xmax, ymax, score, label_id 这种数据格式怎么解析成字典，或者list之类的数据格式？\r\n![image](https://user-images.githubusercontent.com/57173448/210493151-45bd357f-8c12-4b5b-8268-0ef28aed66c8.png)\r\n",
        "state": "closed",
        "user": "Firestick-Xia",
        "closed_by": "Firestick-Xia",
        "created_at": "2023-01-04T05:56:35+00:00",
        "updated_at": "2023-01-05T02:46:24+00:00",
        "closed_at": "2023-01-05T02:46:24+00:00",
        "comments_count": [
            "jiangjiajun",
            "Firestick-Xia",
            "Firestick-Xia",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1046,
        "title": "自己训练的模型，c++ 运行picodet报错。",
        "body": "## 环境\r\n- 【FastDeploy版本】： 1.0.2\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n```\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build \r\ncd build\r\ncmake .. -DENABLE_ORT_BACKEND=OFF \\\r\n\t\t-DENABLE_PADDLE_BACKEND=OFF \\\r\n\t\t-DENABLE_OPENVINO_BACKEND=ON \\\r\n\t\t-DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n\t\t-DENABLE_VISION=ON \\\r\n\t\t-DOPENCV_DIRECTORY=${OPENCV4} \\\r\n\t\t-DENABLE_TEXT=ON\r\nmake -j8\r\nmake install\r\n```\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号. 使用的openvino 作为后端\r\n- 【编译语言】： C++ \r\n-  log\r\n```\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nterminate called after throwing an instance of 'ov::Exception'\r\n  what():  [ NETWORK_NOT_READ ] Unable to read the model: /xxx/heaven7/libs/paddle/paddle_ocr_module/icu_cls/model.pdmodel Please check that model format: pdmodel is supported and the model is correct. Available frontends: \r\n13:24:48: The program has unexpectedly finished.\r\n```\r\n\r\n- 参考代码, [picodet](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet)\r\n",
        "state": "closed",
        "user": "LightSun",
        "closed_by": "LightSun",
        "created_at": "2023-01-04T05:28:29+00:00",
        "updated_at": "2023-01-04T07:51:16+00:00",
        "closed_at": "2023-01-04T07:51:16+00:00",
        "comments_count": [
            "jiangjiajun",
            "LightSun",
            "jiangjiajun",
            "LightSun",
            "jiangjiajun",
            "LightSun",
            "LightSun",
            "LightSun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1050,
        "title": "请问预计PaddleX模型什么时候能支持？",
        "body": "平时多数模型都是paddlex训练的，但是FastDeploy加载PaddleX的模型会报错，从别的issue上得到消息是暂时不支持。想问一下，计划什么时候会将PaddleX的模型也支持起来呀？",
        "state": "closed",
        "user": "CashBai",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-04T06:54:44+00:00",
        "updated_at": "2024-02-06T04:24:13+00:00",
        "closed_at": "2024-02-06T04:24:13+00:00",
        "comments_count": [
            "jiangjiajun",
            "CashBai",
            "jiangjiajun",
            "CashBai",
            "jiangjiajun",
            "CashBai",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1051,
        "title": "用trition server 21.01编译服务器镜像的时候出错",
        "body": "cmake .. -DFASTDEPLOY_DIR=/workspace/fastdeploy/build/fastdeploy_install -DTRITON_COMMON_REPO_TAG=r21.02 -DTRITON_CORE_REPO_TAG=r21.02 -DTRITON_BACKEND_REPO_TAG=r21.02;\r\nmake -j`nproc`'\r\n出错：\r\n![1672816720(1)](https://user-images.githubusercontent.com/23567603/210503399-0ccedb0b-fc66-47eb-a449-4d5072401db8.jpg)\r\n\r\n",
        "state": "closed",
        "user": "intjun",
        "closed_by": "intjun",
        "created_at": "2023-01-04T07:19:00+00:00",
        "updated_at": "2023-02-03T09:59:57+00:00",
        "closed_at": "2023-02-03T09:59:56+00:00",
        "comments_count": [
            "heliqi",
            "intjun",
            "heliqi",
            "heliqi",
            "intjun",
            "heliqi",
            "heliqi",
            "intjun"
        ],
        "labels": [
            "Serving",
            "Build"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1055,
        "title": "paddle很多模型是训练模型，不是推理模型，希望增加直接适用训练模型的方式",
        "body": "不少模块并没有提供从训练模型转推理模型的方法，能否直接适用训练模型的参数文件直接转onnx。\r\n",
        "state": "closed",
        "user": "song4875343",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-04T10:58:09+00:00",
        "updated_at": "2024-02-06T04:24:12+00:00",
        "closed_at": "2024-02-06T04:24:12+00:00",
        "comments_count": [
            "jiangjiajun",
            "song4875343",
            "jiangjiajun",
            "song4875343",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1056,
        "title": "能否提升paddle模型读取，库导入的速度",
        "body": "paddle训练的模型可圈可点，但是paddle模型python import导入很多库时，时间非常长，这样很多企业应用时代码头一次适用，时间非常长，影响使用体验。对用pytorch类似问题，导入库和模型用0.3秒，而paddle进程导入一个paddle就要花2秒，其他乱七八糟的，一起需要4~5秒钟。",
        "state": "closed",
        "user": "song4875343",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-04T11:14:27+00:00",
        "updated_at": "2024-02-06T04:24:11+00:00",
        "closed_at": "2024-02-06T04:24:11+00:00",
        "comments_count": [
            "jiangjiajun",
            "song4875343",
            "jiangjiajun",
            "song4875343",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1059,
        "title": " RobustVideoMatting source. License question ",
        "body": "I'm wondering if it's possible to use [PaddlePaddle RVM built-in C++ module and models](https://github.com/PaddlePaddle/FastDeploy/tree/release/1.0.2/examples/vision/matting/rvm), due to the fact that [original model](https://github.com/PeterL1n/RobustVideoMatting) has GPL-3.0 license ( Disclose source), and PaddlePaddle has  Apache-2.0 license (not disclose source)",
        "state": "closed",
        "user": "ox-prog",
        "closed_by": "ox-prog",
        "created_at": "2023-01-04T15:49:08+00:00",
        "updated_at": "2023-01-15T08:24:15+00:00",
        "closed_at": "2023-01-15T08:24:15+00:00",
        "comments_count": [
            "jiangjiajun",
            "ox-prog",
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1061,
        "title": "加载自己编译的libtriton_fastdeploy.so出现问题",
        "body": "自己尝试编译不同cuda版本的server镜像，启动时出现bug\r\n![1672885753(1)](https://user-images.githubusercontent.com/23567603/210688011-2d8ac7f6-a840-4b64-b11b-7c94a937431f.png)\r\n",
        "state": "closed",
        "user": "intjun",
        "closed_by": "heliqi",
        "created_at": "2023-01-05T02:31:18+00:00",
        "updated_at": "2023-01-05T05:47:23+00:00",
        "closed_at": "2023-01-05T05:47:23+00:00",
        "comments_count": [
            "heliqi",
            "intjun",
            "heliqi"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1064,
        "title": "rv1126上部署demo，交叉编译失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】: 交叉编译，编译器为 gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf\r\n- 【系统平台】: host为Ubuntu 20.04\r\n- 【硬件】: RV1126\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n1. 交叉编译fastdeploy库成功\r\n2. 编译[rv1126平台上的yolov5例子](https://github.com/PaddlePaddle/FastDeploy/tree/release/1.0.2/examples/vision/detection/yolov5/rv1126/cpp)出错\r\n     - 错误日志为：\r\n![image](https://user-images.githubusercontent.com/37280580/210696604-8372228b-5af3-4bf3-839a-e6d00c0d55fb.png)\r\n     - 可能原因：paddlelite和opencv预编译库所用的 gcc版本高于8.3",
        "state": "closed",
        "user": "richjjj",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-05T03:44:49+00:00",
        "updated_at": "2024-02-06T04:24:09+00:00",
        "closed_at": "2024-02-06T04:24:09+00:00",
        "comments_count": [
            "richjjj",
            "yeliang2258",
            "richjjj",
            "yeliang2258",
            "richjjj",
            "yeliang2258",
            "richjjj",
            "yeliang2258",
            "jiangjiajun"
        ],
        "labels": [
            "timvx"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1077,
        "title": "fastdeploy docker 镜像部署 [ERROR] fastdeploy/runtime.cc(271)::UseOrtBackend       The FastDeploy didn't compile with OrtBackend. ",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nregistry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-cpu-only-21.10 \r\n\r\n## 问题日志及出现问题的操作流程\r\n按：https://github.com/PaddlePaddle/FastDeploy/tree/release/1.0.2/examples/text/uie/serving 例子部署，\r\n出现ERROR：\r\nI0106 03:03:01.817417 97 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: uie_0_0 (CPU device 0)\r\nmodel_config: {'name': 'uie', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'INPUT_1', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'execution_accelerators': {'gpu_execution_accelerator': [], 'cpu_execution_accelerator': [{'name': 'onnxruntime', 'parameters': {'cpu_threads': '3'}}]}, 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'uie_0', 'kind': 'KIND_CPU', 'count': 2, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\ninput: ['INPUT_0', 'INPUT_1']\r\noutput: ['OUTPUT_0']\r\n[ERROR] fastdeploy/runtime.cc(322)::UseOrtBackend       The FastDeploy didn't compile with OrtBackend.\r\n\r\n",
        "state": "closed",
        "user": "hzm-2018",
        "closed_by": "heliqi",
        "created_at": "2023-01-06T03:08:23+00:00",
        "updated_at": "2024-09-29T08:06:44+00:00",
        "closed_at": "2023-01-10T09:35:22+00:00",
        "comments_count": [
            "heliqi",
            "heliqi",
            "hzm-2018",
            "heliqi",
            "blakeliu",
            "micsama"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1063,
        "title": "yolov5  1126例子部署",
        "body": "cmake -DCMAKE_TOOLCHAIN_FILE=${PWD}/../fastdeploy-timvx/toolchain.cmake -DFASTDEPLOY_INSTALL_DIR=${PWD}/../fastdeploy-timvx -DTARGET_ABI=armhf ..\r\n-- PADDLELITE_URL will be configured if WITH_TIMVX=ON.\r\n-- The path of OpenCV is /media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/fastdeploy-timvx/third_libs/install/opencv.\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/arm-linux-gnueabihf-g++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   WITH_GPU                  : OFF\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_LITE_BACKEND       : ON\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   DEPENDENCY_LIBS           : /media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/fastdeploy-timvx/lib/libfastdeploy.so;/media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/fastdeploy-timvx/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_gapi;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_stitching;opencv_video;opencv_videoio;/media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/fastdeploy-timvx/third_libs/install/flycv/lib/libflycv_shared.so\r\n-- Configuring done\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    CMAKE_TOOLCHAIN_FILE\r\n\r\n\r\n-- Build files have been written to: /media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/build\r\n(base) @:/media//extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/build$ make  -j4\r\nScanning dependencies of target infer_demo\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer.cc.o\r\n[100%] Linking CXX executable infer_demo\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_gapi.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_highgui.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_ml.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_objdetect.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_photo.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_stitching.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_videoio.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgcodecs.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_video.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_imgproc.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_dnn.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_calib3d.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_features2d.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 5 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 6 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 7 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 8 (>= sh_info of 3)\r\n/usr/lib/gcc-cross/arm-linux-gnueabihf/9/../../../../arm-linux-gnueabihf/bin/ld: ../fastdeploy-timvx/third_libs/install/opencv/lib/libopencv_flann.so.4.6.0: .dynsym local symbol at index 9 (>= sh_info of 3)\r\n[100%] Built target infer_demo\r\n(base) @:/media/extend/FastDeploy/examples/vision/detection/yolov5/rv1126/cpp/build$ make install\r\n\r\n",
        "state": "closed",
        "user": "yutao007",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-05T03:44:23+00:00",
        "updated_at": "2024-02-06T04:24:10+00:00",
        "closed_at": "2024-02-06T04:24:10+00:00",
        "comments_count": [
            "yutao007",
            "yutao007",
            "yutao007",
            "yeliang2258",
            "jiangjiajun"
        ],
        "labels": [
            "timvx"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1079,
        "title": "fd_serving容器没有ip",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n参考此案例运行\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README.md\r\n\r\ndocker run -it --net=host --name fd_serving --shm-size=\"1g\"  -v D:/pkg/FastDeploy/examples/vision/detection/paddledetection/serving/:/serving registry.baidubce.com/paddlepaddle/fastdeploy:1.0.1-cpu-only-21.10  bash\r\n\r\n\r\n新建容器fd_serving 没有分配ip, win, ubuntu均是 \r\n![image](https://user-images.githubusercontent.com/112528302/210937046-adb394a3-03e8-48eb-90ba-22ccafcd3585.png)\r\n![image](https://user-images.githubusercontent.com/112528302/210937183-5a7ea149-4a1e-4f76-a170-3eade71f858a.png)\r\n请问执行代码是否有问题？ host需要改成bridge吗？\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "heliqi",
        "created_at": "2023-01-06T05:37:48+00:00",
        "updated_at": "2023-01-30T08:07:07+00:00",
        "closed_at": "2023-01-30T08:07:07+00:00",
        "comments_count": [
            "heliqi",
            "heliqi"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1081,
        "title": "可以使用fastdeploy部署GPLink模型吗（UIE的蒸馏模型）？",
        "body": "如题，有的话是否有实例呢？",
        "state": "closed",
        "user": "996-icu-FuJian",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-06T07:27:45+00:00",
        "updated_at": "2024-02-06T04:24:08+00:00",
        "closed_at": "2024-02-06T04:24:08+00:00",
        "comments_count": [
            "joey12300",
            "996-icu-FuJian",
            "jiangjiajun"
        ],
        "labels": [
            "Text"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1086,
        "title": "[Android] 使用预编译的cpu android动态库 运行失败",
        "body": "[FastDeploy][ERROR] fastdeploy/runtime.cc(262)::UsePaddleBackend\tThe FastDeploy didn't compile with Paddle Inference.\r\n\r\n看起来像是因为提供的预编译库没有编Paddle Inference，只支持Paddle Lite。\r\n请问有Lite的调用方法吗？我这边的实现参考的：\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.2/docs/cn/quick_start/runtime/cpp.md",
        "state": "closed",
        "user": "MarsMeng1994",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-06T11:44:51+00:00",
        "updated_at": "2024-02-06T04:24:07+00:00",
        "closed_at": "2024-02-06T04:24:07+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": [
            "Question",
            "Android"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1087,
        "title": "Why does importing an image of an ONNX model exported with yolov5 --img-size 1280 get an error? How to solve it.为什么用 yolov5  --img-size 1280 导出的onnx模型图片输入会报错?如何解决",
        "body": "`[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\n[INFO] fastdeploy/runtime.cc(528)::fastdeploy::Runtime::Init    Runtime initialized with Backend::OPENVINO in Device::CPU.\r\nTraceback (most recent call last):\r\n  File \"d:/AHONG_Space/AHONG/mytools/rcnnOKgpu copy.py\", line 40, in <module>\r\n    result = model.predict(img_array)\r\n  File \"D:\\anaconda3\\envs\\paddleX\\lib\\site-packages\\fastdeploy\\vision\\detection\\contrib\\yolov5.py\", line 154, in predict\r\n    return self._model.predict(input_image)\r\nRuntimeError: Can't set input blob with name: images, because model input (shape={1,3,1280,1280}) and blob (shape=(1.3.640.640)) are incompatible`\r\n\r\n默认640就可以正常运行\r\n",
        "state": "closed",
        "user": "HiaHong",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-06T18:20:35+00:00",
        "updated_at": "2024-02-06T04:24:06+00:00",
        "closed_at": "2024-02-06T04:24:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "lhh753159",
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1098,
        "title": "build Fast deploy on Raspbian(bullseye) 32 bit ",
        "body": "## Environment\r\n\r\nFastDeploy version:  0.6.0 \r\nOS Platform: Raspbian(bullseye) 32 bit \r\n\r\n## Problem description\r\n\r\nHi\r\nThanks for sharing this project.\r\nI want to build Fast Deploy on Raspbian(bullseye) 32 bit . I use the below command for building.\r\n\r\n`sudo cmake .. -DENABLE_ORT_BACKEND=ON -DOPENCV_DIRECTORY=/usr/local/ -DCMAKE_INSTALL_PREFIX=${PWD}/myjob -DENABLE_VISION=ON`\r\n\r\nbut I get this error.\r\n\r\n![Screenshot from 2023-01-09 14-39-31](https://user-images.githubusercontent.com/65589645/211295076-c81934be-dfdb-4f67-ba66-fd84f2a576aa.png)\r\n\r\nCan you help me?\r\nThanks\r\n\r\n",
        "state": "closed",
        "user": "saeedkhanehgir",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-09T11:10:43+00:00",
        "updated_at": "2024-02-06T04:24:05+00:00",
        "closed_at": "2024-02-06T04:24:04+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1103,
        "title": "什么时候支持SSE指令集CPU",
        "body": "\r\n### 支持SSE指令集CPU，在一些低功耗CPU上使用\r\n工控机上多采用低功耗的cpu，例如 Intel Atom系列，它不支持AVX 及 FMA指令集，只支持SSE。\r\n在windows开发机上编译PaddleOCR工程示例，正常运行，部署到工控机的windows10上却无法运行，用cpu-z工具查看cpu信息时才注意到是 Intel Atom x5-Z8350，不支持AVX 及 FMA指令集。\r\n\r\nTensorFlow Lite好像是支持SSE指令集cpu。",
        "state": "closed",
        "user": "yzm0080",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-09T16:48:55+00:00",
        "updated_at": "2024-07-16T06:40:52+00:00",
        "closed_at": "2024-07-16T06:40:52+00:00",
        "comments_count": [
            "ZeyuChen",
            "jiangjiajun",
            "nefusmzj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1105,
        "title": "加载Lite模型时出错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： release/1.0.2\r\n- 【编译命令】-DENABLE_LITE_BACKEND=ON -DENABLE_VISION=ON\r\n- 【系统平台】: Android AArch64\r\n- 【硬件】： msm8998\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n我们的库本来是运行在桌面平台的，所以使用了ORT backend，但最近我想要把它移植到android平\r\n台，所以将后端切换到了paddle lite，也使用opt_linux工具把模型转换到了Lite格式，沿用了之前的\r\n加载代码，不过为了方便我把lite的模型改成了和paddle一样的名字，但在运行时会报错\r\n# 转换代码\r\n`opt_linux --model_dir=./det/ --valid_targets=arm --optimize_out_type=naive_buffer --optimize_out=./det/lite_det`\r\n\r\n# 代码位置\r\nhttps://github.com/MaaAssistantArknights/MaaAssistantArknights/blob/00f6725d7b49a17e5028ba796b0e6a465a1d1fc3/src/MaaCore/Config/Miscellaneous/OcrPack.cpp#L37\r\n\r\n# 报错日志\r\n```\r\n~/install $ ./test\r\nlibpng warning: sBIT: invalid\r\nlibpng warning: sBIT: invalid\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\nlibpng warning: sBIT: invalid\r\nlibpng warning: sBIT: invalid\r\nlibpng warning: sBIT: invalid\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\nlibpng warning: iCCP: known incorrect sRGB profile\r\n[I  1/10 15:26:49.428 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: HARDWARE  : QUALCOMM TECHNOLOGIES, INC MSM8998\r\n_MSM8998_MSM8998_\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 8\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 1900, min freq: 1900, cluster ID: 1, CPU ARCH: A53\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 1900, min freq: 1900, cluster ID: 1, CPU ARCH: A53\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 1900, min freq: 1900, cluster ID: 1, CPU ARCH: A53\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 1900, min freq: 1900, cluster ID: 1, CPU ARCH: A53\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 4, max freq: 2457, min freq: 2457, cluster ID: 0, CPU ARCH: A73\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 5, max freq: 2457, min freq: 2457, cluster ID: 0, CPU ARCH: A73\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 6, max freq: 2457, min freq: 2457, cluster ID: 0, CPU ARCH: A73\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 7, max freq: 2457, min freq: 2457, cluster ID: 0, CPU ARCH: A73\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is:\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 64 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 64 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 64 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 64 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is:\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is:\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 7901028KB\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  1/10 15:26:49.429 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  1/10 15:26:49.430 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  1/10 15:26:49.430 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from ./resource/PaddleOCR/det/inference.pdmodel\r\n[I  1/10 15:26:49.433 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from ./resource/PaddleOCR/det/inference.pdiparams\r\n[F  1/10 15:26:49.433 .../lite/core/model/general/program_desc.cc:51 GetBlock] Check failed: (idx < static_cast<int32_t>(BlocksSize())): 0!<0 idx >= blocks.size() \r\n\r\n```\r\n是不是我们加载模型的方式有问题？\r\n\r\n",
        "state": "closed",
        "user": "aa889788",
        "closed_by": "aa889788",
        "created_at": "2023-01-10T08:35:53+00:00",
        "updated_at": "2023-01-11T05:52:26+00:00",
        "closed_at": "2023-01-10T12:20:20+00:00",
        "comments_count": [
            "jiangjiajun",
            "aa889788",
            "aa889788",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1106,
        "title": "GPU发布服务错误 Error: Failed to initialize NVML",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nnvidia-docker images serving registry.baidubce.com/paddlepaddle/fastdeploy:1.0.1-gpu-cuda11.4-trt8.4-21.10 \r\n\r\nwin\r\n\r\n参考案例\r\n(https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README.md)\r\n\r\ngpu部署已经修改runtime config.pbtxt\r\n![image](https://user-images.githubusercontent.com/112528302/211511749-90e0a6a4-a5ac-476c-9103-11252235190c.png)\r\n\r\nyoloe 的name也改了，也注释了\r\n![image](https://user-images.githubusercontent.com/112528302/211512055-92ee30d7-7922-44fc-bf00-ff1fd4c22148.png)\r\n\r\n现在发布gpu服务报错\r\n\r\n![image](https://user-images.githubusercontent.com/112528302/211512222-f7275ac4-3a4a-48b5-a926-dc0909623854.png)\r\n![image](https://user-images.githubusercontent.com/112528302/211512683-33aaed16-d26f-414f-ba89-1c0b43bfb230.png)\r\n\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "cunjing56",
        "created_at": "2023-01-10T09:25:43+00:00",
        "updated_at": "2023-01-10T10:04:20+00:00",
        "closed_at": "2023-01-10T10:04:19+00:00",
        "comments_count": [
            "heliqi",
            "cunjing56"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1119,
        "title": " convert failed node:onnx__Reshape_586, op_type is Conv",
        "body": "- 【FastDeploy版本】： fastdeploy-develop、paddle2onnx=1.0.3 、paddlepaddle-gpu=0.0.0.post111\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3090TI\r\n- 【编译语言】：Python3.8.3\r\n\r\n------------------------------------------\r\n使用fastdeploy压缩yolov8的模型时出现如下问题，应该是在将onnx转paddle时出错了，问题如下：\r\n指令： fastdeploy --auto_compress --config_path=。。。/FastDeploy-develop/tools/auto_compression/configs/detection/yolov8_quant_myself.yaml  --method='QAT' --save_dir='。。。/FastDeploy-develop/tools/auto_compression/yolov8_ptq_model_test/'\r\nWARNING:root:No module named 'parl'\r\n/usr/local/anaconda3/envs/paddle_yolov8/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n/usr/local/anaconda3/envs/paddle_yolov8/lib/python3.8/site-packages/paddle/fluid/framework.py:477: UserWarning: PaddlePaddle version 2.3.0 or higher is required, but 0.0.0 installed, Maybe you are using a develop version, please make sure the version is good with your code.\r\n/usr/local/anaconda3/envs/paddle_yolov8/lib/python3.8/site-packages/paddleslim-0.0.0.dev0-py3.8.egg/paddleslim/quant/reconstruction_quantization.py:179: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n/usr/local/anaconda3/envs/paddle_yolov8/lib/python3.8/site-packages/paddleslim-0.0.0.dev0-py3.8.egg/paddleslim/quant/reconstruction_quantization.py:179: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\nWelcome to use FastDeploy Auto Compression Toolkit!\r\n-----------  Running Arguments -----------\r\n Distillation:\r\n         alpha: 1.0\r\n         loss: soft_label\r\n Global:\r\n         arch: YOLOv7\r\n         format: onnx\r\n         input_list: ['x2paddle_images']\r\n         model_dir: 。。/Desktop/workspace/yolov8/weights/yolov8s.onnx\r\n         model_filename: model.pdmodel\r\n         params_filename: model.pdiparams\r\n         ptq_image_path: 。。/Desktop/workspace/Dateset/NewDataset-aver2/images/val_small_aug\r\n         ptq_preprocess: yolo_image_preprocess\r\n         qat_batch_size: 8\r\n         qat_image_path: 。。/Desktop/workspace/Dateset/NewDataset-aver2/images/val_small_aug\r\n         qat_preprocess: yolo_image_preprocess\r\n PTQ:\r\n         calibration_method: avg\r\n         skip_tensor_list: None\r\n Quantization:\r\n         activation_quantize_type: moving_average_abs_max\r\n         onnx_format: True\r\n         quantize_op_types: ['conv2d', 'depthwise_conv2d']\r\n TrainConfig:\r\n         epochs: 20\r\n         learning_rate:\r\n                 T_max: 8000\r\n                 learning_rate: 3e-05\r\n                 type: CosineAnnealingDecay\r\n         optimizer_builder:\r\n                 optimizer:\r\n                         type: SGD\r\n                 weight_decay: 4e-05\r\n         train_iter: 80000\r\n\r\n2023-01-11 17:18:48,795-INFO: Now translating model from onnx to paddle.\r\nmodel ir_version: 7, op version: 13\r\nUnknown shape for input tensor[tensor name: 'x2paddle_images'] -> shape: ['batch', 3, 640, 640], Please define shape of input here,\r\nNote:you can use visualization tools like Netron to check input shape.\r\nShape of Input(e.g. -1,3,224,224), enter 'N' to skip: **-1,3,640,640**\r\nshape inferencing ...\r\n[WARNING] Incomplete symbolic shape inference\r\nshape inferenced.\r\nNow, onnx2paddle support convert onnx model opset_verison [7, 8, 9, 10, 11, 12, 13, 14, 15], opset_verison of your onnx model is 13.\r\nTotal nodes: 259\r\nNodes converting ...\r\n**Converting node 386 ...     2023-01-11 17:18:57,088-WARNING: convert failed node:onnx__Reshape_586, op_type is Conv\r\n2023-01-11 17:18:57,088-ERROR: x2paddle threw an exception, you can ask for help at:** https://github.com/PaddlePaddle/X2Paddle/issues\r\n\r\n请问，出现如上问题是因为fastdeploy还不支持yolov8吗？\r\n我所使用的yolov8s.onnx : 链接：https://pan.baidu.com/s/1U77yKCFvq9L2dl22Glvmdg  提取码：uew0\r\n",
        "state": "closed",
        "user": "xiaohui0225",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-11T09:33:35+00:00",
        "updated_at": "2025-02-04T06:41:26+00:00",
        "closed_at": "2025-02-04T06:41:26+00:00",
        "comments_count": [
            "wjj19950828",
            "wjj19950828",
            "xiaohui0225",
            "ZJX-CV",
            "xiaohui0225",
            "ZJX-CV",
            "nkhlS141",
            "xiaohui0225",
            "xiaohui0225",
            "ZJX-CV"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1121,
        "title": "使用FastDeploy部署PPOCRv3模型存在精度问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 1.0.2与fastdeploy-python 1.0.2均能复现\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： R9-5900HS/RTX3060 Laptop\r\n- 【编译语言】：Python 3.7.9\r\n- 【测试语言】：日语\r\n- 【测试模型】：PPOCRv3-日语模型\r\n\r\n## 问题日志及出现问题的操作流程\r\n您好，我们于近期测试发现，FastDeploy在PPOCRv3-日语模型下的识别精度似乎远不及PaddleServing Pipeline(0.8.3)。\r\n👇这是我们用于测试的图片\r\n![image](https://user-images.githubusercontent.com/61458603/211816872-72f7f736-95fb-4b60-948c-0a1066a997c7.png)\r\n👆\r\nFD这边测试的服务端代码采用FD提供的example去除CLS流程，加载日语模型后使用: https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCRv3/python/serving\r\nServing这边的服务端代码: https://github.com/PaddlePaddle/Serving/blob/v0.9.0/examples/Pipeline/PaddleOCR/ocr/web_service.py\r\n**请注意，Serving的样例的RecOp内的OCRReader()需要对char_dict_path进行初始化，否则您无法顺利在Pipeline上进行推理。**\r\n**请注意，由于Serving的兼容性较差，您可能无法在Windows和MacOS上顺利完成推理，需要Linux复现。**\r\n我们用于启动FD的命令是:   \r\n\r\n```\r\nfastdeploy simple_serving --app server:app\r\n```  \r\n\r\n在上述环境下，我们使用FD进行识别得到的结果是:   \r\n\r\n```\r\n{\r\n    \"result\":\"{\\\"boxes\\\": [[28, 18, 265, 18, 265, 32, 28, 32], [23, 36, 209, 36, 209, 50, 23, 50]], \\\"text\\\": [\\\"(\\あ\\れ?\\ス\\ズ\\ぐ\\ん\\、\\台\\本\\持\\っ\\て\\る\\\", \\\"\\で\\も\\公\\温\\の\\色\\衣\\じ\\ゃ\\な\\い\\な)\\\"], \\\"rec_scores\\\": [0.8192606568336487, 0.7393889427185059], \\\"cls_scores\\\": [], \\\"cls_labels\\\": []}\"\r\n}\r\n```  \r\n\r\n使用PaddleServing Pipeline加载同样模型识别的结果是:  \r\n\r\n```\r\n{\r\n    \"err_no\": 0,\r\n    \"err_msg\": \"\",\r\n    \"key\": [\r\n        \"result\"\r\n    ],\r\n    \"value\": [\"{\\\"Result\\\": [{\\\"Coordinate\\\": {\\\"UpperLeft\\\": [31.0, 74.0], \\\"UpperRight\\\": [553.0, 74.0], \\\"LowerRight\\\": [553.0, 101.0], \\\"LowerLeft\\\": [31.0, 101.0]}, \\\"Words\\\": \\\"(\\あ\\れ?\\ス\\ズ\\く\\ん\\、\\台\\本\\持\\っ\\て\\る\\…\\\", \\\"Score\\\": 0.978085994720459}, {\\\"Coordinate\\\": {\\\"UpperLeft\\\": [20.0, 117.0], \\\"UpperRight\\\": [394.0, 117.0], \\\"LowerRight\\\": [394.0, 141.0], \\\"LowerLeft\\\": [20.0, 141.0]}, \\\"Words\\\": \\\"\\で\\も\\公\\演\\の\\台\\本\\じ\\ゃ\\な\\い\\な)\\\", \\\"Score\\\": 0.8853415250778198}], \\\"Code\\\": 0, \\\"Message\\\": \\\"Success\\\"}\"\r\n    ],\r\n    \"tensors\": []\r\n}\r\n```  \r\n\r\n按理来说同样的模型不该产生如此巨大的差异，我们认为FastDeploy进行推理的参数存在问题，导致实际精度不如PaddleServing（例如FD的batch_size是写死的）。详细情况劳烦您进一步代为分析。\r\n\r\n\r\n",
        "state": "open",
        "user": "C4a15Wh",
        "closed_by": null,
        "created_at": "2023-01-11T13:24:51+00:00",
        "updated_at": "2023-11-26T02:59:14+00:00",
        "closed_at": null,
        "comments_count": [
            "yunyaoXYY",
            "C4a15Wh",
            "yunyaoXYY",
            "yunyaoXYY",
            "C4a15Wh",
            "yunyaoXYY",
            "C4a15Wh",
            "laugh12321",
            "huangjun11",
            "C4a15Wh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1122,
        "title": "fastdeploy支持jetson jetpack4.5及其以下版本吗",
        "body": "看了官方要求是jetpack必须是4.6，但是现在市场上主流用的最多还是jetpack4.5及其以下版本,这些版本cuda都是10.2，建议提供对jetpack4.5及其以下版本支持，不然jetson只支持4.6版本使用上基本没有什么意义。目前绝大部分还是用的cuda10.2版本jetson",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-12T01:46:15+00:00",
        "updated_at": "2024-10-29T06:43:34+00:00",
        "closed_at": "2024-10-29T06:43:34+00:00",
        "comments_count": [
            "JoeyZhu"
        ],
        "labels": [
            "Enhancement",
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1125,
        "title": "None, <repr raised Error>, <ModelFormat.ONNX: 2>",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python-1.0.2\r\n- 【编译命令】无\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： NVIDIA Quadro P400, 2048MiB， CUDA 11.3.55 CUDNN 8.6.0\r\n- 【编译语言】： Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\nexample代码全部可以跑通，但换成自己的onnx文件运行infer代码后，出现报错。\r\n此onnx文件是通过yolov5v6.2官方代码的export.py文件导出的，经实验通过Netron可以查看这个onnx文件的网络结构\r\n\r\n代码：\r\n````\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\nfrom time import time\r\n\r\n\r\ndef build_option(device, trt=True):\r\n    option = fd.RuntimeOption()\r\n    if device.lower() == \"kunlunxin\":\r\n        option.use_kunlunxin()\r\n\r\n    if device.lower() == \"gpu\":\r\n        option.use_gpu()\r\n\r\n    if trt:\r\n        option.use_trt_backend()\r\n        option.set_trt_input_shape(\"images\", [1, 3, 640, 640])\r\n        return option\r\n\r\n\r\nif __name__ == '__main__':\r\n    root = 'D:/CodeLibraby/FastDeploy/examples/vision/detection/yolov5/python'\r\n\r\n    # 配置runtime，加载模型\r\n    t = time()\r\n    runtime_option = build_option(device='gpu')\r\n    model_file = os.path.join(root, \"yolov5s_infer/model.pdmodel\")\r\n    params_file = os.path.join(root, \"yolov5s_infer/model.pdiparams\")\r\n    model = fd.vision.detection.YOLOv5(\r\n        model_file='D:/CodeLibraby/FastDeploy/examples/vision/detection/yolov5/python/best.onnx',\r\n        params_file=None,\r\n        runtime_option=runtime_option,\r\n        model_format=fd.ModelFormat.ONNX)\r\n    print(f'加载模型用时: {time() - t}秒')\r\n\r\n    # 预测图片检测结果\r\n\r\n    t = time()\r\n    image = os.path.join(root, '000000014439.jpg')\r\n    im = cv2.imread(image)\r\n    result = model.predict(im)\r\n    print(f'推理图片用时: {time() - t}秒')\r\n    print(result.boxes)\r\n\r\n    # 预测结果可视化\r\n    vis_im = fd.vision.vis_detection(im, result)\r\n    cv2.imwrite(\"visualized_result.jpg\", vis_im)\r\n    print(\"Visualized result save in ./visualized_result.jpg\")`\r\n```\r\n\r\n问题报错\r\n`[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Anaconda3\\envs\\pytorch\\Library\\bin\r\nTraceback (most recent call last):\r\n  File \"D:/CodeLibraby/equipment_deploy/infer.py\", line 29, in <module>\r\n    model = fd.vision.detection.YOLOv5(\r\n  File \"C:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\fastdeploy\\vision\\detection\\contrib\\yolov5.py\", line 181, in __init__\r\n    self._model = C.vision.detection.YOLOv5(\r\nTypeError: __init__(): incompatible constructor arguments. The following argument types are supported:\r\n    1. fastdeploy.libs.fastdeploy_main.vision.detection.YOLOv5(arg0: str, arg1: str, arg2: fastdeploy.libs.fastdeploy_main.RuntimeOption, arg3: fastdeploy.libs.fastdeploy_main.ModelFormat)\r\n\r\nInvoked with: 'D:/CodeLibraby/FastDeploy/examples/vision/detection/yolov5/python/best.onnx', None, <repr raised Error>, <ModelFormat.ONNX: 2>\r\n\r\nProcess finished with exit code 1\r\n`\r\nonnx文件下载：\r\n[https://pan.baidu.com/s/1HtSGAd9QDPIbItRhK8lq7Q?pwd=m9cj](url)\r\n提取码：m9cj\r\n\r\n\r\n",
        "state": "closed",
        "user": "WanderAlphonse",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-12T02:04:18+00:00",
        "updated_at": "2024-02-06T04:24:03+00:00",
        "closed_at": "2024-02-06T04:24:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1128,
        "title": "python安装fastdeploy之后显示__init__中没有vision",
        "body": "遵照要求使用pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html 成功安装fastdeploy后，打算部署Yolov5模型，但显示__init__中没有vision，请问是我缺少了哪部操作吗\r\n",
        "state": "closed",
        "user": "guoyunqingyue",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-12T10:03:47+00:00",
        "updated_at": "2024-02-06T04:24:02+00:00",
        "closed_at": "2024-02-06T04:24:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "guoyunqingyue",
            "jiangjiajun",
            "guoyunqingyue",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1132,
        "title": "Yolov8  feature request",
        "body": "Yolov8 is released by ultralytics,do you have any plans on adding the model in the near future ?",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "UygarUsta99",
        "created_at": "2023-01-12T14:43:55+00:00",
        "updated_at": "2023-01-21T21:17:02+00:00",
        "closed_at": "2023-01-21T21:17:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "wjj19950828",
            "UygarUsta99",
            "UygarUsta99",
            "wjj19950828",
            "jiangjiajun",
            "wjj19950828",
            "UygarUsta99",
            "jiangjiajun",
            "UygarUsta99",
            "UygarUsta99",
            "jiangjiajun",
            "UygarUsta99"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1133,
        "title": "windows部署C++代码（gpu）",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，fastdeploy-win-x64-gpu-1.0.2\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号，Nvidia GPU 3060I， CUDA 11.2 CUDNN 8.0.4\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n![微信图片_20230113014025](https://user-images.githubusercontent.com/14051049/212139825-c966f971-4c52-4225-ad89-dcdf046d7415.png)\r\n![微信图片_20230113014032](https://user-images.githubusercontent.com/14051049/212139854-c741ceda-410c-4f65-93a3-6b087a0ebf4d.png)\r\n\r\nwindows上部署fastdeploy，debug报错“没有为 openvino.dll 加载的符号文件”。\r\nC++代码编译不报错，运行报错。\r\n\r\n",
        "state": "closed",
        "user": "David-Wang001",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-12T17:41:11+00:00",
        "updated_at": "2024-02-06T04:24:01+00:00",
        "closed_at": "2024-02-06T04:24:01+00:00",
        "comments_count": [
            "jiangjiajun",
            "lhh753159",
            "jiangjiajun",
            "lhh753159",
            "leeguandong",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1134,
        "title": "为什么推理速度会越来越慢？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.1\r\n- 【编译命令】pip install\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1050， CUDA 11.7 CUDNN 8.4\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n<img width=\"551\" alt=\"1673576862899\" src=\"https://user-images.githubusercontent.com/13444641/212223094-44538ead-ce69-4222-af96-e0892db6abf4.png\">\r\n",
        "state": "closed",
        "user": "universea",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-13T02:29:27+00:00",
        "updated_at": "2024-02-06T04:24:00+00:00",
        "closed_at": "2024-02-06T04:24:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "universea",
            "wjj19950828",
            "wjj19950828",
            "universea",
            "DefTruth",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1135,
        "title": "fastdeploy使用nuitka打包后无法运行",
        "body": "nuitka打包命令 nuitka demo.py --mingw64 --standalone\r\n打包后报错为\r\n![b2ceea8934d1020e31511d3bb60e8b7](https://user-images.githubusercontent.com/48303845/212227349-64d8563a-acee-461a-8632-7b08704a290a.png)\r\n",
        "state": "closed",
        "user": "Hr-Song",
        "closed_by": "chenqianhe",
        "created_at": "2023-01-13T03:01:47+00:00",
        "updated_at": "2023-05-05T08:26:34+00:00",
        "closed_at": "2023-01-19T01:51:08+00:00",
        "comments_count": [
            "Hr-Song",
            "chenqianhe",
            "chenqianhe",
            "Hr-Song",
            "chenqianhe",
            "cuinfo",
            "yangf11",
            "yangf11",
            "chenqianhe",
            "zachary-zheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1138,
        "title": "yolov8",
        "body": "yolov8 有支持计划吗",
        "state": "closed",
        "user": "sl00001",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-13T08:05:15+00:00",
        "updated_at": "2024-02-06T04:23:59+00:00",
        "closed_at": "2024-02-06T04:23:59+00:00",
        "comments_count": [
            "jiangjiajun",
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1140,
        "title": "picodet模型,.onnx模型转.rknn时报错ValueError: Unknow elem_type for multiclass_nms3_0.tmp_0!",
        "body": "根据官网教程安装了rknntoolkit环境，对picodet模型转成onnx格式以后，继续用export.py转rknn的过程中报如下错误：\r\n{'mean': [[128.5, 128.5, 128.5]], 'std': [[128.5, 128.5, 128.5]], 'model_path': './picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx', 'outputs_nodes': None, 'do_quantization': False, 'output_folder': './picodet_s_416_coco_lcnet_infer', 'target_platform': 'RK3588'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\nE load_onnx: Unknow elem_type for multiclass_nms3_0.tmp_0!\r\nW load_onnx: ===================== WARN(2) =====================\r\nE rknn-toolkit2 version: 1.4.0-22dcfef4\r\nE load_onnx: Catch exception when loading onnx model: /FastDeploy/tools/rknpu2/picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1152, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 575, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 44, in rknn.api.ir_graph.IRGraph.__init__\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 380, in rknn.api.ir_graph.IRGraph.rebuild\r\nE load_onnx:   File \"rknn/api/rknn_log.py\", line 113, in rknn.api.rknn_log.RKNNLog.e\r\nE load_onnx: ValueError: Unknow elem_type for multiclass_nms3_0.tmp_0!\r\nTraceback (most recent call last):\r\n  File \"export.py\", line 47, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n请问是因为什么原因导致？",
        "state": "closed",
        "user": "chenzhizui",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-01-13T09:45:50+00:00",
        "updated_at": "2023-02-26T01:19:36+00:00",
        "closed_at": "2023-02-26T01:19:36+00:00",
        "comments_count": [
            "jiangjiajun",
            "chenzhizui",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenzhizui",
            "Zheng-Bicheng",
            "chenzhizui",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1142,
        "title": "ocr部署中文乱码问题",
        "body": "win11 + vs2019 + c++\r\n部署ocr时，识别到的中文为乱码，模型由官网下载\r\n",
        "state": "closed",
        "user": "GreenAvocado92",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-13T14:47:08+00:00",
        "updated_at": "2024-02-06T04:23:57+00:00",
        "closed_at": "2024-02-06T04:23:57+00:00",
        "comments_count": [
            "jiangjiajun",
            "GreenAvocado92",
            "yunyaoXYY",
            "GreenAvocado92",
            "GreenAvocado92",
            "jiangjiajun"
        ],
        "labels": [
            "paddleocr"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1143,
        "title": "C++使用fastdeploy多线程Paddle Inference后端推理OCR模型出现崩溃",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-x64-gpu-1.0.2\r\n- 【编译命令】直接使用预编译库\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： Nvidia GPU T4， CUDA 11.2 CUDNN 8.1\r\n- 【编译语言】： C++\r\n\r\n\r\n还有一个显存占用的问题：\r\n程序开1个线程显存占1208M\r\n程序开8个线程却只占2040M，一些显存各线程间复用？\r\n\r\n\r\n\r\n\r\n## 崩溃时堆栈信息\r\n--------------------- thread id 7 --------------------------------------\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff3e88f700 (LWP 105347)]\r\n0x00007fff40aee858 in ?? () from /usr/lib64/libcuda.so\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-324.el7_9.x86_64 libgcc-4.8.5-44.el7.x86_64 libgomp-4.8.5-44.el7.x86_64\r\n(gdb) bt\r\n#0  0x00007fff40aee858 in ?? () from /usr/lib64/libcuda.so\r\n#1  0x00007fff4089f20a in ?? () from /usr/lib64/libcuda.so\r\n#2  0x00007fff40af3e60 in ?? () from /usr/lib64/libcuda.so\r\n#3  0x00007fff4082d706 in ?? () from /usr/lib64/libcuda.so\r\n#4  0x00007fff4082f6f2 in ?? () from /usr/lib64/libcuda.so\r\n#5  0x00007fff408c92f5 in ?? () from /usr/lib64/libcuda.so\r\n#6  0x00007ffd102d13fb in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#7  0x00007ffd1031df2e in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#8  0x00007ffd0f6c475c in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#9  0x00007ffd0f6c477e in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#10 0x00007ffd0f46e4f9 in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#11 0x00007ffd0f46f77f in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#12 0x00007ffd0f3e6f3a in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#13 0x00007ffd0f420fb1 in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#14 0x00007ffd0f2a9aab in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#15 0x00007ffd0f289235 in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#16 0x00007ffd0f28acec in ?? () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#17 0x00007ffd0ed843bc in cudnn::cublasGemmEx(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, void const*, void const*, cudaDataType_t, int, void const*, cudaDataType_t, int, void const*, void*, cudaDataType_t, int, cudaDataType_t, cublasGemmAlgo_t) () from /usr/local/cuda/lib64/libcudnn_ops_infer.so.8\r\n#18 0x00007ffc2e5e3889 in ?? () from /usr/local/cuda/lib64/libcudnn_adv_infer.so.8\r\n#19 0x00007ffc2e57327b in ?? () from /usr/local/cuda/lib64/libcudnn_adv_infer.so.8\r\n#20 0x00007ffc2e5bdf87 in RNN_forwardGeneric(cudnnContext*, cudnnRNNStruct*, int, cudnnTensorStruct* const*, void const*, cudnnTensorStruct*, void const*, cudnnTensorStruct*, void const*, cudnnFilterStruct*, void const*, cudnnTensorStruct* const*, void*, cudnnTensorStruct*, void*, cudnnTensorStruct*, void*, void*, unsigned long, void*, unsigned long, cudnnForwardMode_t) () from /usr/local/cuda/lib64/libcudnn_adv_infer.so.8\r\n#21 0x00007ffc2e5be447 in cudnnRNNForwardInference () from /usr/local/cuda/lib64/libcudnn_adv_infer.so.8\r\n#22 0x00007fff835cbfaf in void phi::RNNInferece<float>(bool, cudnnContext* const&, int, phi::RNNDescriptors*, float const*, float const*, float const*, float const*, float*, float*, float*, phi::DenseTensor*, unsigned long) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#23 0x00007fff835d08f4 in void phi::RnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#24 0x00007fff835d122a in void phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*), &(void phi::RnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*))>::KernelCallHelper<paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*, phi::TypeTag<int> >::Compute<1, 3, 0, 0, phi::GPUContext const, phi::DenseTensor const, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > >(phi::KernelContext*, phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >&) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#25 0x00007fff835d15be in phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*), &(void phi::RnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, int, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, phi::DenseTensor*, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::DenseTensor*))>::Compute(phi::KernelContext*) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#26 0x00007fff7ff760f1 in paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#27 0x00007fff7ff635cb in paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#28 0x00007fff7ffc2e03 in paddle::framework::NaiveExecutor::Run() () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#29 0x00007fff7f98136d in paddle::AnalysisPredictor::ZeroCopyRun() () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n#30 0x00007ffff757035c in fastdeploy::PaddleBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#31 0x00007ffff754724e in fastdeploy::Runtime::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#32 0x00007ffff70d8387 in fastdeploy::FastDeployModel::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n---Type <return> to continue, or q <return> to quit---\r\n#33 0x00007ffff766df3d in fastdeploy::vision::ocr::Recognizer::BatchPredict(std::vector<cv::Mat, std::allocator<cv::Mat> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >*, std::vector<float, std::allocator<float> >*, unsigned long, unsigned long, std::vector<int, std::allocator<int> > const&) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#34 0x00007ffff7667dff in fastdeploy::pipeline::PPOCRv2::BatchPredict(std::vector<cv::Mat, std::allocator<cv::Mat> > const&, std::vector<fastdeploy::vision::OCRResult, std::allocator<fastdeploy::vision::OCRResult> >*) ()\r\n   from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#35 0x00007ffff76673e5 in fastdeploy::pipeline::PPOCRv2::Predict(cv::Mat const&, fastdeploy::vision::OCRResult*) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#36 0x00007ffff76672f7 in fastdeploy::pipeline::PPOCRv2::Predict(cv::Mat*, fastdeploy::vision::OCRResult*) () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/lib/libfastdeploy.so.1.0.2\r\n#37 0x000000000041b864 in InitAndInfer (det_model_dir=..., cls_model_dir=..., rec_model_dir=..., rec_label_file=..., image_file=..., option=..., thread_id=1) at /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/infer_thread.cc:93\r\n#38 0x0000000000422082 in std::__invoke_impl<void, void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, fastdeploy::RuntimeOption, int>(std::__invoke_other, void (*&&)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, fastdeploy::RuntimeOption&&, int&&) (__f=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5ed4b>, \r\n    __args#0=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5ed87>, \r\n    __args#1=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5ed97>, \r\n    __args#2=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5eda7>, \r\n    __args#3=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5edb7>, \r\n    __args#4=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5edc7>, \r\n    __args#5=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5edd7>, \r\n    __args#6=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x5ede6>) at /usr/local/gcc-8.2/include/c++/8.2.0/bits/invoke.h:60\r\n#39 0x000000000041fb93 in std::__invoke<void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, fastdeploy::RuntimeOption, int>(void (*&&)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&, fastdeploy::RuntimeOption&&, int&&) (__fn=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62ace>, \r\n    __args#0=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b0a>, \r\n    __args#1=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b1a>, \r\n    __args#2=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b2a>, \r\n    __args#3=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b3a>, \r\n    __args#4=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b4a>, \r\n    __args#5=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b5a>, \r\n    __args#6=<unknown type in /home/share/disk1/bzp/cudnn8.1/FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/build/infer_demo_thread, CU 0x0, DIE 0x62b69>) at /usr/local/gcc-8.2/include/c++/8.2.0/bits/invoke.h:95\r\n#40 0x0000000000426f44 in std::thread::_Invoker<std::tuple<void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, fastdeploy::RuntimeOption, int> >::_M_invoke<0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul> (this=0x2df30c8)\r\n    at /usr/local/gcc-8.2/include/c++/8.2.0/thread:234\r\n#41 0x0000000000426e5a in std::thread::_Invoker<std::tuple<void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, fastdeploy::RuntimeOption, int> >::operator() (this=0x2df30c8) at /usr/local/gcc-8.2/include/c++/8.2.0/thread:243\r\n#42 0x0000000000426e3e in std::thread::_State_impl<std::thread::_Invoker<std::tuple<void (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fastdeploy::RuntimeOption const&, int), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, fastdeploy::RuntimeOption, int> > >::_M_run (this=0x2df30c0)\r\n    at /usr/local/gcc-8.2/include/c++/8.2.0/thread:186\r\n#43 0x00007fff5696d05f in execute_native_thread_routine () from /home/share/disk1/bzp/cudnn8.1/fastdeploy-linux-x64-gpu-1.0.2/third_libs/install/opencv/lib64/libopencv_core.so.3.4\r\n---Type <return> to continue, or q <return> to quit---\r\n#44 0x00007fff5a044ea5 in start_thread () from /usr/lib64/libpthread.so.0\r\n#45 0x00007fff555609fd in clone () from /usr/lib64/libc.so.6\r\n\r\n\r\n## 复现过程及代码\r\n修改FastDeploy-release-1.0.2/examples/vision/ocr/PP-OCRv2/cpp/infer.cc为多线程\r\n选择Paddle推理\r\n修改CMakeLists.txt编译通过后执行：\r\ngdb --args ./build/infer_demo_thread ./model/ch_PP-OCRv2_det_infer ./model/ch_ppocr_mobile_v2.0_cls_infer ./model/ch_PP-OCRv2_rec_infer ./model/ppocr_keys_v1.txt ./imgs/1.jpg 1\r\n\r\n\r\n\r\n~~~\r\n\r\n// Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\r\n//\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n//     http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\n#include \"fastdeploy/vision.h\"\r\n\r\n#include <unistd.h>\r\n#include <iostream>\r\n#include <thread>\r\n\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\nint num_threads = 8;\r\n\r\nvoid InitAndInfer(const std::string& det_model_dir, const std::string& cls_model_dir, const std::string& rec_model_dir, const std::string& rec_label_file, const std::string& image_file, const fastdeploy::RuntimeOption& option, int thread_id) {\r\n  auto det_model_file = det_model_dir + sep + \"inference.pdmodel\";\r\n  auto det_params_file = det_model_dir + sep + \"inference.pdiparams\";\r\n\r\n  auto cls_model_file = cls_model_dir + sep + \"inference.pdmodel\";\r\n  auto cls_params_file = cls_model_dir + sep + \"inference.pdiparams\";\r\n\r\n  auto rec_model_file = rec_model_dir + sep + \"inference.pdmodel\";\r\n  auto rec_params_file = rec_model_dir + sep + \"inference.pdiparams\";\r\n\r\n  auto det_option = option;\r\n  auto cls_option = option;\r\n  auto rec_option = option;\r\n\r\n  // The cls and rec model can inference a batch of images now.\r\n  // User could initialize the inference batch size and set them after create PPOCR model.\r\n  int cls_batch_size = 1;\r\n  int rec_batch_size = 6;\r\n\r\n  // If use TRT backend, the dynamic shape will be set as follow.\r\n  // We recommend that users set the length and height of the detection model to a multiple of 32.\r\n  det_option.SetTrtInputShape(\"x\", {1, 3, 64,64}, {1, 3, 640, 640},\r\n                                {1, 3, 960, 960});\r\n  \r\n  cls_option.SetTrtInputShape(\"x\", {1, 3, 48, 10}, {cls_batch_size, 3, 48, 320}, {cls_batch_size, 3, 48, 1024});\r\n  \r\n  rec_option.SetTrtInputShape(\"x\", {1, 3, 32, 10}, {rec_batch_size, 3, 32, 320},\r\n                                {rec_batch_size, 3, 32, 2304});\r\n\r\n  // Users could save TRT cache file to disk as follow. \r\n  // det_option.SetTrtCacheFile(det_model_dir + sep + \"det_trt_cache.trt\");\r\n  // cls_option.SetTrtCacheFile(cls_model_dir + sep + \"cls_trt_cache.trt\");\r\n  // rec_option.SetTrtCacheFile(rec_model_dir + sep + \"rec_trt_cache.trt\");\r\n\r\n  auto det_model = fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, det_option);\r\n  auto cls_model = fastdeploy::vision::ocr::Classifier(cls_model_file, cls_params_file, cls_option);\r\n  auto rec_model = fastdeploy::vision::ocr::Recognizer(rec_model_file, rec_params_file, rec_label_file, rec_option);\r\n\r\n  assert(det_model.Initialized());\r\n  assert(cls_model.Initialized());\r\n  assert(rec_model.Initialized());\r\n  \r\n  // The classification model is optional, so the PP-OCR can also be connected in series as follows\r\n  // auto ppocr_v2 = fastdeploy::pipeline::PPOCRv2(&det_model, &rec_model);\r\n  auto ppocr_v2 = fastdeploy::pipeline::PPOCRv2(&det_model, &cls_model, &rec_model);\r\n\r\n  // Set inference batch size for cls model and rec model, the value could be -1 and 1 to positive infinity.\r\n  // When inference batch size is set to -1, it means that the inference batch size \r\n  // of the cls and rec models will be the same as the number of boxes detected by the det model.  \r\n  ppocr_v2.SetClsBatchSize(cls_batch_size);\r\n  ppocr_v2.SetRecBatchSize(rec_batch_size);\r\n\r\n  if(!ppocr_v2.Initialized()){\r\n    std::cerr << \"Failed to initialize PP-OCR.\" << std::endl;\r\n    return;\r\n  }\r\n\r\n  while(1) {\r\n    std::cout << \"--------------------- thread id \" << thread_id << \" --------------------------------------\" << std::endl;\r\n    auto im = cv::imread(image_file);\r\n    auto im_bak = im.clone();\r\n    \r\n    fastdeploy::vision::OCRResult result;\r\n    if (!ppocr_v2.Predict(&im, &result)) {\r\n      std::cerr << \"Failed to predict.\" << std::endl;\r\n      return;\r\n    }\r\n\r\n    std::cout << result.Str() << std::endl;\r\n\r\n    //auto vis_im = fastdeploy::vision::VisOcr(im_bak, result);\r\n    //cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    //std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n  }\r\n}\r\n\r\nint main_system(const std::string& det_model_dir, const std::string& cls_model_dir, const std::string& rec_model_dir, const std::string& rec_label_file, const std::string& image_file, const fastdeploy::RuntimeOption& option) {\r\n    for(int i = 0; i < num_threads; i++) {\r\n\t\tstd::thread t(InitAndInfer, det_model_dir, cls_model_dir, rec_model_dir, rec_label_file, image_file, option, i);\r\n\t\tt.detach();\r\n\t\tsleep(2);\r\n\t}\r\n\twhile(1){\r\n\t\tsleep(100000);\r\n\t}\r\n    return 0;\r\n}\r\n\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc < 7) {\r\n    std::cout << \"Usage: infer_demo path/to/det_model path/to/cls_model \"\r\n                 \"path/to/rec_model path/to/rec_label_file path/to/image \"\r\n                 \"run_option, \"\r\n                 \"e.g ./infer_demo ./ch_PP-OCRv2_det_infer \"\r\n                 \"./ch_ppocr_mobile_v2.0_cls_infer ./ch_PP-OCRv2_rec_infer \"\r\n                 \"./ppocr_keys_v1.txt ./12.jpg 0\"\r\n              << std::endl;\r\n    std::cout << \"The data type of run_option is int, 0: run with cpu; 1: run \"\r\n                 \"with gpu; 2: run with gpu and use tensorrt backend; 3: run with gpu and use Paddle-TRT; 4: run with kunlunxin.\"\r\n              << std::endl;\r\n    return -1;\r\n  }\r\n\r\n  fastdeploy::RuntimeOption option;\r\n  int flag = std::atoi(argv[6]);\r\n\r\n  if (flag == 0) {\r\n    option.UseCpu(); \r\n  } else if (flag == 1) {\r\n    option.UseGpu();\r\n  } else if (flag == 2) {\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n  } else if (flag == 3) {\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n    option.EnablePaddleTrtCollectShape();\r\n    option.EnablePaddleToTrt();\r\n  } else if (flag == 4) {\r\n    option.UseKunlunXin();\r\n  }\r\n\r\n  std::string det_model_dir = argv[1];\r\n  std::string cls_model_dir = argv[2];\r\n  std::string rec_model_dir = argv[3];\r\n  std::string rec_label_file = argv[4];\r\n  std::string test_image = argv[5];\r\n  main_system(det_model_dir, cls_model_dir, rec_model_dir, rec_label_file, test_image, option);\r\n  return 0;\r\n}\r\n\r\n",
        "state": "closed",
        "user": "marsbzp",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-14T03:35:33+00:00",
        "updated_at": "2024-03-19T06:40:01+00:00",
        "closed_at": "2024-03-19T06:40:01+00:00",
        "comments_count": [
            "jiangjiajun",
            "shenmayufei",
            "shenmayufei"
        ],
        "labels": [
            "paddleocr"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1148,
        "title": "同一张图片paddleseg python下推理和导出模型后，在fastdeploy下差别很大",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如FastDeploy-release-1.0.0 Windows\r\n\r\n同一张图片paddleseg python下推理和导出模型后，在fastdeploy下差别很大，paddleseg下能区分左右道路和中间隔离栏，fastdeploy下前端黏连\r\n原图：\r\n![roi](https://user-images.githubusercontent.com/11229566/212478245-bc9f5c00-0012-448d-830c-b1dba9cd7d97.jpg)\r\npaddleseg 项目自带的推理结果\r\n![paddleseg](https://user-images.githubusercontent.com/11229566/212478257-8c19b006-98d6-442d-80c7-1b46a65c8048.jpg)\r\nFastDeploy 推理结果\r\n![fastdeploy](https://user-images.githubusercontent.com/11229566/212478264-2718254a-1b83-4cce-a23c-1453c477a66b.jpg)\r\nFastDeploy 推理代码\r\nvoid PaddleSegTrtInfer(const std::string& model_dir, const std::string& image_file) {\r\n\tauto model_file = model_dir + sep + \"model.pdmodel\";\r\n\tauto params_file = model_dir + sep + \"model.pdiparams\";\r\n\tauto config_file = model_dir + sep + \"deploy.yaml\";\r\n\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\toption.UseTrtBackend();\r\n\toption.EnableTrtFP16();\r\n\toption.SetTrtCacheFile(\"bin/paddleseg_road2/trtcache_paddleseg_road_fp16\");\r\n\t/*option.UseCpu();\r\n\toption.UsePaddleBackend();*/\r\n\tauto model = fastdeploy::vision::segmentation::PaddleSegModel(\r\n\t\tmodel_file, params_file, config_file, option);\r\n\r\n\tif (!model.Initialized()) {\r\n\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n\r\n\tauto im = cv::imread(image_file);\r\n\tfastdeploy::vision::SegmentationResult res;\r\n\tif (!model.Predict(&im, &res)) {\r\n\t\tstd::cerr << \"Failed to predict.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n\r\n\tstd::cout << res.Str() << std::endl;\r\n\tauto vis_im = fastdeploy::vision::VisSegmentation(im, res, 0.5);\r\n\tcv::imwrite(\"vis_result.jpg\", vis_im);\r\n\tstd::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\n用的configs/quick_start/pp_liteseg_optic_disc_512x512_1k.yml训练\r\n\r\n训练模型和导出后模型地址;\r\n链接：https://pan.baidu.com/s/186wnFcRmGfiT5v9sF1aPFw?pwd=fear \r\n提取码：fear \r\n",
        "state": "closed",
        "user": "yueyue0574",
        "closed_by": "yueyue0574",
        "created_at": "2023-01-14T15:05:39+00:00",
        "updated_at": "2023-01-16T12:01:16+00:00",
        "closed_at": "2023-01-16T10:36:50+00:00",
        "comments_count": [
            "yueyue0574",
            "yueyue0574",
            "felixhjh",
            "yueyue0574"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1150,
        "title": "paddleseg RTFomer运行报错",
        "body": "RTFomer 运行tensorrt报错，gpu可以正常运行\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.2\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】：  Python3.7\r\n\r\n\r\n~/data/deeplearning/program/deploy/FastDeploy/examples/vision/segmentation/paddleseg/python$ python infer.py --model RTFomer/ --image cityscapes_demo.png --device gpu --use_trt True\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   p2o.Conv.26: two inputs (data and weights) are allowed only in explicit-quantization mode.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(637)::CreateTrtEngineFromOnnx       Failed to parse ONNX model by TensorRT.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(263)::InitFromOnnx  Failed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime.cc(864)::CreateTrtBackend    Load model from Paddle failed while initliazing TrtBackend.\r\nAborted (core dumped)\r\n\r\n",
        "state": "closed",
        "user": "Tim4AI",
        "closed_by": "Tim4AI",
        "created_at": "2023-01-15T10:44:21+00:00",
        "updated_at": "2023-04-18T06:15:12+00:00",
        "closed_at": "2023-02-03T12:44:57+00:00",
        "comments_count": [
            "felixhjh",
            "Tim4AI",
            "felixhjh",
            "Tim4AI",
            "superprogrammai"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1149,
        "title": "RKNUP2部署RKYOLO模型时出现的AttributeError: module 'fastdeploy.vision.detection' has no attribute 'RKYOLOV5'问题!",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： git clone https://github.com/PaddlePaddle/FastDeploy.git\r\n- 【系统平台】: Linux firefly 4.19.219 aarch64 GNU/Linux (Ubuntu20.04.3)\r\n- 【硬件】： AIO-3568J开发板\r\n- 【编译语言】： Python 3.8.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【运行日志】\r\n- 运行/FastDeploy/examples/vision/detection/rkyolo/python目录下面的infer脚本文件提示下面错误,运行代码为python3 infer.py --model_file ./model --image 000000014439.jpg,模型文件为从rk官方仓库中提供的rknn格式模型(https://eyun.baidu.com/enterprise/share/init?cid=8272257679089781337&uk=2751701137&sid=202211118572878233).\r\n- Log:\r\n![image](https://user-images.githubusercontent.com/85120075/212519875-63edc2c4-0218-4e2c-a47c-d6186f7cbc5c.png)\r\n- model_file:\r\n![image](https://user-images.githubusercontent.com/85120075/212519895-3e0f6642-4c4f-4c13-b544-4da3a979c733.png)\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-01-15T02:31:31+00:00",
        "updated_at": "2023-01-31T09:11:53+00:00",
        "closed_at": "2023-01-31T09:11:53+00:00",
        "comments_count": [
            "MrMzl",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1151,
        "title": "PaddleX模型Predict崩溃",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.2\r\n- 【编译命令】: 未编译\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 有3080，但是使用option.UseCpu(); option.UseOpenVINOBackend();\r\n- 【编译语言】： 未编译\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n-   使用`examples`下的部署示例的代码，调用paddleX导出基于PPLCNet训练后的分类模型。模型能正常加载，但是到了Predict直接崩溃。代码如下：\r\nvoid CpuInfer(const std::string& model_dir, const std::string& image_file) {\r\n    auto model_file = model_dir + \"model.pdmodel\";\r\n    auto params_file = model_dir + \"model.pdiparams\";\r\n    auto config_file = model_dir + \"model.yml\";\r\n\r\n    cv::Mat im = cv::imread(image_file);\r\n    \r\n    std::vector<int64_t> vec = {1, im.channels() ,im.size().height ,im.size().width };\r\n    const std::map<std::string, std::vector<int64_t>> imMap = { {\"image\",vec} };\r\n\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseCpu();\r\n    option.UseOpenVINOBackend();\r\n    option.SetOpenVINODevice(\"CPU\");\r\n    option.SetOpenVINOShapeInfo(imMap);\r\n    auto model = fastdeploy::vision::classification::PaddleClasModel(\r\n        model_file, params_file, config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    fastdeploy::vision::ClassifyResult res;\r\n    if (!model.Predict(im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    // print res\r\n    std::cout << res.Str() << std::endl;\r\n}\r\n\r\n[即使注释了option的配置代码，Predict还是会崩溃。图像是3通道，尺寸512，512.\r\n",
        "state": "closed",
        "user": "nOObVs",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-15T12:21:35+00:00",
        "updated_at": "2024-02-06T04:23:56+00:00",
        "closed_at": "2024-02-06T04:23:56+00:00",
        "comments_count": [
            "nOObVs",
            "nOObVs",
            "jiangjiajun",
            "nOObVs",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1154,
        "title": "服务化部署：PP-Matting Python Deployment 如何获取抠图的透明背景图",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，cpu： fastdeploy-python  1.0.2\r\n- 【编译命令】pip\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： \r\n- 【编译语言】： Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n    模型：\r\n    [pp-matting-hrnet_w18-human_512](https://bj.bcebos.com/paddlehub/fastdeploy/PP-Matting-512.tgz)\r\n   代码：\r\n    [client.py 和 serving.py 参考](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/python/serving)\r\n\r\n![image](https://user-images.githubusercontent.com/13762699/212601523-37929cfa-ec47-43c5-9683-5a5bc44923d3.png)\r\n\r\n",
        "state": "closed",
        "user": "liuziyingbeidou",
        "closed_by": "liuziyingbeidou",
        "created_at": "2023-01-16T04:55:09+00:00",
        "updated_at": "2023-01-16T10:08:34+00:00",
        "closed_at": "2023-01-16T10:08:34+00:00",
        "comments_count": [
            "felixhjh",
            "liuziyingbeidou",
            "felixhjh",
            "felixhjh",
            "liuziyingbeidou"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1155,
        "title": "fastdeploy::vision::VisSegmentation如何使用自定义颜色",
        "body": "fastdeploy::vision::SegmentationResult获得的结果用fastdeploy::vision::VisSegmentation显示，如果我需要自定义颜色该如何处理？或者过滤掉SegmentationResult中的某些类，剩下的类进行着色",
        "state": "closed",
        "user": "yueyue0574",
        "closed_by": "yueyue0574",
        "created_at": "2023-01-16T08:49:27+00:00",
        "updated_at": "2023-01-16T12:01:36+00:00",
        "closed_at": "2023-01-16T10:36:58+00:00",
        "comments_count": [
            "felixhjh",
            "yueyue0574"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1157,
        "title": "FastDeploy Streamer支持并行多模型吗？",
        "body": "Supporting parallel multiple models inferencing in one pipeline",
        "state": "closed",
        "user": "eyu11",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-17T01:52:33+00:00",
        "updated_at": "2024-02-06T04:23:55+00:00",
        "closed_at": "2024-02-06T04:23:55+00:00",
        "comments_count": [
            "wang-xinyu",
            "eyu11",
            "wang-xinyu",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1159,
        "title": "基于现有的接口，如何得到透明背景的方案呢，请指点迷津",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python             1.0.2\r\n- 【编译命令】pip\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： \r\n- 【编译语言】： Python(3.9）\r\n\r\n或者多个模型协作完成\r\n谢谢！\r\n",
        "state": "closed",
        "user": "liuziyingbeidou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-17T05:59:34+00:00",
        "updated_at": "2024-02-20T06:43:06+00:00",
        "closed_at": "2024-02-20T06:43:06+00:00",
        "comments_count": [
            "felixhjh"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1163,
        "title": "按照新版fastdeploy编译yolov5 demo，在rk1126板端执行报缺少/lib/libstdc++.so.6这个包",
        "body": "./infer_demo ./models/yolov5s_ptq_model ./images/000000014439.jpg \r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by ./infer_demo)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by ./infer_demo)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by ./infer_demo)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by ./infer_demo)\r\n./infer_demo: /lib/libm.so.6: version `GLIBC_2.29' not found (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libfastdeploy.so.0.0.0)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgcodecs.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_core.so.406)\r\n./infer_demo: /userdata/documents/install/lib/libgomp.so.1: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /userdata/documents/install/lib/libgomp.so.1: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libpaddle_full_api_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_video.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libflycv_shared.so)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_imgproc.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_dnn.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_calib3d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_features2d.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n./infer_demo: /lib/libstdc++.so.6: no version information available (required by /userdata/documents/install/lib/libopencv_flann.so.406)\r\n\r\n",
        "state": "closed",
        "user": "yutao007",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-17T08:49:40+00:00",
        "updated_at": "2024-05-14T06:42:07+00:00",
        "closed_at": "2024-05-14T06:42:07+00:00",
        "comments_count": [
            "yeliang2258",
            "yutao007",
            "yutao007",
            "yeliang2258",
            "yeliang2258",
            "TsingWei",
            "TsingWei",
            "yiyang19",
            "yiyang19"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1166,
        "title": "Can you update new release  ",
        "body": "There has been lot of update to codebase, it will be great if you release new build\r\nthanks\r\n",
        "state": "closed",
        "user": "soham24",
        "closed_by": "soham24",
        "created_at": "2023-01-17T17:53:16+00:00",
        "updated_at": "2023-01-18T04:49:35+00:00",
        "closed_at": "2023-01-18T04:49:34+00:00",
        "comments_count": [
            "jiangjiajun",
            "soham24"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1164,
        "title": "fastdeploy yolov5",
        "body": "linux下  yolov5 v7.0原版onnx 没问题\r\nfastdeploy yolov5 识别同一位置会出现2次结果\r\n713.923828,531.345703, 792.430664, 638.601562, 0.967754, 2\r\n712.833984,530.601562, 874.623047, 639.972656, 0.300570, 2\r\n类似这样",
        "state": "closed",
        "user": "taojishou",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-17T09:25:14+00:00",
        "updated_at": "2024-02-06T04:23:54+00:00",
        "closed_at": "2024-02-06T04:23:54+00:00",
        "comments_count": [
            "wjj19950828",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1174,
        "title": "请问现在支持从内存中加载模型吗",
        "body": null,
        "state": "closed",
        "user": "hujiongqazqq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-19T15:07:46+00:00",
        "updated_at": "2024-02-20T06:43:07+00:00",
        "closed_at": "2024-02-20T06:43:07+00:00",
        "comments_count": [
            "leiqing1",
            "monkeycc",
            "heliqi",
            "felixhjh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1177,
        "title": "对于PaddleClas部署中遇到的问题",
        "body": "使用PaddleClas中ResNet50网络进行训练，并按照要求转换网络，得到inference.pdiparams，inference.pdmodel，并且添加了.yaml文件。在部署过程中，使用实例程序inter.py，输入图片为训练中使用的图片，但显示直接采用cv2.imread读取的图片为unit8，而程序要求float，错误的提示为：\r\nFailed to Infer: Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))\r\n\r\n经过im.astype(np.float32) / 255后成功将dtype转换为float32，运行后显示错误：\r\nFailed to Infer: Got invalid dimensions for input: x for the following indices\r\nindex: 1 Got: 227 Expected: 3\r\nindex: 3 Got: 3 Expected: 227\r\nPlease fix either the inputs or the model.\r\n\r\n经过np.transpose(image,(2,0,1))，后改变维度顺序，再次运行，报错为：\r\n> Invalid number of channels in input image:\r\n>     'VScn::contains(scn)'\r\n> where\r\n>     'scn' is 227\r\n\r\n维度顺序仍然错误，即前面要求通道数位置为1，后面要求通道数位置为3，请问应该如何解决这个矛盾？是否是我输入图片格式的问题（.jpg）或者我设置的问题？程序设置如下：\r\ndef parse_arguments():\r\n    import argparse\r\n    import ast\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \"--model\", default=\"models/ResNet50\", help=\"Path of PaddleClas model.\")\r\n    parser.add_argument(\r\n        \"--image\", type=str, default=\"data2/00010.jpg\",help=\"Path of test image file.\")\r\n    parser.add_argument(\r\n        \"--topk\", type=int, default=1, help=\"Return topk results.\")\r\n    parser.add_argument(\r\n        \"--device\",\r\n        type=str,\r\n        default='gpu',\r\n        help=\"Type of inference device, support 'cpu' or 'gpu' or 'ipu' or 'kunlunxin' or 'ascend' .\"\r\n    )\r\n    parser.add_argument(\r\n        \"--use_trt\",\r\n        type=ast.literal_eval,\r\n        default=False,\r\n        help=\"Wether to use tensorrt.\")\r\n    return parser.parse_args()\r\n",
        "state": "open",
        "user": "guoyunqingyue",
        "closed_by": null,
        "created_at": "2023-01-28T06:19:12+00:00",
        "updated_at": "2023-11-21T00:49:57+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "guoyunqingyue",
            "guoyunqingyue",
            "xpzwzwz",
            "guoyunqingyue"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1179,
        "title": "希望fastdeploy提供java调用的demo",
        "body": "因为公司的项目基本都是java，想把fastdeploy结合到java项目中，注！不是android。谢谢",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-29T02:30:04+00:00",
        "updated_at": "2024-02-06T04:23:53+00:00",
        "closed_at": "2024-02-06T04:23:53+00:00",
        "comments_count": [
            "DefTruth",
            "truthsun22",
            "DefTruth",
            "truthsun22",
            "chenqianhe",
            "jiangjiajun"
        ],
        "labels": [
            "Java",
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1180,
        "title": "model.predict希望加入其它参数",
        "body": "比如置信度，类别名等参数，目前置信度信息等都是通过配置文件读取进去的，不太灵活，结合到自己项目中如果想更改置信度等参数，只能重新生成参数文件",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-29T03:01:02+00:00",
        "updated_at": "2024-02-06T04:23:52+00:00",
        "closed_at": "2024-02-06T04:23:52+00:00",
        "comments_count": [
            "DefTruth",
            "truthsun22",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1181,
        "title": "服务化部署客户端安装tritionclient和opencv失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n服务化部署案例\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README.md\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python(3.7）\r\n\r\n部署两个客户端，一个是ubuntu18, 一个是ubuntu18内的python容器\r\nubuntu18 安装tritionclient失败，一遍遍的增加依赖，最后在abc停住了\r\n![image](https://user-images.githubusercontent.com/112528302/215305004-bbd8b250-bf09-425f-86e3-5ba74382bd6f.png)\r\n\r\npython容器安装opencv失败\r\n![image](https://user-images.githubusercontent.com/112528302/215304999-791cfe2e-ae49-433d-b3f6-a8b8194c2d3f.png)\r\n\r\n请问如何解决啊？",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "heliqi",
        "created_at": "2023-01-29T04:31:49+00:00",
        "updated_at": "2023-01-30T08:43:43+00:00",
        "closed_at": "2023-01-30T07:46:05+00:00",
        "comments_count": [
            "heliqi",
            "cunjing56"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1182,
        "title": "服务化部署客户端如何发送多个图片请求？",
        "body": "参考部署文档\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README_CN.md\r\n请问如何发送多个图片请求？有没有可能实现切图、拼图处理？\r\n![image](https://user-images.githubusercontent.com/112528302/215308091-349c2350-37e2-490c-a470-32553e89700c.png)\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "heliqi",
        "created_at": "2023-01-29T06:01:36+00:00",
        "updated_at": "2023-06-16T05:35:58+00:00",
        "closed_at": "2023-02-24T10:09:06+00:00",
        "comments_count": [
            "heliqi",
            "cunjing56",
            "heliqi",
            "sniperking1234"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1184,
        "title": "🎁 FastDeploy快乐开源活动表单",
        "body": "#FastDeploy快乐开源活动# 是飞桨快乐开源的子项目。旨在鼓励更多的开发者参与到开源社区建设中，帮助社区修复 bug 或贡献 feature，加入开源🇨🇳、充实自己🧗‍♀️、广交好友🤝、共建生态💞。\r\n\r\n为了帮助大家循序渐进地了解、建设FastDeploy开源项目，我们提供了以下几种类型的贡献任务，并为完成任务的贡献者**准备了不同的展示平台**，**认识更多志同道合的朋友**！快来一起提 PR啦~\r\n\r\n |任务类型 |任务描述 | 任务成功标准 | 技术要求 | 难易程度 | mentor |\r\n |:----: |--|:--:|:--:|:--:|:--:|\r\n |模型接入任务|【3D模型】完成PointPillars模型接入FastDeploy，并在GPU硬件上验证 Paddle Inference TensorRT 精度对齐，完成Paddle Inference、TensorRT、ONNX Runtime性能对比数据<br>  [1]   模型链接：https://github.com/PaddlePaddle/Paddle3D<br> [2]  模型接入参考文档：https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.2/docs/cn/faq/develop_a_new_model.md<br>  [3]  如开发者没有机器资源，可以先在AI Studio验证通过后提交pr，如果对NVIDIA Jetson感兴趣，提交pr后issue联系，提供开发板充分测。| 见备注|3D、Python、C++|中|@DefTruth|\r\n |模型接入任务|【3D模型】完成SMOKE模型接入FastDeploy，并在GPU硬件上验证 Paddle Inference TensorRT 精度对齐，完成Paddle Inference、TensorRT、ONNX Runtime性能对比数据<br>  [1]   模型链接：https://github.com/PaddlePaddle/Paddle3D<br> [2]  模型接入参考文档：https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.2/docs/cn/faq/develop_a_new_model.md<br>  [3]  如开发者没有机器资源，可以先在AI Studio验证通过后提交pr，如果对NVIDIA Jetson感兴趣，提交pr后issue联系，提供开发板充分测试。|见备注|3D、Python、C++|中|@DefTruth|\r\n |推理后端开发|完成NCNN推理引擎集成到FastDeploy中，并完成picodet的测试工作|见备注|C++、JNI、Java|中|@DefTruth|\r\n\r\n\r\n\r\n\r\n### 备注：\r\n\r\n1. **模型任务成功标准**\r\n\r\n  - a. 精度评估 验收标准，至少可以考虑两个层面，即案例层面的精度和批量评估层面的精度：\r\n   \r\n    i. 案例层面（必须）：对于每个集成的模型，要求能给出>=5个的实际案例来验证结果正确性，包含：\r\n      1. 可视化结果基本一致，要求必须有至少1个可视化结果验证。\r\n      2. 张量输出的diff越小越优。C++集成后输出张量和原repo中原生python推理输出张量的数值diff，求diff的均值、最大值和方差，各个指标越小越好。\r\n     \r\n    ii. 批量评估（可选）：如果开发者能够参考原repo的python脚本在FD中集成完模型后，进行规范精度评估，则可得更高分。\r\n\r\n  - b. 性能评估 验收标准：\r\n  \r\n    i. 支持的引擎数量：一般情况下，越多越好。至少能支持Paddle Inference、ORT推理。\r\n    ii. 支持的系统数量：一般情况下，越多越好。比如测试过Linux(x86_64/aarch64)、Mac OSX(x86_64/arm64)、Windows(x86_64) 上的支持情况。\r\n    iii. 支持的硬件数量：由于限于开发者的拥有硬件资源限制，硬件这项不好强制要求。\r\n\r\n  - c. 额外加分项：\r\n \r\n    i. 自定义算子是否能实现，实现的质量如何等等（开发者能给出OP级别的性能，精度评估）\r\n\r\n\r\n2. **后端接入成功标准**\r\n\r\n- a. 对于Runtime各项功能的均能程度（必须）：这是必须完成的。\r\n\r\n- b. 对于FastDpeloy中模型在新推理后端上的测试覆盖率：能覆盖的模型越多，得分越高，并且能支持Paddle的模型，得分更高。比如支持了MNN。NCNN，则要将Paddle模型转ONNX再转NCNN/NCNN进行支持，能支持的数量越多越好。\r\n\r\n- c. 额外加分项：如果能实现ONNX/Paddle->MNN/NCNN的自动转换（比如集成MNN的Converter模块，则可paddle2onnx -> MNNConverter -> MNN，实现Paddle -> MNN直接转换）",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-29T09:08:35+00:00",
        "updated_at": "2024-02-06T04:17:29+00:00",
        "closed_at": "2024-02-06T04:17:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1183,
        "title": "没有为 paddle_inference.dll 加载的符号文件",
        "body": "*********************************************\r\n测试了一下vs2017是不支持吗\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：    2023年1月29   fastdeploy- develop\r\n- 【编译命令】自行编译的FastDeploy， Windows使用CMakeGUI + Visual Studio 2017 IDE编译\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n-【cpu编译FastDeploy参考文档】https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/build_on_win_with_gui.md\r\n-【在 Windows 使用 FastDeploy C++ SDK参考文档】https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows.md\r\n报错：\r\n![image](https://user-images.githubusercontent.com/60127735/215309492-f969652e-0616-42f7-b4eb-c9088328b196.png)\r\n\r\n配置选项：\r\n![1lfmynFGHu](https://user-images.githubusercontent.com/60127735/215309533-6df5ab92-43eb-4266-a814-b7ce0d27c3b9.jpg)\r\n\r\n.exe直接运行报错提示：\r\n![image](https://user-images.githubusercontent.com/60127735/215309706-444bb024-ce7b-4a30-a5c3-5d04a696a89c.png)\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "teymur-git",
        "closed_by": "teymur-git",
        "created_at": "2023-01-29T06:40:54+00:00",
        "updated_at": "2023-01-29T07:51:53+00:00",
        "closed_at": "2023-01-29T06:53:45+00:00",
        "comments_count": [
            "teymur-git",
            "DefTruth",
            "DefTruth",
            "teymur-git"
        ],
        "labels": [
            "Bug",
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1185,
        "title": "带参数接口的客户端程序",
        "body": "参考\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README.md\r\n\r\n客户端程序paddledet_grpc_client.py，现想让图片路径与请求服务地址变成显示参数，参考slice_image的参数写法\r\n![image](https://user-images.githubusercontent.com/112528302/215318456-be2042c9-7e39-4a70-ad43-12e600230db7.png)\r\n\r\n客户端执行的时候报错\r\n![image](https://user-images.githubusercontent.com/112528302/215318475-9adf9d9f-8c1d-4f18-ab2f-e67155498626.png)\r\n\r\n请问是什么问题呢？\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "cunjing56",
        "created_at": "2023-01-29T09:51:27+00:00",
        "updated_at": "2023-01-30T08:42:59+00:00",
        "closed_at": "2023-01-30T07:14:50+00:00",
        "comments_count": [
            "heliqi",
            "cunjing56"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1189,
        "title": "from .libs.fastdeploy_main import *DLL load failed while importing fastdeploy_main: 拒绝访问RuntimeError: FastDeploy initalized failed!",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： pip install fastdeploy-python==0.0.0 -f https://www.paddlepaddle.org.cn/whl/fastdeploy_nightly_build.html\r\n- 【编译命令】#\r\n- 【系统平台】:Windows 10 专业版   64 位操作系统, 基于 x64 的处理器\r\n  - 【硬件】： Intel(R) Atom(TM) x5-Z8300  CPU @ 1.44GHz   1.44 GHz   win10平板电脑\r\n- 【编译语言】：Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python38\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed while importing fastdeploy_main: 拒绝访问。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"DEMOcpu.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"C:\\Python38\\lib\\site-packages\\fastdeploy\\__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"C:\\Python38\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n```\r\n\r\n```\r\n\r\n\r\nimport fastdeploy as fd\r\nimport numpy as np\r\n\r\nmodel_url = \"https://bj.bcebos.com/fastdeploy/models/mobilenetv2.tgz\"\r\nfd.download_and_decompress(model_url, path=\".\")\r\n\r\n\r\noption = fd.RuntimeOption()\r\n\r\noption.set_model_path(\"mobilenetv2/inference.pdmodel\",\r\n                      \"mobilenetv2/inference.pdiparams\")\r\n\r\n# **** CPU 配置 ****\r\noption.use_cpu()\r\noption.use_ort_backend()\r\noption.set_cpu_thread_num(12)\r\n\r\n# 初始化构造runtime\r\nruntime = fd.Runtime(option)\r\n\r\n# 获取模型输入名\r\ninput_name = runtime.get_input_info(0).name\r\n\r\n# 构造随机数据进行推理\r\nresults = runtime.infer({\r\n    input_name: np.random.rand(1, 3, 224, 224).astype(\"float32\")\r\n})\r\n\r\nprint(results[0].shape)\r\n```\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-01-30T07:07:49+00:00",
        "updated_at": "2023-02-07T02:25:35+00:00",
        "closed_at": "2023-02-07T02:25:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1190,
        "title": "C++编译包时加上paddle后端会报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy_mac_cpu_0.0.0\r\n- 【系统平台】: Mac OSX arm(12.0) \r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 在开发模型时进行C++包编译，加上paddle后端后报错\r\n- \r\n![截屏2023-01-30 下午3 08 49](https://user-images.githubusercontent.com/97090522/215414388-16f9e62f-9dc7-4156-bf66-7a0b5e0d8d7d.png)\r\n![截屏2023-01-30 下午3 10 54](https://user-images.githubusercontent.com/97090522/215414431-6edd7a86-9018-4dba-8153-bd1beea39674.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "totorolin",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-30T07:31:31+00:00",
        "updated_at": "2024-02-06T04:23:51+00:00",
        "closed_at": "2024-02-06T04:23:51+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Bug",
            "Mac OSX x86_64",
            "Mac OSX Arm64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1193,
        "title": "silero-vad 在 Linux x64 推理时结果异常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： https://github.com/PaddlePaddle/FastDeploy/tree/09d8e0f19ab52f368533c1a2591d5ff9d50f493e\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=ON -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\ncpp 测试代码见 https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/audio/silero-vad\r\n\r\n实际使用代码见 https://github.com/chenqianhe/VAD-addon\r\n\r\n![](https://user-images.githubusercontent.com/54462604/213858784-6e8f0806-cd7f-4000-b10e-3a6ede642b4e.png)\r\n\r\n![581675081775_ pic](https://user-images.githubusercontent.com/54462604/215477271-206f31a1-1fe6-4a28-847d-2678cd4083ea.jpg)\r\n\r\n",
        "state": "closed",
        "user": "chenqianhe",
        "closed_by": "chenqianhe",
        "created_at": "2023-01-30T12:31:00+00:00",
        "updated_at": "2023-01-31T08:12:08+00:00",
        "closed_at": "2023-01-31T08:12:08+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1191,
        "title": "FastDeploy 服务化部署 为什么不用 Paddle Serving",
        "body": "FastDeploy 服务化部署 \r\n为什么不用 [Paddle Serving](https://github.com/PaddlePaddle/Serving)\r\n而要使用 [Triton Inference Server](https://github.com/triton-inference-server/server)",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-01-30T08:57:00+00:00",
        "updated_at": "2023-01-30T09:07:56+00:00",
        "closed_at": "2023-01-30T09:07:52+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1194,
        "title": "Android12系统源码导入FastDeploy时编译错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n你好，我希望再Android12系统中使用FastDeploy，在尝试导入Android版本的动态库时，编译过程中出现了如下报错：\r\nld.lld: error: undefined symbol: fastdeploy::vision::matting::PPMatting::PPMatting(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, fastdeploy::RuntimeOption const&, fastdeploy::ModelFormat const&)\r\n目前推测是ndk命名空间与Android系统源码中的命名空间不同导致的（std::__1和std::__ndk1），请问这种情况应该如何解决呢？",
        "state": "closed",
        "user": "Guilty39",
        "closed_by": "Guilty39",
        "created_at": "2023-01-30T13:10:12+00:00",
        "updated_at": "2023-02-19T04:10:42+00:00",
        "closed_at": "2023-02-19T04:10:42+00:00",
        "comments_count": [
            "DefTruth",
            "Guilty39",
            "DefTruth",
            "DefTruth",
            "Guilty39",
            "DefTruth"
        ],
        "labels": [
            "Android"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1196,
        "title": "是否提供对FreeYolo支持？",
        "body": "开源地址：https://github.com/yjh0410/FreeYOLO，是否能提供对这个框架支持",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "jiangjiajun",
        "created_at": "2023-01-31T01:00:01+00:00",
        "updated_at": "2024-02-06T04:23:50+00:00",
        "closed_at": "2024-02-06T04:23:50+00:00",
        "comments_count": [
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1198,
        "title": "fastdeploy 是否可以提供 rotate 旋转目标检测ppyoloe-r的推理脚本",
        "body": null,
        "state": "closed",
        "user": "forword-1234",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-31T08:57:55+00:00",
        "updated_at": "2024-11-22T12:41:41+00:00",
        "closed_at": "2024-04-12T08:18:52+00:00",
        "comments_count": [
            "forword-1234",
            "DefTruth",
            "monkeycc",
            "Splendon",
            "DreamMaker777",
            "njflove"
        ],
        "labels": [
            "New Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1200,
        "title": "在RK3568平台上实现RKYOLO模型部署过程中遇到的“Segmentation fault”问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-release-1.0.3\r\n- 【编译命令】按照仓库提供的环境编译方式完成C++和Python环境的编译，编译日志见后面附件。\r\n- 【系统平台】: Linux firefly 4.19.232  / Debian 10\r\n- 【硬件】： RK3568J\r\n- 【编译语言】： C++ & Python(3.7）\r\n\r\n## 运行FastDeploy-release-1.0.3/examples/vision/detection/rkyolo/cpp中的demo\r\n- 部署内容\r\n- - c++方式实现yolov5模型的部署(yolov5s_relu_tk2_RK356X_i8.rknn,该模型为转换后的模型)\r\n- - https://pan.baidu.com/s/1JDvgZnPWruSRNdhMO1oYkg 提取码: yy4w\r\n- 操作流程\r\n- - 参考 https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/build.md 完成板端编译C++ SDK和Python SDK；\r\n- -  进入FastDeploy-release-1.0.3/examples/vision/detection/rkyolo/cpp文件夹中进行编译，参考链接:https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/rkyolo/cpp/README_CN.md 编译后的文件夹中内容如下:\r\n![image](https://user-images.githubusercontent.com/85120075/215730783-711da9b1-4f73-45ff-822d-6a6f845828de.png)\r\n- - 编译日志:https://pan.baidu.com/s/1JDvgZnPWruSRNdhMO1oYkg 提取码: yy4w \r\n- - 进入FastDeploy-release-1.0.3/examples/vision/detection/rkyolo/cpp/build/install中执行如下命令: ./infer_rkyolo model/ images/000000014439.jpg\r\n- - 执行后不报错，提示“Segmentation fault”。\r\n\r\n## 运行FastDeploy-release-1.0.3/examples/vision/detection/rkyolo/python中的demo\r\n- 部署内容\r\n- - python方式实现yolov5模型的部署(yolov5s_relu_tk2_RK356X_i8.rknn,该模型为转换后的模型)\r\n- 操作流程\r\n- - 参考 https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/build.md 完成板端编译C++ SDK和Python SDK；\r\n- -  进入FastDeploy-release-1.0.3/examples/vision/detection/rkyolo/python文件夹中进行推理，文件夹中文件如下\r\n![image](https://user-images.githubusercontent.com/85120075/215733061-5c034383-53e4-4028-b064-0ad825aa5b2c.png)\r\n- - 执行命令 python3 infer.py --model_file ./model/ --image 000000014439.jpg 后不报错，提示“Segmentation fault”，运行日志如下。\r\n![image](https://user-images.githubusercontent.com/85120075/215733638-a69c61ef-2ebc-4677-a2d4-6e508cfb9d8b.png)\r\n\r\n## 环境编译日志\r\nhttps://pan.baidu.com/s/1R9kIj34-0qPdTK86uCnRMw 提取码: eeba \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-01-31T10:34:16+00:00",
        "updated_at": "2023-02-01T14:31:03+00:00",
        "closed_at": "2023-02-01T14:31:03+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1206,
        "title": "会支持Java版本的吗？类似于ai.djl一样的功能",
        "body": null,
        "state": "closed",
        "user": "xiayongtao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-01T06:46:18+00:00",
        "updated_at": "2024-04-02T06:40:02+00:00",
        "closed_at": "2024-04-02T06:40:02+00:00",
        "comments_count": [
            "DefTruth",
            "QQR1"
        ],
        "labels": [
            "Java"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1208,
        "title": "RK3588 YOLOV8 onnx转rknn",
        "body": "v8的onnx转rknn报这个错怎么办\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1152, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 617, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx: TypeError: '<=' not supported between instances of 'NoneType' and 'int'\r\nrknn对应接口都封装了，也改不了呀\r\nv8这个版本的onnx不管是否有nms都报这个bug\r\n望解答。。。。。。",
        "state": "closed",
        "user": "jielanZhang",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-01T07:52:31+00:00",
        "updated_at": "2023-04-12T05:15:48+00:00",
        "closed_at": "2023-02-26T01:19:12+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "lizheng-1",
            "Zheng-Bicheng",
            "TachibanaYoshino"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1209,
        "title": "rk3588部署yolov5报错",
        "body": "[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(315)::Infer      The input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[ERROR] fastdeploy/vision/detection/ppdet/ppdet_decode.cc(146)::DecodeAndNMS    The size of output must be fpn_stride * 2.\r\nAborted\r\n这个怎么解决",
        "state": "closed",
        "user": "jielanZhang",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-01T07:54:52+00:00",
        "updated_at": "2023-02-26T01:19:04+00:00",
        "closed_at": "2023-02-26T01:19:04+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1215,
        "title": "服务化部署 use backend 预编译不支持",
        "body": "拉取 registry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-cpu-only-21.10\r\n\r\n启动时分别试了三个backend：paddle、paddle_lite、onnx_runtime 分别报\r\n\r\nUsePaddleBackend The FastDeploy didn't compile with Paddle Inference.\r\n\r\nUseLiteBackend The FastDeploy didn't compile with Paddle Lite. \r\n\r\nUseTrtBackend The FastDeploy didn't compile with TrtBackend. \r\n\r\n难道各种backend都不支持吗？这是怎么回事呢？\r\n\r\n\r\n附上日志：\r\n\r\n\r\n<img width=\"1179\" alt=\"image\" src=\"https://user-images.githubusercontent.com/9245996/216023706-fb29b0d0-5c94-4741-a9da-67b758f01d7f.png\">",
        "state": "closed",
        "user": "suparek",
        "closed_by": "suparek",
        "created_at": "2023-02-01T10:55:13+00:00",
        "updated_at": "2023-02-14T11:04:33+00:00",
        "closed_at": "2023-02-14T11:04:32+00:00",
        "comments_count": [
            "heliqi",
            "suparek"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/Paddle-Lite",
        "number": 9959,
        "title": "PPLITE模型如何被flutter及rust开发的app(ios及安卓)调用，困扰好久了",
        "body": "如题\r\n",
        "state": "closed",
        "user": "YK7458",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-01T08:08:09+00:00",
        "updated_at": "2024-03-05T06:41:12+00:00",
        "closed_at": "2024-03-05T06:41:12+00:00",
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1219,
        "title": "在RK3568平台上实现RKYOLOv5模型部署过程中遇到的“Segmentation fault”问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】： fastdeploy-1.0.2\r\n【编译命令】按照仓库提供的环境编译方式完成C++和Python环境的编译。\r\n【系统平台】: Linux FriendlyElec 5.10.110 \r\n【硬件】： RK3568\r\n【编译语言】： C++ & Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n自己的数据集，用rknn的训练代码https://github.com/airockchip/yolov5训练出的模型，转到torchscript，再做量化转成rknn看起来都没什么问题，到rk3568设备上推理时候直接报错退出，修改postprocess.h里的类别数量之后还是报错\r\n<img width=\"1283\" alt=\"image\" src=\"https://user-images.githubusercontent.com/27794711/216072491-9b34b601-650f-4a2e-8888-bd8922c2b148.png\">\r\ndemo路径：FastDeploy/examples/vision/detection/rkyolo/cpp和FastDeploy/examples/vision/detection/rkyolo/python",
        "state": "closed",
        "user": "Rsndmmm",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-01T14:43:19+00:00",
        "updated_at": "2023-02-15T09:48:59+00:00",
        "closed_at": "2023-02-15T09:48:59+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1220,
        "title": "FastDeploy导出paddleyolo模型为rknn模型报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】python tools/rknpu2/export.py --config_path tools/rknpu2/config/yolov7_tiny_unquantized.yaml --target_platform rk3588\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： Nvidia GPU GTX 850M， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【paddleyolo导出的onnx模型FastDeploy无法转化为RKNN】\r\n- -RKNN2环境（python3.8）\r\n[requirements.txt](https://github.com/PaddlePaddle/FastDeploy/files/10563699/requirements.txt)\r\n- -配置文件\r\n[yolov7_tiny_unquantized.zip](https://github.com/PaddlePaddle/FastDeploy/files/10565101/yolov7_tiny_unquantized.zip)\r\n\r\n- -报错详情\r\n{'mean': None, 'std': None, 'model_path': './yolov7_tiny.onnx', 'outputs_nodes': ['p2o.Div.1', 'p2o.Concat.58'], 'do_quantization': False, 'dataset': './dataset.txt', 'output_folder': './yolov7_tiny'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nW load_onnx: If you don't need to crop the model, don't set 'inputs'/'input_size_list'/'outputs'!\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\nW load_onnx: The config.mean_values is None, zeros will be set for input 0!\r\nW load_onnx: The config.std_values is None, ones will be set for input 0!\r\nW load_onnx: The config.mean_values is None, zeros will be set for input 1!\r\nW load_onnx: The config.std_values is None, ones will be set for input 1!\r\nI base_optimize ...\r\nI base_optimize done.\r\nI \r\nI fold_constant ...\r\nE build: Catch exception when building RKNN model!\r\nE build: Traceback (most recent call last):\r\nE build:   File \"rknn/api/rknn_base.py\", line 1541, in rknn.api.rknn_base.RKNNBase.build\r\nE build:   File \"rknn/api/graph_optimizer.py\", line 608, in rknn.api.graph_optimizer.GraphOptimizer.fold_constant\r\nE build:   File \"rknn/api/ir_graph.py\", line 941, in rknn.api.ir_graph.IRGraph.make_model\r\nE build:   File \"rknn/api/ir_graph.py\", line 93, in rknn.api.ir_graph.IRGraph.infer_shapes\r\nE build:   File \"/home/wen/anaconda3/envs/rknn2-38/lib/python3.8/site-packages/onnx/checker.py\", line 104, in check_model\r\nE build:     C.check_model(protobuf_string)\r\nE build: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of type is required but missing.\r\nTraceback (most recent call last):\r\n  File \"tools/rknpu2/export.py\", line 58, in <module>\r\n    assert ret == 0, \"Build model failed!\"\r\nAssertionError: Build model failed!\r\n- -paddleyolo导出的模型文件\r\n\r\n[yolov7_tiny.zip](https://github.com/PaddlePaddle/FastDeploy/files/10565098/yolov7_tiny.zip)\r\n\r\n",
        "state": "closed",
        "user": "wen-flow",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-02T00:53:13+00:00",
        "updated_at": "2023-03-06T03:23:41+00:00",
        "closed_at": "2023-02-03T10:16:18+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "wen-flow",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "wen-flow",
            "wen-flow",
            "Zheng-Bicheng",
            "wen-flow",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "wen-flow",
            "wen-flow",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "wen-flow",
            "wen-flow",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1225,
        "title": "请问是否可以给fastdeploy::vision::classification::YOLOv5Cls加上batchPredict",
        "body": "请问是否可以给fastdeploy::vision::classification::YOLOv5Cls加上batchPredict",
        "state": "closed",
        "user": "yueyue0574",
        "closed_by": "yueyue0574",
        "created_at": "2023-02-02T11:22:55+00:00",
        "updated_at": "2023-02-26T14:44:14+00:00",
        "closed_at": "2023-02-26T14:44:14+00:00",
        "comments_count": [
            "GodIsBoom",
            "yueyue0574",
            "jiangjiajun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1233,
        "title": "辛苦更新相应的二维码～",
        "body": null,
        "state": "closed",
        "user": "daisygxr",
        "closed_by": "leiqing1",
        "created_at": "2023-02-03T09:57:36+00:00",
        "updated_at": "2023-02-03T13:27:27+00:00",
        "closed_at": "2023-02-03T13:27:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1229,
        "title": " FastDeploy ppocrv2 python infer.py使用trt模式推理生成rec推理trt文件报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-1.0.2\r\n- 【编译命令】python infer.py \"args\": [\r\n                \"--det_model\",\"/home/qintao/FastDeploy/examples/vision/ocr/PP-OCRv2/inference/ch_PP-OCRv3_det_infer\",\r\n                \"--cls_model\",\"/home/qintao/FastDeploy/examples/vision/ocr/PP-OCRv2/inference/ch_ppocr_mobile_v2.0_cls_infer\",\r\n                \"--rec_model\",\"/home/qintao/FastDeploy/examples/vision/ocr/PP-OCRv2/inference/rec_chinese_common_v2.0_9.27\",\r\n                \"--rec_label_file\",\"/home/qintao/FastDeploy/examples/vision/ocr/PP-OCRv2/ppocr_keys_v1.txt\",\r\n                \"--image\", \"/home/qintao/FastDeploy/examples/vision/ocr/PP-OCRv2/12.jpg\",\r\n                \"--device\",\"gpu\",\r\n                \"--backend\",\"trt\",\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Nvidia GPU 3090， CUDA 11.2 CUDNN 8.1 tensorrt 8.4.3.1\r\n- 【编译语言】：Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用默认模式对infer.py进行调试，可以正常输出结果，使用trt模式进行调试ocr文本检测和ocr文本方向分类可以正常编译生成trt文件，文本识别trt编译时报错，报错内容如下：\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(500)::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   1: [codeGenerator.cpp::compileGraph::476] Error Code 1: Myelin (myelinExceededMemBudget : Exceeded mem budget of 1073741824. Need 4035485696\r\n)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   2: [builder.cpp::buildSerializedNetwork::399] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed.)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(563)::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(676)::CreateTrtEngineFromOnnx       Failed to build tensorrt engine.\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n\r\n我对c的代码不太熟悉，无法定位到报错的位置，希望可以帮忙解答一下，十分感谢。\r\n\r\n",
        "state": "closed",
        "user": "QQQTAO",
        "closed_by": "yunyaoXYY",
        "created_at": "2023-02-03T03:13:57+00:00",
        "updated_at": "2023-04-23T08:42:06+00:00",
        "closed_at": "2023-02-06T02:26:01+00:00",
        "comments_count": [
            "yunyaoXYY",
            "QQQTAO",
            "yunyaoXYY",
            "QQQTAO",
            "yunyaoXYY",
            "QQQTAO",
            "yunyaoXYY",
            "QQQTAO",
            "Jaccica",
            "QQQTAO",
            "QQQTAO"
        ],
        "labels": [
            "OCR"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1231,
        "title": "yolov8 使用",
        "body": "![bb819eb183e2cf293b5ce458ada33ff](https://user-images.githubusercontent.com/106507366/216543107-eff1239e-1dbc-454e-b78b-f53a07ca59da.png)\r\n\r\n\r\n问下yolov8里面推理的时候需要输入conf_threshold 和nms_threshold  怎么处理  看了下和yolov5 不一样  谢谢",
        "state": "closed",
        "user": "sl00001",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-03T07:53:56+00:00",
        "updated_at": "2024-02-20T06:43:07+00:00",
        "closed_at": "2024-02-20T06:43:07+00:00",
        "comments_count": [
            "jiangjiajun",
            "sl00001",
            "liuyiche",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1235,
        "title": "建议Readme默认语言建议用中文，其他语言可选",
        "body": "还是要有国产框架强大的属性和底气，哈哈",
        "state": "closed",
        "user": "AI-Mart",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-06T03:45:14+00:00",
        "updated_at": "2024-02-13T06:40:02+00:00",
        "closed_at": "2024-02-13T06:40:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "zhiqwang",
            "szufafa"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1234,
        "title": "PaddleDetection YOLOv3 (python)训练的结果，与导出后用FastDeploy(cpu+openvino)部署的结果有差别",
        "body": "\r\nPaddleDetection YOLOv3 (python)训练的结果，与导出后用FastDeploy部署的结果有差别：\r\n\r\n原版yolov3 example代码，直接将读取的原图送入推理，这样部署的结果差很多：\r\n\r\n          auto model = fastdeploy::vision::detection::YOLOv3(model_file, params_file,\r\n                                                               config_file, option);\r\n            if (!model.Initialized()) {\r\n              std::cerr << \"Failed to initialize.\" << std::endl;\r\n              return;\r\n            }\r\n          \r\n            auto im = cv::imread(image_file);\r\n          \r\n            fastdeploy::vision::DetectionResult res;\r\n            if (!model.Predict(im, &res)) { ///////直接将读取的原图送入推理\r\n              std::cerr << \"Failed to predict.\" << std::endl;\r\n              return;\r\n            }\r\n\r\n我加上resize代码，结果就比较接近了，检测出来的框与paddledetection基本一致，但是存在漏检，且在置信度方面有差异：\r\n            auto model = fastdeploy::vision::detection::YOLOv3(model_file, params_file,\r\n                                                               config_file, option);\r\n            if (!model.Initialized()) {\r\n              std::cerr << \"Failed to initialize.\" << std::endl;\r\n              return;\r\n            }\r\n          \r\n            auto im1 = cv::imread(image_file);\r\n            cv::resize(im1, im, cv::Size(608, 608),0,0,cv::INTER_AREA);   /////////加入resize的代码\r\n            fastdeploy::vision::DetectionResult res;\r\n            if (!model.Predict(im, &res)) {    ///////直接将读取的原图送入推理\r\n              std::cerr << \"Failed to predict.\" << std::endl;\r\n              return;\r\n            }\r\n\r\n第一张是原图(部分截图)，第二张是paddledetection结果，第三张是未resize的结果，第四张是resize后的结果\r\n![44](https://user-images.githubusercontent.com/31379701/216756066-058d6d6f-2591-42c9-875a-15b2390d5f21.jpg)\r\n![11](https://user-images.githubusercontent.com/31379701/216756069-0250e7e3-77f5-47f7-bf5b-fefa1c92a565.jpg)\r\n![22](https://user-images.githubusercontent.com/31379701/216756072-446a315f-5700-400c-bc0f-7a62c91b1713.jpg)\r\n![33](https://user-images.githubusercontent.com/31379701/216756076-4cc835e6-7fd7-4f9d-894f-372aad21a471.jpg)\r\n\r\n未resize的结果普遍检测错误、漏检；resize后结果更好，但是置信度有差距(这张图不明显，但是很多相差10%以上，可能由此导致漏检)；查看了导出模型时生成的infer_cfg.yml文件，此文件是用于fastDeploy预处理的吗？里面含有resize操作是否在部署时没有用到？NormalizeImage、Permute等操作是否也需要自己实现？\r\n![55](https://user-images.githubusercontent.com/31379701/216756419-6a1516ab-53a7-49d8-8e9b-962fdd18d044.jpg)\r\n\r\n\r\n",
        "state": "closed",
        "user": "DreamMaker777",
        "closed_by": "DreamMaker777",
        "created_at": "2023-02-04T08:09:46+00:00",
        "updated_at": "2023-02-04T08:53:09+00:00",
        "closed_at": "2023-02-04T08:53:09+00:00",
        "comments_count": [
            "jiangjiajun",
            "DreamMaker777"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1236,
        "title": "服务化部署label问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [说明具体的版本，如fastdeploy-linux-gpu-0.8.0](fastdeploy:1.0.1-gpu-cuda11.4-trt8.4-21.10)\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\nPaddleDetection 服务化部署示例\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/serving\r\n\r\n参考服务化部署案例，我实现了自训练的fasterRCNN模型的服务化部署，但是输出的结果里默认的类型id为0。我实际文件标注编号是1，类型是riceplant\r\n![image](https://user-images.githubusercontent.com/112528302/216896934-85dd6785-28e8-4002-b390-6fd31a01b8da.png)\r\n这种情况在本地预测的时候也出现过，是这个Testdataset的标注路径的失效导致的。现在是这个json应该放在服务化部署的哪个路径上？\r\n![image](https://user-images.githubusercontent.com/112528302/216897215-38e000de-5b79-49bf-949f-94c69df6d31e.png)\r\n\r\n当前我只是有一个标记，可以自己改一下编号就过去了，但是当类型多起来的时候，谁是谁，这怎么分辨？所以希望能知道这样的问题根本的解决方法是什么？\r\n\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "heliqi",
        "created_at": "2023-02-06T06:18:55+00:00",
        "updated_at": "2023-02-06T08:06:38+00:00",
        "closed_at": "2023-02-06T08:06:38+00:00",
        "comments_count": [
            "jiangjiajun",
            "cunjing56",
            "cunjing56",
            "jiangjiajun",
            "cunjing56"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1241,
        "title": "调用ErnieFastTokenizer.EncodePairStrings抛异常std::range_error ",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： \r\n- 【编译命令】[官方编译](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/download_prebuilt_libraries.md#c-sdk%E5%AE%89%E8%A3%85)\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： Nvidia GPU 3060， CUDA 11.7 CUDNN 8.4\r\n- 【编译语言】： C++\r\n- 【IDE】： VS2022\r\n\r\n测试代码\r\n```c++\r\n#include <fast_tokenizer/tokenizers/ernie_fast_tokenizer.h>\r\n\r\nusing namespace paddlenlp::fast_tokenizer;\r\n\r\nint main() {\r\n  std::cout << \"start ...\" << std::endl;\r\n  tokenizers_impl::ErnieFastTokenizer ernie_fast_tokenizer(\"model/vocab1.txt\");\r\n  std::vector<core::Encoding> encodings(2);\r\n  ernie_fast_tokenizer.EncodePairStrings(\"今天天气真好\", &encodings[0]);\r\n  std::cout << encodings[0].DebugString() << std::endl;\r\n  ernie_fast_tokenizer.EncodePairStrings(\r\n      \"don't know how this missed award nominations.\", &encodings[1]);\r\n  std::cout << encodings[0].DebugString() << std::endl;\r\n  std::cout << \"end ...\" << std::endl;\r\n}\r\n```\r\n报错截图\r\n![Dingtalk_20230207101145](https://user-images.githubusercontent.com/44420757/217130186-835c5841-7e7a-404c-b081-f7809972b8a2.jpg)\r\n输出\r\n```bash\r\n0x00007FFFF86506BC 处(位于 TokenizerTest.exe 中)引发的异常: Microsoft C++ 异常: std::range_error，位于内存位置 0x000000EE868FEE30 处。\r\n0x00007FFFF86506BC 处(位于 TokenizerTest.exe 中)引发的异常: Microsoft C++ 异常: [rethrow]，位于内存位置 0x0000000000000000 处。\r\n0x00007FFFF86506BC 处(位于 TokenizerTest.exe 中)引发的异常: Microsoft C++ 异常: std::range_error，位于内存位置 0x000000EE868FEE30 处。\r\n0x00007FFFF86506BC 处(位于 TokenizerTest.exe 中)有未经处理的异常: Microsoft C++ 异常: std::range_error，位于内存位置 0x000000EE868FEE30 处。\r\n```\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "hhxdestiny",
        "closed_by": "hhxdestiny",
        "created_at": "2023-02-07T02:13:24+00:00",
        "updated_at": "2023-02-07T02:30:21+00:00",
        "closed_at": "2023-02-07T02:30:21+00:00",
        "comments_count": [
            "hhxdestiny"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1244,
        "title": "RKNPU2待更新内容",
        "body": "# Bug 修复\r\n\r\n## PaddleDetetcion\r\n\r\n* PaddleDetection多输入时div算子失效(Rockchip库问题，rk暂时未列入计划中)",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-07T08:17:29+00:00",
        "updated_at": "2024-02-27T01:43:03+00:00",
        "closed_at": "2024-02-27T01:43:03+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1242,
        "title": "BuildPreprocessPipelineFromConfig  Failed to load yaml ， PaddleDetPreprocessor      Failed to create PaddleDetPreprocessor.",
        "body": "https://bj.bcebos.com/paddlehub/fastdeploy/picodet_l_320_coco_lcnet.tgz\r\n\r\n```\r\nmodel_file  = user_Model_ydizhi + \"model.pdmodel\"\r\nparams_file = user_Model_ydizhi + \"model.pdiparams\"\r\nconfig_file = user_Model_ydizhi + \"infer_cfg.yml\"\r\nai_model =  fd.vision.detection.PicoDet(model_file, params_file, config_file,   runtime_option=runtime_option)\r\n```\r\n\r\n```\r\n\r\n[ERROR] fastdeploy/vision/detection/ppdet/preprocessor.cc(38)::fastdeploy::vision::detection::PaddleDetPreprocessor::BuildPreprocessPipelineFromConfig  Failed to load yaml file f:/2022/infer_cfg.yml, maybe you should check this file.\r\n[ERROR] fastdeploy/vision/detection/ppdet/preprocessor.cc(28)::fastdeploy::vision::detection::PaddleDetPreprocessor::PaddleDetPreprocessor      Failed to create PaddleDetPreprocessor.\r\n```\r\n\r\n\r\ninfer_cfg.yml\r\n```\r\nmode: paddle\r\ndraw_threshold: 0.5\r\nmetric: COCO\r\nuse_dynamic_shape: false\r\narch: GFL\r\nmin_subgraph_size: 3\r\nPreprocess:\r\n- interp: 2\r\n  keep_ratio: false\r\n  target_size:\r\n  - 320\r\n  - 320\r\n  type: Resize\r\n- is_scale: true\r\n  mean:\r\n  - 0.485\r\n  - 0.456\r\n  - 0.406\r\n  std:\r\n  - 0.229\r\n  - 0.224\r\n  - 0.225\r\n  type: NormalizeImage\r\n- type: Permute\r\nlabel_list:\r\n- person\r\n- bicycle\r\n- car\r\n- motorcycle\r\n- airplane\r\n- bus\r\n- train\r\n- truck\r\n- boat\r\n- traffic light\r\n- fire hydrant\r\n- stop sign\r\n- parking meter\r\n- bench\r\n- bird\r\n- cat\r\n- dog\r\n- horse\r\n- sheep\r\n- cow\r\n- elephant\r\n- bear\r\n- zebra\r\n- giraffe\r\n- backpack\r\n- umbrella\r\n- handbag\r\n- tie\r\n- suitcase\r\n- frisbee\r\n- skis\r\n- snowboard\r\n- sports ball\r\n- kite\r\n- baseball bat\r\n- baseball glove\r\n- skateboard\r\n- surfboard\r\n- tennis racket\r\n- bottle\r\n- wine glass\r\n- cup\r\n- fork\r\n- knife\r\n- spoon\r\n- bowl\r\n- banana\r\n- apple\r\n- sandwich\r\n- orange\r\n- broccoli\r\n- carrot\r\n- hot dog\r\n- pizza\r\n- donut\r\n- cake\r\n- chair\r\n- couch\r\n- potted plant\r\n- bed\r\n- dining table\r\n- toilet\r\n- tv\r\n- laptop\r\n- mouse\r\n- remote\r\n- keyboard\r\n- cell phone\r\n- microwave\r\n- oven\r\n- toaster\r\n- sink\r\n- refrigerator\r\n- book\r\n- clock\r\n- vase\r\n- scissors\r\n- teddy bear\r\n- hair drier\r\n- toothbrush\r\nNMS:\r\n  keep_top_k: 100\r\n  name: MultiClassNMS\r\n  nms_threshold: 0.5\r\n  nms_top_k: 1000\r\n  score_threshold: 0.3\r\nfpn_stride:\r\n- 8\r\n- 16\r\n- 32\r\n- 64\r\n\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-07T02:26:55+00:00",
        "updated_at": "2024-04-24T02:12:33+00:00",
        "closed_at": "2023-02-07T02:41:44+00:00",
        "comments_count": [
            "monkeycc",
            "mengze666",
            "smalie2222"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1253,
        "title": "后端切换没反应,切换GPU没用",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： \r\nfastapi               0.89.1\r\nfastdeploy-gpu-python 1.0.3\r\nfastdeploy-tools      0.0.2\r\n\r\n- 【编译命令】pip install numpy opencv-python fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： Nvidia GPU 3080TI， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n\r\n```python\r\n\r\ndef build_option():\r\n    option = fd.RuntimeOption()\r\n\r\n    option.use_gpu()\r\n    option.use_paddle_backend()\r\n\r\n\r\nruntime_option = build_option()\r\nfd.vision.detection.YOLOv8(dizhi,  runtime_option=runtime_option)\r\n\r\n```\r\n\r\n```\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\n[WARN] 2023-02-08T01:55:46z frontends\\onnx\\frontend\\src\\ops_bridge.cpp 237      Currently ONNX operator set version: 17 is unsupported. Falling back to: 16\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(329)::fastdeploy::OpenVINOBackend::InitFromOnnx       Compile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(116)::fastdeploy::Runtime::Init    Runtime initialized with Backend::OPENVINO in Device::CPU.\r\n\r\n```\r\n\r\n不管怎么换目标模型 分类模型 \r\n换后端和GPU\r\n这里都是\r\nOpenVINO model on device_name:CPU\r\nBackend::OPENVINO in Device::CPU.\r\n\r\n\r\n问题解决\r\n忘记加\r\nreturn option",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-08T01:59:44+00:00",
        "updated_at": "2023-03-29T10:05:21+00:00",
        "closed_at": "2023-02-08T02:20:53+00:00",
        "comments_count": [
            "Firestick-Xia"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1257,
        "title": "Support C API",
        "body": "fastdeploy有c的api吗？我们想通过cgo调用",
        "state": "closed",
        "user": "taojishou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T02:27:17+00:00",
        "updated_at": "2024-02-13T06:40:03+00:00",
        "closed_at": "2024-02-13T06:40:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "taojishou",
            "jiangjiajun",
            "taojishou",
            "taojishou",
            "jiangjiajun",
            "taojishou",
            "rainyfly"
        ],
        "labels": [
            "c api"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1258,
        "title": "paddle模型转换为rknn模型报错",
        "body": "运行环境：\r\nrknn版本：rknn_toolkit2-1.3.0_11912b58-cp38-cp38-linux_x86_64.whl\r\npaddlepaddle：2.4.1\r\npaddle2onnx:1.0.5\r\nonnx:1.7.0\r\n\r\n问题描述：\r\n在github上下载的picodet_s_416_coco_lcnet模型进行模型的转换，pd-->onnx-->rknn进行转换\r\n其中在onnx-->rknn模型报错：\r\nroot@linux:/media/workspace/fd_serving/FastDeploy# python3 tools/rknpu2/export.py --config_path tools/rknpu2/config/RK3588/picodet_s_416_coco_lcnet.yaml \r\n{'model_path': '/media/workspace/rknn_rknpu2/picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx', 'output_folder': '/media/workspace/rknn_rknpu2/picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet', 'target_platform': 'RK3588', 'normalize': {'mean': [[0.485, 0.456, 0.406]], 'std': [[0.229, 0.224, 0.225]]}, 'outputs': ['p2o.Div.79', 'p2o.Concat.9']}\r\nW __init__: rknn-toolkit2 version: 1.3.0-11912b58\r\n<class 'list'>\r\nyaml_config[\"outputs\"] =  ['p2o.Div.79', 'p2o.Concat.9']\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\nE load_onnx: Catch exception when loading onnx model: /media/workspace/rknn_rknpu2/picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1157, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx: UnboundLocalError: local variable 'inp' referenced before assignment\r\nTraceback (most recent call last):\r\n  File \"tools/rknpu2/export.py\", line 57, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n",
        "state": "closed",
        "user": "chenjf2015103095",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-08T02:33:39+00:00",
        "updated_at": "2023-02-26T01:18:51+00:00",
        "closed_at": "2023-02-26T01:18:51+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1259,
        "title": "YOLOV8大尺寸模型推理报错。",
        "body": "\r\n模型：\r\n[YOLOV8_1760.zip](https://github.com/PaddlePaddle/FastDeploy/files/10681209/YOLOV8_1760.zip)\r\n报错图片：\r\n![image](https://user-images.githubusercontent.com/33145332/217416200-f22bdd9b-1697-4683-84c3-2c6a17cb53d1.png)\r\n推理代码：\r\nvoid TrtInfer(const std::string& model_file, const std::string& image_file) {\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\toption.UseTrtBackend();\r\n\toption.SetTrtInputShape(\"images\", { 1, 3, 1760, 1760 });\r\n\tauto model = fastdeploy::vision::detection::YOLOv8(model_file, \"\", option);\r\n\tmodel.GetPreprocessor().SetSize({ 1760,1760 });\r\n\tif (!model.Initialized()) {\r\n\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n\r\n\tauto im = cv::imread(image_file);\r\n\tfastdeploy::vision::DetectionResult res;\r\n\tif (!model.Predict(im, &res)) {\r\n\t\tstd::cerr << \"Failed to predict.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n}",
        "state": "closed",
        "user": "chuxuming",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T02:50:23+00:00",
        "updated_at": "2024-02-13T06:40:04+00:00",
        "closed_at": "2024-02-13T06:40:04+00:00",
        "comments_count": [
            "wjj19950828",
            "chuxuming",
            "chuxuming",
            "wjj19950828"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1261,
        "title": "多线程下，模型并行效率线性递增",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.8 CUDNN 8.6\r\nProgram Language: e.g. C++\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n[yolox_deploy.zip](https://github.com/PaddlePaddle/FastDeploy/files/10682133/yolox_deploy.zip)\r\n\r\n问题描述：\r\n1.进行多线程调用时，\r\n一个线程  是内部推理 10次 （单个模型4ms）40 ms， cuda 利用率 59% \r\n 两个线程 是内部推理 10次 （单个模型4ms）80 ms，cuda 利用率 67% \r\n三个线程 是内部推理 10次 （单个模型4ms）110 ms，cuda 利用率 96% \r\n以上cuda利用率均正常。 \r\n\r\n\r\n\r\n实验1：以下是另外一个测试验证，nvpp分析，模型不存在并行，效率是随着模型并行个数的增多，线性增加。\r\n![7](https://user-images.githubusercontent.com/21118441/217446877-79c058e1-2c2d-47f0-9481-8908a0579f43.png)\r\n\r\n实验2（fasterdeploy推理）\r\n![_20230208134611](https://user-images.githubusercontent.com/21118441/217445269-f3dde66e-fbac-49d7-a70c-6a24738a3264.png)\r\n通过nvpp 分析，cuda 流已经有并发情况，但是时间为啥还是线性增加。按道理cuda流并行后，效率会有所提速。\r\n\r\n能否帮忙分析问题所在！\r\n\r\n\r\n",
        "state": "open",
        "user": "longsy316",
        "closed_by": null,
        "created_at": "2023-02-08T05:51:51+00:00",
        "updated_at": "2024-07-17T15:54:28+00:00",
        "closed_at": null,
        "comments_count": [
            "longsy316",
            "Hr-Song",
            "luameows",
            "sanersbug"
        ],
        "labels": [
            "Performance"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1262,
        "title": "预编译库Linux aarch64  C++ SDK下载链接失效",
        "body": "(https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/download_prebuilt_libraries.md)\r\n预编译库  C++ SDK\r\n\r\nLinux aarch64   | fastdeploy-linux-aarch64-1.0.3.tgz\r\n下载链接失效\r\n\r\n",
        "state": "closed",
        "user": "cgisky1980",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T06:15:02+00:00",
        "updated_at": "2024-02-13T06:40:05+00:00",
        "closed_at": "2024-02-13T06:40:05+00:00",
        "comments_count": [
            "jiangjiajun",
            "cgisky1980",
            "cgisky1980",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1267,
        "title": "AttributeError: module 'fastdeploy.vision.detection' has no attribute 'YOLOv8'",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：\r\nfastapi            0.89.1\r\nfastdeploy-python  1.0.2\r\nfastdeploy-tools   0.0.2\r\n- 【编译命令】pip install\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： CPU\r\n- 【编译语言】： Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n```\r\n\r\nruntime_option RuntimeOption(\r\n  backend : Backend.UNKOWN\r\n  cpu_thread_num : 12\r\n  device : Device.CPU\r\n  device_id : 0\r\n  external_stream : None\r\n  ipu_available_memory_proportion : 1.0\r\n  ipu_batches_per_step : 1\r\n  ipu_device_num : 1\r\n  ipu_enable_fp16 : False\r\n  ipu_enable_half_partial : False\r\n  ipu_enable_pipelining : False\r\n  ipu_micro_batch_size : 1\r\n  ipu_replica_num : 1\r\n  is_dynamic : False\r\n  kunlunxin_adaptive_seqlen : False\r\n  kunlunxin_autotune : True\r\n  kunlunxin_autotune_file :\r\n  kunlunxin_enable_multi_stream : False\r\n  kunlunxin_l3_workspace_size : 16776192\r\n  kunlunxin_locked : False\r\n  kunlunxin_precision : int16\r\n  long_to_int : True\r\n  model_buffer_size : 0\r\n  model_file :\r\n  model_format : ModelFormat.???\r\n  model_from_memory : False\r\n  ort_execution_mode : -1\r\n  ort_graph_opt_level : -1\r\n  ort_inter_op_num_threads : -1\r\n  params_buffer_size : 0\r\n  params_file :\r\n  poros_file :\r\n  trt_enable_fp16 : False\r\n  trt_enable_int8 : False\r\n  trt_max_batch_size : 1\r\n  trt_max_shape : {}\r\n  trt_max_workspace_size : 1073741824\r\n  trt_min_shape : {}\r\n  trt_opt_shape : {}\r\n  trt_serialize_file :\r\n  unconst_ops_thres : -1\r\n  use_nvidia_tf32 : False\r\n)\r\nTraceback (most recent call last):\r\n  File \"1.py\", line 2190, in <module>\r\n    ai_model =  fd.vision.detection.YOLOv8(dizhi,  runtime_option=runtime_option)\r\nAttributeError: module 'fastdeploy.vision.detection' has no attribute 'YOLOv8'\r\n```\r\n\r\n\r\n在这台电脑不行\r\n另外电脑可以\r\n\r\n切换几个后端也是不行\r\nBackend.UNKOWN\r\nBackend.??????",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T08:54:46+00:00",
        "updated_at": "2024-02-13T06:40:07+00:00",
        "closed_at": "2024-02-13T06:40:07+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1268,
        "title": "Windows下编译后报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.3\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】： i7-12700K/Redeon RX 6750XT\r\n- 【编译语言】： C++ \r\n- 【编译平台】： cmake + vs 2022 \r\n\r\n## 问题日志及出现问题的操作流程\r\n 准备用预编译好的CPU环境下Windows C++ SDK加载PPYOLOE coco 300的模型进行推理，先写了一点基本的代码加载库，可以编译成功，将所需dll复制到build/DEBUG目录下运行程序，结果报错\r\n\r\n![WXWorkLocal_167584613342](https://user-images.githubusercontent.com/103732515/217480364-69e2819a-0609-40d6-a751-c65d93f8c254.png)\r\n\r\n代码只有短短一行\r\n```\r\n#include \"fastdeploy/vision.h\"\r\n\r\nint main(){\r\n\tfastdeploy::RuntimeOption option;\r\n\t\r\n\treturn 0;\r\n}\r\n\r\n```\r\n直接运行的报错如下图\r\n\r\n![image](https://user-images.githubusercontent.com/103732515/217480980-544b144a-94f1-413b-a5cf-40ccb2fc8ddb.png)\r\n\r\n编译的cmakelists文件如下\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.0.0)\r\nproject(__TIMESTAMP__ VERSION 0.1.0)\r\n\r\nset(CMAKE_CXX_STANDARD 11)\r\n\r\ninclude(CTest)\r\n\r\nfind_package(FastDeploy REQUIRED)\r\ninclude_directories(${FASTDEPLOY_INCS})\r\n\r\nadd_executable(main test.cpp)\r\n\r\ntarget_link_libraries(main ${FASTDEPLOY_LIBS})\r\n\r\n\r\nset(CPACK_PROJECT_NAME ${PROJECT_NAME})\r\nset(CPACK_PROJECT_VERSION ${PROJECT_VERSION})\r\n\r\ninclude(CPack)\r\n```\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "fyy0114",
        "closed_by": "fyy0114",
        "created_at": "2023-02-08T08:55:05+00:00",
        "updated_at": "2023-02-09T01:42:17+00:00",
        "closed_at": "2023-02-09T01:42:17+00:00",
        "comments_count": [
            "DefTruth",
            "jiangjiajun",
            "fyy0114"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1263,
        "title": "怎样把模型转换为json格式",
        "body": "`model = fd.vision.detection.YOLOv5(\r\nmodel_file=os.path.join(ROOT, 'best.onnx'),\r\nruntime_option=runtime_option)\r\n\r\nmodel = model.to_json() #这个方法似乎是不可用的\r\n`",
        "state": "closed",
        "user": "WanderAlphonse",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T08:16:38+00:00",
        "updated_at": "2024-02-13T06:40:06+00:00",
        "closed_at": "2024-02-13T06:40:06+00:00",
        "comments_count": [
            "jiangjiajun",
            "WanderAlphonse",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1271,
        "title": "服务化部署PPOCRv3,内存占用不释放",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： paddlepaddle/fastdeploy:1.0.3-cpu-only-21.10 参考示例服务化部署ppocrv3\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【性能问题】描述清楚对比的方式\r\n- - backend为paddle和openvino,多并发请求OCR服务（500张图片，10并发）\r\n1、默认配置（动态shape）,内存占用快速增长且推理结束后不释放\r\n2、固定infer shape后，内存占用增长速度变缓，但内存占用依旧不释放\r\n\r\n",
        "state": "closed",
        "user": "wxz5459",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T09:49:07+00:00",
        "updated_at": "2024-07-02T06:40:17+00:00",
        "closed_at": "2024-07-02T06:40:17+00:00",
        "comments_count": [
            "heliqi",
            "wxz5459",
            "heliqi",
            "dzz10",
            "dzz10",
            "wxz5459",
            "dzz10"
        ],
        "labels": [
            "OCR",
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1272,
        "title": "MaskRCNN的结果图片中没有mask",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop branch\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=OFF \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=OFF \\\r\n         -DWITH_GPU=ON \\\r\n         -DTRT_DIRECTORY=/workspace/TensorRT-8.4.1.5 \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=OFF \\\r\n         -DENABLE_CVCUDA=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-dev\r\n- 【系统平台】: Linux x64(Ubuntu 20.04.4 LTS)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU T4， CUDA 11.7\r\n- 【编译语言】： python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n```\r\ncd FastDeploy/examples/vision/detection/paddledetection/python\r\n\r\npython3 infer_mask_rcnn.py --model mask_rcnn_r50_1x_coco --image 000000014439.jpg\r\n```\r\n\r\n![1222638b12698c1b04a08970c04274fd](https://user-images.githubusercontent.com/15235574/217498258-c6104d99-b7fd-4ae9-8f96-d95eefc8dd2b.jpg)\r\n",
        "state": "closed",
        "user": "wang-xinyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T10:03:15+00:00",
        "updated_at": "2024-12-17T06:42:02+00:00",
        "closed_at": "2024-12-17T06:42:02+00:00",
        "comments_count": [
            "qianbin1989228",
            "panp4n",
            "panp4n",
            "qianbin1989228"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1277,
        "title": " paddlenlp 的C++库 在哪?",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-aarch64-1.0.3\r\n- 【系统平台】: Linux aarch64(Ubuntu 18.04) \r\n- 【硬件】： RK3588\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/text/uie/cpp/\r\nc++的例子 make 的时候报错\r\nerror: ‘paddlenlp’ is not a namespace-name\r\n using namespace paddlenlp;\r\n这个 paddlenlp 的C++库 在哪?\r\n\r\n",
        "state": "closed",
        "user": "cgisky1980",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-08T18:26:57+00:00",
        "updated_at": "2024-04-16T09:02:52+00:00",
        "closed_at": "2024-04-16T09:02:52+00:00",
        "comments_count": [
            "jiangjiajun",
            "cgisky1980"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1278,
        "title": "YOLOV7Face",
        "body": "Face detection希望能够更新支持一下yolov7-face",
        "state": "closed",
        "user": "szufafa",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-09T02:19:27+00:00",
        "updated_at": "2024-04-16T09:02:53+00:00",
        "closed_at": "2024-04-16T09:02:53+00:00",
        "comments_count": [
            "jiangjiajun",
            "szufafa"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1279,
        "title": "如何筛选 yolov5  推理的结果",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform:  Windows x64 \r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.7\r\n\r\n`DetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n1110.355835,425.777344, 1257.300659, 522.001709, 0.829359, 0\r\n558.526306,659.826904, 719.997986, 751.881592, 0.823671, 0\r\n1248.770752,426.186157, 1408.032959, 524.551147, 0.814135, 0\r\n759.281067,682.981812, 840.883850, 774.683960, 0.809666, 0\r\n750.875183,540.860107, 925.318909, 660.229492, 0.806654, 0\r\n1808.166016,467.900391, 2105.863281, 555.792969, 0.806545, 2\r\n1341.073242,541.609497, 1469.515625, 648.473511, 0.804522, 0\r\n1154.162842,691.180542, 1220.338623, 774.633179, 0.795894, 0\r\n412.432709,659.394653, 554.675232, 799.230347, 0.795412, 0\r\n605.183289,479.122070, 714.999939, 615.235107, 0.794305, 0\r\n1016.549805,423.201172, 1118.640625, 522.117188, 0.794164, 1`\r\n\r\n比如要筛选score大于 0.8 或者 label_id 为 '1' 的结果\r\n\r\n\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n",
        "state": "closed",
        "user": "HiaHong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-09T02:19:45+00:00",
        "updated_at": "2024-04-16T09:02:54+00:00",
        "closed_at": "2024-04-16T09:02:54+00:00",
        "comments_count": [
            "jiangjiajun",
            "HiaHong",
            "jiangjiajun",
            "HiaHong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1280,
        "title": "DeepLabV3+  预测结果存在问题，原图进原图出",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如    fastdeploy-win-x64-gpu-1.0.3\r\n- 【编译命令】\r\n- 【系统平台】: / Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号  ，  用的CPU，后端默认openvision\r\n- 【编译语言】： C++\r\n 模型：  PaddleSeg/configs/deeplabv3p/deeplabv3p_resnet101_os8_cityscapes_769x769_80k.yml \r\n",
        "state": "closed",
        "user": "teymur-git",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-09T02:30:34+00:00",
        "updated_at": "2024-04-16T09:02:55+00:00",
        "closed_at": "2024-04-16T09:02:55+00:00",
        "comments_count": [
            "felixhjh",
            "teymur-git"
        ],
        "labels": [
            "paddleseg"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1281,
        "title": "yolov7-face",
        "body": "fastdeploy-win-x64-1.0.2\r\n\r\n识别结果没有关键点\r\n替换成yolov5-face就有\r\n",
        "state": "closed",
        "user": "szufafa",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-09T03:39:59+00:00",
        "updated_at": "2024-04-16T09:02:56+00:00",
        "closed_at": "2024-04-16T09:02:56+00:00",
        "comments_count": [
            "wjj19950828",
            "szufafa",
            "szufafa",
            "KunMengcode",
            "KunMengcode",
            "wjj19950828",
            "wjj19950828",
            "szufafa",
            "KunMengcode",
            "szufafa"
        ],
        "labels": [
            "Model"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1284,
        "title": "fastdeploy 对jenston 平台是否可以增加 开启/关闭DLA，以及 支持 Int8 量化相关API",
        "body": "Nvidia  DLA:\r\n<img width=\"284\" alt=\"b2bbfa6814264b0c0885461b42c449f\" src=\"https://user-images.githubusercontent.com/27769380/217740560-ae8b8a8d-9e6d-4a1d-aca1-318b392a6ff4.png\">\r\n\r\n\r\nkINT8 :\r\n![image](https://user-images.githubusercontent.com/27769380/217743128-c432fc8f-1430-409c-8e8a-e530bb70a09c.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "shanyu155",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-09T07:14:58+00:00",
        "updated_at": "2024-02-13T06:40:08+00:00",
        "closed_at": "2024-02-13T06:40:07+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1286,
        "title": "FastDeploy预编译库中的一键自动压缩工具无法使用",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu/cpu-python==1.0.3 以及develop版本都有这个问题\r\n          预编译库是严格按照这个文档做的：https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/download_prebuilt_libraries.md\r\n          \r\n- 【系统平台】: Linux x64(Ubuntu 18.04)  aistudio 和 VMware虚拟机里都有这个问题\r\n- 【硬件】： aistudio的16G和32G的GPU都有这个问题\r\n- 【编译语言】： Python3.7\r\n- PaddlePaddle 2.4.0\r\n- PaddleSlim 2.4.0\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 严格按照这个文档操作的：https://github.com/PaddlePaddle/FastDeploy/tree/develop/tools/common_tools/auto_compression\r\n日志：\r\n\u001b[32m[02-08 14:55:39 MainThread @logger.py:242]\u001b[0m Argv: /opt/conda/envs/python35-paddle120-env/bin/fastdeploy compress --config_path=./configs/detection/yolov5s_quant.yaml --method=PTQ --save_dir=./yolov5s_ptq_model/\r\n\u001b[33m[02-08 14:55:39 MainThread @utils.py:79]\u001b[0m \u001b[5m\u001b[33mWRN\u001b[0m paddlepaddle version: 2.4.0. The dynamic graph version of PARL is under development, not fully tested and supported\r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/parl/remote/communication.py:38: FutureWarning: 'pyarrow.default_serialization_context' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\r\n  context = pyarrow.default_serialization_context()\r\nWARNING:root:cannot import name 'layers' from 'parl' (/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/parl/__init__.py)\r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n  from collections import MutableMapping\r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n  from collections import Iterable, Mapping\r\n/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n  from collections import Sized\r\nWelcome to use FastDeploy Auto Compression Toolkit!\r\n-----------  Running Arguments -----------\r\n Distillation:\r\n\t alpha: 1.0\r\n\t loss: soft_label\r\n Global:\r\n\t format: onnx\r\n\t input_list: ['x2paddle_images']\r\n\t model_dir: ./model_exported/yolov5s.onnx\r\n\t model_filename: model.pdmodel\r\n\t params_filename: model.pdiparams\r\n\t ptq_image_path: ./dataset/COCO_val_320\r\n\t ptq_preprocess: yolo_image_preprocess\r\n\t qat_batch_size: 1\r\n\t qat_image_path: ./dataset/COCO_train_320\r\n\t qat_preprocess: yolo_image_preprocess\r\n PTQ:\r\n\t calibration_method: avg\r\n\t skip_tensor_list: None\r\n QuantAware:\r\n\t activation_quantize_type: moving_average_abs_max\r\n\t onnx_format: True\r\n\t quantize_op_types: ['conv2d', 'depthwise_conv2d']\r\n\t use_pact: True\r\n TrainConfig:\r\n\t learning_rate: 1e-05\r\n\t optimizer_builder:\r\n\t\t optimizer:\r\n\t\t\t type: SGD\r\n\t\t weight_decay: 4e-05\r\n\t target_metric: 0.365\r\n\t train_iter: 3000\r\n------------------------------------------\r\n2023-02-08 14:55:40,318-INFO: Now translating model from onnx to paddle.\r\n2023-02-08 14:55:40,318-WARNING: __init__() missing 1 required positional argument: 'enable_onnx_checker'\r\n2023-02-08 14:55:40,319-ERROR: x2paddle threw an exception, you can ask for help at: https://github.com/PaddlePaddle/X2Paddle/issues\r\n   \r\n截图：\r\n![c6c40e9aab5d44cf79b4e1aef496cc5](https://user-images.githubusercontent.com/94025121/217786462-332eda2d-71b6-4ea3-b6f4-7d27e7c2b8cc.png)\r\n\r\n![9afddbc4bfcca51216e71c3a8935118](https://user-images.githubusercontent.com/94025121/217787033-9f4997f8-339d-48c7-84b0-28c29fac9799.png)\r\n\r\n\r\n- 换用自己的模型也不行\r\n   错误提示2：\r\n![8a30c700b5652e991e3f5db33f237b4](https://user-images.githubusercontent.com/94025121/217786099-a99ca118-126a-4737-8bb6-139c1e418925.png)\r\n\r\n\r\n个人感觉是预编译库里面的x2paddle工具有问题。\r\n\r\n",
        "state": "closed",
        "user": "Taichipeace",
        "closed_by": "Taichipeace",
        "created_at": "2023-02-09T10:29:22+00:00",
        "updated_at": "2023-02-16T11:35:51+00:00",
        "closed_at": "2023-02-16T11:35:51+00:00",
        "comments_count": [
            "wjj19950828",
            "Taichipeace",
            "Taichipeace",
            "wjj19950828",
            "wjj19950828",
            "Taichipeace"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1291,
        "title": "Jetson nano 编译报错问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：  \r\n- 【编译命令】 python编译\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)   Jetson nano\r\n- 【硬件】： Jetson nano  cuda 10.2\r\n- 【编译语言】：Python 3.6.9\r\n-   cmake  3.10.2\r\n-   gcc  7.5.0 \r\n-   jetpack   4.6.1\r\n## 问题日志及出现问题的操作流程\r\n\r\n\r\npython3设置为默认的python\r\n```\r\nsudo rm /usr/bin/python\r\nsudo ln -s /usr/bin/python3.6 /usr/bin/python\r\n```\r\n\r\n\r\n编译\r\nPADDLEINFERENCE_DIRECTORY 这里有问题\r\n\r\nPaddle Inference预编译库 有对应的python版本\r\nJetpack4.6：nv_jetson-cuda10.2-trt8.0-nano\r\n我直接安装了\r\n\r\n但是这里还要写一个PADDLEINFERENCE_DIRECTORY目录\r\n我下载了选择对应的Jetpack C++包 作为这个目录\r\nC++ Jetson(Nano) Jetpack 4.6.1  paddle_inference_install_dir\r\n不知道对不对\r\n\r\n那么我这里 是不是就不需要安装python预编译库\r\n还是安装了python预编译库 这里就不需要了？\r\n还是这里安装   不要安装python预编译库？\r\n或者安装了python预编译库  PADDLEINFERENCE_DIRECTORY就不用写了？\r\n```\r\n\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\n\r\n# ENABLE_PADDLE_BACKEND & PADDLEINFERENCE_DIRECTORY为可选项\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport PADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_install_dir\r\n\r\npython setup.py build\r\n```\r\n\r\n\r\n编译报错\r\njetpack   4.6.1\r\nPython 3.6.9\r\ncmake  3.10.2\r\ngcc  7.5.0 \r\n```\r\n\r\n~/FastDeploy/python$ python setup.py build\r\nfatal: not a git repository (or any of the parent directories): .git\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\n-- The C compiler identification is GNU 7.5.0\r\n-- The CXX compiler identification is GNU 7.5.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /home/MM/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 5% complete]\r\n-- [download 10% complete]\r\n-- [download 15% complete]\r\n-- [download 20% complete]\r\n-- [download 26% complete]\r\n-- [download 31% complete]\r\n-- [download 36% complete]\r\n-- [download 41% complete]\r\n-- [download 46% complete]\r\n-- [download 51% complete]\r\n-- [download 56% complete]\r\n-- [download 61% complete]\r\n-- [download 67% complete]\r\n-- [download 72% complete]\r\n-- [download 77% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 98% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/MM/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/MM/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\nCMake Error at cmake/paddle_inference.cmake:72 (find_package):\r\n  By not providing \"FindPython.cmake\" in CMAKE_MODULE_PATH this project has\r\n  asked CMake to find a package configuration file provided by \"Python\", but\r\n  CMake did not find one.\r\n\r\n  Could not find a package configuration file provided by \"Python\" with any\r\n  of the following names:\r\n\r\n    PythonConfig.cmake\r\n    python-config.cmake\r\n\r\n  Add the installation prefix of \"Python\" to CMAKE_PREFIX_PATH or set\r\n  \"Python_DIR\" to a directory containing one of the above files.  If \"Python\"\r\n  provides a separate development package or SDK, be sure it has been\r\n  installed.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:225 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/MM/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 437, in <module>\r\n    license='Apache 2.0')\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 129, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.6/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 280, in run\r\n    self.run_command('cmake_build')\r\n  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 266, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.6m', '-DPYTHON_EXECUTABLE=/usr/bin/python', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-36m-aarch64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DWITH_ASCEND=OFF', '-DENABLE_ORT_BACKEND=OFF', '-DENABLE_OPENVINO_BACKEND=OFF', '-DENABLE_PADDLE_BACKEND=ON', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=OFF', '-DENABLE_LITE_BACKEND=OFF', '-DPADDLELITE_URL=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=OFF', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=OFF', '-DWITH_IPU=OFF', '-DWITH_KUNLUNXIN=OFF', '-DBUILD_ON_JETSON=ON', '-DTRT_DIRECTORY=UNDEFINED', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=/home/MM/FastDeploy/paddle_inference_jetson', '-DRKNN2_TARGET_SOC=', '/home/MM/FastDeploy']' returned non-zero exit status 1.\r\n\r\n\r\n```\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-10T01:24:46+00:00",
        "updated_at": "2024-10-29T06:43:35+00:00",
        "closed_at": "2024-10-29T06:43:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "zachary-zheng",
            "jiangjiajun",
            "wang-xinyu",
            "qiulongquan",
            "jiangjiajun",
            "wang-xinyu",
            "qiulongquan",
            "qiulongquan",
            "jiangjiajun",
            "qiulongquan",
            "jiangjiajun",
            "qiulongquan",
            "zachary-zheng",
            "827346462",
            "827346462",
            "zachary-zheng",
            "monkeycc",
            "jiangjiajun",
            "zachary-zheng",
            "monkeycc",
            "zachary-zheng",
            "827346462",
            "827346462",
            "827346462",
            "kewuyu",
            "Petal99"
        ],
        "labels": [
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1282,
        "title": "rk3588编译fastdeploy成功后，不能使用",
        "body": "运行脚本如下：\r\nimport fastdeploy as fd\r\nimport cv2\r\n\r\nruntime_option = fd.RuntimeOption()\r\nruntime_option.use_rknpu2()\r\n\r\nmodel_file = \"./picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet_rk3588.rknn\"\r\n\r\nconfig_file = \"./picodet_s_416_coco_lcnet/infer_cfg.yml\"\r\n\r\nimage_path = \"/picodet_s_416_coco_lcnet/000000014439.jpg\"\r\n\r\nmodel = fd.vision.detection.RKPicoDet(model_file,\"\",\r\n                                      config_file,\r\n                                      runtime_option=runtime_option,\r\n                                      model_format=fd.ModelFormat.RKNN)\r\nim = cv2.imread(image_path)\r\nresult = model.predict(im.copy())\r\nvis_im = fd.vision.vis_detection(im,result,socre_threshold=0.5)\r\ncv2.imwrite(\"vis_result.jpg\",vis_im)\r\n\r\n报错信息如下：\r\n\r\nTraceback (most recent call last):\r\n  File \"rknn_infer.py\", line 11, in <module>\r\n    runtime_option = fd.RuntimeOption()\r\nAttributeError: module 'fastdeploy' has no attribute 'RuntimeOption'\r\n",
        "state": "closed",
        "user": "chenjf2015103095",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-09T03:52:54+00:00",
        "updated_at": "2023-02-17T00:59:53+00:00",
        "closed_at": "2023-02-17T00:59:53+00:00",
        "comments_count": [
            "jiangjiajun",
            "chenjf2015103095",
            "jiangjiajun",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "liutingxi",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "liutingxi",
            "Zheng-Bicheng",
            "liutingxi",
            "Zheng-Bicheng",
            "chenjf2015103095",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "leiqing1"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1292,
        "title": "YOLOv7Face Result 不包含landmarks ，(模型是有关键点值的)",
        "body": null,
        "state": "closed",
        "user": "KunMengcode",
        "closed_by": "KunMengcode",
        "created_at": "2023-02-10T01:37:07+00:00",
        "updated_at": "2023-02-15T01:54:07+00:00",
        "closed_at": "2023-02-15T01:54:07+00:00",
        "comments_count": [
            "wjj19950828"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1296,
        "title": "fastdeploy::vision::segmentation::PaddleSegModel(model_dir） 如何解决加载模型路径为中文  ",
        "body": "******\r\nfastdeploy::vision::segmentation::PaddleSegModel(model_dir）中文路径就会失败，该如何解决\r\n![image](https://user-images.githubusercontent.com/60127735/218004720-5d8c99cf-90f7-4247-883a-1df555ed0bc6.png)\r\n\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-win10-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： 用的cpu\r\n- 【编译语言】： C++ \r\n\r\n",
        "state": "closed",
        "user": "teymur-git",
        "closed_by": "heliqi",
        "created_at": "2023-02-10T05:00:05+00:00",
        "updated_at": "2024-04-02T01:41:47+00:00",
        "closed_at": "2024-04-01T09:24:31+00:00",
        "comments_count": [
            "DefTruth",
            "teymur-git",
            "teymur-git",
            "hhxdestiny",
            "GengGode",
            "MistEO"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1298,
        "title": "RuntimeError: FastDeploy initalized failed!",
        "body": "环境：ubuntu\r\ncuda：11.4 cudnn：8.1\r\n安装fastdeploy-gpu-python     1.0.3\r\n报错信息：\r\n`Traceback (most recent call last):\r\n  File \"test.py\", line 2, in <module>\r\n    import fastdeploy.vision as vision\r\n  File \"/home/xia.shuaipeng/ls/envs/test_api/lib/python3.8/site-packages/fastdeploy/__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/home/xia.shuaipeng/ls/envs/test_api/lib/python3.8/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!`\r\n",
        "state": "closed",
        "user": "Firestick-Xia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-10T09:54:50+00:00",
        "updated_at": "2024-02-20T06:43:08+00:00",
        "closed_at": "2024-02-20T06:43:08+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1299,
        "title": "使用fastdeploy部署ppocrv3， npu推断，时间很长",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- fastdeploy\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n-  Linux x64 麒麟10\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 华为atlas300i，cann5.1.2\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\nPython3.9\r\n## 问题日志及出现问题的操作流程\r\n使用fastdeploy，ppocrv3， npu推断，时间很长\r\n- 附上详细的问题日志有助于快速定位分析\r\n[log.txt](https://github.com/PaddlePaddle/FastDeploy/files/10707227/log.txt)\r\n\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "closed",
        "user": "EasyIsAllYouNeed",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-10T12:15:57+00:00",
        "updated_at": "2024-02-20T06:43:09+00:00",
        "closed_at": "2024-02-20T06:43:09+00:00",
        "comments_count": [
            "yunyaoXYY"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1306,
        "title": "fastdepoly无识别结果",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/quick_start/models/cpp.md\r\n运行官方demo，没有识别结果",
        "state": "closed",
        "user": "GreenAvocado92",
        "closed_by": "GreenAvocado92",
        "created_at": "2023-02-13T06:22:08+00:00",
        "updated_at": "2023-02-13T06:35:08+00:00",
        "closed_at": "2023-02-13T06:34:57+00:00",
        "comments_count": [
            "GreenAvocado92"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1303,
        "title": "[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(278)::fastdeploy::OrtBackend::Infer      Failed to Infer: Got invalid dimensions for input: images for the following indices",
        "body": "[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(278)::fastdeploy::OrtBackend::Infer      Failed to Infer: Got invalid dimensions for input: images for the following indices\r\n\r\nyolov8 onnx  1280\r\n报错\r\n\r\n解决方法 \r\n`model.preprocessor.size = [1280, 1280]`",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-12T01:15:57+00:00",
        "updated_at": "2023-02-13T08:08:38+00:00",
        "closed_at": "2023-02-13T08:08:38+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1307,
        "title": "发现fastdeploy加载模型的路径不能为中文，这个问题有解决办法吗？",
        "body": "*****\r\n发现fastdeploy加载模型的路径不能为中文，这个问题有解决办法吗？\r\n****\r\n",
        "state": "closed",
        "user": "teymur-git",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-13T07:22:44+00:00",
        "updated_at": "2024-02-20T06:43:10+00:00",
        "closed_at": "2024-02-20T06:43:10+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1314,
        "title": "gpu compile error",
        "body": "Environment: debian 11, OpenCV 4, cmake 3.18, cuda-12-0, gcc 10.2.\r\n\r\nCMake Option: \r\n```shell\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n-DENABLE_PADDLE_BACKEND=ON \\\r\n-DENABLE_OPENVINO_BACKEND=ON \\\r\n-DENABLE_TRT_BACKEND=ON \\\r\n-DWITH_GPU=ON \\\r\n-DTRT_DIRECTORY=/Paddle/TensorRT-8.4.1.5 \\\r\n-DCUDA_DIRECTORY=/usr/local/cuda \\\r\n-DCMAKE_INSTALL_PREFIX=/opt/paddlepaddle/fastdeploy-gpu  \\\r\n-DENABLE_VISION=ON \\\r\n-DOPENCV_DIRECTORY=/usr/local/lib/cmake/opencv4 \\\r\n-DENABLE_TEXT=ON\r\n```\r\n\r\n```shell\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\nIn file included from /usr/include/c++/10/ios:41,\r\n                 from /usr/include/c++/10/ostream:38,\r\n                 from /usr/include/c++/10/iostream:39,\r\n                 from /home/sixqaq/FastDeploy/third_party/yaml-cpp/src/stream.cpp:1:\r\n/usr/include/c++/10/bits/localefwd.h:190:38: internal compiler error: 段错误\r\n  190 |   template<typename _CharT, typename _InIter =  istreambuf_iterator<_CharT> >\r\n      |                                      ^~~~~~~\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n0x7f05ec0bbd5f ???\r\n        ./signal/../sysdeps/unix/sysv/linux/x86_64/sigaction.c:0\r\n0x7f05ec0a6d09 __libc_start_main\r\n        ../csu/libc-start.c:308\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <file:///usr/share/doc/gcc-10/README.Bugs> for instructions.\r\nmake[2]: *** [third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/build.make:459：third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o] 错误 1\r\nmake[2]: *** 正在等待未完成的任务....\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/sixqaq/FastDeploy/gpu-build/third_libs/paddle2onnx/src/paddle2onnx-linux-x64-1.0.5.tgz'\r\n     dst='/home/sixqaq/FastDeploy/gpu-build/third_libs/paddle2onnx/src/extern_paddle2onnx'\r\n-- extracting... [tar xfz]\r\n```",
        "state": "closed",
        "user": "sixsixQAQ",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T02:55:45+00:00",
        "updated_at": "2024-02-20T06:43:11+00:00",
        "closed_at": "2024-02-20T06:43:11+00:00",
        "comments_count": [
            "sixsixQAQ",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1315,
        "title": "RK3568部署自训练YOLO模型中遇到的segmentation fault问题(rkyolo部署)!",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-release-1.0.3\r\n- 【编译命令】按照仓库提供的环境编译方式完成C++和Python环境的编译，编译日志见后面附件。\r\n- 【系统平台】: Linux firefly 4.19.232 / Debian 10\r\n- 【硬件】： firefly AIO-3568J\r\n- 【编译语言】： C++ & Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- c++/python两种方式实现自己训练的YOLOv5s模型时出现Segmentation fault问题，具体运行过程如下所示:\r\n- 【1. 自训练模型及测试数据链接】\r\n- - 链接: https://pan.baidu.com/s/1eqfYr-Bf2W37conTH19XQQ?pwd=aupk 提取码: aupk\r\n- - 备注:连接中的模型为自己转换后的模型文件，参考模型转换文档链接:https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo/RKNN_model_convert\r\n- 【2. C++/pyrthon部署操作】\r\n- - 均按照官方文档操作，部署rk官方提供的测试模型可以正常推理出结果。\r\n- - https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.3/examples/vision/detection/rkyolo/cpp/README_CN.md\r\n- - https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.3/examples/vision/detection/rkyolo/python/README_CN.md\r\n- 【3. 执行结果日志】\r\n- - ./infer_rkyolo model/yolov5s_sar_RK356X_i8.rknn images/Gao_ship_hh_02017010717010304.jpg\r\n- - python3 infer.py --model_file ./model_sar/yolov5s_sar_RK356X_i8.rknn --image Gao_ship_hh_02017010717010304.jpg\r\n- - 执行结果日志链接: https://pan.baidu.com/s/1EZkL5YUEWf-F0eQXS2EEAg?pwd=zyv5 提取码: zyv5 \r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-14T02:56:37+00:00",
        "updated_at": "2023-02-17T06:08:09+00:00",
        "closed_at": "2023-02-17T06:08:09+00:00",
        "comments_count": [
            "jiangjiajun",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "MrMzl",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "MrMzl"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1316,
        "title": "onnx部署问题",
        "body": "根据官网教程编译，模型也没官网提供，在win下运行，报错如下：\r\n![image](https://user-images.githubusercontent.com/91200482/218648365-023a53b8-82d6-4766-a71d-1a9e87db2851.png)\r\n",
        "state": "closed",
        "user": "GreenAvocado92",
        "closed_by": "GreenAvocado92",
        "created_at": "2023-02-14T05:34:28+00:00",
        "updated_at": "2023-02-20T05:21:45+00:00",
        "closed_at": "2023-02-20T05:21:45+00:00",
        "comments_count": [
            "jiangjiajun",
            "GreenAvocado92",
            "jiangjiajun",
            "GreenAvocado92",
            "ShawnXsw",
            "GreenAvocado92",
            "GreenAvocado92"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1319,
        "title": "The valid cpu backends of model yolov8 are [ Backend::OPENVINO ,Backend::ORT ], Backend::TRT is not supported.",
        "body": "```python\r\nfd.vision.detection.YOLOv8\r\n\r\ndef build_option1280():\r\n    option = fd.RuntimeOption()\r\n    option.use_trt_backend()\r\n    option.set_trt_input_shape(\"images\", [1, 3, 1280, 1280])\r\n```\r\n\r\n```\r\n\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\n[ERROR] fastdeploy/fastdeploy_model.cc(126)::fastdeploy::FastDeployModel::InitRuntimeWithSpecifiedBackend       The valid cpu backends of model yolov8 are [ Backend::OPENVINO ,Backend::ORT ], Backend::TRT is not supported.\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov8/yolov8.cc(40)::fastdeploy::vision::detection::YOLOv8::Initialize     Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"f:/2022/slidingwindow/jianFDAI.py\", line 36, in <module>\r\n    deploy_model_1 = fd.vision.detection.YOLOv8(\"F:/2022/2023021208best.onnx\", runtime_option=runtime_option1280)\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\site-packages\\fastdeploy\\vision\\detection\\contrib\\yolov8.py\", line 181, in __init__\r\n    assert self.initialized, \"YOLOv8 initialize failed.\"\r\nAssertionError: YOLOv8 initialize failed.\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T08:09:33+00:00",
        "updated_at": "2024-02-20T06:43:12+00:00",
        "closed_at": "2024-02-20T06:43:12+00:00",
        "comments_count": [
            "wjj19950828"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1321,
        "title": "支持sahi检测不",
        "body": "PaddleDetection 已经实现\r\nhttps://github.com/PaddlePaddle/PaddleDetection/blob/develop/deploy/python/infer.py\r\nhttps://github.com/PaddlePaddle/PaddleDetection/blob/develop/tools/infer.py\r\n\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T08:41:17+00:00",
        "updated_at": "2024-02-20T06:43:13+00:00",
        "closed_at": "2024-02-20T06:43:13+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1325,
        "title": "Could not load library libcublasLt.so.12",
        "body": "系统：debian 11 自编译FastDeploy，gcc 10.2, cmake 3.18，cuda 11.5, cudnn 8.8。\r\n编译选项：\r\n```shell\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n-DENABLE_PADDLE_BACKEND=ON \\\r\n-DENABLE_OPENVINO_BACKEND=ON \\\r\n-DWITH_GPU=ON \\\r\n-DCUDA_DIRECTORY=/usr/local/cuda-11.5 \\ \r\n-DCMAKE_INSTALL_PREFIX=/opt/paddlapaddle/fastdeploy \\\r\n -DENABLE_VISION=ON \\\r\n-DOPENCV_DIRECTORY=/usr/local/lib/cmake/opencv4 \\\r\n-DENABLE_TEXT=ON\r\n```\r\n\r\n运行时报错：\r\n```shell\r\n=====before=====\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW    Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert       BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nCould not load library libcublasLt.so.12. Error: libcublasLt.so.12: cannot open shared object file: No such file or directory\r\n已放弃\r\n```\r\n\r\n出错代码：\r\n```cpp\r\nstd::string\r\nemotion_GpuInfer (const std::string &model_dir, const cv::Mat &image_file)\r\n{\r\n\tauto model_file = model_dir + sep + \"model.pdmodel\";\r\n\tauto params_file = model_dir + sep + \"model.pdiparams\";\r\n\tauto config_file = model_dir + sep + \"inference.yml\";\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\tcerr << \"=====before=====\" << endl;\r\n\t\r\n\tauto model = fastdeploy::vision::classification::PaddleClasModel (\r\n\t                 model_file, params_file, config_file, option);\r\n\tcerr << \"=====after=====\" << endl;\r\n......\r\n}\r\n```",
        "state": "closed",
        "user": "sixsixQAQ",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-14T12:00:30+00:00",
        "updated_at": "2024-04-16T09:02:57+00:00",
        "closed_at": "2024-04-16T09:02:57+00:00",
        "comments_count": [
            "sixsixQAQ",
            "sixsixQAQ",
            "jiangjiajun",
            "sixsixQAQ"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1327,
        "title": "ExternalError: CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED.",
        "body": "环境：debian 11, gcc 10.2, cuda 12.0, cudnn 8.8\r\n\r\n### 运行结果：\r\n\r\n\r\n\r\n```shell\r\n[INFO] fastdeploy/runtime/runtime.cc(264)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\nbefore\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  \r\n\r\n  Compile Traceback (most recent call last):\r\n    File \"tools/export.py\", line 116, in <module>\r\n      main(args)\r\n    File \"tools/export.py\", line 94, in main\r\n      paddle.jit.save(net, save_path)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/jit.py\", line 631, in wrapper\r\n      func(layer, path, input_spec, **configs)\r\n    File \"<decorator-gen-106>\", line 2, in save\r\n      \r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/jit.py\", line 861, in save\r\n      inner_input_spec, with_hook=with_hook)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 528, in concrete_program_specify_input_spec\r\n      *desired_input_spec, with_hook=with_hook)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 436, in get_concrete_program\r\n      concrete_program, partial_program_layer = self._program_cache[cache_key]\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 801, in __getitem__\r\n      self._caches[item_id] = self._build_once(item)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 790, in _build_once\r\n      **cache_key.kwargs)\r\n    File \"<decorator-gen-104>\", line 2, in from_func_spec\r\n      \r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 733, in from_func_spec\r\n      outputs = static_func(*inputs)\r\n    File \"/ssd1/home/chenguowei01/github/PaddleSeg/Matting/tools/../ppmatting/models/ppmattingv2.py\", line 152, in forward\r\n      paddle.shape(feats_backbone[-1])[-2:])  # 32x\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/github/PaddleSeg/Matting/tools/../ppmatting/models/layers/tensor_fusion.py\", line 105, in forward\r\n      atten = F.sigmoid(self.conv_atten(atten))\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/container.py\", line 98, in forward\r\n      input = layer(input)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/github/PaddleSeg/paddleseg/models/layers/layer_libs.py\", line 109, in forward\r\n      x = self._conv(x)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/layer/conv.py\", line 678, in forward\r\n      use_cudnn=self._use_cudnn)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/functional/conv.py\", line 169, in _conv_nd\r\n      type=op_type, inputs=inputs, outputs=outputs, attrs=attrs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/layer_helper.py\", line 44, in append_op\r\n      return self.main_program.current_block().append_op(*args, **kwargs)\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 3621, in append_op\r\n      attrs=kwargs.get(\"attrs\", None))\r\n    File \"/ssd1/home/chenguowei01/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 2635, in __init__\r\n      for frame in traceback.extract_stack():\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::AnalysisPredictor::ZeroCopyRun()\r\n1   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)\r\n2   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const\r\n3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&, paddle::framework::RuntimeContext*) const\r\n4   void phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::KernelCallHelper<paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::TypeTag<int> >::Compute<1, 3, 0, 0, phi::GPUContext const, phi::DenseTensor const, phi::DenseTensor const, phi::DenseTensor const>(phi::KernelContext*, phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&)\r\n5   void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)\r\n6   phi::DnnWorkspaceHandle::RunFunc(std::function<void (void*)> const&, unsigned long)\r\n7   std::_Function_handler<void (void*), phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)::{lambda(void*)#4}>::_M_invoke(std::_Any_data const&, void*&&)\r\n8   phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n9   phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. \r\n  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnStatus_t) to get Nvidia's official solution and advice about CUDNN Error.] (at /build/Paddle/paddle/phi/kernels/fusion/gpu/conv_fusion_kernel.cu:612)\r\n  [operator < conv2d_fusion > error]\r\n已放弃\r\n```\r\n### 出错代码\r\n```cpp\r\ncv::Mat\r\nGpuInfer (const std::string &model_dir, const cv::Mat &image, const std::string &background_file)\r\n{\r\n\tauto model_file = model_dir + sep + \"model.pdmodel\";\r\n\tauto params_file = model_dir + sep + \"model.pdiparams\";\r\n\tauto config_file = model_dir + sep + \"deploy.yaml\";\r\n\t\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\t\r\n\toption.UsePaddleInferBackend();\r\n\tauto model = fastdeploy::vision::matting::PPMatting (model_file, params_file,\r\n\t             config_file, option);\r\n\tcv::Mat vis_im;\r\n\tif (!model.Initialized()) {\r\n\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\treturn vis_im;\r\n\t}\r\n\tauto im = image;\r\n\tfastdeploy::vision::MattingResult res;\r\n\tcerr << \"before\" << endl;\r\n\r\n\tif (!model.Predict (&im, &res)) {\r\n\t\tstd::cerr << \"Failed to predict.\" << std::endl;\r\n\t\treturn vis_im;\r\n\t}\r\n\tcerr << \"after\" << endl;\r\n......\r\n}\r\n```\r\nNVIDIA 官网对`CUDNN_STATUS_NOT_SUPPORTED`的解释：`The functionality requested is not presently supported by cuDNN.`",
        "state": "open",
        "user": "sixsixQAQ",
        "closed_by": null,
        "created_at": "2023-02-14T15:34:41+00:00",
        "updated_at": "2024-09-03T05:12:38+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "EveningLin",
            "guoyunqingyue",
            "intothephone",
            "shiyutang",
            "xxddccaa",
            "WangShengFeng1",
            "zhangjin2233",
            "yes-github"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1328,
        "title": "什么时候支持python 3.11",
        "body": "什么时候支持python 3.11",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-15T00:09:04+00:00",
        "updated_at": "2024-02-20T06:43:14+00:00",
        "closed_at": "2024-02-20T06:43:14+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1329,
        "title": "serving docker error：unable to find 'libtriton_onnxruntime.so'",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 如fastdeploy-linux-gpu-1.0.3\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia GPU 3090， CUDA 11.4 CUDNN 8.4\r\ndocker：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.3-gpu-cuda11.4-trt8.4-21.10\r\nmodel：yolov5，serving\r\n\r\nUNAVAILABLE: Invalid argument: unable to find 'libtriton_onnxruntime.so' for model 'runtime', searched: /models/runtime/1,  |                 |         | /models/runtime, /opt/tritonserver/backends/onnxruntime\r\n",
        "state": "closed",
        "user": "ShawnXsw",
        "closed_by": "heliqi",
        "created_at": "2023-02-15T02:01:22+00:00",
        "updated_at": "2023-03-15T01:24:22+00:00",
        "closed_at": "2023-02-24T10:07:38+00:00",
        "comments_count": [
            "rainyfly",
            "ShawnXsw",
            "rainyfly",
            "ShawnXsw",
            "rainyfly",
            "ShawnXsw",
            "ShawnXsw"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1332,
        "title": "主体检测模型picodet_lcnet_x2_5_640_mainbody_infer报错",
        "body": "主体检测模型是picodet_lcnet_x2_5_640_mainbody_infer，调用examples\\vision\\detection\\paddledetection\\python\\infer_picodet.py来检测，报如下错误\r\n[ERROR] fastdeploy/vision/detection/ppdet/postprocessor.cc(26)::fastdeploy::vision::detection::PaddleDetPostprocessor::ProcessMask\tThe data type of out mask tensor should be INT32, but now it's FDDataType::FP32\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(75)::fastdeploy::vision::detection::PPDetBase::BatchPredict\tFailed to postprocess the inference results by runtime.\r\n另外，lite版的主体检测模型picodet_PPLCNet_x2_5_mainbody_lite_v1.0_infer，也不行",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-15T07:03:12+00:00",
        "updated_at": "2024-07-09T06:40:36+00:00",
        "closed_at": "2024-07-09T06:40:36+00:00",
        "comments_count": [
            "jiangjiajun",
            "truthsun22",
            "jiangjiajun",
            "truthsun22",
            "jiangjiajun",
            "huge3286",
            "tomjeans"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1333,
        "title": "Got error when running infer_onnx_tensorrt example",
        "body": "## Environment\r\n\r\nFastDeploy version: \r\nfastdeploy-gpu-python 1.0.3\r\n\r\nOS Platform: \r\nLinux cm.bigdata 3.10.0-1160.76.1.el7.x86_64 #1 SMP Wed Aug 10 16:21:17 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\nCentOS Linux release 7.9.2009 (Core)\r\n\r\nHardware: \r\nNvidia GPU RTX A4000  CUDA 11.2 CUDNN 8.2\r\n\r\nProgram Language:\r\nPython 3.10\r\n\r\n## Problem description\r\nWhen running the infer_onnx_tensorrt example\r\n[FastDeploy](https://github.com/PaddlePaddle/FastDeploy)/[examples](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples)/[runtime](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/runtime)/[python](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/runtime/python)/infer_onnx_tensorrt.py\r\nError message occur as follow:\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(256)::InitFromOnnx  [ERROR] Error occurs while calling cudaStreamCreate().",
        "state": "closed",
        "user": "zhz17",
        "closed_by": "zhz17",
        "created_at": "2023-02-15T07:34:04+00:00",
        "updated_at": "2023-02-15T09:29:07+00:00",
        "closed_at": "2023-02-15T09:28:53+00:00",
        "comments_count": [
            "zhz17"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1334,
        "title": "在windows上编译exe成功，最后预测时报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 如fastdeploy-windows-gpu\r\n- 【编译命令】自行编译的FastDeploy，采用的是cmake GUI\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：Nvidia GPU 3090TI， CUDA 11.7  CUDNN 8.6\r\n- 【编译语言】： C++\r\n\r\n\r\n![图片](https://user-images.githubusercontent.com/66996161/218964407-8ee90c48-4369-4f3d-97fd-a6828bb6d228.png)\r\n",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "happybear1015",
        "created_at": "2023-02-15T07:46:32+00:00",
        "updated_at": "2023-02-15T08:20:33+00:00",
        "closed_at": "2023-02-15T08:20:00+00:00",
        "comments_count": [
            "happybear1015"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1335,
        "title": "win11人脸检测SCRFD模型predict方法异常结束",
        "body": "- 【FastDeploy版本】： fastdeploy-win-x64-1.0.3\r\n- 【编译命令】官方C++ Release SDK\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】： 使用CPU版本\r\n- 【编译语言】： C++\r\n- 【编译工具】： vs2019/cmake3.24.3/gcc8.1\r\n\r\n首先感谢大佬提供的超赞项目！\r\n![1676448283070](https://user-images.githubusercontent.com/68574803/218968480-f93fd315-5322-44d6-bad4-411be9fcd8e2.jpg)\r\n\r\n![1676447310675](https://user-images.githubusercontent.com/68574803/218968079-55692810-b074-4e9e-b283-93232738601f.jpg)\r\n\r\n体验中遇到一个问题：按教程[https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows_build.md](url)中sln及cmake方法，SCRFD模型均无法得到预测结果，运行至`model.Predict(&im, &res)`处就终止，并未打印res以及后续输出人脸检测结果；同样按教程配置的yolov5s-face模型可正常输出预测结果:\r\n![1676448466154](https://user-images.githubusercontent.com/68574803/218974751-cbd0006f-ff50-43eb-9655-69be50f1c71a.jpg)\r\n请问是何原因？\r\n",
        "state": "closed",
        "user": "Plz-Thx",
        "closed_by": "Plz-Thx",
        "created_at": "2023-02-15T08:39:55+00:00",
        "updated_at": "2023-02-17T01:01:14+00:00",
        "closed_at": "2023-02-17T01:01:13+00:00",
        "comments_count": [
            "DefTruth",
            "Plz-Thx"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1336,
        "title": "RKNPU2 支持TinyPose",
        "body": "期望可以支持TinyPose在RKNPU2设备上运行，例如RK3588\r\n\r\n原始Paddle模型为wget https://bj.bcebos.com/paddlehub/fastdeploy/PP_TinyPose_256x192_infer.tgz\r\n转换后的onnx模型，配置文件和rknn模型见附件。\r\n\r\n非常感谢！\r\n\r\n\r\n[PP_TinyPose_256x192_RK3588.zip](https://github.com/PaddlePaddle/FastDeploy/files/10741623/PP_TinyPose_256x192_RK3588.zip)\r\n",
        "state": "closed",
        "user": "liutingxi",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-15T09:44:21+00:00",
        "updated_at": "2023-02-17T00:59:39+00:00",
        "closed_at": "2023-02-17T00:59:39+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "liutingxi"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1338,
        "title": "[Serving] FastDeployServing 配置项添加collect shape开关",
        "body": "## 现状\r\n\r\n目前 FastDeployServing PaddleTensorRT后端强制开启collect shape开关。需要添加一个配置项允许关闭collect shape功能",
        "state": "closed",
        "user": "joey12300",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-15T12:06:34+00:00",
        "updated_at": "2024-02-20T06:43:14+00:00",
        "closed_at": "2024-02-20T06:43:14+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1340,
        "title": "能否有多进程multiprocessing调用FastDeploy的示例",
        "body": "要对图片文件夹进行检测\r\n\r\n只要有图片进来 就检测\r\n\r\n多线程只是并发  \r\n属于伪多线程  实际上就一个线程\r\n\r\n多进程\r\n能并行处理多张图片\r\n但是每个进程 创建FastDeploy 又会占用显存\r\n十几张图同时处理 就会显卡爆满\r\nGUI交互也麻烦\r\n\r\n服务端部署的话\r\n还要传送图片数据 交互太慢",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-16T00:55:43+00:00",
        "updated_at": "2023-02-16T02:00:17+00:00",
        "closed_at": "2023-02-16T02:00:16+00:00",
        "comments_count": [
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1341,
        "title": "模型ONNX加密问题",
        "body": "是否支持onnx \r\n\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/tutorials/encrypt_model/README_CN.md\r\n我看支持了 但是不知道怎么用\r\n\r\n理论上来说 都是对文件的加密\r\n那么其他模型文件 也是应该可以的\r\n\r\n@felixhjh\r\n\r\n```\r\n# pip install fastdeploy-gpu-python==0.0.0 -f https://www.paddlepaddle.org.cn/whl/fastdeploy_nightly_build.html\r\n# 已经安装最新版\r\n\r\nfile = \"F:/123.onnx\"\r\n\r\nmodel_file = open(file,'rb').read()\r\n\r\nencrypted_model, key = fd.encryption.encrypt(model_file)\r\n\r\nprint(\"key\",key)\r\n\r\nprint(encrypted_model)\r\n```\r\n```\r\n\r\n  File \"f:/A/jiami.py\", line 12, in <module>\r\n    encrypted_model, key = fd.encryption.encrypt(model_file)\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\site-packages\\fastdeploy\\encryption\\encryption.py\", line 31, in encrypt\r\n    key = generate_key()\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\site-packages\\fastdeploy\\encryption\\encryption.py\", line 21, in generate_key\r\n    return C.encryption.generate_key()\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'encryption'\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-16T02:43:34+00:00",
        "updated_at": "2024-08-13T06:42:47+00:00",
        "closed_at": "2024-08-13T06:42:47+00:00",
        "comments_count": [
            "jiangjiajun",
            "Junxiang-Zeng8443"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1342,
        "title": "构建镜像出现 CMake Error: Could not find CMAKE_ROOT !!! 问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】bash scripts/build_fd_cuda_11_2.sh\r\n- 【系统平台】: centos 7\r\n- 【硬件】： Quadro RTX 6000\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n由于修改了源代码，想重新构建镜像。参考如下文档：\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN/compile.md\r\n当执行：bash scripts/build_fd_cuda_11_2.sh 时，出现如下错误\r\n看着有两个问题，\r\n一个fatal: detected dubious ownership in repository at '/workspace/fastdeploy'：不确定这个是否需要处理\r\n一个是：CMake Error: Could not find CMAKE_ROOT !!!\r\n\r\nSetting up git (1:2.25.1-1ubuntu3.10) ...\r\nSetting up python3-pip (20.0.2-5ubuntu1.7) ...\r\nSetting up python3.8-dev (3.8.10-0ubuntu1~20.04.6) ...\r\nSetting up libpython3-dev:amd64 (3.8.2-0ubuntu2) ...\r\nSetting up python3-dev (3.8.2-0ubuntu2) ...\r\nProcessing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\nfatal: detected dubious ownership in repository at '/workspace/fastdeploy'\r\nTo add an exception for this directory, call:\r\n\r\n\tgit config --global --add safe.directory /workspace/fastdeploy\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\nCMake Error: Could not find CMAKE_ROOT !!!\r\nCMake has most likely not been installed correctly.\r\nModules directory not found in\r\n/workspace/fastdeploy/serving/cmake-3.18.6-Linux-x86_64/share/cmake-3.18\r\nCMake Error: Error executing cmake::LoadCache(). Aborting.\r\n\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 417, in <module>\r\n    setuptools.setup(\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 144, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.8/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.8/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 266, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/workspace/fastdeploy/serving/cmake-3.18.6-Linux-x86_64/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.8', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-38-x86_64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DWITH_ASCEND=OFF', '-DENABLE_ORT_BACKEND=OFF', '-DENABLE_OPENVINO_BACKEND=OFF', '-DENABLE_PADDLE_BACKEND=OFF', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=OFF', '-DENABLE_LITE_BACKEND=OFF', '-DPADDLELITE_URL=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=ON', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=ON', '-DWITH_IPU=OFF', '-DWITH_KUNLUNXIN=OFF', '-DBUILD_ON_JETSON=OFF', '-DTRT_DIRECTORY=/workspace/fastdeploy/serving/TensorRT-8.4.1.5/', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=', '-DRKNN2_TARGET_SOC=', '/workspace/fastdeploy']' returned non-zero exit status 1.\r\nfatal: detected dubious ownership in repository at '/workspace/fastdeploy'\r\nTo add an exception for this directory, call:\r\n\r\n\tgit config --global --add safe.directory /workspace/fastdeploy\r\nDidn't detect path: fastdeploy/libs/third_libs exist, please execute `python setup.py build` first\r\nCMake Error: Could not find CMAKE_ROOT !!!\r\nCMake has most likely not been installed correctly.\r\nModules directory not found in\r\n/workspace/fastdeploy/serving/cmake-3.18.6-Linux-x86_64/share/cmake-3.18\r\nCMake Error: Error executing cmake::LoadCache(). Aborting.\r\n\r\nmake: *** No targets specified and no makefile found.  Stop.\r\nmake: *** No rule to make target 'install'.  Stop.\r\nCMake Error: Could not find CMAKE_ROOT !!!\r\nCMake has most likely not been installed correctly.\r\nModules directory not found in\r\n/workspace/fastdeploy/serving/cmake-3.18.6-Linux-x86_64/share/cmake-3.18\r\nCMake Error: Error executing cmake::LoadCache(). Aborting.\r\n\r\nmake: *** No targets specified and no makefile found.  Stop.\r\n\r\n\r\n",
        "state": "closed",
        "user": "Lennon-cheng",
        "closed_by": "Lennon-cheng",
        "created_at": "2023-02-16T03:23:50+00:00",
        "updated_at": "2023-02-16T11:36:22+00:00",
        "closed_at": "2023-02-16T11:36:22+00:00",
        "comments_count": [
            "heliqi",
            "Lennon-cheng",
            "heliqi",
            "Lennon-cheng",
            "heliqi",
            "Lennon-cheng"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1343,
        "title": "fd暂时不支持在RK3588上使用OCR",
        "body": "需要在RK3588上使用OCR，请PADDLE的工程早日支持\r\n\r\n",
        "state": "closed",
        "user": "alanzheng800",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-16T08:59:35+00:00",
        "updated_at": "2023-02-27T07:06:40+00:00",
        "closed_at": "2023-02-27T07:06:40+00:00",
        "comments_count": [
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1345,
        "title": "onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: scale_factor Got: 1 Expected: 2 Please fix either the inputs or the model.",
        "body": "## Environment\r\n算能tpu-mlir容器\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 \r\nHardware: e.g. Nvidia GPU 3060 \r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\n按照官方文档进行容器的创建，然后是模型转换出现下面问题\r\n`root@ce2aa914b856:/workspace/ppyoloe_crn_s_300e_coco/workspace# model_transform.py     --model_name ppyoloe_crn_s_300e_coco     --model_def ../ppyoloe_crn_s_300e_coco.onnx     --input_shapes [[1,3,640,640],[1,2]]     --keep_aspect_ratio     --pixel_format rgb     --output_names p2o.Div.1,p2o.Concat.29     --test_input ../inputs.npz     --test_result ppyoloe_crn_s_300e_coco_top_outputs.npz     --mlir ppyoloe_crn_s_300e_coco.mlir\r\nSOPHGO Toolchain v0.8.beta.0-20230215\r\n2023/02/16 17:26:44 - INFO : \r\n\t _____________________________________________________ \r\n\t| preprocess:                                           |\r\n\t|   (x - mean) * scale                                  |\r\n\t'-------------------------------------------------------'\r\n  config Preprocess args : \r\n\tresize_dims           : same to net input dims\r\n\tkeep_aspect_ratio     : True\r\n\tpad_value             : 0\r\n\tpad_type              : center\r\n\t--------------------------\r\n\tmean                  : [0.0, 0.0, 0.0]\r\n\tscale                 : [1.0, 1.0, 1.0]\r\n\t--------------------------\r\n\tpixel_format          : rgb\r\n\tchannel_format        : nchw\r\n\tmodel_format          : image\r\n\r\nRun onnxsim 1 times, model simplified: True\r\nRun onnxsim 1 times, model simplified: True\r\nSave mlir file: ppyoloe_crn_s_300e_coco_origin.mlir\r\n[Running]: tpuc-opt --init --canonicalize --mark-FLOPs --save-weight --mlir-print-debuginfo ppyoloe_crn_s_300e_coco_origin.mlir -o ppyoloe_crn_s_300e_coco.mlir \r\n[Success]: tpuc-opt --init --canonicalize --mark-FLOPs --save-weight --mlir-print-debuginfo ppyoloe_crn_s_300e_coco_origin.mlir -o ppyoloe_crn_s_300e_coco.mlir \r\nMlir file generated:ppyoloe_crn_s_300e_coco.mlir\r\nTraceback (most recent call last):\r\n  File \"/workspace/tpu-mlir/python/tools/model_transform.py\", line 192, in <module>\r\n    tool.model_validate(args.test_input, args.tolerance, args.excepts, args.test_result)\r\n  File \"/workspace/tpu-mlir/python/tools/model_transform.py\", line 65, in model_validate\r\n    ref_outputs = self.origin_inference(inputs)\r\n  File \"/workspace/tpu-mlir/python/tools/model_transform.py\", line 102, in origin_inference\r\n    return onnx_inference(inputs, self.converter.onnx_file)\r\n  File \"/workspace/tpu-mlir/python/tools/model_runner.py\", line 221, in onnx_inference\r\n    outs = session.run(None, data)\r\n  File \"/usr/local/lib/python3.7/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 200, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: scale_factor Got: 1 Expected: 2 Please fix either the inputs or the model.\r\n`\r\n![image](https://user-images.githubusercontent.com/38728358/219325037-ebe7e8c8-4e15-4362-8284-40718cd2c8ed.png)\r\n\r\n",
        "state": "closed",
        "user": "jo-dean",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-16T09:31:00+00:00",
        "updated_at": "2024-07-16T06:40:53+00:00",
        "closed_at": "2024-07-16T06:40:53+00:00",
        "comments_count": [
            "KhantiInNaraka",
            "jo-dean",
            "jo-dean",
            "xyzeng4657",
            "xyzeng4657",
            "srd2018"
        ],
        "labels": [
            "Sophgo"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1346,
        "title": "MacOS M1平台 ，使用预编译的lib编译example报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [fastdeploy-osx-arm64-1.0.3.tgz](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-osx-arm64-1.0.3.tgz)\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Mac OSX arm(12.0) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n编译examples/vision/detection/fastestdet/cpp路径下时，出现如下错误\r\n\r\n```\r\n-- The path of ONNXRuntime is /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/onnxruntime/lib.\r\n-- The path of OpenCV is /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv.\r\n-- Found OpenCV: /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv (found version \"3.4.16\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.23.1\r\n--   CMake command             : /opt/homebrew/Cellar/cmake/3.23.1/bin/cmake\r\n--   System                    : Darwin\r\n--   C++ compiler              : /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++\r\n--   C++ compiler version      : 13.0.0.13000029\r\n--   CXX flags                 : -Wno-format\r\n--   WITH_GPU                  : OFF\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   DEPENDENCY_LIBS           : /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/lib/libfastdeploy.dylib;/Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/onnxruntime/lib/libonnxruntime.dylib;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;/Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.dylib;/Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/paddle2onnx/lib/libpaddle2onnx.dylib\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/examples/vision/detection/fastestdet/cpp/build\r\n(base) yangjun@yangjun build % make -j4\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer.cc.o\r\n[100%] Linking CXX executable infer_demo\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/lib/libfastdeploy.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/onnxruntime/lib/libonnxruntime.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_highgui.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_dnn.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_objdetect.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_ml.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_shape.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_stitching.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_superres.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_videostab.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/paddle2onnx/lib/libpaddle2onnx.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_calib3d.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_flann.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_features2d.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_photo.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_video.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_imgcodecs.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_videoio.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_core.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nld: warning: ignoring file /Users/yangjun/Downloads/fastdeploy-osx-arm64-1.0.3/third_libs/install/opencv/lib/libopencv_imgproc.3.4.16.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64\r\nUndefined symbols for architecture arm64:\r\n  \"fastdeploy::RuntimeOption::UseTrtBackend()\", referenced from:\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::RuntimeOption::SetTrtInputShape(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::RuntimeOption::UseGpu(int)\", referenced from:\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::vision::VisDetection(cv::Mat const&, fastdeploy::vision::DetectionResult const&, float, int, float)\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::vision::DetectionResult::Str()\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::vision::detection::FastestDet::Predict(cv::Mat const&, fastdeploy::vision::DetectionResult*)\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::vision::detection::FastestDet::FastestDet(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, fastdeploy::RuntimeOption const&, fastdeploy::ModelFormat const&)\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"fastdeploy::FDTensor::FreeFn()\", referenced from:\r\n      fastdeploy::FDTensor::~FDTensor() in infer.cc.o\r\n  \"cv::Mat::~Mat()\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"cv::String::deallocate()\", referenced from:\r\n      cv::String::~String() in infer.cc.o\r\n  \"cv::String::allocate(unsigned long)\", referenced from:\r\n      cv::String::String(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      cv::String::String(char const*) in infer.cc.o\r\n  \"cv::imread(cv::String const&, int)\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"cv::imwrite(cv::String const&, cv::_InputArray const&, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n      CpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      GpuInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n      TrtInfer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in infer.cc.o\r\n  \"vtable for fastdeploy::FastDeployModel\", referenced from:\r\n      fastdeploy::FastDeployModel::~FastDeployModel() in infer.cc.o\r\n  NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\n  \"vtable for fastdeploy::vision::detection::FastestDet\", referenced from:\r\n      fastdeploy::vision::detection::FastestDet::~FastestDet() in infer.cc.o\r\n  NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[2]: *** [infer_demo] Error 1\r\nmake[1]: *** [CMakeFiles/infer_demo.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```\r\n",
        "state": "closed",
        "user": "ANDROIDTODO",
        "closed_by": "ANDROIDTODO",
        "created_at": "2023-02-16T10:41:38+00:00",
        "updated_at": "2023-02-23T03:28:38+00:00",
        "closed_at": "2023-02-23T03:28:38+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1347,
        "title": "【急！】在荣品A311D上部署PPYOLOE+_s时，遇到NPU跑不起来的情况",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 拉取fastdeploy-develop版本源码，自行编译\r\n- 【编译命令】cmake -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DWITH_TIMVX=ON  -DTARGET_ABI=arm64 -DENABLE_FLYCV=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-timvx -DENABLE_VISION=ON -DENABLE_LITE_BACCKEND=ON -Wno-dev ..\r\n- 【系统平台】: aarch64 linux kernel 4.9.113，gcc、g++=9.3.0，make=4.2.1，cmake=3.22.6，ubuntu 20.04 arm64\r\n- 【硬件】： 荣品A311D，NPU驱动已经安装为 6.4.4.3\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n### 问题描述：【模型跑不通】\r\n\r\n* 已经按照FD官方文档跑通 [PP-YOLOE 量化模型 C++ 部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/a311d/cpp/README_CN.md)。这个Demo的Arm cpu和NPU都可以跑通。\r\n\r\n* 然后我换上了自己训练的PPYOLOE+_s模型，infer_cfg.yml文件也替换了。此时可以在 Arm Cpu 上跑通，精度也OK。\r\n\r\n* 然后，按照文档的方法，打开NPU推理方式：\r\n```\r\n  // fastdeploy::vision::EnableFlyCV();\r\n  // fastdeploy::RuntimeOption option;\r\noption.UseTimVX();\r\n // option.SetLiteSubgraphPartitionPath(subgraph_file);\r\n```\r\n* 编译执行，此时已经有报错。\r\n记录Log信息，找到其中的Subgraph operators之后的内容，单独存到 subgrapph.txt中，用新的 txt文件替换原有的subgraph.txt文件。\r\n* 然后打开读取异构计算的txt文件的语句：\r\n```\r\n  // fastdeploy::vision::EnableFlyCV();\r\n  // fastdeploy::RuntimeOption option;\r\noption.UseTimVX();\r\noption.SetLiteSubgraphPartitionPath(subgraph_file);\r\n```\r\n* 再次执行，仍然报错，且报错内容相同。\r\n* 以上测试流程，不管是用PTQ还是QAT方式压缩的模型，都是一样的现象，一样的报错。\r\n\r\n- 日志信息，见附件：log_ppyoloe_plus.txt\r\n- 使用的模型包括subgraph.txt，在zip附件中：ppyoloe_plus_qat_model_atai.zip\r\n- 测试图片，见附件：test03.zip\r\n- 运行程序的脚本为：\r\n```\r\nexport GLOG_v=5; \r\nexport SUBGRAPH_ONLINE_MODE=true; \r\nexport RKNPU_LOGLEVEL=5; \r\nexport RKNN_LOG_LEVEL=5; \r\nulimit -c unlimited; \r\nexport VIV_VX_ENABLE_GRAPH_TRANSFORM=-pcq:1; \r\nexport VIV_VX_SET_PER_CHANNEL_ENTROPY=100; \r\nexport TIMVX_BATCHNORM_FUSION_MAX_ALLOWED_QUANT_SCALE_DEVIATION=300000; \r\nexport VSI_NN_LOG_LEVEL=5;\r\n\r\nexport LD_LIBRARY_PATH=/home/rpdzkj/Downloads/FastDeploy-develop/examples/vision/detection/paddledetection/a311d/cpp/build/install/lib/;\r\n\r\n#./build/install/infer_demo ./build/install/models/ppyoloe_noshare_qat ./build/install/images/000000014439.jpg\r\n#./build/infer_demo ./models/ppyoloe_plus_ptq_model_new ./images/test03.jpg\r\n./build/infer_demo ./models/ppyoloe_plus_qat_model_new ./images/test03.jpg\r\n```\r\n\r\n\r\n\r\n\r\n[ppyoloe_plus_qat_model_atai.zip](https://github.com/PaddlePaddle/FastDeploy/files/10755738/ppyoloe_plus_qat_model_atai.zip)\r\n[log_ppyoloe_plus.txt](https://github.com/PaddlePaddle/FastDeploy/files/10755739/log_ppyoloe_plus.txt)\r\n[test03.zip](https://github.com/PaddlePaddle/FastDeploy/files/10755779/test03.zip)\r\n\r\n",
        "state": "closed",
        "user": "Taichipeace",
        "closed_by": "Taichipeace",
        "created_at": "2023-02-16T12:15:23+00:00",
        "updated_at": "2023-02-17T15:02:58+00:00",
        "closed_at": "2023-02-17T15:02:57+00:00",
        "comments_count": [
            "yingshengBD",
            "Taichipeace"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1348,
        "title": "PP-OCRv3   fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, det_option); 出现：段错误 (核心已转储)",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-x64-1.0.3\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.10) \r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n\r\n只是添加了日志，其他的代码没有修改\r\n![111](https://user-images.githubusercontent.com/47800318/219533269-8e6f7728-a777-44e7-97bd-45f662ce06e6.png)\r\n![222](https://user-images.githubusercontent.com/47800318/219533303-278292ad-9b2e-40ca-ad18-59f7cb685763.png)\r\n![333](https://user-images.githubusercontent.com/47800318/219533287-d8f37cfc-ad92-4494-9dba-ac3824e092db.png)\r\n![4444](https://user-images.githubusercontent.com/47800318/219536945-fe80eb47-a66f-45c3-af64-75f661ce46c0.png)\r\n![555](https://user-images.githubusercontent.com/47800318/219536955-7c3b1784-7964-48fc-942d-03ee231ec1f2.png)\r\n\r\n![666](https://user-images.githubusercontent.com/47800318/219537947-bd0f25a7-6a9a-4570-bd51-4ded4fc68682.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "kekezhu0000",
        "closed_by": "yunyaoXYY",
        "created_at": "2023-02-17T02:55:13+00:00",
        "updated_at": "2023-02-20T02:52:53+00:00",
        "closed_at": "2023-02-20T02:52:53+00:00",
        "comments_count": [
            "wang-xinyu",
            "kekezhu0000",
            "wang-xinyu",
            "kekezhu0000",
            "yunyaoXYY",
            "yunyaoXYY",
            "kekezhu0000",
            "yunyaoXYY",
            "kekezhu0000",
            "kekezhu0000",
            "yunyaoXYY"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1350,
        "title": "fastdeploy 无法在 jetson nano 上面实现tensorRT功能",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： jetson nano编译版本，fastdeploy-python    0.0.0\r\n- 【编译命令】：Python编译安装\r\n- 【系统平台】: Linux (Ubuntu 18.04) \r\n- 【硬件】：  Nvidia jetson nano b01 4G， CUDA 10.2 CUDNN 8.2.1\r\n- 【编译语言】： Python 3.6.15\r\n\r\n## 问题日志及出现问题的操作流程\r\njetson nano硬件环境下多个example模型在tensorRT模式下无法正常预测（在GPU模式下都可以正常预测并返回结果）\r\n\r\n预测命令\r\nFastDeploy/examples/vision/detection/paddledetection/python$ python infer_ppyoloe.py --model_dir ppyoloe_crn_l_300e_coco --image 000000014439.jpg --device gpu --use_trt True\r\n\r\n日志log\r\n(paddle3.6) qiulongquan@qiulongquan-desktop:~/ml/FastDeploy/examples/vision/detection/paddledetection/python$ python infer_ppyoloe.py --model_dir ppyoloe_crn_l_300e_coco --image 000000014439.jpg --device gpu --use_trt True\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(552)::CreateTrtEngineFromOnnx     Cannot build engine right now, because there's dynamic input shape exists, list as below,\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(556)::CreateTrtEngineFromOnnx     Input 0: TensorInfo(name: image, shape: [-1, 3, 640, 640], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(556)::CreateTrtEngineFromOnnx     Input 1: TensorInfo(name: scale_factor, shape: [-1, 2], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(558)::CreateTrtEngineFromOnnx     FastDeploy will build the engine while inference with input data, and will also collect the input shape range information. You should be noticed that FastDeploy will rebuild the engine while new input shape is out of the collected shape range, this may bring some time consuming problem, refer https://github.com/PaddlePaddle/FastDeploy/docs/backends/tensorrt.md for more details.\r\n[INFO] fastdeploy/runtime.cc(289)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/vision/detection/ppdet/ppyoloe.cc(67)::Initialize     Detected operator multiclass_nms3 in your model, will replace it with fastdeploy::backend::MultiClassNMS(background_label=-1, keep_top_k=300, nms_eta=1, nms_threshold=0.7, score_threshold=0.01, nms_top_k=10000, normalized=1).\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update     [New Shape Out of Range] input name: image, shape: [1, 3, 640, 640], The shape range before: min_shape=[-1, 3, 640, 640], max_shape=[-1, 3, 640, 640].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update     [New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 640, 640], max_shape=[1, 3, 640, 640].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(40)::Update     [New Shape Out of Range] input name: scale_factor, shape: [1, 2], The shape range before: min_shape=[-1, 2], max_shape=[-1, 2].\r\n[WARNING] fastdeploy/backends/tensorrt/utils.cc(52)::Update     [New Shape Out of Range] The updated shape range now: min_shape=[1, 2], max_shape=[1, 2].\r\n[WARNING] fastdeploy/backends/tensorrt/trt_backend.cc(281)::Infer       TensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/docs/backends/tensorrt.md for more details.\r\n[INFO] fastdeploy/backends/tensorrt/trt_backend.cc(416)::BuildTrtEngine Start to building TensorRT Engine...\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(228)::log   1: [executionResources.cpp::setTacticSources::156] Error Code 1: Cudnn (Could not initialize cudnn, please check cudnn installation.)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(228)::log   2: [builder.cpp::buildSerializedNetwork::609] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed. )\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(465)::BuildTrtEngine        Failed to call buildSerializedNetwork().\r\nSegmentation fault (core dumped)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/36796264/219549393-0c6a694a-04ff-4f89-ba02-aee2ca00bb7d.png)\r\n",
        "state": "closed",
        "user": "qiulongquan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-17T04:27:34+00:00",
        "updated_at": "2024-04-16T06:40:24+00:00",
        "closed_at": "2024-04-16T06:40:24+00:00",
        "comments_count": [
            "wang-xinyu",
            "qiulongquan",
            "leiqing1",
            "qiulongquan",
            "qiulongquan",
            "Phoenix8215"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1354,
        "title": "RKYOLO部署结果分析相关问题!",
        "body": "@Zheng-Bicheng 郑工您好，按照您的指导，目前我这边已经能够基本实现自训练YOLOv5模型在rk3568平台上的推理部署全流程，(https://github.com/PaddlePaddle/FastDeploy/issues/1315#issue-1583401136)\r\n另外想咨询您几个相关的问题:\r\n1、我看您这边给我提供的解决方案是基于c++的，比如接口model.GetPostprocessor().SetClassNum(3)，我这边想多了解一些这些类里面的相关接口的内容，比如都有什么接口，接口功能，参数定义(比如这个3的具体意义)，调用方式，实现代码这些，这些在fastdeploy的官方仓库里面有体现吗？\r\n2、另外一个就是推理结果分析相关的问题，我这边测试了几组数据，发现在pc端训练完成后测试集测试的结果和在板端推理的结果还有点不太一样，相同的一张图片，我的真实目标可能就两个，pc端可以准确检测识别目标，但是板端会在这些小目标的周围生成其他的冗余目标框，体现在rusult里面就是会打印输出很多目标框、置信度及目标类别，而且这些目标label_id我看还会有个别的标签是2，我用的这组数据就只有一种目标，我想咨询一下您我这边应该从什么方向和角度去调整解决这个问题？",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-02-17T06:25:16+00:00",
        "updated_at": "2023-02-19T05:25:52+00:00",
        "closed_at": "2023-02-19T05:25:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1352,
        "title": "The speed of inference with gpu is slower than cpu when i use PP-Mattingv2 deployed by FastDeloy.",
        "body": "## Environment\r\n\r\nFastDeploy version: fastdeploy-win-gpu-1.0.3\r\nOS Platform: Windows x64\r\nHardware: Nvidia GPU 3060 Lap  CUDA 11.7 CUDNN 8.5\r\nProgram Language: C++\r\n\r\n## Problem description\r\nThis is cpu inference.\r\n![image](https://user-images.githubusercontent.com/93067563/219555626-f2fa722e-9438-418e-8377-3bbe5f49d4a9.png)\r\nThis is gpu inference.\r\n![image](https://user-images.githubusercontent.com/93067563/219555954-a32abb0e-f84a-47ff-aac3-3a734c09fa36.png)\r\n\r\nThe follow is all code:\r\n\r\n```\r\n#include <iostream>\r\n#include <opencv2/core/core.hpp>\r\n#include <opencv2/core/cuda.hpp>\r\n#include <opencv2/core/version.hpp>\r\n#include \"fastdeploy/vision.h\"      \r\n\r\n\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\ncv::Mat CpuInfer(const std::string& model_dir, const cv::Mat& image, const std::string& background_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"deploy.yaml\";\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseCpu();\r\n    auto model = fastdeploy::vision::matting::PPMatting(model_file, params_file,\r\n        config_file, option);\r\n    cv::Mat vis_im;\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n    auto im = image;\r\n    fastdeploy::vision::MattingResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n\r\n    if (!background_file.empty()) {\r\n        auto bg = cv::imread(background_file);\r\n        vis_im =\r\n            fastdeploy::vision::SwapBackground(im, bg, res);\r\n    }\r\n    else {\r\n        vis_im = fastdeploy::vision::VisMatting(im, res);\r\n    }\r\n    return vis_im;\r\n}\r\n\r\ncv::Mat GpuInfer(const std::string& model_dir, const cv::Mat& image, const std::string& background_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"deploy.yaml\";\r\n\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    option.UsePaddleInferBackend();\r\n    auto model = fastdeploy::vision::matting::PPMatting(model_file, params_file,\r\n        config_file, option);\r\n    cv::Mat vis_im;\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n    auto im = image;\r\n    fastdeploy::vision::MattingResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n\r\n    if (!background_file.empty()) {\r\n        auto bg = cv::imread(background_file);\r\n        vis_im =\r\n            fastdeploy::vision::SwapBackground(im, bg, res);\r\n    }\r\n    else {\r\n        vis_im = fastdeploy::vision::VisMatting(im, res);\r\n    }\r\n    return vis_im;\r\n}\r\n\r\ncv::Mat TrtInfer(const std::string& model_dir, const cv::Mat& image,\r\n    const std::string& background_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"deploy.yaml\";\r\n\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n    option.SetTrtInputShape(\"img\", { 1, 3, 512, 512 });\r\n    auto model = fastdeploy::vision::matting::PPMatting(model_file, params_file,\r\n        config_file, option);\r\n    cv::Mat vis_im;\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n    auto im = image;\r\n    fastdeploy::vision::MattingResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return vis_im;\r\n    }\r\n\r\n    if (!background_file.empty()) {\r\n        auto bg = cv::imread(background_file);\r\n        vis_im =\r\n            fastdeploy::vision::SwapBackground(im, bg, res);\r\n    }\r\n    else {\r\n        vis_im = fastdeploy::vision::VisMatting(im, res);\r\n    }\r\n    return vis_im;\r\n}\r\n\r\nint infer_by_camera(const std::string& device, const std::string& model_dir,\r\n    const std::string& window_name = \"video\", const std::string& background_file = \"\") {\r\n    cv::VideoCapture cap;\r\n    cap.open(0);\r\n    if (!cap.isOpened()) {\r\n        std::cout << \"open camera failed!\" << std::endl;\r\n        return 0;\r\n    }\r\n    cv::namedWindow(window_name, 1);\r\n    while (1) {\r\n        time_t t_now = time(0);\r\n        cv::Mat frame;\r\n        cap >> frame;\r\n        if (frame.empty()) {\r\n            return 0;\r\n        }\r\n        if (device == \"gpu\" || device == \"GPU\")\r\n            cv::imshow(window_name, GpuInfer(model_dir, frame, background_file));\r\n        else if (device == \"Trt\")\r\n            cv::imshow(window_name, TrtInfer(model_dir, frame, background_file));\r\n\r\n        else\r\n            cv::imshow(window_name, CpuInfer(model_dir, frame, background_file));\r\n        std::cout << \"Matting此帧共消耗\" << (time(0) - t_now) << \"秒\" << std::endl;\r\n        if (cv::waitKey(30) >= 0) break;\r\n    }\r\n    cap.release();\r\n    return 1;\r\n}\r\n\r\nint main() {\r\n\tinfer_by_camera(\"GPU\", \"model\");\r\n\treturn 0;\r\n}\r\n```\r\n\r\nI make sure ther is only one GPU which is 3060 Lap in my computer.",
        "state": "closed",
        "user": "liaosishui",
        "closed_by": "liaosishui",
        "created_at": "2023-02-17T05:25:11+00:00",
        "updated_at": "2023-03-01T07:00:55+00:00",
        "closed_at": "2023-03-01T07:00:54+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1353,
        "title": "编译中加入flycv报错",
        "body": "- 【FastDeploy版本】： fastdeploy-linux-gpu-0.0.0（dev-2023.2.17）\r\n- 【编译命令】\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DTRT_DIRECTORY=/root/3rdparty/TensorRT-8.0.3.4 \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_CUDA_ARCHITECTURES=86 -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/INSTALL/fastdeploy \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/root/3rdparty/opencv \\\r\n         -DENABLE_TEXT=OFF \\\r\n         -DENABLE_CVCUDA=ON \\\r\n         -DENABLE_FLYCV=ON\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：  Nvidia GPU 3090， CUDA 11.2 CUDNN 8.1\r\n- 【编译语言】： C++\r\n\r\n【编译错误】\r\n![image](https://user-images.githubusercontent.com/33308054/219564787-b8f1d7ed-f921-436c-bf05-4c4f0c3312db.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "ShawnXsw",
        "closed_by": "jiangjiajun",
        "created_at": "2023-02-17T06:20:13+00:00",
        "updated_at": "2023-02-20T05:52:33+00:00",
        "closed_at": "2023-02-20T05:52:33+00:00",
        "comments_count": [
            "jiangjiajun",
            "ShawnXsw",
            "DefTruth"
        ],
        "labels": [
            "Bug",
            "Build",
            "Linux x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1357,
        "title": "使用tinypose rknpu2相关",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/keypointdetection/tiny_pose/rknpu2/python/pptinypose_infer.py#L38-L39\r\n这两行代码不支持，运行会报错，可以看一下吗",
        "state": "closed",
        "user": "houyaoqi17",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-17T09:16:47+00:00",
        "updated_at": "2023-02-27T07:06:52+00:00",
        "closed_at": "2023-02-27T07:06:52+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "houyaoqi17",
            "Zheng-Bicheng",
            "houyaoqi17",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1359,
        "title": "init failed fastdeploy",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.3\r\n python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n\r\n- 【编译命令】python 包\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：具体硬件型号 Nvidia GPU 1060 3090 v100， 驱动 CUDA 11.2  CUDNN 8.3\r\n- 【编译语言】： Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用conda管理的包   conda config --add channels conda-forge && conda install cudatoolkit=11.2 cudnn=8.2\r\n![08b88540128497fb5198c7911586715](https://user-images.githubusercontent.com/93702453/219687420-d58dbba1-bfef-4302-94a1-7400780a3229.png)\r\n![f39e8dd1b83c92cda73c61cf0b04f3a](https://user-images.githubusercontent.com/93702453/219687449-bb49b5c6-3d18-41cb-90cf-edb88ed6c133.png)\r\n\r\n",
        "state": "closed",
        "user": "developWmark",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-17T14:48:02+00:00",
        "updated_at": "2024-02-27T06:39:16+00:00",
        "closed_at": "2024-02-27T06:39:16+00:00",
        "comments_count": [
            "jiangjiajun",
            "developWmark",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1358,
        "title": "picodet rknn量化相关问题",
        "body": "使用rknn量化，会出现以下问题\r\n\r\n:~/Documents/code/FastDeploy/examples/vision/detection/paddledetec\r\ntion/rknpu2/python$ python3 infer.py --model_file picodet_xs_320_coco_lcnet/picodet_xs_320_coco_lcn\r\net_rk3588_quantized.rknn  --config_file picodet_xs_320_coco_lcnet/infer_cfg.yml\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion rknn_api/rknnrt version: 1.4.0 (a10f100eb@2022-09-09T09:07:14), driver version: 0.8.0\r\nindex=0, name=image, n_dims=4, dims=[1, 320, 320, 3], n_elems=307200, size=307200, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-14, scale=0.018658, pass_through=0\r\nindex=0, name=p2o.Mul.179, n_dims=4, dims=[1, 2125, 4, 1], n_elems=8500, size=8500, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-63, scale=2.518318, pass_through=0\r\nindex=1, name=p2o.Concat.9, n_dims=4, dims=[1, 80, 2125, 1], n_elems=170000, size=170000, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003438, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(383)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(323)::Infer      The input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\nSegmentation fault (core dumped)\r\n",
        "state": "closed",
        "user": "houyaoqi17",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-17T09:21:08+00:00",
        "updated_at": "2023-02-21T10:34:40+00:00",
        "closed_at": "2023-02-21T10:29:35+00:00",
        "comments_count": [
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1360,
        "title": "考虑推出Gradio类似的框架不",
        "body": "考虑推出Gradio类似的WEB在线推理框架不,集成了GUI\r\nhttps://github.com/gradio-app/gradio\r\n\r\n虽然我们有什么[服务化部署](https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving)\r\n但是部署费劲 还要会前端交互\r\n太重量级了\r\n没这个方便",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-18T00:57:24+00:00",
        "updated_at": "2024-03-05T06:40:29+00:00",
        "closed_at": "2024-03-05T06:40:29+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1362,
        "title": "Python 3.11.2 编译 没有wheel包",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：20230218 github版\r\n- 【编译命令】\r\n```\r\ncd FastDeploy/python\r\n\r\nset ENABLE_ORT_BACKEND=ON\r\nset ENABLE_PADDLE_BACKEND=ON\r\nset ENABLE_OPENVINO_BACKEND=ON\r\nset ENABLE_VISION=ON\r\nset ENABLE_TEXT=ON\r\nset ENABLE_TRT_BACKEND=ON\r\nset WITH_GPU=ON\r\nset DENABLE_ENCRYPTION=ON\r\nset TRT_DIRECTORY=D:\\TensorRT-8.4.3.1\r\nset CUDA_DIRECTORY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\r\n\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n```\r\n\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： CUDA 11.6   CUDNN 8.4\r\n- 【编译语言】： Python 3.11.2\r\n\r\n\r\n\r\nDENABLE_ENCRYPTION=ON\r\nPython 3.11.2\r\n\r\n## 问题日志及出现问题的操作流程\r\n```\r\n\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(53): message : 参见“std::vector<std::vector<fastdeploy::text::UIEResult *,std::allocator<fastdeploy::text::UIEResult *>>,std::allocator<std::vector<fastdeploy::text::UIEResult *,std::allocator<fastdeploy::text::\r\nUIEResult *>>>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(54,27): warning C4251: “fastdeploy::text::SchemaNode::children_”: class“std::vector<fastdeploy::text::SchemaNode,std::allocator<fastdeploy::text::SchemaNode>>”需要有 dll 接口由 struct“fastdeploy::text::SchemaNode”的客\r\n户端使用 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(54): message : 参见“std::vector<fastdeploy::text::SchemaNode,std::allocator<fastdeploy::text::SchemaNode>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(97,33): warning C4099: “fastdeploy::text::UIEModel”: 类型名称以前使用“class”现在使用的是“struct” [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(97): message : 参见“fastdeploy::text::UIEModel”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(201,27): warning C4251: “fastdeploy::text::UIEModel::schema_”: class“std::unique_ptr<fastdeploy::text::Schema,std::default_delete<fastdeploy::text::Schema>>”需要有 dll 接口由 struct“fastdeploy::text::UIEModel”的客户端使用\r\n [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(201): message : 参见“std::unique_ptr<fastdeploy::text::Schema,std::default_delete<fastdeploy::text::Schema>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\n    正在创建库 D:/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.lib 和对象 D:/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.exp\r\n  fastdeploy_main.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy_main.cp311-win_amd64.pyd\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/third_party/yaml-cpp/util/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/third_party/yaml-cpp/util/CMakeLists.txt\r\n  Building Custom Rule D:/FastDeploy/third_party/yaml-cpp/util/CMakeLists.txt\r\ncl : 命令行 warning D9025: 正在重写“/W3”(用“/w”) [D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\yaml-cpp-sandbox.vcxproj]\r\n  sandbox.cpp\r\ncl : 命令行 warning D9025: 正在重写“/W3”(用“/w”) [D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\yaml-cpp-read.vcxproj]\r\n  read.cpp\r\ncl : 命令行 warning D9025: 正在重写“/W3”(用“/w”) [D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\yaml-cpp-parse.vcxproj]\r\n  parse.cpp\r\n  Defining YAML_CPP_API for DLL import\r\n  Defining YAML_CPP_API for DLL import\r\n  Defining YAML_CPP_API for DLL import\r\n  yaml-cpp-sandbox.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\Release\\sandbox.exe\r\n  yaml-cpp-parse.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\Release\\parse.exe\r\n  yaml-cpp-read.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\util\\Release\\read.exe\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\ncreating build\r\ncreating build\\lib.win-amd64-cpython-311\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\code_version.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\c_lib_wrap.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\download.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\model.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\runtime.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncopying fastdeploy\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\encryption\r\ncopying fastdeploy\\encryption\\encryption.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\encryption\r\ncopying fastdeploy\\encryption\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\encryption\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\libs\r\ncopying fastdeploy\\libs\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\libs\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\pipeline\r\ncopying fastdeploy\\pipeline\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\pipeline\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\r\ncopying fastdeploy\\serving\\model_manager.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\r\ncopying fastdeploy\\serving\\server.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\r\ncopying fastdeploy\\serving\\utils.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\r\ncopying fastdeploy\\serving\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\text\r\ncopying fastdeploy\\text\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\text\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncopying fastdeploy\\utils\\example_resource.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncopying fastdeploy\\utils\\hub_config.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncopying fastdeploy\\utils\\hub_env.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncopying fastdeploy\\utils\\hub_model_server.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncopying fastdeploy\\utils\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\utils\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\r\ncopying fastdeploy\\vision\\utils.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\r\ncopying fastdeploy\\vision\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\pipeline\\pptinypose\r\ncopying fastdeploy\\pipeline\\pptinypose\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\pipeline\\pptinypose\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\handler\r\ncopying fastdeploy\\serving\\handler\\base_handler.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\handler\r\ncopying fastdeploy\\serving\\handler\\vision_model_handler.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\handler\r\ncopying fastdeploy\\serving\\handler\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\handler\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\router\r\ncopying fastdeploy\\serving\\router\\base_router.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\router\r\ncopying fastdeploy\\serving\\router\\http_router.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\router\r\ncopying fastdeploy\\serving\\router\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\serving\\router\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\text\\uie\r\ncopying fastdeploy\\text\\uie\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\text\\uie\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\r\ncopying fastdeploy\\vision\\classification\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\common\r\ncopying fastdeploy\\vision\\common\\manager.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\common\r\ncopying fastdeploy\\vision\\common\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\common\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\r\ncopying fastdeploy\\vision\\detection\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\r\ncopying fastdeploy\\vision\\evaluation\\classify.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\r\ncopying fastdeploy\\vision\\evaluation\\detection.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\r\ncopying fastdeploy\\vision\\evaluation\\segmentation.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\r\ncopying fastdeploy\\vision\\evaluation\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\r\ncopying fastdeploy\\vision\\facealign\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\r\ncopying fastdeploy\\vision\\facedet\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\r\ncopying fastdeploy\\vision\\faceid\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\generation\r\ncopying fastdeploy\\vision\\generation\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\generation\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\headpose\r\ncopying fastdeploy\\vision\\headpose\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\headpose\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\keypointdetection\r\ncopying fastdeploy\\vision\\keypointdetection\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\keypointdetection\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\r\ncopying fastdeploy\\vision\\matting\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\ocr\r\ncopying fastdeploy\\vision\\ocr\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\ocr\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\segmentation\r\ncopying fastdeploy\\vision\\segmentation\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\segmentation\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\sr\r\ncopying fastdeploy\\vision\\sr\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\sr\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\tracking\r\ncopying fastdeploy\\vision\\tracking\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\tracking\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\visualize\r\ncopying fastdeploy\\vision\\visualize\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\visualize\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\contrib\r\ncopying fastdeploy\\vision\\classification\\contrib\\resnet.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\contrib\r\ncopying fastdeploy\\vision\\classification\\contrib\\yolov5cls.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\contrib\r\ncopying fastdeploy\\vision\\classification\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\ppcls\r\ncopying fastdeploy\\vision\\classification\\ppcls\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\classification\\ppcls\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\fastestdet.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\nanodet_plus.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\scaled_yolov4.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolor.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov5.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov5lite.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov5seg.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov6.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov7.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov7end2end_ort.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov7end2end_trt.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolov8.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\yolox.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncopying fastdeploy\\vision\\detection\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\ppdet\r\ncopying fastdeploy\\vision\\detection\\ppdet\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\ppdet\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\\rkyolo\r\ncopying fastdeploy\\vision\\detection\\contrib\\rkyolo\\rkyolov5.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\\rkyolo\r\ncopying fastdeploy\\vision\\detection\\contrib\\rkyolo\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\detection\\contrib\\rkyolo\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\cityscapes.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\coco.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\coco_metrics.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\coco_utils.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\fd_logging.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\json_results.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\map_utils.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\seg_metrics.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\util.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncopying fastdeploy\\vision\\evaluation\\utils\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\evaluation\\utils\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\\contrib\r\ncopying fastdeploy\\vision\\facealign\\contrib\\face_landmark_1000.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\\contrib\r\ncopying fastdeploy\\vision\\facealign\\contrib\\pfld.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\\contrib\r\ncopying fastdeploy\\vision\\facealign\\contrib\\pipnet.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\\contrib\r\ncopying fastdeploy\\vision\\facealign\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facealign\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\blazeface.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\centerface.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\retinaface.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\scrfd.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\ultraface.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\yolov5face.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\yolov7face.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncopying fastdeploy\\vision\\facedet\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\facedet\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\r\ncopying fastdeploy\\vision\\faceid\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\\adaface\r\ncopying fastdeploy\\vision\\faceid\\contrib\\adaface\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\\adaface\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\\insightface\r\ncopying fastdeploy\\vision\\faceid\\contrib\\insightface\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\faceid\\contrib\\insightface\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\generation\\contrib\r\ncopying fastdeploy\\vision\\generation\\contrib\\anemigan.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\generation\\contrib\r\ncopying fastdeploy\\vision\\generation\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\generation\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\headpose\\contrib\r\ncopying fastdeploy\\vision\\headpose\\contrib\\fsanet.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\headpose\\contrib\r\ncopying fastdeploy\\vision\\headpose\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\headpose\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\keypointdetection\\pptinypose\r\ncopying fastdeploy\\vision\\keypointdetection\\pptinypose\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\keypointdetection\\pptinypose\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\contrib\r\ncopying fastdeploy\\vision\\matting\\contrib\\modnet.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\contrib\r\ncopying fastdeploy\\vision\\matting\\contrib\\rvm.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\contrib\r\ncopying fastdeploy\\vision\\matting\\contrib\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\contrib\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\ppmatting\r\ncopying fastdeploy\\vision\\matting\\ppmatting\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\matting\\ppmatting\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\ocr\\ppocr\r\ncopying fastdeploy\\vision\\ocr\\ppocr\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\ocr\\ppocr\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\segmentation\\ppseg\r\ncopying fastdeploy\\vision\\segmentation\\ppseg\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\segmentation\\ppseg\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\sr\\ppsr\r\ncopying fastdeploy\\vision\\sr\\ppsr\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\sr\\ppsr\r\ncreating build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\tracking\\pptracking\r\ncopying fastdeploy\\vision\\tracking\\pptracking\\__init__.py -> build\\lib.win-amd64-cpython-311\\fastdeploy\\vision\\tracking\\pptracking\r\nrunning build_ext\r\ncopying D:\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy_main.cp311-win_amd64.pyd -> D:\\FastDeploy\\python\\build\\lib.win-amd64-cpython-311\\fastdeploy\r\n\r\nD:\\FastDeploy\\python>\r\n```\r\n\r\n\r\n[CMakeOutput.log](https://github.com/PaddlePaddle/FastDeploy/files/10773762/CMakeOutput.log)\r\n\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-18T09:12:39+00:00",
        "updated_at": "2023-02-26T01:36:39+00:00",
        "closed_at": "2023-02-26T01:36:39+00:00",
        "comments_count": [
            "jiangjiajun",
            "wang-xinyu",
            "monkeycc",
            "wang-xinyu",
            "monkeycc",
            "wang-xinyu",
            "monkeycc",
            "wang-xinyu",
            "monkeycc",
            "wang-xinyu",
            "jiangjiajun",
            "monkeycc",
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1361,
        "title": "Python 3.11.2 编译报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： https://github.com/PaddlePaddle/FastDeploy/tree/9b482b72fb50139c2354a8a07db9702ed7039edd 20230218版\r\n- 【编译命令】\r\n```\r\ncd FastDeploy/python\r\n\r\nset ENABLE_ORT_BACKEND=ON\r\nset ENABLE_PADDLE_BACKEND=ON\r\nset ENABLE_OPENVINO_BACKEND=ON\r\nset ENABLE_VISION=ON\r\nset ENABLE_TEXT=ON\r\nset ENABLE_TRT_BACKEND=ON\r\nset WITH_GPU=ON\r\nset DENABLE_ENCRYPTION=ON\r\nset TRT_DIRECTORY=D:\\TensorRT-8.4.3.1\r\nset CUDA_DIRECTORY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\r\n\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n```\r\n\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： CUDA 11.6   CUDNN 8.4\r\n- 【编译语言】： Python 3.11.2\r\n\r\n\r\n\r\nDENABLE_ENCRYPTION=ON\r\nPython 3.11.2\r\n\r\n## 问题日志及出现问题的操作流程\r\n```\r\n\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(36,59): warning C4251: “fastdeploy::text::UIEResult::relation_”: class“std:\r\n:unordered_map<std::string,std::vector<fastdeploy::text::UIEResult,std::allocator<fastdeploy::text::UIEResult>>,std::ha\r\nsh<std::string>,std::equal_to<std::string>,std::allocator<std::pair<const std::string,std::vector<fastdeploy::text::UIE\r\nResult,std::allocator<fastdeploy::text::UIEResult>>>>>”需要有 dll 接口由 struct“fastdeploy::text::UIEResult”的客户端使用 [D:\\FastDe\r\nploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(36): message : 参见“std::unordered_map<std::string,std::vector<fastdeploy::te\r\nxt::UIEResult,std::allocator<fastdeploy::text::UIEResult>>,std::hash<std::string>,std::equal_to<std::string>,std::alloc\r\nator<std::pair<const std::string,std::vector<fastdeploy::text::UIEResult,std::allocator<fastdeploy::text::UIEResult>>>>\r\n>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(51,15): warning C4251: “fastdeploy::text::SchemaNode::name_”: class“std::ba\r\nsic_string<char,std::char_traits<char>,std::allocator<char>>”需要有 dll 接口由 struct“fastdeploy::text::SchemaNode”的客户端使用 [D:\r\n\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.29.30133\\include\\xstring(4871): messa\r\nge : 参见“std::basic_string<char,std::char_traits<char>,std::allocator<char>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake\r\n-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(52,41): warning C4251: “fastdeploy::text::SchemaNode::prefix_”: class“std::\r\nvector<std::vector<std::string,std::allocator<std::string>>,std::allocator<std::vector<std::string,std::allocator<std::\r\nstring>>>>”需要有 dll 接口由 struct“fastdeploy::text::SchemaNode”的客户端使用 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdep\r\nloy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(52): message : 参见“std::vector<std::vector<std::string,std::allocator<std::s\r\ntring>>,std::allocator<std::vector<std::string,std::allocator<std::string>>>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cma\r\nke-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(53,40): warning C4251: “fastdeploy::text::SchemaNode::relations_”: class“st\r\nd::vector<std::vector<fastdeploy::text::UIEResult *,std::allocator<fastdeploy::text::UIEResult *>>,std::allocator<std::\r\nvector<fastdeploy::text::UIEResult *,std::allocator<fastdeploy::text::UIEResult *>>>>”需要有 dll 接口由 struct“fastdeploy::te\r\nxt::SchemaNode”的客户端使用 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(53): message : 参见“std::vector<std::vector<fastdeploy::text::UIEResult *,std\r\n::allocator<fastdeploy::text::UIEResult *>>,std::allocator<std::vector<fastdeploy::text::UIEResult *,std::allocator<fas\r\ntdeploy::text::UIEResult *>>>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(54,27): warning C4251: “fastdeploy::text::SchemaNode::children_”: class“std\r\n::vector<fastdeploy::text::SchemaNode,std::allocator<fastdeploy::text::SchemaNode>>”需要有 dll 接口由 struct“fastdeploy::text\r\n::SchemaNode”的客户端使用 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(54): message : 参见“std::vector<fastdeploy::text::SchemaNode,std::allocator<f\r\nastdeploy::text::SchemaNode>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(97,33): warning C4099: “fastdeploy::text::UIEModel”: 类型名称以前使用“class”现在使用的是“\r\nstruct” [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(97): message : 参见“fastdeploy::text::UIEModel”的声明 [D:\\FastDeploy\\python\\.set\r\nuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(201,27): warning C4251: “fastdeploy::text::UIEModel::schema_”: class“std::u\r\nnique_ptr<fastdeploy::text::Schema,std::default_delete<fastdeploy::text::Schema>>”需要有 dll 接口由 struct“fastdeploy::text::\r\nUIEModel”的客户端使用 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\nD:\\FastDeploy\\.\\fastdeploy/text/uie/model.h(201): message : 参见“std::unique_ptr<fastdeploy::text::Schema,std::default_de\r\nlete<fastdeploy::text::Schema>>”的声明 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy_main.vcxproj]\r\n    正在创建库 D:/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.lib 和对象 D:/FastDeploy/python/.setuptools\r\n  -cmake-build/Release/fastdeploy_main.exp\r\n  fastdeploy_main.vcxproj -> D:\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy_main.cp311-win_amd64.pyd\r\n  Building Custom Rule D:/FastDeploy/CMakeLists.txt\r\n  Error copying directory from \"D:/FastDeploy/python/.setuptools-cmake-build/third_libs/install\" to \"D:/FastDeploy/pyth\r\n  on/fastdeploy/libs/third_libs\".\r\nD:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Microsoft\\VC\\v160\\Microsoft.CppCommon.targets(\r\n241,5): error MSB8066: “D:\\FastDeploy\\python\\.setuptools-cmake-build\\CMakeFiles\\a1e623b9921cc5b529b7bbb1fc2d9378\\copy_t\r\nhird_libraries.rule”的自定义生成已退出，代码为 1。 [D:\\FastDeploy\\python\\.setuptools-cmake-build\\copy_third_libraries.vcxproj]\r\nTraceback (most recent call last):\r\n  File \"D:\\FastDeploy\\python\\setup.py\", line 417, in <module>\r\n    setuptools.setup(\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\__init__.py\", line 87, in setup\r\n    return distutils.core.setup(**attrs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 968, in run_commands\r\n    self.run_command(cmd)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 132, in run\r\n    self.run_command(cmd_name)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\FastDeploy\\python\\setup.py\", line 280, in run\r\n    self.run_command('cmake_build')\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\Python3112\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\FastDeploy\\python\\setup.py\", line 274, in run\r\n    subprocess.check_call(build_args)\r\n  File \"D:\\Python3112\\Lib\\subprocess.py\", line 413, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['D:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Professional\\\\Common7\\\\IDE\\\\CommonExtensions\\\\Microsoft\\\\CMake\\\\CMake\\\\bin\\\\cmake.exe', '--build', '.', '--config', 'Release', '--', '/maxcpucount:20']' returned non-zero exit status 1.\r\n\r\n```\r\n![微信截图_20230218161202](https://user-images.githubusercontent.com/6490927/219849643-df2f2d95-c180-481a-9449-bcdd6868ce9c.png)\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-18T08:19:22+00:00",
        "updated_at": "2023-02-18T09:08:29+00:00",
        "closed_at": "2023-02-18T09:08:29+00:00",
        "comments_count": [
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1363,
        "title": "如何由fastdeploy::vision::SegmentationResult的结果  获取到掩模图",
        "body": "fastdeploy::vision::SegmentationResult  获取到掩模图，然后根据掩模图片得到目前区域。\r\n",
        "state": "closed",
        "user": "chuxuming",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-19T03:05:37+00:00",
        "updated_at": "2024-02-27T06:39:17+00:00",
        "closed_at": "2024-02-27T06:39:17+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1364,
        "title": "更新关闭日志记录INFO信息的接口",
        "body": "原版本接口fastdeploy.FDLogger.disable_info在新版本移除，麻烦在新版本添加相同功能接口，谢谢！",
        "state": "closed",
        "user": "JiehangXie",
        "closed_by": "JiehangXie",
        "created_at": "2023-02-19T09:24:12+00:00",
        "updated_at": "2023-03-03T05:13:19+00:00",
        "closed_at": "2023-03-03T05:13:19+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1367,
        "title": "onnx部署yolov5模型问题",
        "body": "win下，用fastdeploy部署yolov5的onnx模型，发现，fastdeploy上提供的*.onnx模型是可以的，但是我从yolov5中下载*pt转成onnx模型是无法识别的，请问，*.onnx模型是怎么生成的（针对ultralytics/yolov5)\r\n",
        "state": "closed",
        "user": "GreenAvocado92",
        "closed_by": "GreenAvocado92",
        "created_at": "2023-02-20T01:32:12+00:00",
        "updated_at": "2023-02-20T05:21:30+00:00",
        "closed_at": "2023-02-20T05:21:30+00:00",
        "comments_count": [
            "wjj19950828",
            "GreenAvocado92"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1372,
        "title": "N5095 cpu上PP-OCR服务化部署失败",
        "body": "使用registry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-cpu-only-21.10镜像在赛扬N5095上部署，环境是ubuntu20.04 。在启动服务后：fastdeployserver --model-repository=/ocr_serving/models\r\n会报错失败\r\n<img width=\"983\" alt=\"1676878678325\" src=\"https://user-images.githubusercontent.com/18228342/220042345-77cab7e6-56fb-4725-acbc-1180e4b51cc3.png\">\r\n同样的model文件和配置，同样的环境，在我的i5-1240p笔记本上是能正常运行的。\r\n是否N5095 CPU不支持avx指令集导致的？\r\n是否有解决办法？",
        "state": "closed",
        "user": "flamebox",
        "closed_by": "heliqi",
        "created_at": "2023-02-20T07:40:02+00:00",
        "updated_at": "2023-02-22T03:53:33+00:00",
        "closed_at": "2023-02-22T03:53:19+00:00",
        "comments_count": [
            "jiangjiajun",
            "rainyfly",
            "flamebox",
            "heliqi",
            "flamebox",
            "heliqi",
            "flamebox",
            "heliqi",
            "flamebox",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1373,
        "title": "【jetson nano环境】facedet脸部检测模型入力尺寸修改 模型无法检测发生错误",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： jetson nano编译版本，fastdeploy-python 0.0.0\r\n- 【编译命令】：Python编译安装\r\n- 【系统平台】: Linux (Ubuntu 18.04)\r\n- 【硬件】： Nvidia jetson nano b01 4G， CUDA 10.2 CUDNN 8.2.1\r\n- 【编译语言】： Python 3.7.12\r\n\r\n## 问题日志及出现问题的操作流程\r\n飞浆官方提供的facedet 脸部检测模型Pytorch_Retinaface（https://github.com/biubug6/Pytorch_Retinaface） 模型入力尺寸是640*640 为了提高jetson nano检测速度，修改成320*320。 然后正常导出onnx模型\r\n\r\n![image](https://user-images.githubusercontent.com/36796264/220047883-77dedddc-7aec-4cdb-a664-5a2d9f057e35.png)\r\n\r\n在FastDeploy部署无法正常检测脸部，出现下面error信息\r\n\r\n(paddle3.7) qiulongquan@qiulongquan-desktop:~/ml/FastDeploy/examples/vision/facedet/retinaface/python$ python face_det_infer.py \r\nWARNING:root:`RuntimeOption.set_trt_max_workspace_size` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.max_workspace_size = 1073741824` instead.\r\nWARNING:root:`RuntimeOption.enable_paddle_trt_collect_shape` will be deprecated in v1.2.0, please use `RuntimeOption.paddle_infer_option.collect_trt_shape = True` instead.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(692)::CreateTrtEngineFromOnnx        Detect serialized TensorRT Engine file in ./tensorrt_cache/model.trt, will load it directly.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache   Build TensorRT Engine from cache file: ./tensorrt_cache/model.trt with shape range information as below,\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache   Input name: input0, shape=[1, 3, 320, 320], min=[1, 3, 320, 320], max=[1, 3, 320, 320]\r\n\r\n[INFO] fastdeploy/runtime/runtime.cc(355)::CreateTrtBackend     Runtime initialized with Backend::TRT in Device::GPU.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(232)::log   3: [executionContext.cpp::setBindingDimensions::944] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::setBindingDimensions::944, condition: profileMaxDims.d[i] >= dimensions.d[i]. Supplied binding dimension [1,3,640,640] for bindings[0] exceed min ~ max range at index 2, maximum dimension in profile is 320, minimum dimension in profile is 320, but supplied dimension is 640.\r\n)\r\n\r\n已经确认 在640*640入力尺寸下可以正常进行脸部检测，系统环境应该没有问题，主要出在入力尺寸转换后的模型上面\r\n请给与帮助 谢谢\r\n",
        "state": "closed",
        "user": "qiulongquan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-20T08:09:37+00:00",
        "updated_at": "2024-04-16T09:02:57+00:00",
        "closed_at": "2024-04-16T09:02:57+00:00",
        "comments_count": [
            "jiangjiajun",
            "qiulongquan",
            "jiangjiajun",
            "qiulongquan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1369,
        "title": "fastdeploy调用yolov5无法使用GPU",
        "body": "![A7O78E%$26~@YIA6GDWM}GT](https://user-images.githubusercontent.com/91200482/220026903-3d094b37-62e8-4477-874c-48ed7698ea3a.jpg)\r\n![XO@Y{L3V79 X I7O}W}B3 F](https://user-images.githubusercontent.com/91200482/220026991-9cedd504-a3ff-4cd2-838a-e5e47a9cef6c.jpg)\r\n![3RG@K$VQY@GM3E@82FBFNTE](https://user-images.githubusercontent.com/91200482/220026992-584dc148-7592-4790-b154-9e920d4451fb.png)\r\n\r\n第一张图是调用yolov5的代码，第二图是yolov5初始化时的提示，使用CPU， 第三张图是YOLOE的初始化提示",
        "state": "closed",
        "user": "GreenAvocado92",
        "closed_by": "GreenAvocado92",
        "created_at": "2023-02-20T06:22:04+00:00",
        "updated_at": "2023-02-20T07:36:11+00:00",
        "closed_at": "2023-02-20T07:36:11+00:00",
        "comments_count": [
            "jiangjiajun",
            "GreenAvocado92"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1378,
        "title": "ppocrV3 预测 报错 module numpy has no attribute astype",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：  fastdeploy-linux-gpu-1.0.2\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】：  如 Nvidia GPU TeslaP40， CUDA 11.4  CUDNN 8.4\r\n- 【编译语言】： C++ / Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【性能问题】描述清楚对比的方式\r\n- 预测特定图片：\r\n  报 module numpy has no attribute astype\r\n  同时gpu占用大于80%\r\n  控制台报显存oom\r\n\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n问题图片\r\n[12345.png.zip](https://github.com/PaddlePaddle/FastDeploy/files/10783432/12345.png.zip)  \r\n[stack.txt](https://github.com/PaddlePaddle/FastDeploy/files/10783482/stack.txt)\r\n\r\n![1099745716](https://user-images.githubusercontent.com/121446530/220081336-27461ee8-82c0-41c3-921b-2414e2dbe41b.jpg)\r\n\r\n![1398360170](https://user-images.githubusercontent.com/121446530/220082136-9d997d5e-62d2-49fb-8d44-6e221226c9b6.jpg)\r\n\r\n",
        "state": "closed",
        "user": "EasyIsAllYouNeed",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-20T10:34:48+00:00",
        "updated_at": "2024-02-27T06:39:17+00:00",
        "closed_at": "2024-02-27T06:39:17+00:00",
        "comments_count": [
            "yunyaoXYY",
            "jiangjiajun",
            "yunyaoXYY"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1383,
        "title": "使用rv1126开发板部署demo示例，需要等待几十秒才输出结果",
        "body": "环境准备：https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rv1126.md\r\n部署Demo示例：https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rv1126\r\n\r\n按照文档操作，运行infer_demo后需要等待几十秒才会输出vis_result.jpg的结果图片，前面一大部分时间输出了好多日志，是否可以去掉前面的部分操作，直接能得到识别的结果？\r\n![7536ab3173381054980b94b707db363](https://user-images.githubusercontent.com/7600926/220112600-e7cf4e9c-314f-480f-b322-2c21f2221814.png)\r\n",
        "state": "closed",
        "user": "com876",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-20T12:49:15+00:00",
        "updated_at": "2024-03-12T06:39:13+00:00",
        "closed_at": "2024-03-12T06:39:12+00:00",
        "comments_count": [
            "jiekechoo",
            "com876",
            "jiekechoo",
            "jiekechoo",
            "jiekechoo",
            "jiekechoo",
            "com876",
            "ZHIZIHUABU",
            "jiekechoo",
            "ZHIZIHUABU",
            "jiekechoo",
            "yeliang2258"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1388,
        "title": "多线程预测时候报错[Paddle2ONNX] Find dumplicate output name p2o.Add.1，it will rename to p2o.p2o.Add.1.0",
        "body": "使用cpp在windows进行多线程预测图像，使用的模型为paddlecls模型。\r\n多线程思路为：将所有图像路径存储在一个容器中，设置线程数，每一个线程预测的图像数为：总图像数/线程数。\r\n线程数设置为1时，可以正常运行，设置为多线程，就会报错如下，并自动退出程序：\r\n![image](https://user-images.githubusercontent.com/66996161/220228944-0b926429-c72f-474f-90a6-520a811872fc.png)\r\n\r\n请求大佬帮忙解决，谢谢！\r\n",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "happybear1015",
        "created_at": "2023-02-21T02:05:59+00:00",
        "updated_at": "2023-02-22T09:55:45+00:00",
        "closed_at": "2023-02-22T09:55:45+00:00",
        "comments_count": [
            "jiangjiajun",
            "happybear1015",
            "happybear1015",
            "jiangjiajun",
            "happybear1015",
            "jiangjiajun",
            "happybear1015",
            "happybear1015"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1390,
        "title": "版面分析模型预测失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.0.0\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型无法预测】\r\n- 模型采用的是版面分析的模型，模型下载地址是 https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/ppstructure/docs/models_list.md#1-%E7%89%88%E9%9D%A2%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B 我们选用的是 \r\n![image](https://user-images.githubusercontent.com/58454582/220231744-a8e6dbf0-fd79-4bea-803c-c9440fcd76f3.png)\r\n模型加载代码为：\r\n![image](https://user-images.githubusercontent.com/58454582/220231816-798fa55c-79fc-450f-a667-22d7ca6604ab.png)\r\n报错为：\r\n[ERROR] fastdeploy/vision/detection/ppdet/postprocessor.cc(27)::ProcessMask\tThe data type of out mask tensor should be INT32, but now it's FDDataType::FP32\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(78)::BatchPredict\tFailed to postprocess the inference results by runtime.\r\n",
        "state": "closed",
        "user": "liuwqiang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-21T02:28:25+00:00",
        "updated_at": "2024-04-16T09:02:58+00:00",
        "closed_at": "2024-04-16T09:02:58+00:00",
        "comments_count": [
            "liuwqiang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1400,
        "title": "文字识别OCR部分支持端侧平台瑞芯微3588吗？",
        "body": "![image](https://user-images.githubusercontent.com/100198198/220315263-74422a6b-40c6-4abd-a806-62bd22d3cb8f.png)\r\n## 目前显示对平台瑞芯微3588，OCR的支持还在进行中，请问这个支持大概将在未来的多久完成？还是说已经支持了，但是文档暂未更新。",
        "state": "closed",
        "user": "KongXCai",
        "closed_by": "KongXCai",
        "created_at": "2023-02-21T10:12:48+00:00",
        "updated_at": "2023-02-27T07:06:22+00:00",
        "closed_at": "2023-02-22T03:22:51+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "KongXCai",
            "Zheng-Bicheng",
            "KongXCai",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1401,
        "title": "【windows环境】使用fastdeploy + tensorRT(INT8)  图像分类模型无法预测结果",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 官网 python安装\r\n- （   pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html  ）\r\n- 【编译命令】无编译\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 1050TI， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】：Python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\npaddlecls图像分类模型（PP-HGNet small）使用tensorRT INT8模式预测图片无法顺利执行\r\ntensorRT配置参数\r\n```\r\nconfig.enable_tensorrt_engine(workspace_size=1 << 30,\r\n                                max_batch_size=1,\r\n                                min_subgraph_size=5,\r\n                                precision_mode=PrecisionType.Int8,\r\n                                use_static=False,\r\n                                use_calib_mode=True)\r\n```\r\n程序执行log\r\n(fastdeploy) PS C:\\Users\\QLQ\\Desktop\\jetson\\total_test_for_emotion> python .\\emotion_infer.py\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system \r\nPATH env -> C:\\ProgramData\\Anaconda3\\envs\\fastdeploy\\Library\\bin \r\nW0221 19:40:43.204571  4024 analysis_predictor.cc:1391] The one-time configuration of analysis predictor failed, which may be due \r\nto native predictor called first and its configurations taken effect.\r\nI0221 19:40:43.235571  4024 analysis_predictor.cc:1099] TensorRT \r\nsubgraph engine is enabled\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_graph_clean_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [adaptive_pool2d_convert_global_pass]e[0m\r\nI0221 19:40:43.422009  4024 fuse_pass_base.cc:59] ---  detected 6 subgraphs\r\ne[32m--- Running IR pass [shuffle_channel_detect_pass]e[0m\r\ne[32m--- Running IR pass [quant_conv2d_dequant_fuse_pass]e[0m    \r\nW0221 19:40:43.440047  4024 op_compat_sensible_pass.cc:221]  Attribute(round_type) of Op(fake_quantize_range_abs_max) is not defined in opProto or is in extra set!The compatable check for this attribute is not use. Please remove it from the precondition of pass: quant_conv2d_dequant_fuse_pass\r\ne[32m--- Running IR pass [delete_fill_constant_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_quant_dequant_op_pass]e[0m      \r\ne[32m--- Running IR pass [delete_quant_dequant_filter_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_weight_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_quant_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [identity_scale_op_clean_pass]e[0m      \r\ne[32m--- Running IR pass [add_support_int8_pass]e[0m\r\nI0221 19:40:43.694061  4024 fuse_pass_base.cc:59] ---  detected 160 subgraphs\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [trt_embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [preln_embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [delete_c_identity_op_pass]e[0m\r\ne[32m--- Running IR pass [trt_multihead_matmul_fuse_pass_v2]e[0m \r\ne[32m--- Running IR pass [trt_multihead_matmul_fuse_pass_v3]e[0m\r\ne[32m--- Running IR pass [vit_attention_fuse_pass]e[0m\r\ne[32m--- Running IR pass [trt_skip_layernorm_fuse_pass]e[0m      \r\ne[32m--- Running IR pass [preln_skip_layernorm_fuse_pass]e[0m    \r\ne[32m--- Running IR pass [preln_residual_bias_fuse_pass]e[0m     \r\ne[32m--- Running IR pass [layernorm_shift_partition_fuse_pass]e[0m\r\ne[32m--- Running IR pass [unsqueeze2_eltwise_fuse_pass]e[0m\r\ne[32m--- Running IR pass [trt_squeeze2_matmul_fuse_pass]e[0m     \r\ne[32m--- Running IR pass [trt_flatten2_matmul_fuse_pass]e[0m     \r\ne[32m--- Running IR pass [trt_map_matmul_v2_to_mul_pass]e[0m     \r\nI0221 19:40:43.750034  4024 fuse_pass_base.cc:59] ---  detected 1 subgraphs\r\ne[32m--- Running IR pass [trt_map_matmul_v2_to_matmul_pass]e[0m  \r\ne[32m--- Running IR pass [trt_map_matmul_to_mul_pass]e[0m        \r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\nI0221 19:40:43.758036  4024 fuse_pass_base.cc:59] ---  detected 1 subgraphs\r\ne[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m    \r\nI0221 19:40:43.769050  4024 fuse_pass_base.cc:59] ---  detected 5 subgraphs\r\ne[32m--- Running IR pass [remove_padding_recover_padding_pass]e[0m\r\ne[32m--- Running IR pass [delete_remove_padding_recover_padding_pass]e[0m\r\ne[32m--- Running IR pass [dense_fc_to_sparse_pass]e[0m\r\ne[32m--- Running IR pass [dense_multihead_matmul_to_sparse_pass]e[0m\r\ne[32m--- Running IR pass [constant_folding_pass]e[0m\r\ne[32m--- Running IR pass [tensorrt_subgraph_pass]e[0m\r\nI0221 19:40:43.829036  4024 tensorrt_subgraph_pass.cc:244] ---  detect a sub-graph with 153 nodes\r\ne[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0me[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0me[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\nI0221 19:40:43.860035  4024 ir_params_sync_among_devices_pass.cc:89] Sync params from CPU to GPU\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m    \r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m     \r\nI0221 19:40:43.948082  4024 analysis_predictor.cc:1314] ======= optimize end =======\r\nI0221 19:40:43.950038  4024 naive_executor.cc:110] ---  skip [feed], feed -> x\r\nI0221 19:40:43.951036  4024 naive_executor.cc:110] ---  skip [softmax_1.tmp_0], fetch -> fetch\r\n\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(661)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx Detect serialized TensorRT Engine file in tensorrt_cache/windows_model.trt, will load it directly.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(108)::fastdeploy::TrtBackend::LoadTrtCache    Build TensorRT Engine from cache file: tensorrt_cache/windows_model.trt with shape range information as below,\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(111)::fastdeploy::TrtBackend::LoadTrtCache    Input name: input0, shape=[1, 3, 320, 320], min=[1, 3, 320, 320], max=[1, 3, 320, 320]\r\n\r\n[INFO] fastdeploy/runtime/runtime.cc(90)::fastdeploy::Runtime::Init      Runtime initialized with Backend::TRT in Device::GPU.\r\nface detection processing time:0.307 Seconds.\r\nemotion preprocess processing time:0.002 Seconds.\r\nW0221 19:41:03.635134  4024 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 11.4, Runtime API Version: 11.2\r\nW0221 19:41:03.635134  4024 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\nI0221 19:41:03.636133  4024 tensorrt_engine_op.h:421] This process is generating calibration table for Paddle TRT int8...\r\n\r\n在执行到创建 calibration table 的时候就停止了，无法继续运行\r\n当使用HALF fp16模式时候可以正常运行程序，我们需要在INT8 模式下能够运行\r\n\r\n分类模型的 _opt_cache 目录下 没有生成任何文件\r\n\r\n基本可以判断 系统环境没问题 可能是tensorRT配置问题\r\n请问 我错误在哪里呢？",
        "state": "closed",
        "user": "qiulongquan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-21T10:48:59+00:00",
        "updated_at": "2024-04-16T09:02:59+00:00",
        "closed_at": "2024-04-16T09:02:59+00:00",
        "comments_count": [
            "qiulongquan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1412,
        "title": "ImportError  undefined symbol",
        "body": null,
        "state": "closed",
        "user": "mhb0928",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T09:10:04+00:00",
        "updated_at": "2024-04-16T09:03:01+00:00",
        "closed_at": "2024-04-16T09:03:01+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 1414
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1407,
        "title": "支持自己训练的模型 android 端",
        "body": "你好  ，我这边利用 yolov5训练的自定义的数据集  ，部署在android上可以吗 ，有例子吗",
        "state": "closed",
        "user": "sl00001",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T06:39:08+00:00",
        "updated_at": "2024-02-27T06:39:18+00:00",
        "closed_at": "2024-02-27T06:39:18+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": [
            "Android"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1406,
        "title": "如何在推理时开启paddle.inference.Config()中的配置？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 0.1.0\r\n- 【编译命令】不是自编译的\r\n- 【系统平台】: Linux x64(Ubuntu 18.04.5 LTS)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3090， CUDA 11.2 CUDNN 8.2\r\n- 【编译语言】： Python 3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n【问题】在使用fastdeploy推理时，如何设置 [paddle.inference.Config()](https://paddle-inference.readthedocs.io/en/latest/api_reference/python_api_doc/Config/OtherFunction.html#profile) 中的配置？例如：\r\npaddle inference 可以开启 enable_profile()，用于查看性能；也可以开启enable_memory_optim()，启动内存优化，等等。\r\n\r\n请问fastdeploy中如何设置这些paddle inference支持的功能？\r\n\r\n感谢。",
        "state": "closed",
        "user": "kehuo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T02:25:51+00:00",
        "updated_at": "2024-04-16T09:03:00+00:00",
        "closed_at": "2024-04-16T09:03:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "kehuo"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1413,
        "title": "编译DLL 无法使用",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy-release-1.0.0 源代码\r\n- 【系统平台】Windows x64(Windows10)\r\n- 【硬件】：Nvidia GPU 3080， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ \r\n\r\n编译GPU版本DLL编译成功， 添加到demo程序运行时会把程序卡掉了。也没有报错，任务管理器只显示程序挂起，这是怎么回事？\r\n但是demo程序使用官方编译DLL的没问题。\r\n\r\n**CMake配置如下：**，请问编译GPU DLL 时CMake配置有什么问题吗？？\r\n![image](https://user-images.githubusercontent.com/68001817/220580737-4607e130-82d3-48a7-88af-b924fddd9ea6.png)\r\n\r\n![image](https://user-images.githubusercontent.com/68001817/220580150-fb616c37-7e16-4451-b7f6-1b10569ad231.png)\r\n\r\n",
        "state": "closed",
        "user": "chccc1994",
        "closed_by": "chccc1994",
        "created_at": "2023-02-22T09:36:44+00:00",
        "updated_at": "2023-02-22T13:40:10+00:00",
        "closed_at": "2023-02-22T13:40:10+00:00",
        "comments_count": [
            "DefTruth",
            "chccc1994",
            "DefTruth",
            "chccc1994",
            "jiangjiajun",
            "chccc1994"
        ],
        "labels": [
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1416,
        "title": "HardSigmoid op not support now for sophon example",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】:  未编译\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) /\r\n- 【硬件】： BM1684X\r\n- 【编译语言】： python\r\n\r\n完全按照例子https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/sophgo，生成onnx并转mlir模型时出现两个问题：\r\n1.  -input_shapes [[1,3,640,640],[1,2]]　这也写检查shape会报错\r\n2. 绕过第一步，会出现算子不支持, HardSigmoid op not support now\r\n",
        "state": "closed",
        "user": "thunder95",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-22T12:15:06+00:00",
        "updated_at": "2024-03-26T06:39:46+00:00",
        "closed_at": "2024-03-26T06:39:46+00:00",
        "comments_count": [
            "jiangjiajun",
            "thunder95",
            "Yi-sir",
            "thunder95",
            "Yi-sir",
            "Yi-sir"
        ],
        "labels": [
            "Sophgo"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1419,
        "title": "RK3588S PicoDet输入图片识别不到目标时自动退出程序并且不报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】与官方提供的编译方式相同\r\n- 【系统平台】: ubuntu20.04 RK3588s\r\n- 【编译语言】： C++ / Python 3.9.16\r\n\r\n## 问题日志及出现问题的操作流程\r\n自己按照百度paddle对应的方式部署自己训练的PicoDet到RK3588S板上，正常测试运行一切正常。\r\n但在输入批量照片时，发现在图片中能识别到对象时一切正常，能继续运行，当在输入图片中识别不到目标时，python调用对应的c++代码时返回\r\nProcess finished with exit code 139(interrupted by signal 11 :SIGSEGV)\r\n并且程序自动退出，不报错\r\n",
        "state": "closed",
        "user": "XDUwsk",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-22T13:02:06+00:00",
        "updated_at": "2023-02-26T01:18:08+00:00",
        "closed_at": "2023-02-26T01:18:08+00:00",
        "comments_count": [
            "Zheng-Bicheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1417,
        "title": "调用paddleseg模型内存持续增长，大佬救命",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： windows develop\r\n- 【编译命令】vs2019\r\n- 【系统平台】: win10\r\n- 【硬件】： cpu\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 模型是512x512导出的paddleliteseg，直接输入分辨率512x512的内存不会增长，分辨率只要不为512x512必然内存暴涨\r\n- 模型配置文件里面有target_size 512x512\r\n```\r\n fastdeploy::vision::SegmentationResult res;\r\n    //cv::Mat bgr_mat = cv::Mat::ones(1024, 1024, CV_8UC3);\r\n    cv::Mat bgr_mat = cv::Mat::ones(512, 512, CV_8UC3);\r\n    m_model->Predict(bgr_mat, &res); \r\n    if (res.contain_score_map)\r\n    {\r\n      for (int i = 0; i < res.score_map.size(); ++i)\r\n      {\r\n        if (res.score_map[i] < m_init_param.score_thresh)\r\n        {\r\n          res.label_map[i] = 0;\r\n        }\r\n      }\r\n    }\r\n```\r\n\r\n\r\n",
        "state": "closed",
        "user": "zhuqiang00099",
        "closed_by": "zhuqiang00099",
        "created_at": "2023-02-22T12:23:06+00:00",
        "updated_at": "2023-02-23T11:36:22+00:00",
        "closed_at": "2023-02-23T11:36:22+00:00",
        "comments_count": [
            "jiangjiajun",
            "zhuqiang00099",
            "jiangjiajun",
            "jiangjiajun",
            "zhuqiang00099"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1425,
        "title": "Windows sever环境初始化失败",
        "body": "## 环境\r\n- 【FastDeploy版本】： fastdeploy-windows-gpu-1.0.3\r\n- 【编译命令】预编译\r\n- 【系统平台】: Windows sever2016\r\n- 【硬件】： CPU hygon C86 7255，如 Nvidia GPU RTX5000， CUDA 11.4 CUDNN 8.2\r\n- 【编译语言】：Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\nimport fastdeploy失败\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed while importing fastdeploy_main: 动态链接库(DLL)初始化例程失败。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n\r\n请问是CPU不支持，还是操作系统的原因呢？",
        "state": "closed",
        "user": "UstbLjc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-23T08:22:13+00:00",
        "updated_at": "2024-09-03T06:41:52+00:00",
        "closed_at": "2024-09-03T06:41:52+00:00",
        "comments_count": [
            "jiangjiajun",
            "wall-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1426,
        "title": "cpu 100%",
        "body": "ubuntu下 cpu版为何只用了一个核心，多线程有什么参考代码吗？",
        "state": "closed",
        "user": "taojishou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-23T08:44:16+00:00",
        "updated_at": "2024-03-05T06:40:30+00:00",
        "closed_at": "2024-03-05T06:40:30+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1427,
        "title": "failed to allocate pinned system memory: no pinned memory pool 可能是什么原因",
        "body": "\r\n使用环境一台阿里云 linux （centos7.2）测试机\r\n运行方式：docker\r\n版本： registry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-cpu-only-21.10 \r\n参照用例部署OCR： FastDeploy/blob/release/1.0.3/examples/vision/ocr/PP-OCRv3/serving/README_CN.md\r\n部署上没有出现错误信息\r\n\r\n测试：  python3 client.py\r\nClient返回：\r\ntext=    score= 0.0\r\n\r\nServer报错：\r\n![image](https://user-images.githubusercontent.com/2025317/220864612-1d78d96f-fd0e-49ef-bbfd-ec95b6133153.png)\r\n\r\n",
        "state": "closed",
        "user": "cuinfo",
        "closed_by": "heliqi",
        "created_at": "2023-02-23T09:25:11+00:00",
        "updated_at": "2023-02-28T06:50:12+00:00",
        "closed_at": "2023-02-28T06:50:12+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1434,
        "title": "How to use fastdeploy on rknpu2  + android environment ?",
        "body": "I have found documents on how to deploy on rknpu2 and how to deploy on android. But I could not find how to deploy on rk3588 android environment. Any help?",
        "state": "closed",
        "user": "3togo",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-02-24T12:43:43+00:00",
        "updated_at": "2023-03-24T16:14:17+00:00",
        "closed_at": "2023-03-24T16:14:17+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "3togo",
            "3togo",
            "Zheng-Bicheng",
            "3togo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1436,
        "title": "请问能直接推理gpu地址的图像数据么",
        "body": "\n\n摄像头数据通过gpu硬解后，数据在gpu里，能直接推理么？不然得copy到cpu再传给fastdeploy去gpu推理",
        "state": "closed",
        "user": "yueyue0574",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-25T23:34:15+00:00",
        "updated_at": "2024-04-16T09:03:02+00:00",
        "closed_at": "2024-04-16T09:03:02+00:00",
        "comments_count": [
            "wang-xinyu",
            "yueyue0574",
            "wang-xinyu",
            "yueyue0574"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1438,
        "title": "import fastdeploy 失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python     1.0.4\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 3060， CUDA 11.7 CUDNN 8.3\r\n- 【编译语言】： （conda ）Python3.8\r\n\r\n## 问题\r\n![%U3SLO% W~`(5RDXK1_W@H9](https://user-images.githubusercontent.com/109637491/221401837-3ccacbef-5637-4bf0-ad9f-70cfa51481a0.png)\r\n![RUR_GU0OQAESW4NBVHC%84L](https://user-images.githubusercontent.com/109637491/221401844-9984d35f-e079-4ab7-a4e1-1934b94d048b.png)\r\n![F91LNTJZ{I{P}L8)H6{9 1R](https://user-images.githubusercontent.com/109637491/221401847-dcf0ceed-9bf5-4cfc-bb7a-7052694003b6.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "kewuyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-26T09:12:02+00:00",
        "updated_at": "2024-03-05T06:40:30+00:00",
        "closed_at": "2024-03-05T06:40:30+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1439,
        "title": "ocr部署示例中，通过simple server部署还是triton部署的方式更好？",
        "body": "如题",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-27T01:38:52+00:00",
        "updated_at": "2024-03-05T06:40:31+00:00",
        "closed_at": "2024-03-05T06:40:31+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1443,
        "title": "如何直接获取预测类别名称",
        "body": "工程师您好：\r\n   请问一下，部署ppcls模型的时候，想要直接获取预测类别名称，怎么办呢？\r\n   \r\n   目前，源代码返回的是id号。",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "happybear1015",
        "created_at": "2023-02-27T04:26:29+00:00",
        "updated_at": "2023-03-08T04:33:44+00:00",
        "closed_at": "2023-03-08T04:33:34+00:00",
        "comments_count": [
            "leiqing1",
            "happybear1015"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1444,
        "title": "plugins.xml文件编译后会被自动删除",
        "body": "plugins.xml文件, 为什么每次编译后，都会自动删除这个文件?\r\n\r\n每次都要粘贴到文件夹，程序才可以运行，有点麻烦，请问这文件的用处是什么？为何会被自动删除？",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-27T05:28:05+00:00",
        "updated_at": "2024-06-18T06:41:07+00:00",
        "closed_at": "2024-06-18T06:41:07+00:00",
        "comments_count": [
            "jiangjiajun",
            "happybear1015",
            "jiangjiajun",
            "happybear1015",
            "jiangjiajun",
            "happybear1015",
            "jiangjiajun",
            "happybear1015",
            "zzaozhuang",
            "happybear1015",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1447,
        "title": "mobileseg部署到rv1126上推理时间过慢，256*256图片推理时间800ms",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "closed",
        "user": "ZHIZIHUABU",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-27T08:22:32+00:00",
        "updated_at": "2024-03-12T06:39:13+00:00",
        "closed_at": "2024-03-12T06:39:13+00:00",
        "comments_count": [
            "ZHIZIHUABU",
            "yeliang2258",
            "ZHIZIHUABU",
            "yeliang2258",
            "ZHIZIHUABU",
            "ZHIZIHUABU",
            "ZHIZIHUABU",
            "yeliang2258",
            "ZHIZIHUABU",
            "ZHIZIHUABU",
            "yeliang2258"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1454,
        "title": "Yolov8 Python多线程/进程部署示例",
        "body": "`fd.vision.detection.YOLOv8`\r\n\r\n线程\r\n```\r\nTraceback (most recent call last):\r\n  File \"multi_thread_process.py\", line 168, in <module>\r\n    args=(model.clone(), imgs_list[i * image_num_each_thread:],\r\nAttributeError: 'YOLOv8' object has no attribute 'clone'\r\n\r\n```\r\n\r\n进程\r\n```\r\n\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\nTraceback (most recent call last):\r\n  File \"multi_thread_process.py\", line 150, in <module>\r\n    with Pool(\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\context.py\", line 119, in Pool\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\pool.py\", line 212, in __init__\r\n    self._repopulate_pool()\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\pool.py\", line 303, in _repopulate_pool\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\pool.py\", line 326, in _repopulate_pool_static\r\n    w.start()\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\context.py\", line 327, in _Popen\r\n    return Popen(process_obj)\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nTypeError: cannot pickle 'fastdeploy.libs.fastdeploy_main.RuntimeOption' object\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\spawn.py\", line 107, in spawn_main\r\n    new_handle = reduction.duplicate(pipe_handle,\r\n  File \"D:\\anaconda31114\\envs\\FastDeployGPU\\lib\\multiprocessing\\reduction.py\", line 79, in duplicate\r\n    return _winapi.DuplicateHandle(\r\nPermissionError: [WinError 5] 拒绝访问。\r\n```\r\n\r\n有没yolov8的 多线程/进程 代码示例\r\n别用argparse\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-02-27T12:58:03+00:00",
        "updated_at": "2023-02-27T13:07:34+00:00",
        "closed_at": "2023-02-27T13:07:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1453,
        "title": "paddle_ocr返回模型无法正常预测",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python-1.0.1\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】：TITAN X ， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】：Python3.8等\r\n~~~\r\ndef modelinit():\r\n    ...\r\n    ppocr_v3 = fd.vision.ocr.PPOCRv3(det_model=det_model, cls_model=cls_model, rec_model=rec_model)\r\n    return ppocr_v3\r\n~~~\r\n封装上述语句后，返回模型无法调用预测函数,在函数内可以正常调用，\r\n程序结束的错误代码为：Process finished with exit code -1073741819 (0xC0000005)\r\n\r\n",
        "state": "closed",
        "user": "LiQiang0307",
        "closed_by": "LiQiang0307",
        "created_at": "2023-02-27T12:36:09+00:00",
        "updated_at": "2023-03-01T04:45:01+00:00",
        "closed_at": "2023-03-01T04:45:01+00:00",
        "comments_count": [
            "jiangjiajun",
            "LiQiang0307"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1455,
        "title": "对文件夹进行监控,有图片就检测,detection.YOLOv8,Python多进程怎么部署",
        "body": "对文件夹进行监控,有图片就检测,\r\n`fd.vision.detection.YOLOv8`\r\n多进程怎么部署\r\n\r\n比如一下子几十张图片进来,如何高并发的检测,快速检测",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-27T13:09:49+00:00",
        "updated_at": "2024-04-24T00:45:04+00:00",
        "closed_at": "2024-04-16T09:03:03+00:00",
        "comments_count": [
            "heliqi",
            "monkeycc",
            "heliqi",
            "monkeycc",
            "heliqi",
            "monkeycc",
            "laborer123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1457,
        "title": "请问什么时候有IOS 的支持呀",
        "body": "如题\r\n",
        "state": "closed",
        "user": "ANDROIDTODO",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-28T02:28:10+00:00",
        "updated_at": "2024-05-14T06:42:08+00:00",
        "closed_at": "2024-05-14T06:42:08+00:00",
        "comments_count": [
            "leiqing1",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1471,
        "title": "PP-YOLOE模型使用服务器部署请求失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 使用registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-cpu-only-21.10 镜像环境\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： i5 1240P cpu\r\n- 【编译语言】： Python\r\n\r\n## 问题日志及出现问题的操作流程\r\n完全按照文档《PaddleDetection 服务化部署示例》进行操作\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README_CN.md\r\ntriton服务器能成功启动起来，使用[paddledet_grpc_client.py](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/paddledet_grpc_client.py)进行请求时，客户端抛出以下错误：\r\n<img width=\"817\" alt=\"1677638843678\" src=\"https://user-images.githubusercontent.com/18228342/222032147-73df361f-c118-4647-9572-97cac4ef3ca7.png\">\r\n此时服务器端打印以下错误信息：\r\n<img width=\"741\" alt=\"8378a74c2a73c31aa3862e0df7b6e0d\" src=\"https://user-images.githubusercontent.com/18228342/222032193-7da8f945-77eb-440a-8765-d02abebca1f8.png\">\r\n\r\n## 排查思路：\r\n将镜像换成registry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-cpu-only-21.10后，加载一样的model目录，使用client请求就能成功。就是将docker镜像回退两个版本后，没有这个错误。\r\n",
        "state": "closed",
        "user": "flamebox",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T02:59:25+00:00",
        "updated_at": "2024-04-16T09:03:06+00:00",
        "closed_at": "2024-04-16T09:03:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1458,
        "title": "AttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'text'",
        "body": "执行 python infer.py --model_dir uie-base/\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 134, in <module>\r\n    uie = UIEModel(\r\n  File \"/home/kwx/Software/Anaconda/envs/uie/lib/python3.8/site-packages/fastdeploy/text/uie/__init__.py\", line 62, in __init__\r\n    schema_tmp += [SchemaNode(key, val)._schema_node]\r\n  File \"/home/kwx/Software/Anaconda/envs/uie/lib/python3.8/site-packages/fastdeploy/text/uie/__init__.py\", line 34, in __init__\r\n    schema_node_children += [C.text.SchemaNode(child, [])]\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'text'\r\n\r\n编译：\r\n按照readme里的python SDK编译然后pip install wheel\r\n",
        "state": "closed",
        "user": "Juelianqvq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-28T03:25:31+00:00",
        "updated_at": "2024-04-16T09:03:04+00:00",
        "closed_at": "2024-04-16T09:03:04+00:00",
        "comments_count": [
            "jiangjiajun",
            "Juelianqvq",
            "jiangjiajun",
            "Juelianqvq",
            "jiangjiajun",
            "Juelianqvq"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1462,
        "title": "PPYOLOE 行人追踪 region_type  region_polygon 参数设置",
        "body": "[PP-Human检测跟踪模块](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.6/deploy/pipeline/docs/tutorials/pphuman_mot.md#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95), [PP-Human检测跟踪模块](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.6/deploy/pipeline/docs/tutorials/pphuman_mot.md#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95)中可以设置 `region_type` `region_polygon `区域l类型和坐标，在`fastdeploy`中怎么设置`region_type` `region_polygon `参数？？ \r\n\r\n```python\r\npython deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml \\\r\n                                                   --video_file=test_video.mp4 \\\r\n                                                   --device=gpu \\\r\n                                                   --draw_center_traj \\\r\n                                                   --do_break_in_counting \\\r\n                                                   --region_type=custom \\\r\n                                                   --region_polygon 200 200 400 200 300 400 100 400\r\n```",
        "state": "closed",
        "user": "chccc1994",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-28T08:37:02+00:00",
        "updated_at": "2024-03-05T06:40:32+00:00",
        "closed_at": "2024-03-05T06:40:32+00:00",
        "comments_count": [
            "chccc1994",
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1469,
        "title": "fastdeploy compress 报错",
        "body": "您好！我安装了fastdeploy：pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html；以及\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n也安装了paddle：python -m pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\r\n目前我可以正常训练模型；但是调用compress进行自动压缩量化时报错（已从头重装过2/3次，报错一致）：\r\n  File \"/home/chenailin/.local/lib/python3.8/site-packages/common_tools/auto_compression/fd_auto_compress/fd_auto_compress.py\", line 21, in <module>\r\n    import paddle\r\nModuleNotFoundError: No module named 'paddle'",
        "state": "closed",
        "user": "sdreamforchen",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T01:50:48+00:00",
        "updated_at": "2024-03-05T06:40:33+00:00",
        "closed_at": "2024-03-05T06:40:33+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1463,
        "title": "How to pass \"CMAKE_HOST_SYSTEM_PROCESSOR=aarch64\" to opencv.cmake when cross compiling",
        "body": "I need to use opencv-linux-aarch64-3.4.14 to do the cross-compilation. To do so, I need  to set \r\nCMAKE_HOST_SYSTEM_PROCESSOR=aarch64\r\nHowever adding  -DCMAKE_HOST_SYSTEM_PROCESSOR=aarch64  to cmake command could not pass this variable assignment  to opencv.cmake\r\nMy question is how to set CMAKE_HOST_SYSTEM_PROCESSOR=aarch64 on opencv.cmake\r\n\r\nAny help?\r\n\r\n\r\n\r\n\r\n```\r\ncmake -DCMAKE_TOOLCHAIN_FILE=cmake/toolchain.cmake \\\r\n-DCMAKE_HOST_SYSTEM_PROCESSOR=aarch64 \\\r\n-DCMAKE_SYSTEM_NAME=Linux -DTARGET_SOC=${TARGET_SOC} \\\r\n-DCMAKE_SYSTEM_PROCESSOR=aarch64 \\\r\n-DCMAKE_CROSSCOMPILING=TRUE \\\r\n-DCMAKE_C_COMPILER=${BIN}/aarch64-rockchip-linux-gnu-gcc \\\r\n-DCMAKE_CXX_COMPILER=${BIN}/aarch64-rockchip-linux-gnu-g++ \\\r\n-DCMAKE_FIND_ROOT_PATH=${SYSROOT} \\\r\n-DCMAKE_SYSROOT=${SYSROOT} \\\r\n-DENABLE_VISION=ON \\\r\n-DENABLE_TEXT=OFF \\\r\n-DENABLE_ENCRYPTION=OFF \\\r\n.\"\r\n\r\n\r\n```\r\n\r\n\r\n\r\nBelow is extracted from CMakeLists.txt at the root of FastDeploy Folder\r\n```\r\nif(ENABLE_VISION)\r\n  add_definitions(-DENABLE_VISION)\r\n  add_subdirectory(${PROJECT_SOURCE_DIR}/third_party/yaml-cpp)\r\n  list(APPEND DEPEND_LIBS yaml-cpp)\r\n  list(APPEND ALL_DEPLOY_SRCS ${DEPLOY_VISION_SRCS})\r\n  list(APPEND ALL_DEPLOY_SRCS ${DEPLOY_PIPELINE_SRCS})\r\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/yaml-cpp/include)\r\n  include(${PROJECT_SOURCE_DIR}/cmake/opencv.cmake)\r\n\r\n```\r\n\r\nBelow is extracted from cmake/opencv.cmake\r\n\r\n```\r\n# Linux\r\nelse()\r\n  if(CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"aarch64\")\r\n    set(OPENCV_FILENAME \"opencv-linux-aarch64-3.4.14\")\r\n  endif()\r\nendif()\r\n\r\nif(NOT OPENCV_FILENAME)\r\n  set(OPENCV_FILENAME \"opencv-linux-x64-3.4.16\")\r\nendif()\r\n```",
        "state": "closed",
        "user": "3togo",
        "closed_by": "3togo",
        "created_at": "2023-02-28T11:24:45+00:00",
        "updated_at": "2023-03-01T02:55:59+00:00",
        "closed_at": "2023-03-01T02:55:58+00:00",
        "comments_count": [
            "jiangjiajun",
            "3togo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1467,
        "title": "ocr example运行报错",
        "body": "示例的图片，12.jpg可以正常运行，但是自测了一张文档，报错rec模型和cls模型**batch_size**太小。之后将两个模型的batch_size调整成256之后，报显存太小，因此之后对**det模型的后处理进行调整**，将det_box切割成batch_size=5去预测，发现预测的速度非常慢，一张普通图片的时间要5s，显卡配置为rtx3090ti，请问下想提高性能怎么解决？\r\n\r\n修改之后的/data/FastDeploy-develop/examples/vision/ocr/PP-OCRv3/serving/models/det_postprocess/1/model.py内容，从228行之后如下：\r\n```\r\nfor i in range(0, len(image_list), predict_size):\r\n    image_list_batch = image_list[i:i+predict_size]\r\n    rec_texts_, rec_scores_, cls_labels_, cls_scores_ = self.predict_batch_images(image_list_batch)\r\n    # print('jer, so,why', rec_texts_, type(rec_texts_))\r\n    rec_texts += rec_texts_.tolist()\r\n    rec_scores += rec_scores_.tolist()\r\n    cls_labels += cls_labels_.tolist()\r\n    cls_scores += cls_scores_.tolist()\r\n```\r\n下面为对box拆分为小batch预测的代码：\r\n```\r\ndef predict_batch_images(self, image_list):\r\n        cls_labels = []\r\n        cls_scores = []\r\n        rec_texts = []\r\n        rec_scores = []\r\n\r\n        cls_pre_tensors = self.cls_preprocessor.run(image_list)\r\n        cls_dlpack_tensor = cls_pre_tensors[0].to_dlpack()\r\n        # print(type(cls_dlpack_tensor), 'haha')\r\n        cls_input_tensor = pb_utils.Tensor.from_dlpack(\r\n            \"x\", cls_dlpack_tensor)\r\n\r\n        inference_request = pb_utils.InferenceRequest(\r\n            model_name='cls_pp',\r\n            requested_output_names=['cls_labels', 'cls_scores'],\r\n            inputs=[cls_input_tensor])\r\n        inference_response = inference_request.exec()\r\n        if inference_response.has_error():\r\n            raise pb_utils.TritonModelException(\r\n                inference_response.error().message())\r\n        else:\r\n            # Extract the output tensors from the inference response.\r\n            cls_labels = pb_utils.get_output_tensor_by_name(\r\n                inference_response, 'cls_labels')\r\n            cls_labels = cls_labels.as_numpy()\r\n\r\n            cls_scores = pb_utils.get_output_tensor_by_name(\r\n                inference_response, 'cls_scores')\r\n            cls_scores = cls_scores.as_numpy()\r\n\r\n        for index in range(len(image_list)):\r\n            if cls_labels[index] == 1 and cls_scores[\r\n                    index] > self.cls_threshold:\r\n                image_list[index] = cv2.rotate(\r\n                    image_list[index].astype(np.float32), 1)\r\n                image_list[index] = np.astype(np.uint8)\r\n\r\n        rec_pre_tensors = self.rec_preprocessor.run(image_list)\r\n        rec_dlpack_tensor = rec_pre_tensors[0].to_dlpack()\r\n        rec_input_tensor = pb_utils.Tensor.from_dlpack(\r\n            \"x\", rec_dlpack_tensor)\r\n\r\n        inference_request = pb_utils.InferenceRequest(\r\n            model_name='rec_pp',\r\n            requested_output_names=['rec_texts', 'rec_scores'],\r\n            inputs=[rec_input_tensor])\r\n        inference_response = inference_request.exec()\r\n        if inference_response.has_error():\r\n            raise pb_utils.TritonModelException(\r\n                inference_response.error().message())\r\n        else:\r\n            # Extract the output tensors from the inference response.\r\n            rec_texts = pb_utils.get_output_tensor_by_name(\r\n                inference_response, 'rec_texts')\r\n            rec_texts = rec_texts.as_numpy()\r\n\r\n            rec_scores = pb_utils.get_output_tensor_by_name(\r\n                inference_response, 'rec_scores')\r\n            rec_scores = rec_scores.as_numpy()\r\n        return rec_texts, rec_scores, cls_labels, cls_scores\r\n```",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-02-28T12:20:37+00:00",
        "updated_at": "2024-04-16T09:03:05+00:00",
        "closed_at": "2024-04-16T09:03:05+00:00",
        "comments_count": [
            "jiangjiajun",
            "yywangfei",
            "yunyaoXYY",
            "yywangfei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1472,
        "title": "ocr serving bug修正",
        "body": "建议/data/FastDeploy-develop/examples/vision/ocr/PP-OCRv3/serving/models/det_postprocess/1/model.py 第194行修改为：\r\nimage_list[index] = image_list[index].astype(np.uint8)\r\n原始的代码为：\r\nimage_list[index] = np.astype(np.uint8)\r\n\r\n",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T03:18:45+00:00",
        "updated_at": "2024-03-05T06:40:34+00:00",
        "closed_at": "2024-03-05T06:40:34+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1473,
        "title": "参考官方C++多线程示例使用PPYOLOE进行多线程推理测试报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.4\r\n- 【使用方式】 [Visual Studio 2019 创建 CMake 工程使用 C++ SDK](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows_build.md#VisualStudio2019)\r\n- 【系统平台】:  Windows x64(Windows11)\r\n- 【硬件】： Nvidia GPU 3060TI CUDA 11.2.2 CUDNN 8.2.1 TensorRT 8.2.4.2\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 参考官方[多线程示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/tutorials/multi_thread/cpp/single_model/multi_thread.cc)使用PPYOLOE进行多线程测试。\r\n- 仅做如下更改：\r\n  - 将`inference.pdmodel`、`inference.pdiparams`、`inference_cls.yaml`分别替换为`model.pdmodel`、`model.pdiparams`、`infer_cfg.yml`;\r\n  - 将`fastdeploy::vision::classification::PaddleClasModel`替换为`fastdeploy::vision::detection::PPYOLOE`;\r\n  - 将`fastdeploy::vision::ClassifyResult res`替换为`fastdeploy::vision::DetectionResult res`;\r\n  - 将`option.SetTrtInputShape(\"inputs\", { 1, 3, 224, 224 });`替换为`option.SetTrtInputShape(\"inputs\", { 1, 3, 640, 640 });`;\r\n- 进行构建时出现报错, 直接构建官方的`multi_thread.cc`是成功的，经过定位后发现报错出现以下代码位置:\r\n```cpp\r\n    std::vector<std::thread> threads;\r\n    for (int i = 0; i < thread_num; ++i) {\r\n        threads.emplace_back(Predict, models[i].get(), i, image_list[i]);\r\n    }\r\n```\r\n- 报错如下:\r\n\r\n![image](https://user-images.githubusercontent.com/38065710/222040598-7ce1a503-6ce1-41a3-8fc6-5a961235ef6a.png)\r\n\r\n```\r\nD:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread(55,14): error C2672: “std::invoke”: 未找到匹配的重载函数 \r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread(62): message : 查看对正在编译的函数 模板 实例化“unsigned int std::thread::_Invoke<_Tuple,0,1,2,3>(void *) noexcept”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Tuple=_Tuple\r\n            ]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread(69): message : 查看对正在编译的函数 模板 实例化“unsigned int (__cdecl *std::thread::_Get_invoke<_Tuple,0,1,2,3>(std::integer_sequence<size_t,0,1,2,3>) noexcept)(void *)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread(90): message : 查看对正在编译的函数 模板 实例化“void std::thread::_Start<void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),_Ty,int&,std::vector<std::string,std::allocator<std::string>>&>(_Fn,_Ty &&,int &,std::vector<std::string,std::allocator<std::string>> &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Ty=fastdeploy::vision::detection::PPDetBase *,\r\n                _Fn=void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &)\r\n            ]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\xmemory(681): message : 查看对正在编译的函数 模板 实例化“std::thread::thread<void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),_Ty,int&,std::vector<std::string,std::allocator<std::string>>&,0>(_Fn,_Ty &&,int &,std::vector<std::string,std::allocator<std::string>> &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Ty=fastdeploy::vision::detection::PPDetBase *,\r\n                _Fn=void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &)\r\n            ]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\vector(713): message : 查看对正在编译的函数 模板 实例化“void std::_Default_allocator_traits<_Alloc>::construct<_Ty,void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase*,int&,std::vector<std::string,std::allocator<std::string>>&>(_Alloc &,_Objty *const ,void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase *&&,int &,std::vector<std::string,std::allocator<std::string>> &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Alloc=std::allocator<std::thread>,\r\n                _Ty=std::thread,\r\n                _Objty=std::thread\r\n            ]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\vector(713): message : 查看对正在编译的函数 模板 实例化“void std::_Default_allocator_traits<_Alloc>::construct<_Ty,void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase*,int&,std::vector<std::string,std::allocator<std::string>>&>(_Alloc &,_Objty *const ,void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase *&&,int &,std::vector<std::string,std::allocator<std::string>> &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Alloc=std::allocator<std::thread>,\r\n                _Ty=std::thread,\r\n                _Objty=std::thread\r\n            ]\r\n  D:\\Laugh\\Programs\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\vector(731): message : 查看对正在编译的函数 模板 实例化“void std::vector<std::thread,std::allocator<std::thread>>::_Emplace_back_with_unused_capacity<void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),_Ty,int&,std::vector<std::string,std::allocator<std::string>>&>(void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),_Ty &&,int &,std::vector<std::string,std::allocator<std::string>> &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Ty=fastdeploy::vision::detection::PPDetBase *\r\n            ]\r\n  E:\\Laugh\\Projects\\infer_ppyoloe\\infer_ppyoloe.cpp(151): message : 查看对正在编译的函数 模板 实例化“void std::vector<std::thread,std::allocator<std::thread>>::emplace_back<void(__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase*,int&,_Ty&>(void (__cdecl &)(fastdeploy::vision::detection::PPYOLOE *,int,const std::vector<std::string,std::allocator<std::string>> &),fastdeploy::vision::detection::PPDetBase *&&,int &,_Ty &)”的引用 [E:\\Laugh\\Projects\\infer_ppyoloe\\out\\build\\x64-Release\\infer_ppyoloe_demo.vcxproj]\r\n            with\r\n            [\r\n                _Ty=std::vector<std::string,std::allocator<std::string>>\r\n```\r\n",
        "state": "closed",
        "user": "laugh12321",
        "closed_by": "laugh12321",
        "created_at": "2023-03-01T03:52:48+00:00",
        "updated_at": "2023-03-01T12:47:35+00:00",
        "closed_at": "2023-03-01T04:02:39+00:00",
        "comments_count": [
            "laugh12321",
            "KunMengcode",
            "laugh12321"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1474,
        "title": "在rv1126开发板部署分割模型后，调用npu推理时间需要800ms",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "closed",
        "user": "ZHIZIHUABU",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T05:38:05+00:00",
        "updated_at": "2024-04-16T09:03:07+00:00",
        "closed_at": "2024-04-16T09:03:07+00:00",
        "comments_count": [
            "ZHIZIHUABU"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1475,
        "title": "FastDeploy版本1.0.4预编译库下载好， samples/yolov5 文件夹里面没有csharp ，自己复制吗？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "closed",
        "user": "zhinangubei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T07:39:27+00:00",
        "updated_at": "2024-03-05T06:40:35+00:00",
        "closed_at": "2024-03-05T06:40:35+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1476,
        "title": "FastDeploy支持PaddleYOLO系列和PaddleDetection系列进行部署吗",
        "body": "FastDeploy支持PaddleYOLO系列和PaddleDetection系列进行部署吗，小白不知道怎么使用FastDeploy调用PaddleYOLO训练的模型配置文件，使用PaddleYOLO训练之后output里只有pdiparams文件，不知道pdmodel配置文件和infer_cfg.yml文件在哪里",
        "state": "closed",
        "user": "Wei0205",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T08:12:57+00:00",
        "updated_at": "2024-11-09T15:54:11+00:00",
        "closed_at": "2024-03-05T06:40:35+00:00",
        "comments_count": [
            "jiangjiajun",
            "NyquistBodeTu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1477,
        "title": "编译fastdeploy streamer出现 <dlpack/dlpack.h><SYCL/sycl.hpp> \"unsupported/Eigen/CXX11/Tensor\"\"unsupported/Eigen/CXX11/Tensor\"错误",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.4\r\n- 【编译命令】cmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-x64-x.x.x\r\n                        make -j ------>>>执行此句报出如下错误\r\n- 【系统平台】: wls2 Ubuntu 22.04\r\n- 【硬件】： Nvidia GPU T4， CUDA 11.7 CUDNN 8.8\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n[ 21%] Built target yaml-cpp\r\n[ 22%] Built target yaml-cpp-parse\r\n[ 24%] Built target yaml-cpp-sandbox\r\n[ 25%] Built target yaml-cpp-read\r\n[ 26%] Building CXX object CMakeFiles/fd_streamer.dir/src/dlpack/contrib/mock_main.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/contraction_benchmarks_cpu.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/tensor_benchmarks_cpu.cc.o\r\n[ 29%] Building CXX object CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/tensor_contract_sycl_bench.cc.o\r\n/root/build/FastDeploy/streamer/src/dlpack/contrib/mock_main.cc:2:10: fatal error: dlpack/dlpack.h: No such file or directory\r\n    2 | #include <dlpack/dlpack.h>\r\n      |          ^~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n/root/build/FastDeploy/streamer/src/eigen/bench/tensors/tensor_contract_sycl_bench.cc:18:10: fatal error: SYCL/sycl.hpp: No such file or directory\r\n   18 | #include <SYCL/sycl.hpp>\r\n      |          ^~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/fd_streamer.dir/build.make:132: CMakeFiles/fd_streamer.dir/src/dlpack/contrib/mock_main.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[2]: *** [CMakeFiles/fd_streamer.dir/build.make:202: CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/tensor_contract_sycl_bench.cc.o] Error 1\r\nIn file included from /root/build/FastDeploy/streamer/src/eigen/bench/tensors/tensor_benchmarks_cpu.cc:5:\r\n/root/build/FastDeploy/streamer/src/eigen/bench/tensors/tensor_benchmarks.h:8:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\n    8 | #include \"unsupported/Eigen/CXX11/Tensor\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nIn file included from /root/build/FastDeploy/streamer/src/eigen/bench/tensors/contraction_benchmarks_cpu.cc:5:\r\n/root/build/FastDeploy/streamer/src/eigen/bench/tensors/tensor_benchmarks.h:8:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\n    8 | #include \"unsupported/Eigen/CXX11/Tensor\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/fd_streamer.dir/build.make:174: CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/tensor_benchmarks_cpu.cc.o] Error 1\r\nmake[2]: *** [CMakeFiles/fd_streamer.dir/build.make:160: CMakeFiles/fd_streamer.dir/src/eigen/bench/tensors/contraction_benchmarks_cpu.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:148: CMakeFiles/fd_streamer.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2\r\n",
        "state": "closed",
        "user": "jiangxinufo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T08:43:31+00:00",
        "updated_at": "2024-03-05T06:40:36+00:00",
        "closed_at": "2024-03-05T06:40:36+00:00",
        "comments_count": [
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1478,
        "title": "目标检测如何返回目标的类别名称？",
        "body": "\r\nDetectionResult 返回的是id，请问如何能返回目标的类别名称？",
        "state": "closed",
        "user": "jbnjbn9",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-01T08:57:30+00:00",
        "updated_at": "2024-06-25T06:41:00+00:00",
        "closed_at": "2024-06-25T06:41:00+00:00",
        "comments_count": [
            "jiangjiajun",
            "Gentek36"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1483,
        "title": "InvalidArgumentError: fail to get creator of CustomSkipLayerNormPluginDynamic",
        "body": "self._model = C.text.UIEModel(model_file, params_file, vocab_file,\r\nRuntimeError:\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle_infer::CreatePredictor(paddle::AnalysisConfig const&)\r\n1   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n2   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n3   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n4   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n5   paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n6   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)\r\n7   paddle::inference::analysis::IRPassManager::Apply(std::unique_ptr<paddle::framework::ir::Graph, std::default_delete<paddle::framework::ir::Graph> >)\r\n8   paddle::framework::ir::Pass::Apply(paddle::framework::ir::Graph*) const\r\n9   paddle::inference::analysis::TensorRtSubgraphPass::ApplyImpl(paddle::framework::ir::Graph*) const\r\n10  paddle::inference::analysis::TensorRtSubgraphPass::CreateTensorRTOp(paddle::framework::ir::Node*, paddle::framework::ir::Graph*, std::vector<std::string, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > >*) const\r\n11  phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n12  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: fail to get creator of CustomSkipLayerNormPluginDynamic\r\n  [Hint: Expected creator != nullptr, but received creator == nullptr.] (at /build/Paddle/paddle/fluid/inference/tensorrt/convert/skip_layernorm.cc:109)\r\n\r\n",
        "state": "closed",
        "user": "Juelianqvq",
        "closed_by": "Juelianqvq",
        "created_at": "2023-03-01T14:53:58+00:00",
        "updated_at": "2023-03-02T09:59:11+00:00",
        "closed_at": "2023-03-02T09:59:11+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1484,
        "title": "根据paddleseg rknpu2 pythonREADME，运行报参数出错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy         2.2.6 \r\n```\r\npip install fastdeploy\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\n```\r\n\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】:  Linux rock-5b 5.10.110-rockchip-rk3588 #23.02.2 SMP Fri Feb 17 23:59:20 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux\r\n- 【硬件】： Rock5b  https://wiki.radxa.com/Rock5/5b/getting_started\r\n- 【编译语言】：Python 3.9.16\r\n- \r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n\r\npip list\r\n```\r\n(rknn2) rock-5b:python:% pip list                                                                                     <develop ✗>\r\nPackage            Version\r\n------------------ ---------\r\ncertifi            2022.12.7\r\ncharset-normalizer 3.0.1\r\ndiskcache          5.4.0\r\nfalcon             3.1.1\r\nfastdeploy         2.2.6\r\ngevent             22.10.2\r\ngevent-websocket   0.10.1\r\ngreenlet           2.0.2\r\ngunicorn           20.1.0\r\nidna               3.4\r\npip                22.3.1\r\nrequests           2.28.2\r\nsetuptools         65.6.3\r\nujson              5.7.0\r\nurllib3            1.26.14\r\nwheel              0.38.4\r\nzope.event         4.6\r\nzope.interface     5.5.2\r\n\r\n```\r\n- 【运行问题】 无法进行推演\r\n```\r\n(rknn2) rock-5b:python:% pwd                                                                                          <develop ✗>\r\n/home/edwardzhou/rknn2/FastDeploy/examples/vision/segmentation/paddleseg/rockchip/rknpu2/python\r\n\r\n\r\npython3 infer.py --model_file ./Portrait_PP_HumanSegV2_Lite_256x144_infer/Portrait_PP_HumanSegV2_Lite_256x144_infer_rk3588.rknn \\\r\n                --config_file ./Portrait_PP_HumanSegV2_Lite_256x144_infer/deploy.yaml \\\r\n                --image images/portrait_heng.jpg\r\n\r\nusage: infer.py [-h] --recipe RECIPE --mode MODE [--queue_dir QUEUE_DIR] [--base BASE] [--docker_args DOCKER_ARGS]\r\ninfer.py: error: the following arguments are required: --recipe, --mode\r\n\r\n```\r\n\r\n",
        "state": "closed",
        "user": "edwardzhou",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-02T01:51:29+00:00",
        "updated_at": "2024-08-09T02:00:31+00:00",
        "closed_at": "2023-03-03T07:53:55+00:00",
        "comments_count": [
            "edwardzhou",
            "Zheng-Bicheng",
            "edwardzhou",
            "Zheng-Bicheng",
            "SeaSpring17",
            "Zheng-Bicheng",
            "zhazhaming",
            "LiaoYuWei1"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1485,
        "title": "没看到支持IOS的SDK？",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n",
        "state": "closed",
        "user": "lmq5294249",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T02:11:07+00:00",
        "updated_at": "2024-03-05T06:40:37+00:00",
        "closed_at": "2024-03-05T06:40:37+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1487,
        "title": "python版本的rkyolov5的anchor设置问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop版本\r\n- 【编译命令】官方编译命令\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】：firefly的ROC-RK3588S-PC\r\n- 【编译语言】： python\r\n\r\nrkyolov5模型转换从pt格式转换为onnx然后转换为rknn，并且设置model.postprocessor.class_num之后运行成功。但目前尚未找到如何在python上设置yolov5的anchor大小。\r\n\r\n",
        "state": "closed",
        "user": "XDUwsk",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-02T07:04:35+00:00",
        "updated_at": "2023-04-07T02:22:45+00:00",
        "closed_at": "2023-04-07T02:22:45+00:00",
        "comments_count": [],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1488,
        "title": "编译fastdeploy streamer examples ppyoloe示例报错",
        "body": "【FastDeploy版本】： fastdeploy-linux-gpu-1.0.4\r\n【编译命令】cmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-x64-gpu-1.0.4\r\nmake -j ------>>>执行此句报出如下错误\r\n【系统平台】: wls2 Ubuntu 20.04\r\n【硬件】： Nvidia GPU 3070， CUDA 11.7 CUDNN 8.8\r\n【编译语言】： C++\r\n报错如下：\r\n/usr/bin/ld: /root/test/FastDeploy/streamer/examples/ppyoloe/cpp/../../../build/libfd_streamer.so: undefined reference to `YAML::detail::node_data::empty_scalar[abi:cxx11]()'\r\n/usr/bin/ld: /root/test/FastDeploy/streamer/examples/ppyoloe/cpp/../../../build/libfd_streamer.so: undefined reference to `YAML::detail::node_data::convert_to_map(std::shared_ptr<YAML::detail::memory_holder> const&)'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/streamer_demo.dir/build.make:97: streamer_demo] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:110: CMakeFiles/streamer_demo.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2",
        "state": "closed",
        "user": "zly19540609",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T07:09:19+00:00",
        "updated_at": "2024-04-16T09:03:07+00:00",
        "closed_at": "2024-04-16T09:03:07+00:00",
        "comments_count": [
            "wang-xinyu",
            "zly19540609",
            "wang-xinyu",
            "zly19540609",
            "wang-xinyu",
            "zly19540609",
            "wang-xinyu",
            "zly19540609",
            "wang-xinyu",
            "zly19540609"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1496,
        "title": "rk3588 的python 推理中如何使用3核心加速",
        "body": "./examples/vision/segmentation/paddleseg/rockchip/rknpu2/python/infer.py 如何配置3核心加速\r\n\r\n如c++\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/fastdeploy/runtime/runtime_option.h\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/fastdeploy/runtime/backends/rknpu2/option.h\r\nUserknpu2",
        "state": "closed",
        "user": "edwardzhou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T16:12:21+00:00",
        "updated_at": "2024-04-16T09:03:09+00:00",
        "closed_at": "2024-04-16T09:03:09+00:00",
        "comments_count": [],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1489,
        "title": "我在算能sc7 pcie设备中，循环运行推理， 设备显存会爆然后coredump, 另外还有不能指定运行核心的问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.3\r\n- 【系统平台】: centos7.8\r\n- 【硬件】： 算能sc7 pcie\r\n\r\n## 问题日志及出现问题的操作流程\r\n1.设备显存会爆然后coredump\r\nwhile( true ){\r\n    model.Predict(&im, &res) \r\n} ，你们可以跑下试试。\r\n\r\n2.而且sc7这种多芯卡，不能指定运行在多核上，之运行在第一个核\r\n\r\n![5d570b51bddc9a45e954a99da297bc2](https://user-images.githubusercontent.com/59901551/222374827-cd7da1cc-47f9-43a5-8adf-fea743372e5f.png)\r\n",
        "state": "closed",
        "user": "wangxu372848892",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T08:36:03+00:00",
        "updated_at": "2024-04-16T09:03:08+00:00",
        "closed_at": "2024-04-16T09:03:08+00:00",
        "comments_count": [
            "KhantiInNaraka",
            "wangxu372848892"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1492,
        "title": "c_api的接口相关",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【系统平台】: Linux x64(Ubuntu 20.04.1)\r\n- 【编译语言】： C++ / golang(1.19）\r\n\r\n## 问题日志及出现问题的操作流程\r\n1.\r\n使用FD_C_Imread正常读取图片文件，是没问题的。\r\n现在使用的时候，通过其他方式抽取了视频帧，只能先创建一个文件，通过FD_C_Imread来返回FD_C_Mat进行预测。\r\n希望可以提供一个api，通过图片数组、[]byte、指针地址或者其他方式，创建FD_C_Mat使用。\r\n\r\n2.\r\n在实际使用的时候，经常会有模型堆叠的问题，希望提供一个api，通过score_threshold，来过滤结果数据。\r\n\r\n",
        "state": "closed",
        "user": "oldma3095",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-02T10:06:42+00:00",
        "updated_at": "2025-04-22T06:44:20+00:00",
        "closed_at": "2025-04-22T06:44:20+00:00",
        "comments_count": [
            "rainyfly",
            "oldma3095",
            "oldma3095",
            "rainyfly",
            "rainyfly",
            "oldma3095",
            "cumthxy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1497,
        "title": "执行FastDeploy Streamer PP-YOLOE C++ Example中./streamer_demo时报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.4\r\n- 【编译命令】cmke\r\n- 【系统平台】: Linux x64(wls2 Ubuntu 18.04) \r\n- 【硬件】： Nvidia GPU 3080， CUDA 11.7 CUDNN 8.8\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\ndeepstream6.1，streamer在此之前都已经编译成功，在编译yoloe示例的时候出现以下错误：\r\nroot@jiangxin:~/build/FastDeploy/streamer/examples/ppyoloe/cpp/build# ./streamer_demo\r\napp\r\nnvurisrcbin_list\r\nnvstreammux\r\nnvinfer\r\nnvtracker\r\nnvmultistreamtiler\r\nnvosdbin\r\nnvvideoencfilesinkbin\r\n[INFO] /root/build/FastDeploy/streamer/src/gstreamer/utils.cc(60)::CreatePipeline       Trying to launch pipeline: nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_walk.mov gpu-id=0 ! mux.sink_0  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_walk.mov gpu-id=0 ! mux.sink_1  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_walk.mov gpu-id=0 ! mux.sink_2  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_walk.mov gpu-id=0 ! mux.sink_3  nvstreammux name=mux gpu-id=0 batch-size=4 width=1920 height=1080 batched-push-timeout=40000 ! nvinfer gpu-id=0 config-file-path=nvinfer_config.txt ! nvtracker gpu-id=0 tracker-width=640 tracker-height=640 ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_NvDCF_perf.yml enable-batch-process=true ! nvmultistreamtiler gpu-id=0 rows=2 columns=2 ! nvosdbin gpu-id=0 ! nvvideoencfilesinkbin gpu-id=0 bitrate=4000 output-file=out.mp4\r\n[ERROR] /root/build/FastDeploy/streamer/src/app/base_app.cc(112)::SetupPerfMeasurement  Can't find a properly sink bin in the pipeline\r\nAborted\r\n\r\n",
        "state": "closed",
        "user": "jiangxinufo",
        "closed_by": "wang-xinyu",
        "created_at": "2023-03-03T00:20:31+00:00",
        "updated_at": "2023-03-03T05:47:37+00:00",
        "closed_at": "2023-03-03T05:47:36+00:00",
        "comments_count": [
            "jiangxinufo",
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu",
            "jiangxinufo",
            "wang-xinyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1498,
        "title": "使用Python API中fastdeploy.vision.detection.PicoDet的batch_predict报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python-1.0.3\r\n- 【系统平台】: Linux x64(CentOS Linux release 7.5.1804 (Core))\r\n- 【硬件】： i7-8700K CPU @ 3.70GHz\r\n- 【镜像】： PaddlePaddle-2.3.1-GPU\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n在Jupyter Lab中的运行代码如下：\r\n```\r\nimport os\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.PicoDet(\"/my/path/to/model.pdmodel\",\r\n                                 \"/my/path/to/model.pdiparams\",\r\n                                 \"/my/path/to/infer_cfg.yml\")\r\nimages = list()\r\n\r\nfor img in os.listdir(\"infer_imgs/fd_batch/\"):\r\n    images.append(cv2.imread(\"infer_imgs/fd_batch/\" + img))\r\n\r\nresult = model.batch_predict(images)\r\nlen(result)\r\n```\r\n结果输出信息如下：\r\n```\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(199)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(116)::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_20549/1388148344.py in <module>\r\n     11     images.append(cv2.imread(\"infer_imgs/fd_batch/\" + img))\r\n     12 \r\n---> 13 result = model.batch_predict(images)\r\n     14 len(result)\r\n\r\n/usr/local/lib/python3.7/dist-packages/fastdeploy/vision/detection/ppdet/__init__.py in batch_predict(self, images)\r\n    121         \"\"\"\r\n    122 \r\n--> 123         return self._model.batch_predict(images)\r\n    124 \r\n    125     def clone(self):\r\n\r\nRuntimeError: Can't set input blob with name: scale_factor, because model input (shape={1,2}) and blob (shape=(6.2)) are incompatible\r\n```\r\n\r\n但是相同的代码，换成PPYOLOE的模型就可以跑通，代码如下：\r\n```\r\nimport os\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.PPYOLOE(\"/my/path/to/model.pdmodel\",\r\n                                 \"/my/path/to/model.pdiparams\",\r\n                                 \"/my/path/to/infer_cfg.yml\")\r\nimages = list()\r\n\r\nfor img in os.listdir(\"infer_imgs/fd_batch/\"):\r\n    images.append(cv2.imread(\"infer_imgs/fd_batch/\" + img))\r\n\r\nresult = model.batch_predict(images)\r\nlen(result)\r\n```\r\n输出结果如下：\r\n```\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(199)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(116)::Init\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\n6\r\n```\r\n",
        "state": "closed",
        "user": "dium6i",
        "closed_by": "dium6i",
        "created_at": "2023-03-03T01:38:42+00:00",
        "updated_at": "2023-03-03T03:36:20+00:00",
        "closed_at": "2023-03-03T03:36:20+00:00",
        "comments_count": [
            "jiangjiajun",
            "dium6i"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1499,
        "title": "参考官方C++示例使用PPYOLOE进行推理测试时没有检测结果",
        "body": "系统：windows10\r\n编译器：vs2022\r\nFastDeploy版本：最新develop版本，按教程自行编译的版本。\r\ncuda版本：cuda11.6\r\n显卡:英伟达2060\r\n\r\n运行结果显示如下：\r\n\r\n![微信截图_20230303100705](https://user-images.githubusercontent.com/22534064/222613474-936c8f46-38bd-4fd8-b9ff-a8bed7944cac.png)\r\n\r\n图片路径无误，模型加载正确，程序正常运行，就是检测结果为空",
        "state": "closed",
        "user": "whuhit",
        "closed_by": "DefTruth",
        "created_at": "2023-03-03T02:11:27+00:00",
        "updated_at": "2023-03-06T06:00:08+00:00",
        "closed_at": "2023-03-06T06:00:08+00:00",
        "comments_count": [
            "whuhit",
            "DefTruth",
            "whuhit"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1501,
        "title": "测试调用  编译的so文件出现   undefined reference to `paddle::AnalysisConfig::SetExecStream(void*)'",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-x64-1.0.3\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.10)\r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n调用  fastdeploy-linux-x64-1.0.3    matting  编译的SO文件出错，前段事件使用的 fastdeploy-linux-x64-0.2.0 这个版本没有问题！\r\n![微信图片_20230303084524](https://user-images.githubusercontent.com/47800318/222618532-ce5d6ba5-0467-4425-b3a4-a8395da5ea0c.png)\r\n这是编译  libkt_demo.so 的CMakelists.txt\r\n![微信图片_20230303084918](https://user-images.githubusercontent.com/47800318/222618554-814a6bd0-aa21-4d9c-ac30-76357385d4d2.png)\r\n这是编译 测试调用libkt_demo.so的CMakelists.txt\r\n![微信图片_20230303102626](https://user-images.githubusercontent.com/47800318/222618586-338f158d-7ce8-4502-ba73-cc5e5f5160df.png)\r\n",
        "state": "closed",
        "user": "kekezhu0000",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-03T02:48:55+00:00",
        "updated_at": "2024-04-16T09:03:10+00:00",
        "closed_at": "2024-04-16T09:03:10+00:00",
        "comments_count": [
            "jiangjiajun",
            "kekezhu0000"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1502,
        "title": "什么时候也来个特征图可视化",
        "body": "https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.6/docs/tutorials/GradCAM_cn.md",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "leiqing1",
        "created_at": "2023-03-03T03:34:51+00:00",
        "updated_at": "2023-03-09T06:04:35+00:00",
        "closed_at": "2023-03-09T06:04:35+00:00",
        "comments_count": [
            "leiqing1",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1505,
        "title": "执行FastDeploy Streamer PP-YOLOE C++ Example中./streamer_demo时Can't find a properly sink bin in the pipeline",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.4\r\n- 【编译命令】cmake\r\n- 【系统平台】: Linux x64(wls Ubuntu 22.04)\r\n- 【硬件】： Nvidia GPU T4， CUDA 11.7 CUDNN 8.8\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n*********************************************************************************************************************************************\r\n相关问题已经在#1497中进行了沟通，@wang-xinyu 同学建议重新安装的deepstream或者使用docker，我已经拉了docker，想请教下运行起来这个docker的命令是什么呢？怎么能让我能顺利运行起来示例呢？\r\n*********************************************************************************************************************************************\r\nroot@jiangxin:~/build/FastDeploy/streamer/examples/ppyoloe/cpp/build# ./streamer_demo\r\napp\r\nnvurisrcbin_list\r\nnvstreammux\r\nnvinfer\r\nnvtracker\r\nnvmultistreamtiler\r\nnvosdbin\r\nnvvideoencfilesinkbin\r\n[INFO] /root/build/FastDeploy/streamer/src/gstreamer/utils.cc(60)::CreatePipeline       Trying to launch pipeline: nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_0  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_1  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_2  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_3  nvstreammux name=mux gpu-id=0 batch-size=4 width=1920 height=1080 batched-push-timeout=40000 ! nvinfer gpu-id=0 config-file-path=nvinfer_config.txt ! nvtracker gpu-id=0 tracker-width=640 tracker-height=640 ll-lib-file=/opt/nvidia/deepstream/deepstream-6.1/lib/libnvds_nvmultiobjecttracker.so ll-config-file=/opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/config_tracker_NvDCF_perf.yml enable-batch-process=true ! nvmultistreamtiler gpu-id=0 rows=2 columns=2 ! nvosdbin gpu-id=0 ! nvvideoencfilesinkbin gpu-id=0 bitrate=4000 output-file=out.mp4\r\n[ERROR] /root/build/FastDeploy/streamer/src/app/base_app.cc(112)::SetupPerfMeasurement  Can't find a properly sink bin in the pipeline\r\nAborted\r\n\r\n",
        "state": "closed",
        "user": "jiangxinufo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-04T01:10:35+00:00",
        "updated_at": "2024-04-16T09:03:11+00:00",
        "closed_at": "2024-04-16T09:03:11+00:00",
        "comments_count": [
            "wang-xinyu",
            "jiangxinufo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1504,
        "title": "请问 KIE 任务应该如何配置？",
        "body": "已经根据 https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/ppstructure/kie/README_ch.md 训练好模型。\r\n\r\n但现在Fastdeploy 的 API尚未支持？参数只能传 det，cls 这几个模型参数\r\n\r\n<img width=\"802\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2182116/222757852-f5b8414b-2576-4a31-8196-e8d44043eb3f.png\">\r\n\r\n",
        "state": "closed",
        "user": "sunzhaoyang",
        "closed_by": "sunzhaoyang",
        "created_at": "2023-03-03T15:20:39+00:00",
        "updated_at": "2023-03-07T00:13:59+00:00",
        "closed_at": "2023-03-07T00:13:59+00:00",
        "comments_count": [
            "sunzhaoyang",
            "jiangjiajun",
            "sunzhaoyang",
            "jiangjiajun"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1506,
        "title": "MacOSX 13.2.1 上，编译cpu模式失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 当前主线代码 https://github.com/PaddlePaddle/FastDeploy\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n```bash\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON\r\n\r\n```\r\n\r\n- 【系统平台】:  Mac OSX intel(13.2.1)\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n上面cmake的输出\r\n\r\n```bash\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON\r\n-- The C compiler identification is AppleClang 14.0.0.14000029\r\n-- The CXX compiler identification is AppleClang 14.0.0.14000029\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /Users/edwardzhou/ff_work/FastDeploy/build/third_libs/install/onnxruntime\r\nCMake Warning (dev) at /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:104 (ExternalProject_Add)\r\n  CMakeLists.txt:208 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/paddle_inference.cmake:114 (ExternalProject_Add)\r\n  CMakeLists.txt:226 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/m_openvino_toolkit_osx_2022.2.0.dev20220829.tgz to /Users/edwardzhou/ff_work/FastDeploy/build/m_openvino_toolkit_osx_2022.2.0.dev20220829.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n....\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /Users/edwardzhou/ff_work/FastDeploy/build/m_openvino_toolkit_osx_2022.2.0.dev20220829.tgz ...\r\nOPENVINO_LIBS = /Users/edwardzhou/ff_work/FastDeploy/build/third_libs/install/openvino/runtime/lib/libopenvino.dylib;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy\r\n-- Use the default OpenCV lib from: https://bj.bcebos.com/paddle2onnx/libs/opencv-osx-x86_64-3.4.16.tgz\r\nDownloading file from https://bj.bcebos.com/paddle2onnx/libs/opencv-osx-x86_64-3.4.16.tgz to /Users/edwardzhou/ff_work/FastDeploy/build/opencv-osx-x86_64-3.4.16.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n...\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /Users/edwardzhou/ff_work/FastDeploy/build/opencv-osx-x86_64-3.4.16.tgz ...\r\n-- Found OpenCV: /Users/edwardzhou/ff_work/FastDeploy/build/third_libs/install/opencv (found version \"3.4.16\")\r\nFASTTOKENIZER_COMPILE_LIB = /Users/edwardzhou/ff_work/FastDeploy/build/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.dylib\r\nCMake Warning (dev) at /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/fast_tokenizer.cmake:117 (ExternalProject_Add)\r\n  CMakeLists.txt:410 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/Cellar/cmake/3.25.2/share/cmake/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/paddle2onnx.cmake:67 (ExternalProject_Add)\r\n  CMakeLists.txt:423 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.2\r\n--   CMake command             : /usr/local/Cellar/cmake/3.25.2/bin/cmake\r\n--   System                    : Darwin\r\n--   C++ compiler              : /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++\r\n--   C++ compiler version      : 14.0.0.14000029\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          :\r\n--   Shared linker flags       :\r\n--   Build type                :\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;ENABLE_OPENVINO_BACKEND;ENABLE_VISION;ENABLE_TEXT;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : 2.4-dev5\r\n--   OpenVINO version          : 2022.2.0.dev20220829\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/edwardzhou/ff_work/FastDeploy/build\r\n```\r\n\r\n编译时出错\r\n```bash\r\n\r\nmake -j12\r\n[  1%] Creating directories for 'extern_fast_tokenizer'\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Creating directories for 'extern_onnxruntime'\r\n[  2%] Creating directories for 'extern_paddle2onnx'\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  3%] Creating directories for 'extern_paddle_inference'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  4%] Performing download step (download, verify and extract) for 'extern_paddle2onnx'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_fast_tokenizer'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_paddle_inference'\r\n-- Downloading...\r\n   dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-osx-x86_64-1.12.0.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-osx-x86_64-1.12.0.tgz'\r\n-- Downloading...\r\n   dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-osx-x86_64-1.0.5.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle2onnx-osx-x86_64-1.0.5.tgz'\r\n-- Downloading...\r\n   dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/fast_tokenizer/src/fast_tokenizer-osx-x86_64-1.0.2.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddlenlp/fast_tokenizer/fast_tokenizer-osx-x86_64-1.0.2.tgz'\r\n-- Downloading...\r\n   dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/paddle_inference/src/paddle_inference-osx-x86_64-2.4-dev5.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle_inference-osx-x86_64-2.4-dev5.tgz'\r\nCMake Error at extern_paddle_inference-stamp/download-extern_paddle_inference.cmake:170 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/paddle_inference-osx-x86_64-2.4-dev5.tgz' failed\r\n          status_code: 22\r\n          status_string: \"HTTP response code said error\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n            Trying [2409:8c04:1001:1002:0:ff:b001:368a]:443...\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN: offers h2\r\n\r\n  ALPN: offers http/1.1\r\n\r\n  (304) (OUT), TLS handshake, Client hello (1):\r\n\r\n  [318 bytes data]\r\n\r\n  (304) (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  (304) (OUT), TLS handshake, Client hello (1):\r\n\r\n  [351 bytes data]\r\n\r\n  (304) (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  (304) (IN), TLS handshake, Unknown (8):\r\n\r\n  [25 bytes data]\r\n\r\n  (304) (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  (304) (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  (304) (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  (304) (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / AEAD-AES256-GCM-SHA384\r\n\r\n  ALPN: server accepted http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  GET /fastdeploy/third_libs/paddle_inference-osx-x86_64-2.4-dev5.tgz\r\n  HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.84.0\r\n\r\n  Accept: */*\r\n\r\n\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  HTTP/1.1 404 Not Found\r\n\r\n  Date: Sat, 04 Mar 2023 02:21:43 GMT\r\n\r\n  Content-Type: application/json; charset=utf-8\r\n\r\n  Content-Length: 117\r\n\r\n  Connection: keep-alive\r\n\r\n  Server: BceBos\r\n\r\n  x-bce-debug-id:\r\n  fTjKmwgrjBSGRStmxV4DwboxqiqCStEW4QxvuXvw61IOf481DJMNWKsxDfTtrS12MjvsH866VdK34wUh7RzZDg==\r\n\r\n\r\n  x-bce-request-id: 9d9c23b0-b4b5-4012-8d5d-51034f6b4930\r\n\r\n  The requested URL returned error: 404\r\n\r\n  Closing connection 0\r\n\r\n\r\n\r\n          --- LOG END ---\r\n\r\n\r\n\r\n\r\nmake[2]: *** [third_libs/paddle_inference/src/extern_paddle_inference-stamp/extern_paddle_inference-download] Error 1\r\nmake[1]: *** [CMakeFiles/extern_paddle_inference.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 13%] Linking CXX static library libyaml-cpp.a\r\n[ 13%] Built target yaml-cpp\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-osx-x86_64-1.0.5.tgz'\r\n     dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/paddle2onnx/src/extern_paddle2onnx'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 14%] No update step for 'extern_paddle2onnx'\r\n[ 14%] No patch step for 'extern_paddle2onnx'\r\n[ 15%] No configure step for 'extern_paddle2onnx'\r\n[ 15%] No build step for 'extern_paddle2onnx'\r\n[ 16%] Performing install step for 'extern_paddle2onnx'\r\n[ 16%] Completed 'extern_paddle2onnx'\r\n[ 16%] Built target extern_paddle2onnx\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-osx-x86_64-1.12.0.tgz'\r\n     dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 17%] No update step for 'extern_onnxruntime'\r\n[ 17%] No patch step for 'extern_onnxruntime'\r\n[ 18%] No configure step for 'extern_onnxruntime'\r\n[ 18%] No build step for 'extern_onnxruntime'\r\n[ 19%] Performing install step for 'extern_onnxruntime'\r\n[ 19%] Completed 'extern_onnxruntime'\r\n[ 19%] Built target extern_onnxruntime\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/fast_tokenizer/src/fast_tokenizer-osx-x86_64-1.0.2.tgz'\r\n     dst='/Users/edwardzhou/ff_work/FastDeploy/build/third_libs/fast_tokenizer/src/extern_fast_tokenizer'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 20%] No update step for 'extern_fast_tokenizer'\r\n[ 20%] No patch step for 'extern_fast_tokenizer'\r\n[ 21%] No configure step for 'extern_fast_tokenizer'\r\n[ 21%] No build step for 'extern_fast_tokenizer'\r\n[ 21%] Performing install step for 'extern_fast_tokenizer'\r\n[ 21%] Completed 'extern_fast_tokenizer'\r\n[ 21%] Built target extern_fast_tokenizer\r\nmake: *** [all] Error 2\r\n\r\n```\r\n",
        "state": "closed",
        "user": "edwardzhou",
        "closed_by": "edwardzhou",
        "created_at": "2023-03-04T02:30:18+00:00",
        "updated_at": "2023-03-09T06:55:34+00:00",
        "closed_at": "2023-03-09T06:55:34+00:00",
        "comments_count": [
            "leiqing1",
            "edwardzhou"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1508,
        "title": "PaddleDetection的cpp例子运行问题",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 当前主线版本 rev: d21d48810e53f6d9075299cd5fa1b5aee500e435\r\n- 【编译命令】\r\n```\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON\r\n```\r\n\r\n- 【系统平台】: Mac OSX intel(13.2.1)\r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n根据 /examples/vision/detection/paddledetection/cpp/README_CN.md 的步骤，进行编译，下载模型与图片，运行出错:\r\n\r\n```\r\n$ pwd\r\n\r\n$ mkdir build && cd build\r\n\r\n$ cmake .. -DFASTDEPLOY_INSTALL_DIR=/Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk\r\n-- The C compiler identification is AppleClang 14.0.0.14000029\r\n-- The CXX compiler identification is AppleClang 14.0.0.14000029\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- The path of ONNXRuntime is /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/onnxruntime/lib.\r\n-- The path of OpenCV is /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/opencv.\r\n-- Found OpenCV: /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/opencv (found version \"3.4.16\")\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.2\r\n--   CMake command             : /usr/local/Cellar/cmake/3.25.2/bin/cmake\r\n--   System                    : Darwin\r\n--   C++ compiler              : /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++\r\n--   C++ compiler version      : 14.0.0.14000029\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          :\r\n--   Shared linker flags       :\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   WITH_GPU                  : OFF\r\n--   WITH_CAPI                  : OFF\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_CVCUDA             : OFF\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   DEPENDENCY_LIBS           : /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.dylib;/Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/onnxruntime/lib/libonnxruntime.dylib;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;/Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.dylib;/Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/paddle2onnx/lib/libpaddle2onnx.dylib\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/edwardzhou/ff_work/FastDeploy/build/compiled_fastdeploy_sdk/examples/vision/detection/paddledetection/cpp/build\r\n(base)\r\n 11:42:59  edwardzhou@EdwardMacBookPro16  ...paddledetection/cpp/build   develop ✔ \r\n$ make -j12\r\n[  4%] Building CXX object CMakeFiles/infer_mask_rcnn_demo.dir/infer_mask_rcnn.cc.o\r\n[  4%] Building CXX object CMakeFiles/infer_yolox_demo.dir/infer_yolox.cc.o\r\n[  7%] Building CXX object CMakeFiles/infer_yolov3_demo.dir/infer_yolov3.cc.o\r\n[  9%] Building CXX object CMakeFiles/infer_picodet_demo.dir/infer_picodet.cc.o\r\n[ 11%] Building CXX object CMakeFiles/infer_yolov6_demo.dir/infer_yolov6.cc.o\r\n[ 14%] Building CXX object CMakeFiles/infer_yolov8_demo.dir/infer_yolov8.cc.o\r\n[ 19%] Building CXX object CMakeFiles/infer_yolov7_demo.dir/infer_yolov7.cc.o\r\n[ 19%] Building CXX object CMakeFiles/infer_ppyoloe_demo.dir/infer_ppyoloe.cc.o\r\n[ 21%] Building CXX object CMakeFiles/infer_ppyolo_demo.dir/infer_ppyolo.cc.o\r\n[ 23%] Building CXX object CMakeFiles/infer_ssd_demo.dir/infer_ssd.cc.o\r\n[ 26%] Building CXX object CMakeFiles/infer_yolov5_demo.dir/infer_yolov5.cc.o\r\n[ 28%] Building CXX object CMakeFiles/infer_faster_rcnn_demo.dir/infer_faster_rcnn.cc.o\r\n[ 30%] Linking CXX executable infer_picodet_demo\r\n[ 33%] Linking CXX executable infer_yolov8_demo\r\n[ 35%] Linking CXX executable infer_yolov3_demo\r\n[ 38%] Linking CXX executable infer_ppyoloe_demo\r\n[ 40%] Linking CXX executable infer_mask_rcnn_demo\r\n[ 42%] Linking CXX executable infer_faster_rcnn_demo\r\n[ 45%] Linking CXX executable infer_yolov6_demo\r\n[ 50%] Linking CXX executable infer_ssd_demo\r\n[ 50%] Linking CXX executable infer_ppyolo_demo\r\n[ 52%] Linking CXX executable infer_yolov5_demo\r\n[ 54%] Linking CXX executable infer_yolox_demo\r\n[ 57%] Linking CXX executable infer_yolov7_demo\r\n[ 57%] Built target infer_picodet_demo\r\n[ 57%] Built target infer_yolov8_demo\r\n[ 57%] Built target infer_yolov6_demo\r\n[ 57%] Built target infer_yolov3_demo\r\n[ 57%] Built target infer_faster_rcnn_demo\r\n[ 57%] Built target infer_ppyoloe_demo\r\n[ 57%] Built target infer_ppyolo_demo\r\n[ 57%] Built target infer_yolov5_demo\r\n[ 57%] Built target infer_mask_rcnn_demo\r\n[ 57%] Built target infer_ssd_demo\r\n[ 57%] Built target infer_yolox_demo\r\n[ 57%] Built target infer_yolov7_demo\r\n[ 59%] Building CXX object CMakeFiles/infer_cascadercnn_demo.dir/infer_cascadercnn.cc.o\r\n[ 61%] Building CXX object CMakeFiles/infer_rtmdet_demo.dir/infer_rtmdet.cc.o\r\n[ 64%] Building CXX object CMakeFiles/infer_pssdet_demo.dir/infer_pssdet.cc.o\r\n[ 66%] Building CXX object CMakeFiles/infer_retinanet_demo.dir/infer_retinanet.cc.o\r\n[ 69%] Building CXX object CMakeFiles/infer_ppyoloesod_demo.dir/infer_ppyoloesod.cc.o\r\n[ 71%] Building CXX object CMakeFiles/infer_ttfnet_demo.dir/infer_ttfnet.cc.o\r\n[ 76%] Building CXX object CMakeFiles/infer_gfl_demo.dir/infer_gfl.cc.o\r\n[ 78%] Building CXX object CMakeFiles/infer_fcos_demo.dir/infer_fcos.cc.o\r\n[ 78%] Building CXX object CMakeFiles/infer_tood_demo.dir/infer_tood.cc.o\r\n[ 80%] Linking CXX executable infer_gfl_demo\r\n[ 83%] Linking CXX executable infer_cascadercnn_demo\r\n[ 90%] Linking CXX executable infer_ppyoloesod_demo\r\n[ 90%] Linking CXX executable infer_rtmdet_demo\r\n[ 90%] Linking CXX executable infer_pssdet_demo\r\n[ 92%] Linking CXX executable infer_retinanet_demo\r\n[ 95%] Linking CXX executable infer_fcos_demo\r\n[ 97%] Linking CXX executable infer_ttfnet_demo\r\n[100%] Linking CXX executable infer_tood_demo\r\n[100%] Built target infer_gfl_demo\r\n[100%] Built target infer_rtmdet_demo\r\n[100%] Built target infer_retinanet_demo\r\n[100%] Built target infer_ppyoloesod_demo\r\n[100%] Built target infer_cascadercnn_demo\r\n[100%] Built target infer_pssdet_demo\r\n[100%] Built target infer_ttfnet_demo\r\n[100%] Built target infer_fcos_demo\r\n[100%] Built target infer_tood_demo\r\n\r\n$ wget https://bj.bcebos.com/paddlehub/fastdeploy/ppyoloe_crn_l_300e_coco.tgz\r\n$ wget https://gitee.com/paddlepaddle/PaddleDetection/raw/release/2.4/demo/000000014439.jpg\r\n$ tar xvf ppyoloe_crn_l_300e_coco.tgz\r\n\r\n$ ./infer_ppyoloe_demo ./ppyoloe_crn_l_300e_coco 000000014439.jpg 0\r\n\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[libprotobuf ERROR /Users/paddle/paddle2onnx_workspace/protobuf/src/google/protobuf/message_lite.cc:123] Can't parse message of type \"paddle2onnx.framework.proto.VarType.TensorDesc\" because it is missing required fields: data_type\r\n[Paddle2ONNX] Unexcepted situation happend, while reading the parameters of PaddlePaddle model.\r\n[Paddle2ONNX] Failed to load parameters of PaddlePaddle model from memory.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(189)::InitFromPaddle\tError occured while export PaddlePaddle to ONNX format.\r\n[ERROR] fastdeploy/runtime/runtime.cc(287)::CreateOrtBackend\tFailed to initialize Backend::ORT.\r\n[1]    61320 abort      ./infer_ppyoloe_demo ./ppyoloe_crn_l_300e_coco 000000014439.jpg 0\r\n\r\n```\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "edwardzhou",
        "closed_by": "edwardzhou",
        "created_at": "2023-03-04T04:25:35+00:00",
        "updated_at": "2023-03-12T06:44:26+00:00",
        "closed_at": "2023-03-12T06:44:26+00:00",
        "comments_count": [
            "edwardzhou",
            "jiangjiajun",
            "edwardzhou",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "edwardzhou",
            "Zheng-Bicheng",
            "edwardzhou",
            "Zheng-Bicheng",
            "edwardzhou"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1510,
        "title": "模型前置处理函数ResizeImage是否支持按照size处理图片",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.3\r\n- 【编译命令】pip install \r\n- 【系统平台】: Mac OSX arm(12.0) \r\n- 【硬件】： \r\n- 【编译语言】： \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【ResizeImage函数不支持按照size处理图片】\r\n在infrence_clas.yaml文件中，设置如下前置处理算子\r\n```\r\nPreProcess:\r\n  transform_ops:\r\n    - ResizeImage:\r\n        size: [192,256]\r\n    - CropImage:\r\n        size: 224\r\n```\r\n报错：invalid node; first invalid key: \"resize_short\"\r\n\r\n在PaddleCls中，ResizeImage函数支持按照resize_short和size两种方式去处理图片的，在FastDeploy中为什么不支持了？\r\nPaddleCls中ResizeImage处理逻辑：\r\n```\r\nclass ResizeImage(object):\r\n    \"\"\" resize image \"\"\"\r\n\r\n    def __init__(self,\r\n                 size=None,\r\n                 resize_short=None,\r\n                 interpolation=None,\r\n                 backend=\"cv2\",\r\n                 return_numpy=True):\r\n        if resize_short is not None and resize_short > 0:\r\n            self.resize_short = resize_short\r\n            self.w = None\r\n            self.h = None\r\n        elif size is not None:\r\n            self.resize_short = None\r\n            self.w = size if type(size) is int else size[0]\r\n            self.h = size if type(size) is int else size[1]\r\n        else:\r\n            raise OperatorParamError(\"invalid params for ReisizeImage for '\\\r\n                'both 'size' and 'resize_short' are None\")\r\n\r\n        self._resize_func = UnifiedResize(\r\n            interpolation=interpolation,\r\n            backend=backend,\r\n            return_numpy=return_numpy)\r\n```\r\nFastDeploy中ResizeImage处理逻辑：\r\n```\r\n    if (op_name == \"ResizeImage\") {\r\n      int target_size = op.begin()->second[\"resize_short\"].as<int>();\r\n      bool use_scale = false;\r\n      int interp = 1;\r\n      processors_.push_back(\r\n          std::make_shared<ResizeByShort>(target_size, 1, use_scale));\r\n```\r\n",
        "state": "closed",
        "user": "shirukai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-04T06:32:18+00:00",
        "updated_at": "2024-04-16T09:03:12+00:00",
        "closed_at": "2024-04-16T09:03:12+00:00",
        "comments_count": [
            "jiangjiajun",
            "shirukai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1509,
        "title": "C API增加FD_C_CreateDetectionResult",
        "body": "编译FastDeploy C API发现\r\n\r\n ### Vision Predict函数 ###\r\n```\r\nFD_C_Bool FD_C_PPYOLOEWrapperPredict(\r\n    __fd_take FD_C_PPYOLOEWrapper* fd_c_ppyoloe_wrapper, FD_C_Mat img,\r\n    FD_C_DetectionResult* fd_c_detection_result)\r\n```\r\n传入的是FD_C_DetectionResult*，而并没有相关初始化API，在[paddledetection_c](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/c/infer_ppyoloe.c)范例中使用的是malloc来初始化\r\n```\r\n  FD_C_DetectionResult* result =\r\n      (FD_C_DetectionResult*)malloc(sizeof(FD_C_DetectionResult));\r\n```\r\n对于一些无法手动分配内存的语言不太友好，建议增加相关初始化函数以便调用。\r\n```\r\nFD_C_DetectionResult* FD_C_CreateDetectionResult() {\r\n    FD_C_DetectionResult* result = (FD_C_DetectionResult*)malloc(sizeof(FD_C_DetectionResult));\r\n    return  result;\r\n}\r\n```\r\n谢谢。",
        "state": "closed",
        "user": "huipeng8",
        "closed_by": "heliqi",
        "created_at": "2023-03-04T05:49:36+00:00",
        "updated_at": "2023-03-09T03:13:21+00:00",
        "closed_at": "2023-03-09T03:13:20+00:00",
        "comments_count": [
            "rainyfly",
            "rainyfly"
        ],
        "labels": [
            "c api"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1511,
        "title": "fastdeploy编译在jetson nano上，推理examples的yolov5face模型报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： git clone https://github.com/PaddlePaddle/FastDeploy.git最新\r\n- 【编译命令】\r\ncd FastDeploy/python\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\n\r\n# ENABLE_PADDLE_BACKEND & PADDLEINFERENCE_DIRECTORY为可选项\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport PADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n安装pip install *****\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 10.2 CUDNN 8.2\r\n- 【编译语言】： Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 执行/FastDeploy/examples/vision/facedet/yolov5face/python\r\n- - 执行： python infer.py --model yolov5s-face.onnx --image a.jpg --device gpu\r\n            **报错**：     WARNING:root:The installed fastdeploy-python package is not built with GPU, will force to use CPU. To use GPU, following the commands to install fas                 tdeploy-gpu-python.\r\nWARNING:root:    ================= Install GPU FastDeploy===============\r\nWARNING:root:    python -m pip uninstall fastdeploy-python\r\nWARNING:root:    python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 42, in <module>\r\n    model = fd.vision.facedet.YOLOv5Face(args.model, runtime_option=runtime_option)\r\n  File \"/home/dlinano/.local/lib/python3.6/site-packages/fastdeploy/vision/facedet/contrib/yolov5face.py\", line 38, in __init__\r\n    self._model = C.vision.facedet.YOLOv5Face(\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'vision'\r\n- - 执行： python infer.py --model yolov5s-face.onnx --image a.jpg --device cpu\r\n- **报错**：Traceback (most recent call last):\r\n  File \"infer.py\", line 42, in <module>\r\n    model = fd.vision.facedet.YOLOv5Face(args.model, runtime_option=runtime_option)\r\n  File \"/home/dlinano/.local/lib/python3.6/site-packages/fastdeploy/vision/facedet/contrib/yolov5face.py\", line 38, in __init__\r\n    self._model = C.vision.facedet.YOLOv5Face(\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'vision'\r\n\r\n\r\n",
        "state": "closed",
        "user": "jm7301",
        "closed_by": "jm7301",
        "created_at": "2023-03-04T07:51:58+00:00",
        "updated_at": "2023-03-08T05:51:26+00:00",
        "closed_at": "2023-03-06T13:11:34+00:00",
        "comments_count": [
            "jiangjiajun",
            "jm7301",
            "jiangjiajun",
            "jm7301",
            "jm7301",
            "vamoslc",
            "jiangjiajun",
            "vamoslc",
            "vamoslc",
            "jiangjiajun",
            "vamoslc",
            "jiangjiajun",
            "vamoslc",
            "jm7301",
            "jm7301",
            "jm7301",
            "jm7301",
            "vamoslc",
            "jm7301"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1514,
        "title": "- arcface 利用export.py转rknn api内部报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-aarch64-1.4.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- (python3.6) wxb@ndsl88:~/programs/FastDeploy/tools/rknpu2$ python ./export.py --config_path=./config/arcface_unquantized.yaml --target_platform rk3588\r\n![QQ截图20230305195654](https://user-images.githubusercontent.com/64680548/222959018-a94be930-095f-47c9-a385-1b020f6c985f.png)\r\n\r\n{'mean': [[127.5, 127.5, 127.5]], 'std': [[127.5, 127.5, 127.5]], 'model_path': './ms1mv3_arcface_r18_o.onnx', 'outputs_nodes': None, 'do_quantization': False, 'dataset': './ms1mv3_arcface_r18/datasets.txt', 'output_folder': './'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\nW load_onnx: Model converted from pytorch, 'opset_version' should be set 12 in torch.onnx.export for successful convert!\r\n             More details can be found in examples/pytorch/resnet18_export_onnx\r\nI base_optimize ...\r\nI base_optimize done.\r\nI \r\nI fold_constant ...\r\nE build: Catch exception when building RKNN model!\r\nE build: Traceback (most recent call last):\r\nE build:   File \"rknn/api/rknn_base.py\", line 1541, in rknn.api.rknn_base.RKNNBase.build\r\nE build:   File \"rknn/api/graph_optimizer.py\", line 627, in rknn.api.graph_optimizer.GraphOptimizer.fold_constant\r\nE build:   File \"rknn/api/session.py\", line 28, in rknn.api.session.Session.__init__\r\nE build:   File \"rknn/api/session.py\", line 71, in rknn.api.session.Session.sess_build\r\nE build:   File \"/home/wxb/programs/anaconda3/envs/python3.6/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 335, in __init__\r\nE build:     self._create_inference_session(providers, provider_options, disabled_optimizers)\r\nE build:   File \"/home/wxb/programs/anaconda3/envs/python3.6/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 364, in _create_inference_session\r\nE build:     \"onnxruntime.InferenceSession(..., providers={}, ...)\".format(available_providers))\r\nE build: ValueError: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)\r\nTraceback (most recent call last):\r\n  File \"./export.py\", line 58, in <module>\r\n    assert ret == 0, \"Build model failed!\"\r\nAssertionError: Build model failed!\r\n\r\n",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-05T11:57:27+00:00",
        "updated_at": "2023-03-06T02:39:39+00:00",
        "closed_at": "2023-03-06T02:39:15+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "XiaBing992",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1513,
        "title": "paddle ocr什么时候可以支持rk3588 npu？",
        "body": "paddle ocr什么时候可以支持rk3588 npu？",
        "state": "closed",
        "user": "leokwu",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-05T09:19:25+00:00",
        "updated_at": "2023-03-08T14:11:05+00:00",
        "closed_at": "2023-03-06T02:40:00+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "leokwu",
            "jiangjiajun",
            "leokwu",
            "leokwu",
            "jiangjiajun",
            "leokwu",
            "leokwu",
            "leokwu"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1515,
        "title": "RK3588 PPOCRv3出现CPU速度快过NPU速度的问题",
        "body": "- 【FastDeploy版本】： develop\r\n- 【编译命令】aarch-64\r\n- 【系统平台】: Linux x64(Debian 10)\r\n- 【硬件】： RK3588\r\n- 【编译语言】： C++ / Python(3.7或3.8等）",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-06T01:54:30+00:00",
        "updated_at": "2024-02-27T01:42:55+00:00",
        "closed_at": "2024-02-27T01:42:55+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "KongXCai",
            "Zheng-Bicheng",
            "KongXCai"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1517,
        "title": "RK3588 推理 picodet模型报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： git clone [https://github.com/PaddlePaddle/FastDeploy.git最新](https://github.com/PaddlePaddle/FastDeploy.git%E6%9C%80%E6%96%B0)\r\n- 【编译命令】\r\n- cd python \r\n- git checkout develop\r\n-  export ENABLE_ORT_BACKEND=ON\r\n-  export ENABLE_RKNPU2_BACKEND=ON\r\n- export ENABLE_VISION=ON\r\n- export RKNN2_TARGET_SOC=RK3588\r\n- python3 setup.py build \r\n-  python3 infer.py --model_file config/picodet_l_640_lcnet_rk3588_unquantized.rknn --config_file config/infer_cfg.yml --image config/0000364_01177_d_0000799_320_0_960_540.jpg\r\n- 【系统平台】: debian  rk3588\r\n- 【编译语言】： Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# export ENABLE_ORT_BACKEND=ON\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# export ENABLE_RKNPU2_BACKEND=ON\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# export ENABLE_VISION=ON\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# export RKNN2_TARGET_SOC=RK3588\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# python3 setup.py build\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\nCMake Warning (dev) at CMakeLists.txt:15 (PROJECT):\r\n  Policy CMP0048 is not set: project() command manages VERSION variables.\r\n  Run \"cmake --help-policy CMP0048\" for policy details.  Use the cmake_policy\r\n  command to set the policy and suppress this warning.\r\n\r\n  The following variable(s) would be set to empty:\r\n\r\n    CMAKE_PROJECT_VERSION\r\n    CMAKE_PROJECT_VERSION_MAJOR\r\n    CMAKE_PROJECT_VERSION_MINOR\r\n    CMAKE_PROJECT_VERSION_PATCH\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nDecompress file /home/linaro/LC/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/linaro/LC/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\r\nDecompress file /home/linaro/LC/FastDeploy/python/.setuptools-cmake-build/rknpu2_runtime-linux-aarch64-1.4.2b0-RK3588.tgz ...\r\n-- Use the default OpenCV lib from: https://bj.bcebos.com/paddle2onnx/libs/opencv-linux-aarch64-3.4.14.tgz\r\nDecompress file /home/linaro/LC/FastDeploy/python/.setuptools-cmake-build/opencv-linux-aarch64-3.4.14.tgz ...\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.18.4\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 10.2.1\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          :\r\n--   Shared linker flags       :\r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_RKNPU2_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : ON\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Python executable         : /usr/bin/python3\r\n--   Python includes           : /usr/include/python3.9\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/linaro/LC/FastDeploy/python/.setuptools-cmake-build\r\n[  2%] Built target extern_onnxruntime\r\n[  8%] Built target extern_paddle2onnx\r\n[ 16%] Built target yaml-cpp\r\n[ 17%] Built target yaml-cpp-read\r\n[ 18%] Built target yaml-cpp-parse\r\n[ 18%] Built target yaml-cpp-sandbox\r\n[ 77%] Built target fastdeploy\r\n[100%] Built target fastdeploy_main\r\n[100%] Built target copy_fd_libraries\r\n[100%] Built target copy_third_libraries\r\ncopying fastdeploy/code_version.py -> build/lib.linux-aarch64-3.9/fastdeploy\r\nrunning build_ext\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# cd /home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python/\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python# python3 infer.py --model_file config/picodet_l_640_lcnet_rk3588_unquantized.rknn --config_file config/infer_cfg.yml --image config/0000364_01177_d_0000799_320_0_960_540.jpg\r\nTraceback (most recent call last):\r\n  File \"/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python/infer.py\", line 50, in <module>\r\n    model = fd.vision.detection.PPYOLOE(\r\n  File \"/usr/local/lib/python3.9/dist-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 101, in __init__\r\n    self._model = C.vision.detection.PPYOLOE(\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'vision'\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python# cd /home/linaro/LC/FastDeploy/python/\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# python3 setup.py bdist_wheel\r\nrunning bdist_wheel\r\nrunning build\r\nrunning build_py\r\nrunning egg_info\r\nwriting fastdeploy_python.egg-info/PKG-INFO\r\nwriting dependency_links to fastdeploy_python.egg-info/dependency_links.txt\r\nwriting requirements to fastdeploy_python.egg-info/requires.txt\r\nwriting top-level names to fastdeploy_python.egg-info/top_level.txt\r\nreading manifest file 'fastdeploy_python.egg-info/SOURCES.txt'\r\nwriting manifest file 'fastdeploy_python.egg-info/SOURCES.txt'\r\ncopying fastdeploy/libs/libfastdeploy.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs\r\ncopying fastdeploy/libs/fastdeploy_main.cpython-39-aarch64-linux-gnu.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs\r\ncopying fastdeploy/libs/libfastdeploy.so.0.0.0 -> build/lib.linux-aarch64-3.9/fastdeploy/libs\r\ncopying fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so.1.0.5 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/paddle2onnx/lib\r\ncopying fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/paddle2onnx/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_highgui.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_videoio.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_imgcodecs.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_imgproc.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_core.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/opencv/lib/libopencv_video.so.3.4 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib\r\ncopying fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/onnxruntime/lib\r\ncopying fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so.1.12.0 -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/onnxruntime/lib\r\ncopying fastdeploy/libs/third_libs/rknpu2_runtime/._.DS_Store -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying fastdeploy/libs/third_libs/rknpu2_runtime/._include -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying fastdeploy/libs/third_libs/rknpu2_runtime/.DS_Store -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying fastdeploy/libs/third_libs/rknpu2_runtime/lib/._librknnrt.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/lib\r\ncopying fastdeploy/libs/third_libs/rknpu2_runtime/lib/librknnrt.so -> build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/lib\r\nrunning build_ext\r\ninstalling to build/bdist.linux-aarch64/wheel\r\nrunning install\r\nrunning install_lib\r\ncreating build/bdist.linux-aarch64/wheel\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/download.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/pipeline\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/pipeline/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/pipeline\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/pipeline/pptinypose\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/pipeline/pptinypose/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/pipeline/pptinypose\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/model.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/code_version.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/fastdeploy_main.cpython-39-aarch64-linux-gnu.so -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/serving\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/utils.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/serving/router\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/router/base_router.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/router\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/router/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/router\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/router/http_router.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/router\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/server.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/model_manager.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/serving/handler\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/handler/vision_model_handler.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/handler\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/handler/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/handler\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/serving/handler/base_handler.py -> build/bdist.linux-aarch64/wheel/fastdeploy/serving/handler\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/libs\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/paddle2onnx\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/paddle2onnx/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so.1.0.5 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/paddle2onnx/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/paddle2onnx/lib\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_highgui.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_videoio.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_imgcodecs.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_imgproc.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_core.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/opencv/lib/libopencv_video.so.3.4 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/opencv/lib\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/onnxruntime\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/onnxruntime/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/onnxruntime/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so.1.12.0 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/onnxruntime/lib\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/._.DS_Store -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/._include -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/lib/._librknnrt.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/lib/librknnrt.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime/lib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/third_libs/rknpu2_runtime/.DS_Store -> build/bdist.linux-aarch64/wheel/fastdeploy/libs/third_libs/rknpu2_runtime\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/libfastdeploy.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/fastdeploy_main.cpython-39-aarch64-linux-gnu.so -> build/bdist.linux-aarch64/wheel/fastdeploy/libs\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/libs/libfastdeploy.so.0.0.0 -> build/bdist.linux-aarch64/wheel/fastdeploy/libs\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/runtime.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/encryption\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/encryption/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/encryption\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/encryption/encryption.py -> build/bdist.linux-aarch64/wheel/fastdeploy/encryption\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/text\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/text/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/text\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/text/uie\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/text/uie/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/text/uie\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/c_lib_wrap.py -> build/bdist.linux-aarch64/wheel/fastdeploy\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/headpose\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/headpose/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/headpose/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/headpose/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/headpose/contrib/fsanet.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/headpose/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/headpose/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/headpose\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/matting/contrib/rvm.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/matting/contrib/modnet.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/matting/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/matting/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/ppmatting\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/matting/ppmatting/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/matting/ppmatting\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/utils.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/nanodet_plus.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov7end2end_ort.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov5lite.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolox.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/fastestdet.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov7end2end_trt.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov7.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolor.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov8.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/scaled_yolov4.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov5seg.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov6.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib/rkyolo\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/rkyolo/rkyolov5.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib/rkyolo\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/rkyolo/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib/rkyolo\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/contrib/yolov5.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/ppdet\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/detection/ppdet/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/detection/ppdet\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/classify.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/segmentation.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/detection.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/coco_metrics.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/map_utils.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/coco.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/cityscapes.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/util.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/json_results.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/fd_logging.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/coco_utils.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/evaluation/utils/seg_metrics.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/evaluation/utils\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/faceid/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib/adaface\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/faceid/contrib/adaface/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib/adaface\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib/insightface\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/faceid/contrib/insightface/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid/contrib/insightface\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/faceid/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/faceid\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facealign/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facealign/contrib/pipnet.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facealign/contrib/pfld.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facealign/contrib/face_landmark_1000.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facealign/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facealign\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/sr\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/sr/ppsr\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/sr/ppsr/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/sr/ppsr\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/sr/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/sr\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/visualize\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/visualize/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/visualize\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/generation\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/generation/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/generation/contrib/anemigan.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/generation/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/generation/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/generation/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/generation/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/generation\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/scrfd.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/yolov7face.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/ultraface.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/yolov5face.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/retinaface.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/centerface.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/contrib/blazeface.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/facedet/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/facedet\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/keypointdetection\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/keypointdetection/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/keypointdetection\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/keypointdetection/pptinypose\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/keypointdetection/pptinypose/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/keypointdetection/pptinypose\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/tracking\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/tracking/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/tracking\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/tracking/pptracking\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/tracking/pptracking/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/tracking/pptracking\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/ocr\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/ocr/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/ocr\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/ocr/ppocr\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/ocr/ppocr/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/ocr/ppocr\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/segmentation\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/segmentation/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/segmentation\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/segmentation/ppseg\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/segmentation/ppseg/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/segmentation/ppseg\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/classification/contrib/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/classification/contrib/yolov5cls.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/classification/contrib/resnet.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/contrib\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/classification/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/ppcls\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/classification/ppcls/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/classification/ppcls\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/vision/common\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/common/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/common\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/vision/common/manager.py -> build/bdist.linux-aarch64/wheel/fastdeploy/vision/common\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/utils/__init__.py -> build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/utils/hub_model_server.py -> build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/utils/hub_config.py -> build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/utils/hub_env.py -> build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\ncopying build/lib.linux-aarch64-3.9/fastdeploy/utils/example_resource.py -> build/bdist.linux-aarch64/wheel/fastdeploy/utils\r\nrunning install_egg_info\r\nCopying fastdeploy_python.egg-info to build/bdist.linux-aarch64/wheel/fastdeploy_python-0.0.0.egg-info\r\nrunning install_scripts\r\ncreating build/bdist.linux-aarch64/wheel/fastdeploy_python-0.0.0.dist-info/WHEEL\r\ncreating 'dist/fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl' and adding 'build/bdist.linux-aarch64/wheel' to it\r\nadding 'fastdeploy/__init__.py'\r\nadding 'fastdeploy/c_lib_wrap.py'\r\nadding 'fastdeploy/code_version.py'\r\nadding 'fastdeploy/download.py'\r\nadding 'fastdeploy/fastdeploy_main.cpython-39-aarch64-linux-gnu.so'\r\nadding 'fastdeploy/model.py'\r\nadding 'fastdeploy/runtime.py'\r\nadding 'fastdeploy/encryption/__init__.py'\r\nadding 'fastdeploy/encryption/encryption.py'\r\nadding 'fastdeploy/libs/__init__.py'\r\nadding 'fastdeploy/libs/fastdeploy_main.cpython-39-aarch64-linux-gnu.so'\r\nadding 'fastdeploy/libs/libfastdeploy.so'\r\nadding 'fastdeploy/libs/libfastdeploy.so.0.0.0'\r\nadding 'fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so'\r\nadding 'fastdeploy/libs/third_libs/onnxruntime/lib/libonnxruntime.so.1.12.0'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_core.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_highgui.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_imgcodecs.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_imgproc.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_video.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/opencv/lib/libopencv_videoio.so.3.4'\r\nadding 'fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so'\r\nadding 'fastdeploy/libs/third_libs/paddle2onnx/lib/libpaddle2onnx.so.1.0.5'\r\nadding 'fastdeploy/libs/third_libs/rknpu2_runtime/.DS_Store'\r\nadding 'fastdeploy/libs/third_libs/rknpu2_runtime/._.DS_Store'\r\nadding 'fastdeploy/libs/third_libs/rknpu2_runtime/._include'\r\nadding 'fastdeploy/libs/third_libs/rknpu2_runtime/lib/._librknnrt.so'\r\nadding 'fastdeploy/libs/third_libs/rknpu2_runtime/lib/librknnrt.so'\r\nadding 'fastdeploy/pipeline/__init__.py'\r\nadding 'fastdeploy/pipeline/pptinypose/__init__.py'\r\nadding 'fastdeploy/serving/__init__.py'\r\nadding 'fastdeploy/serving/model_manager.py'\r\nadding 'fastdeploy/serving/server.py'\r\nadding 'fastdeploy/serving/utils.py'\r\nadding 'fastdeploy/serving/handler/__init__.py'\r\nadding 'fastdeploy/serving/handler/base_handler.py'\r\nadding 'fastdeploy/serving/handler/vision_model_handler.py'\r\nadding 'fastdeploy/serving/router/__init__.py'\r\nadding 'fastdeploy/serving/router/base_router.py'\r\nadding 'fastdeploy/serving/router/http_router.py'\r\nadding 'fastdeploy/text/__init__.py'\r\nadding 'fastdeploy/text/uie/__init__.py'\r\nadding 'fastdeploy/utils/__init__.py'\r\nadding 'fastdeploy/utils/example_resource.py'\r\nadding 'fastdeploy/utils/hub_config.py'\r\nadding 'fastdeploy/utils/hub_env.py'\r\nadding 'fastdeploy/utils/hub_model_server.py'\r\nadding 'fastdeploy/vision/__init__.py'\r\nadding 'fastdeploy/vision/utils.py'\r\nadding 'fastdeploy/vision/classification/__init__.py'\r\nadding 'fastdeploy/vision/classification/contrib/__init__.py'\r\nadding 'fastdeploy/vision/classification/contrib/resnet.py'\r\nadding 'fastdeploy/vision/classification/contrib/yolov5cls.py'\r\nadding 'fastdeploy/vision/classification/ppcls/__init__.py'\r\nadding 'fastdeploy/vision/common/__init__.py'\r\nadding 'fastdeploy/vision/common/manager.py'\r\nadding 'fastdeploy/vision/detection/__init__.py'\r\nadding 'fastdeploy/vision/detection/contrib/__init__.py'\r\nadding 'fastdeploy/vision/detection/contrib/fastestdet.py'\r\nadding 'fastdeploy/vision/detection/contrib/nanodet_plus.py'\r\nadding 'fastdeploy/vision/detection/contrib/scaled_yolov4.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolor.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov5.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov5lite.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov5seg.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov6.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov7.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov7end2end_ort.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov7end2end_trt.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolov8.py'\r\nadding 'fastdeploy/vision/detection/contrib/yolox.py'\r\nadding 'fastdeploy/vision/detection/contrib/rkyolo/__init__.py'\r\nadding 'fastdeploy/vision/detection/contrib/rkyolo/rkyolov5.py'\r\nadding 'fastdeploy/vision/detection/ppdet/__init__.py'\r\nadding 'fastdeploy/vision/evaluation/__init__.py'\r\nadding 'fastdeploy/vision/evaluation/classify.py'\r\nadding 'fastdeploy/vision/evaluation/detection.py'\r\nadding 'fastdeploy/vision/evaluation/segmentation.py'\r\nadding 'fastdeploy/vision/evaluation/utils/__init__.py'\r\nadding 'fastdeploy/vision/evaluation/utils/cityscapes.py'\r\nadding 'fastdeploy/vision/evaluation/utils/coco.py'\r\nadding 'fastdeploy/vision/evaluation/utils/coco_metrics.py'\r\nadding 'fastdeploy/vision/evaluation/utils/coco_utils.py'\r\nadding 'fastdeploy/vision/evaluation/utils/fd_logging.py'\r\nadding 'fastdeploy/vision/evaluation/utils/json_results.py'\r\nadding 'fastdeploy/vision/evaluation/utils/map_utils.py'\r\nadding 'fastdeploy/vision/evaluation/utils/seg_metrics.py'\r\nadding 'fastdeploy/vision/evaluation/utils/util.py'\r\nadding 'fastdeploy/vision/facealign/__init__.py'\r\nadding 'fastdeploy/vision/facealign/contrib/__init__.py'\r\nadding 'fastdeploy/vision/facealign/contrib/face_landmark_1000.py'\r\nadding 'fastdeploy/vision/facealign/contrib/pfld.py'\r\nadding 'fastdeploy/vision/facealign/contrib/pipnet.py'\r\nadding 'fastdeploy/vision/facedet/__init__.py'\r\nadding 'fastdeploy/vision/facedet/contrib/__init__.py'\r\nadding 'fastdeploy/vision/facedet/contrib/blazeface.py'\r\nadding 'fastdeploy/vision/facedet/contrib/centerface.py'\r\nadding 'fastdeploy/vision/facedet/contrib/retinaface.py'\r\nadding 'fastdeploy/vision/facedet/contrib/scrfd.py'\r\nadding 'fastdeploy/vision/facedet/contrib/ultraface.py'\r\nadding 'fastdeploy/vision/facedet/contrib/yolov5face.py'\r\nadding 'fastdeploy/vision/facedet/contrib/yolov7face.py'\r\nadding 'fastdeploy/vision/faceid/__init__.py'\r\nadding 'fastdeploy/vision/faceid/contrib/__init__.py'\r\nadding 'fastdeploy/vision/faceid/contrib/adaface/__init__.py'\r\nadding 'fastdeploy/vision/faceid/contrib/insightface/__init__.py'\r\nadding 'fastdeploy/vision/generation/__init__.py'\r\nadding 'fastdeploy/vision/generation/contrib/__init__.py'\r\nadding 'fastdeploy/vision/generation/contrib/anemigan.py'\r\nadding 'fastdeploy/vision/headpose/__init__.py'\r\nadding 'fastdeploy/vision/headpose/contrib/__init__.py'\r\nadding 'fastdeploy/vision/headpose/contrib/fsanet.py'\r\nadding 'fastdeploy/vision/keypointdetection/__init__.py'\r\nadding 'fastdeploy/vision/keypointdetection/pptinypose/__init__.py'\r\nadding 'fastdeploy/vision/matting/__init__.py'\r\nadding 'fastdeploy/vision/matting/contrib/__init__.py'\r\nadding 'fastdeploy/vision/matting/contrib/modnet.py'\r\nadding 'fastdeploy/vision/matting/contrib/rvm.py'\r\nadding 'fastdeploy/vision/matting/ppmatting/__init__.py'\r\nadding 'fastdeploy/vision/ocr/__init__.py'\r\nadding 'fastdeploy/vision/ocr/ppocr/__init__.py'\r\nadding 'fastdeploy/vision/segmentation/__init__.py'\r\nadding 'fastdeploy/vision/segmentation/ppseg/__init__.py'\r\nadding 'fastdeploy/vision/sr/__init__.py'\r\nadding 'fastdeploy/vision/sr/ppsr/__init__.py'\r\nadding 'fastdeploy/vision/tracking/__init__.py'\r\nadding 'fastdeploy/vision/tracking/pptracking/__init__.py'\r\nadding 'fastdeploy/vision/visualize/__init__.py'\r\nadding 'fastdeploy_python-0.0.0.dist-info/METADATA'\r\nadding 'fastdeploy_python-0.0.0.dist-info/WHEEL'\r\nadding 'fastdeploy_python-0.0.0.dist-info/top_level.txt'\r\nadding 'fastdeploy_python-0.0.0.dist-info/RECORD'\r\nremoving build/bdist.linux-aarch64/wheel\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python# cd dist\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python/dist# pip3 install fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl\r\nProcessing ./fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl\r\nRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from fastdeploy-python==0.0.0) (1.19.5)\r\nRequirement already satisfied: fastdeploy-tools>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from fastdeploy-python==0.0.0) (0.0.5)\r\nRequirement already satisfied: fastapi in /usr/local/lib/python3.9/dist-packages (from fastdeploy-python==0.0.0) (0.92.0)\r\nRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from fastdeploy-python==0.0.0) (2.25.1)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from fastdeploy-python==0.0.0) (4.64.1)\r\nRequirement already satisfied: wheel in /usr/lib/python3/dist-packages (from fastdeploy-python==0.0.0) (0.34.2)\r\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from fastdeploy-python==0.0.0) (6.0)\r\nRequirement already satisfied: uvicorn==0.16.0 in /usr/local/lib/python3.9/dist-packages (from fastdeploy-tools>=0.0.5->fastdeploy-python==0.0.0) (0.16.0)\r\nRequirement already satisfied: asgiref>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn==0.16.0->fastdeploy-tools>=0.0.5->fastdeploy-python==0.0.0) (3.6.0)\r\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn==0.16.0->fastdeploy-tools>=0.0.5->fastdeploy-python==0.0.0) (8.1.3)\r\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn==0.16.0->fastdeploy-tools>=0.0.5->fastdeploy-python==0.0.0) (0.14.0)\r\nRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.9/dist-packages (from fastapi->fastdeploy-python==0.0.0) (1.10.5)\r\nRequirement already satisfied: starlette<0.26.0,>=0.25.0 in /usr/local/lib/python3.9/dist-packages (from fastapi->fastdeploy-python==0.0.0) (0.25.0)\r\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi->fastdeploy-python==0.0.0) (4.5.0)\r\nRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.26.0,>=0.25.0->fastapi->fastdeploy-python==0.0.0) (3.6.2)\r\nRequirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi->fastdeploy-python==0.0.0) (2.10)\r\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi->fastdeploy-python==0.0.0) (1.3.0)\r\nfastdeploy-python is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/python/dist# cd  /home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python/\r\n- 【执行命令】\r\nroot@linaro-alip:/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python# python3 infer.py --model_file config/picodet_l_640_lcnet_rk3588_unquantized.rknn --config_file config/infer_cfg.yml --image \r\nconfig/0000364_01177_d_0000799_320_0_960_540.jpg\r\n- 【报错信息】\r\nTraceback (most recent call last):\r\n  File \"/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python/infer.py\", line 50, in <module>\r\n    model = fd.vision.detection.PPYOLOE(\r\n  File \"/usr/local/lib/python3.9/dist-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 101, in __init__\r\n    self._model = C.vision.detection.PPYOLOE(\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'vision'\r\n\r\n\r\n",
        "state": "closed",
        "user": "vamoslc",
        "closed_by": "vamoslc",
        "created_at": "2023-03-06T02:49:02+00:00",
        "updated_at": "2025-02-07T06:22:49+00:00",
        "closed_at": "2023-03-07T02:54:57+00:00",
        "comments_count": [
            "jiangjiajun",
            "vamoslc",
            "vamoslc",
            "jiangjiajun",
            "vamoslc",
            "apperciation"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1518,
        "title": "A311D部署PP-ShiTuV2的特征提取模型general_PPLCNetV2_base_pretrained_v1.0报错。",
        "body": "- 【FastDeploy版本】： fastdeploy-linux-1.0.4\r\n- 【编译命令】cmake -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DWITH_TIMVX=ON  -DTARGET_ABI=arm64 -DENABLE_FLYCV=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-timvx -DENABLE_VISION=ON -DENABLE_LITE_BACCKEND=ON -Wno-dev ..\r\n- 【系统平台】: aarch64 linux kernel 4.9.113，gcc、g++=9.3.0，make=4.2.1，cmake=3.22.6，ubuntu 20.04 arm64\r\n- 【硬件】： 荣品A311D，NPU驱动已经安装为 6.4.4.3\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n### 【模型跑不通】\r\n- 已经按照FD官方文档跑通 [PaddleClas A311D 开发板 C++ 部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/classification/paddleclas/a311d/cpp/README_CN.md)。\r\n\r\n- 我的目的是在A311D上部署PP-ShiTuV2，主体检测部分已经部署成功，目前是部署特征提取模型时，遇到了问题。直接采用的特征提取模型为：[general_PPLCNetV2_base_pretrained_v1.0.pdparams](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/PPShiTuV2/general_PPLCNetV2_base_pretrained_v1.0.pdparams)，并且通过如下命令导出模型：\r\n\r\n`python tools/export_model.py -c ppcls/configs/GeneralRecognitionV2/GeneralRecognitionV2_PPLCNetV2_base.yaml -o Global.pretrained_model=../general_PPLCNetV2_base_pretrained_v1.0 -o Global.device=cpu use_shared_conv=False`\r\n\r\n导出模型的文件里，并没有inference_cls.yaml文件，所以直接使用这个yaml文件：https://github.com/PaddlePaddle/PaddleClas/blob/release/2.5/ppcls/configs/GeneralRecognitionV2/GeneralRecognitionV2_PPLCNetV2_base.yaml\r\n\r\n用到的文件和模型已经打包上传[source_code.zip](https://github.com/PaddlePaddle/FastDeploy/files/10893915/source_code.zip)，其中包含：\r\n- 推理代码：infer_demo.cc，因为已经跑通主体检测部分，所以直接把特征提取的代码加到里面了。\r\n- 编译脚本：generate_demo.sh\r\n- 运行脚本：run_demo.sh\r\n- 模型文件：feature_extract_convfalse\r\n- 测试图片：Adboard_clas_test_img02.jpg\r\n- 报错信息：\r\n[W  3/ 6  8:51:35.676 ..._install/Paddle-Lite/lite/api/cxx_api.cc:609 CheckInputValid]  Error input tensor precision type. Input index (0) Tensor name (x) Require precision type (float) Input precision type (uint8_t).\r\n\r\n提示看是模型精度的问题，但是不知道该如何设置，请大佬指点。\r\n",
        "state": "closed",
        "user": "Taichipeace",
        "closed_by": "Taichipeace",
        "created_at": "2023-03-06T02:51:34+00:00",
        "updated_at": "2023-04-24T10:07:10+00:00",
        "closed_at": "2023-03-06T10:45:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "Taichipeace",
            "jiangjiajun",
            "Taichipeace",
            "Taichipeace",
            "Hongyuan-Liu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1519,
        "title": "Python编译完成，引入包的时候报‘FastDeploy initalized failed’错误",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.4\r\n- 【编译命令】\r\n```\r\ncd FastDeploy/python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_OPENVINO_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TEXT=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n ```\r\n- 【系统平台】: Linux x64(Centos 7) \r\n- 【硬件】： 8c16g 无gpu\r\n- 【编译语言】： Python3.9\r\n```\r\n(3.9) [root@analysts-master-1 dist]# g++ --version\r\ng++ (GCC) 8.3.1 20190311 (Red Hat 8.3.1-3)\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n(3.9) [root@analysts-master-1 dist]# cmake --version\r\ncmake version 3.18.0-rc1\r\n\r\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n```\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【编译完成过后导入包报错】\r\n编译日志 `python setup.py build`\r\n```\r\nfatal: Not a git repository (or any of the parent directories): .git\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\n-- The C compiler identification is GNU 8.3.1\r\n-- The CXX compiler identification is GNU 8.3.1\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /opt/rh/devtoolset-8/root/usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /opt/rh/devtoolset-8/root/usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-x86_64.tar.gz to /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/patchelf-0.15.0-x86_64.tar.gz ...\r\nDecompress file /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/opencv-linux-x64-3.4.16.tgz ...\r\n-- Found OpenCV: /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/third_libs/install/opencv (found version \"3.4.16\") \r\nFASTTOKENIZER_COMPILE_LIB = /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.so\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.18.0-rc1\r\n--   CMake command             : /opt/cmake-3.18.0-rc1-Linux-x86_64/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /opt/rh/devtoolset-8/root/usr/bin/c++\r\n--   C++ compiler version      : 8.3.1\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;ENABLE_OPENVINO_BACKEND;ENABLE_VISION;ENABLE_TEXT;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 1.0.4\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : 2.4-dev5\r\n--   OpenVINO version          : 2022.2.0.dev20220829\r\n--   Python executable         : /opt/miniconda3/envs/3.9/bin/python\r\n--   Python includes           : /opt/miniconda3/envs/3.9/include/python3.9\r\n-- Found PythonInterp: /opt/miniconda3/envs/3.9/bin/python (found version \"3.9.16\") \r\n-- Found PythonLibs: /opt/miniconda3/envs/3.9/lib/libpython3.9.so (found version \"3.9.16\") \r\n-- Configuring done\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    ONNX_NAMESPACE\r\n    PADDLELITE_URL\r\n\r\n-- Build files have been written to: /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build\r\n/opt/cmake-3.18.0-rc1-Linux-x86_64/bin/cmake -P /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/CMakeFiles/VerifyGlobs.cmake\r\n/opt/cmake-3.18.0-rc1-Linux-x86_64/bin/cmake -S/opt/FastDeploy-release-1.0.4 -B/opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build --check-build-system CMakeFiles/Makefile.cmake 0\r\n/opt/cmake-3.18.0-rc1-Linux-x86_64/bin/cmake -E cmake_progress_start /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/CMakeFiles /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build//CMakeFiles/progress.marks\r\n/usr/bin/gmake -s -f CMakeFiles/Makefile2 all\r\nScanning dependencies of target extern_paddle_inference\r\nScanning dependencies of target yaml-cpp\r\nScanning dependencies of target extern_paddle2onnx\r\nScanning dependencies of target extern_onnxruntime\r\nScanning dependencies of target extern_fast_tokenizer\r\nScanning dependencies of target copy_third_libraries\r\nScanning dependencies of target copy_fd_libraries\r\n[100%] Built target copy_fd_libraries\r\n[100%] Built target copy_third_libraries\r\n/opt/cmake-3.18.0-rc1-Linux-x86_64/bin/cmake -E cmake_progress_start /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/CMakeFiles 0\r\ncreating build\r\n忽略其它日志\r\nrunning build_ext\r\ncopying /opt/FastDeploy-release-1.0.4/python/.setuptools-cmake-build/fastdeploy_main.cpython-39-x86_64-linux-gnu.so -> /opt/FastDeploy-release-1.0.4/python/build/lib.linux-x86_64-cpython-39/fastdeploy\r\n\r\n```\r\n\r\n打包日志`python setup.py bdist_wheel`\r\n\r\n```\r\nfatal: Not a git repository (or any of the parent directories): .git\r\nrunning bdist_wheel\r\nrunning build\r\nrunning build_py\r\nrunning egg_info\r\ncreating fastdeploy_python.egg-info\r\nwriting fastdeploy_python.egg-info/PKG-INFO\r\nwriting dependency_links to fastdeploy_python.egg-info/dependency_links.txt\r\nwriting requirements to fastdeploy_python.egg-info/requires.txt\r\nwriting top-level names to fastdeploy_python.egg-info/top_level.txt\r\nwriting manifest file 'fastdeploy_python.egg-info/SOURCES.txt'\r\nreading manifest file 'fastdeploy_python.egg-info/SOURCES.txt'\r\nwriting manifest file 'fastdeploy_python.egg-info/SOURCES.txt'\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.fast_tokenizer' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.fast_tokenizer' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.fast_tokenizer' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.fast_tokenizer' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.fast_tokenizer.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.fast_tokenizer.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.fast_tokenizer.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.fast_tokenizer.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.fast_tokenizer.third_party.include' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.fast_tokenizer.third_party.include' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.fast_tokenizer.third_party.include' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.fast_tokenizer.third_party.include' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.onnxruntime.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.onnxruntime.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.onnxruntime.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.onnxruntime.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.opencv.lib64' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.opencv.lib64' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.opencv.lib64' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.opencv.lib64' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.openvino.runtime' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.openvino.runtime' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.openvino.runtime' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.openvino.runtime' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.openvino.runtime.cmake' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.openvino.runtime.cmake' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.openvino.runtime.cmake' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.openvino.runtime.cmake' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.openvino.runtime.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.openvino.runtime.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.openvino.runtime.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.openvino.runtime.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle2onnx.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle2onnx.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle2onnx.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle2onnx.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mkldnn.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mkldnn.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mkldnn.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mkldnn.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mklml.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mklml.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mklml.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.mklml.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf.compiler' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf.compiler' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf.compiler' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf.compiler' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.utf8proc.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.utf8proc.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.utf8proc.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.utf8proc.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.xxhash.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.xxhash.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.xxhash.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.xxhash.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n忽略其它日志\r\n```\r\n\r\n\r\n报错日志\r\n```\r\n(3.9) [root@analysts-master-1 dist]# python\r\nPython 3.9.16 (main, Jan 11 2023, 16:05:54) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import fastdeploy as fd\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /opt/miniconda3/envs/3.9/lib/python3.9/site-packages/fastdeploy/libs/libfastdeploy.so.1.0.4: undefined symbol: _ZN2ov3Any4Base9to_stringEv\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/fastdeploy/__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/opt/miniconda3/envs/3.9/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n```\r\n\r\n",
        "state": "closed",
        "user": "shirukai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T03:43:53+00:00",
        "updated_at": "2024-03-20T06:57:20+00:00",
        "closed_at": "2024-03-19T06:40:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "shirukai",
            "jiangjiajun",
            "shirukai",
            "kehuo",
            "kehuo",
            "ITCZhuxy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1520,
        "title": "请问如何在Fastdeploy中使用paddlevideo中的TSM？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不同】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "closed",
        "user": "yangguoping123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T04:41:22+00:00",
        "updated_at": "2024-03-12T06:39:14+00:00",
        "closed_at": "2024-03-12T06:39:14+00:00",
        "comments_count": [
            "jiangjiajun",
            "yangguoping123",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1521,
        "title": "gpu docker 部署 cup占用高",
        "body": "registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n1.0.4 版本  部署正常 调用报错\r\n1.0.2 版本  部署正常 调用正常\r\n1.0.2 版本  空闲时cup占用过大  4核处理器 cup持续占用30%左右  8核处理器 持续占用15%左右\r\nhttps://github.com/triton-inference-server/server/issues/3779  这里可能是triton-inference-server解决方案 \r\n希望飞浆可以尽快更新到 triton-inference-server 可以正常使用的版本",
        "state": "closed",
        "user": "zhangshengsheng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T05:22:10+00:00",
        "updated_at": "2024-07-02T06:40:18+00:00",
        "closed_at": "2024-07-02T06:40:18+00:00",
        "comments_count": [
            "zhangshengsheng",
            "heliqi",
            "zhangshengsheng",
            "teymur-git",
            "teymur-git",
            "polarisunny"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1523,
        "title": "Jetson PP_LiteSeg 不能正常检测",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：  \r\n- 【编译命令】 python编译\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)   Jetson nano\r\n- 【硬件】： Jetson nano  cuda 10.2\r\n- 【编译语言】：Python 3.6.9\r\n-  cmake-3.25.2\r\n-   gcc  7.5.0 \r\n-   jetpack   4.6.1\r\n## 问题日志及出现问题的操作流程\r\n\r\n# GPU推理\r\n`python infer.py --model yolov8.onnx --image 000000014439.jpg --device gpu\r\n`目标检测 没问题\r\n\r\n在尝试 PP_LiteSeg \r\n一直等待  系统卡顿\r\n然后自动 Killed\r\n\r\n```\r\npython infer.py --model /PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer --image /0MX/cityscapes_demo.png --device gpu\r\n\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(264)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\n\r\nKilled\r\n```\r\n\r\n\r\n\r\n切换 CPU 正常\r\n```\r\n\r\npython infer.py --model/PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer --image /cityscapes_demo.png --device CPU\r\n\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0306 15:24:33.741662 14267 analysis_config.cc:563] Please compile with MKLDNN first to use MKLDNN\r\nE0306 15:24:33.968976 14267 analysis_config.cc:574] Please compile with MKLDNN first to set MKLDNN Thread Id\r\n[INFO] fastdeploy/runtime/runtime.cc(264)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nSegmentationResult Image masks 10 rows x 10 cols: \r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, .....]\r\n...........\r\nresult shape is: [1024 2048]\r\n\r\n```\r\n\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T07:31:06+00:00",
        "updated_at": "2024-03-12T06:39:15+00:00",
        "closed_at": "2024-03-12T06:39:15+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1524,
        "title": "Seg模型  without-argmax with-argmax  有什么不同吗",
        "body": "Seg模型\r\n\r\nwithout-argmax\r\nwith-argmax\r\n有什么不同吗",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-03-06T07:56:46+00:00",
        "updated_at": "2023-03-07T03:33:01+00:00",
        "closed_at": "2023-03-07T03:33:01+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1525,
        "title": "rk3588 3核npu的分配问题",
        "body": "rk3588，使用的是ROC-rk3588s-pc，是firefly的板子，希望能在目前的fastdeploy的默认分配核心的基础上添加自定义核心分配的功能。",
        "state": "closed",
        "user": "XDUwsk",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T08:22:50+00:00",
        "updated_at": "2024-03-12T06:39:16+00:00",
        "closed_at": "2024-03-12T06:39:16+00:00",
        "comments_count": [
            "jiangjiajun",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 1532
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1526,
        "title": "FastDeploy 有没手部关键点检测,或者有没考虑手部关键点模型",
        "body": "https://github.com/PaddlePaddle/PaddleHub/tree/develop/modules/image/keypoint_detection/hand_pose_localization\r\n\r\n我看PaddleHub有个手部关键点检测\r\n但是 FastDeploy PaddleDetection 都没\r\n\r\nPaddleDetection  \r\n关键点教程 也是云里雾里的\r\n\r\n希望FastDeploy 增加手部关键点模型,\r\n这样开箱即用",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-06T09:32:33+00:00",
        "updated_at": "2024-03-12T06:39:17+00:00",
        "closed_at": "2024-03-12T06:39:17+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1529,
        "title": "安装的gpu版本，但是使用的却是cpu",
        "body": "怎么使用model在gpu上运行",
        "state": "closed",
        "user": "kankanjiuzou123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-07T02:42:01+00:00",
        "updated_at": "2024-04-16T09:03:13+00:00",
        "closed_at": "2024-04-16T09:03:13+00:00",
        "comments_count": [
            "jiangjiajun",
            "kankanjiuzou123",
            "kankanjiuzou123",
            "jiangjiajun",
            "kankanjiuzou123",
            "kankanjiuzou123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1530,
        "title": "paddleocr3 在转成rknn模型时出错",
        "body": "python3.8 FastDeploy/tools/rknpu2/export.py --config_path FastDeploy/tools/rknpu2/config/ppocrv3_det.yaml                               --target_platform rk3588\r\n{'mean': [[123.675, 116.28, 103.53]], 'std': [[58.395, 57.12, 57.375]], 'model_path': './ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx', 'outputs_nodes': None, 'do_quantization': False, 'dataset': None, 'output_folder': './ch_PP-OCRv3_det_infer'}\r\nTraceback (most recent call last):\r\n  File \"FastDeploy/tools/rknpu2/export.py\", line 35, in <module>\r\n    model = RKNN(config.verbose)\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/rknn/api/rknn.py\", line 56, in __init__\r\n    self.rknn_base = RKNNBase(cur_path, verbose)\r\n  File \"rknn/api/rknn_base.py\", line 75, in rknn.api.rknn_base.RKNNBase.__init__\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 514, in get_distribution\r\n    dist = get_provider(dist)\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 386, in get_provider\r\n    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 683, in find\r\n    if dist is not None and dist not in req:\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 3135, in __contains__\r\n    return self.specifier.contains(item, prereleases=True)\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py\", line 902, in contains\r\n    item = Version(item)\r\n  File \"/home/wuli/.local/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py\", line 197, in __init__\r\n    raise InvalidVersion(f\"Invalid version: '{version}'\")\r\npkg_resources.extern.packaging.version.InvalidVersion: Invalid version: '1.4.0-22dcfef4'",
        "state": "closed",
        "user": "leokwu",
        "closed_by": "leokwu",
        "created_at": "2023-03-07T03:32:07+00:00",
        "updated_at": "2024-01-10T13:15:58+00:00",
        "closed_at": "2023-03-08T14:12:32+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "leokwu",
            "Zheng-Bicheng",
            "leokwu",
            "Zheng-Bicheng",
            "leokwu",
            "Zheng-Bicheng",
            "leokwu",
            "leokwu",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "leokwu",
            "srd2018",
            "srd2018",
            "bai-0829"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1535,
        "title": "Serving 最新镜像1.0.4-gpu-cuda11.4-trt8.5-21.10推理PPYOLOE报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.4\r\n- 【系统平台】: docker\r\n- 【硬件】： Nvidia GPU 3060TI\r\n- 【编译语言】： C++ / Python\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【部署 Serving 1.0.4-gpu-cuda11.4-trt8.5-21.10 docker 镜像推理 PP-YOLOE 报错】\r\n![c084337c599c82db1389bc9c32eff1c](https://user-images.githubusercontent.com/38065710/223346320-06b45baf-5d53-4df9-a9d3-7ffbed32d8b6.png)\r\n- 【使用 Serving 1.0.2-gpu-cuda11.4-trt8.4-21.10 docker 镜像推理 PP-YOLOE 正常】\r\n\r\n",
        "state": "closed",
        "user": "laugh12321",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-07T06:53:43+00:00",
        "updated_at": "2024-04-16T09:03:14+00:00",
        "closed_at": "2024-04-16T09:03:14+00:00",
        "comments_count": [],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1536,
        "title": "FastDeploy或者Paddle支持从内存加载推理模型吗？",
        "body": null,
        "state": "closed",
        "user": "Ruanlt",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-07T06:57:27+00:00",
        "updated_at": "2024-04-16T09:03:15+00:00",
        "closed_at": "2024-04-16T09:03:15+00:00",
        "comments_count": [
            "jiangjiajun",
            "Ruanlt",
            "Ruanlt"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1537,
        "title": "Serving 部署 PPHGNet 报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： docker 1.0.4-gpu-cuda11.4-trt8.5-21.10、1.0.2-gpu-cuda11.4-trt8.4-21.10\r\n- 【硬件】： Nvidia GPU 3060TI\r\n- 【编译语言】： C++ / Python\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 直接复制[models](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/classification/paddleclas/serving/models)所有文件，运行报错如下\r\n![image](https://user-images.githubusercontent.com/38065710/223348190-4c96bc94-314e-4660-a0da-66a1f03f6d42.png)\r\n- 将`models/runtime/config.pbtxt`中\r\n```\r\n# Input configuration of the model\r\ninput [\r\n  {\r\n    # input name\r\n    name: \"inputs\"\r\n    # input type such as TYPE_FP32、TYPE_UINT8、TYPE_INT8、TYPE_INT16、TYPE_INT32、TYPE_INT64、TYPE_FP16、TYPE_STRING\r\n    data_type: TYPE_FP32\r\n    # input shape， The batch dimension is omitted and the actual shape is [batch, c, h, w]\r\n    dims: [ 3, 224, 224 ]\r\n  }\r\n]\r\n```\r\n改为\r\n```\r\n# Input configuration of the model\r\ninput [\r\n  {\r\n    # input name\r\n    name: \"x\"\r\n    # input type such as TYPE_FP32、TYPE_UINT8、TYPE_INT8、TYPE_INT16、TYPE_INT32、TYPE_INT64、TYPE_FP16、TYPE_STRING\r\n    data_type: TYPE_FP32\r\n    # input shape， The batch dimension is omitted and the actual shape is [batch, c, h, w]\r\n    dims: [ 3, 224, 224 ]\r\n  }\r\n]\r\n```\r\n报错如下：\r\n![image](https://user-images.githubusercontent.com/38065710/223348714-3276f2dc-1d47-46ce-bb7b-6e571802a4a9.png)\r\n- 将`models/runtime/config.pbtxt`中\r\n```\r\n# The output of the model is configured in the same format as the input\r\noutput [\r\n  {\r\n    name: \"save_infer_model/scale_0.tmp_1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1000 ]\r\n  }\r\n]\r\n```\r\n改为\r\n```\r\n# The output of the model is configured in the same format as the input\r\noutput [\r\n  {\r\n    name: \"softmax_1.tmp_0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1000 ]\r\n  }\r\n]\r\n```\r\n报错如下：\r\n![image](https://user-images.githubusercontent.com/38065710/223349199-3bf1e5e0-acdf-4eac-81e5-39d80127010b.png)\r\n",
        "state": "closed",
        "user": "laugh12321",
        "closed_by": "laugh12321",
        "created_at": "2023-03-07T07:07:43+00:00",
        "updated_at": "2023-04-03T10:20:12+00:00",
        "closed_at": "2023-03-22T07:31:58+00:00",
        "comments_count": [
            "heliqi",
            "laugh12321"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1539,
        "title": "cuda_runtime_api.h file not found error",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0\r\nOS Platform:  Windows x64 \r\nHardware: e.g. Nvidia GPU 2080Ti  CUDA 11.6 CUDNN 8.3\r\nProgram Language: C++\r\n\r\n## Problem description\r\nWhen trying to include \"fastdeploy/vision.h\" I receive 'cuda_runtime_api.h' file not found error.I have checked and there is cuda_runtime_api.h in my cuda/include folder.I have also added cuda/include to system variables path.\r\n\r\nThank you for your time.\r\n",
        "state": "closed",
        "user": "UygarUsta99",
        "closed_by": "UygarUsta99",
        "created_at": "2023-03-07T07:12:01+00:00",
        "updated_at": "2024-03-07T14:12:44+00:00",
        "closed_at": "2023-03-07T07:28:59+00:00",
        "comments_count": [
            "UygarUsta99",
            "yywangfei",
            "UygarUsta99"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1544,
        "title": "fastdeploy推理ser_vi_layoutxlm_xfund_infer 和re_vi_layoutxlm_xfund_infer模型报错",
        "body": "Signal (11) received.\r\n 0# 0x000055618231A8A9 in fastdeployserver\r\n 1# 0x00007F1940F8D210 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# 0x00007F19410D5885 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# void paddle_infer::Tensor::CopyFromCpu<long>(long const*) in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 4# fastdeploy::ShareTensorFromFDTensor(paddle_infer::Tensor*, fastdeploy::FDTensor&) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.4\r\n 5# fastdeploy::PaddleBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.4\r\n 6# fastdeploy::Runtime::Infer() in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.4\r\n 7# 0x00007F1920705E94 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n 8# 0x00007F1920709726 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n 9# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n10# 0x00007F1941B1783A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n11# 0x00007F1941B1804D in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n12# 0x00007F19419CC801 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n13# 0x00007F1941B11DC7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n14# 0x00007F194137BDE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n15# 0x00007F19417F9609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n16# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\nSegmentation fault (core dumped)\r\n",
        "state": "closed",
        "user": "ChengShuting",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-07T09:32:10+00:00",
        "updated_at": "2024-11-26T06:40:45+00:00",
        "closed_at": "2024-11-26T06:40:45+00:00",
        "comments_count": [
            "heliqi",
            "ChengShuting",
            "ChengShuting",
            "elonzh"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1545,
        "title": "在飞腾平台下编译xpu版报错不通过",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop版本\r\n- 【编译命令】cmake -DWITH_KUNLUNXIN=ON -DWITH_GPU=OFF -DENABLE_ORT_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON- -DCMAKE_INSTALL_PREFIX=fastdeploy-kunlunxin -DENABLE_VISION=ON ..\r\n\r\n- 【系统平台】: 银河麒麟v10 Linux localhost.localdomain 4.19.90-25.2.v2101.gfb01.ky10.aarch64\r\n- 【硬件】： 飞腾2000+CPU  昆仑k200xpu\r\n- 【编译语言】： C++  && Python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n在上述环境下进行依靠Paddlelite后端的FastDeploy编译时出现报错：\r\n[root@localhost build]# cmake -DWITH_KUNLUNXIN=ON -DWITH_GPU=OFF -DENABLE_ORT_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON- -DCMAKE_INSTALL_PREFIX=fastdeploy-kunlunxin -DENABLE_VISION=ON ..\r\nDecompress file /root/fast/FastDeploy/build/patchelf-0.15.0-aarch64.tar.gz ...\r\nCMake Error at cmake/kunlunxin.cmake:7 (message):\r\n  KunlunXin XPU is only supported on Linux x64 platform\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:132 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/root/fast/FastDeploy/build/CMakeFiles/CMakeOutput.log\".\r\n",
        "state": "closed",
        "user": "oushilu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-07T11:35:36+00:00",
        "updated_at": "2024-04-02T06:40:03+00:00",
        "closed_at": "2024-04-02T06:40:03+00:00",
        "comments_count": [
            "yeliang2258"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1540,
        "title": "rk3588 推理 picodet模型出错 生成的图片中无框",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： git clone [https://github.com/PaddlePaddle/FastDeploy.git最新](https://github.com/PaddlePaddle/FastDeploy.git%E6%9C%80%E6%96%B0)\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】:  debian\r\n- 【硬件】： rk3588\r\n- 【编译语言】： Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 按照教程下载的picodet模型 在pc端导出生成rknn之后放在板子上推理出错 错误信息为\r\n- root@linaro-alip:/home/linaro/LC/FastDeploy/examples/vision/detection/paddledetection/rknpu2/python# python3 infer.py --modefile picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet_rk3588_unquantized.rknn --config_file picodet_s_416_coco_lcnet/infer_g.yml --image picodet_s_416_coco_lcnet/11479-0000364_01177_d_0000799_320_0_960_540.jpg\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize inreprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NmalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are sed to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion rknn_api/rknnrt version: 1.4.2b0 (c59ccf9@2023-02-14T17:55:39), driver version: 0.8.0\r\nindex=0, name=image, n_dims=4, dims=[1, 416, 416, 3], n_elems=519168, size=1038336, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp, scale=1.000000, pass_through=0\r\nindex=0, name=p2o.Mul.79, n_dims=4, dims=[1, 96, 52, 52], n_elems=259584, size=519168, fmt=NCHW, type=FP16, qnt_type=AFFINE,p=0, scale=1.000000, pass_through=0\r\nindex=1, name=p2o.Concat.9, n_dims=4, dims=[1, 80, 3598, 1], n_elems=287840, size=575680, fmt=NCHW, type=FP16, qnt_type=AFFI, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(334)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(326)::Infer      The input tensor type != model's inputs typehe input_type need FP16,but inputs[0].type is UINT8\r\n[ERROR] fastdeploy/vision/detection/ppdet/ppdet_decode.cc(115)::DecodeAndNMS    The shape of boxes and scores should be [bat, boxes_num, 4], [batch, classes_num, boxes_num]\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\nVisualized result save in ./visualized_result.jpg\r\n\r\n\r\n\r\n在生成的visualized_result.jpg中没有框显示。\r\n\r\n",
        "state": "closed",
        "user": "vamoslc",
        "closed_by": "vamoslc",
        "created_at": "2023-03-07T07:56:18+00:00",
        "updated_at": "2023-03-09T01:16:50+00:00",
        "closed_at": "2023-03-09T01:16:50+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "vamoslc"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1546,
        "title": "FastDeploy使用trt推理与原yolov5 6.1 版本trt加速冲突",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python-1.0.1\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：TITAN X ， CUDA 11.3 CUDNN 8.2 Tensorrt版本8.2.2.1与8.5.1.7（都会出现如下问题）\r\n- 【编译语言】：Python3.8等\r\n\r\n```\r\n[INFO] fastdeploy/runtime.cc(506)::fastdeploy::Runtime::Init    Runtime initialized with Backend::TRT in Device::GPU.\r\nLoading crowdhuman_yolov5m.engine for TensorRT inference...\r\n[03/07/2023-19:58:45] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\r\n\r\n[03/07/2023-19:58:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 15888, GPU 4476 (MiB)\r\n[03/07/2023-19:58:45] [TRT] [I] Loaded engine size: 109 MiB\r\n[03/07/2023-19:58:45] [TRT] [E] 1: [stdArchiveReader.cpp::nvinfer1::rt::StdArchiveReader::StdArchiveReader::40] Error Code 1: Serialization (Serialization assertion stdVersionRead == serializationVersion failed.Version tag does not match. Note: Current Version: 213, Serialized Engine Version: 232)\r\n[03/07/2023-19:58:45] [TRT] [E] 4: [runtime.cpp::nvinfer1::Runtime::deserializeCudaEngine::50] Error Code 4: Internal Error (Engine deserialization failed.)\r\nNone\r\nTraceback (most recent call last):\r\n  File \"app/app.py\", line 97, in <module>\r\n    yolo_model = DetectMultiBackend('crowdhuman_yolov5m.engine', device=device)  # 👨‍🚀\r\n  File \"C:\\Users/Administrator/Desktop/airun_project\\yolov5\\models\\common.py\", line 389, in __init__\r\n    context = model.create_execution_context()\r\nAttributeError: 'NoneType' object has no attribute 'create_execution_context'\r\n```",
        "state": "closed",
        "user": "LiQiang0307",
        "closed_by": "LiQiang0307",
        "created_at": "2023-03-07T12:07:54+00:00",
        "updated_at": "2024-04-10T07:22:27+00:00",
        "closed_at": "2023-03-09T08:17:34+00:00",
        "comments_count": [
            "LiQiang0307",
            "jiangjiajun",
            "LiQiang0307",
            "jiangjiajun",
            "LiQiang0307",
            "LiQiang0307",
            "jiangjiajun",
            "LiQiang0307",
            "LiQiang0307"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1548,
        "title": "MinGW64无法构建？",
        "body": "## 问题\r\n我想要使用MinGW64来编译，然而CMake报错。我的MinGW是64位的。\r\n```shell\r\n***********************Compile on non 64-bit system now**********************\r\nCMake Error at cmake/check.cmake:10 (message):\r\n  -DENABLE_PADDLE_BACKEND=ON doesn't support on non 64-bit system now.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:105 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n```\r\n报错的文件`cmake/check.cmake`内容如下：\r\n```shell\r\n# Check for 32bit system\r\nif(WIN32)\r\n  if(NOT CMAKE_CL_64)\r\n    message(\"***********************Compile on non 64-bit system now**********************\")\r\n    add_definitions(-DNON_64_PLATFORM)\r\n    if(WITH_GPU)\r\n      message(FATAL_ERROR \"-DWITH_GPU=ON doesn't support on non 64-bit system now.\")\r\n    endif()\r\n    if(ENABLE_PADDLE_BACKEND)\r\n      message(FATAL_ERROR \"-DENABLE_PADDLE_BACKEND=ON doesn't support on non 64-bit system now.\")\r\n    endif()\r\n    if(ENABLE_POROS_BACKEND)\r\n      message(FATAL_ERROR \"-DENABLE_POROS_BACKEND=ON doesn't support on non 64-bit system now.\")\r\n    endif()\r\n  endif()\r\nendif()\r\n```\r\n\r\n## CMake选项\r\n```shell\r\ncmake .. -G \"MinGW Makefiles\" -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON -DCMAKE_INSTALL_PREFIX=\"../install\"\r\n```\r\n## 环境\r\nWindwows 10\r\n\r\nTDM64-GCC 10.3.0：\r\n```shell\r\n$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=D:\\tdm-gcc\\bin\\gcc.exe\r\nCOLLECT_LTO_WRAPPER=D:/tdm-gcc/bin/../libexec/gcc/x86_64-w64-mingw32/10.3.0/lto-wrapper.exe\r\nTarget: x86_64-w64-mingw32\r\nConfigured with: ../../../src/gcc-git-10.3.0/configure --build=x86_64-w64-mingw32 --enable-targets=all --enable-languages=ada,c,c++,fortran,jit,lto,objc,obj-c++ --enable-libgomp --enable-lto --enable-graphite --enable-cxx-flags=-DWINPTHREAD_STATIC --disable-build-with-cxx --disable-build-poststage1-with-cxx --enable-libstdcxx-debug --enable-threads=posix --enable-version-specific-runtime-libs --enable-fully-dynamic-string --enable-libstdcxx-filesystem-ts=yes --disable-libstdcxx-pch --enable-libstdcxx-threads --enable-libstdcxx-time=yes --enable-mingw-wildcard --with-gnu-ld --disable-werror --enable-nls --disable-win32-registry --enable-large-address-aware --disable-rpath --disable-symvers --prefix=/mingw64tdm --with-local-prefix=/mingw64tdm --with-pkgversion=tdm64-1 --with-bugurl=https://github.com/jmeubank/tdm-gcc/issues\r\nThread model: posix\r\nSupported LTO compression algorithms: zlib zstd\r\ngcc version 10.3.0 (tdm64-1) \r\n```",
        "state": "closed",
        "user": "sixsixQAQ",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-08T01:29:47+00:00",
        "updated_at": "2024-03-19T06:40:03+00:00",
        "closed_at": "2024-03-19T06:40:03+00:00",
        "comments_count": [
            "sixsixQAQ",
            "jiangjiajun",
            "sixsixQAQ",
            "jiangjiajun",
            "sixsixQAQ",
            "sixsixQAQ",
            "lifw555"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1553,
        "title": "python API可视化增加标签展示 报错",
        "body": "fastdeploy-gpu-python-0.0.0 fastdeploy-tools-0.0.5\r\n\r\n```python\r\nlabel_list = list()\r\nwith open(\"label_list.txt\", \"r\") as f:\r\n     for line in f:\r\n          label_list.append(line.strip())\r\n\r\nvis_im = fd.vision.vis_detection(im, result,score_threshold=0.7, label_list=label_list)\r\n```\r\n\r\n\r\n`TypeError: vis_detection() got an unexpected keyword argument 'label_list'`",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-08T07:29:58+00:00",
        "updated_at": "2024-04-16T09:03:16+00:00",
        "closed_at": "2024-04-16T09:03:16+00:00",
        "comments_count": [
            "jiangjiajun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1555,
        "title": "量化训练",
        "body": "自动压缩模型的工具是只支持Linux系统吗？量化训练时提示这个\r\n709-WARNING: post-quant-hpo is not support in system other than linux",
        "state": "closed",
        "user": "xiaotailang",
        "closed_by": "yunyaoXYY",
        "created_at": "2023-03-08T12:07:24+00:00",
        "updated_at": "2023-07-14T06:31:11+00:00",
        "closed_at": "2023-03-09T02:22:41+00:00",
        "comments_count": [
            "lazyn1997",
            "lazyn1997"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1557,
        "title": "对于有多个box的图片，grpclient每次只能获取检测模型的一个box，而且返回的唯一的box的logits是完全正确的",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 使用docker构建，镜像是：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.2-gpu-cuda11.4-trt8.4-21.10\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: (gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3090TI\r\n- 【编译语言】： \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 使用paddleDetection的ppyoloe_plus_crn_l_80e_coco.yml 训练一个检测模型\r\n- - 通过官方脚本export成 \".pdiparams\"、\"pdmodel\"两个推理用文件\r\n- -  使用paddleDetection的 deploy/python/infer.py中的Detector类加载检测模型，推理正常\r\n- -  使用fastdeploy部署service，配置文件如下：\r\n>input [\r\n  {\r\n    name: \"image\"\r\n    data_type: TYPE_FP32]\r\n    dims: [ 3, 640, 640 ]\r\n  },\r\n  {\r\n    name: \"scale_factor\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 2 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"multiclass_nms3_0.tmp_0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 6 ]\r\n  },\r\n  {\r\n    name: \"multiclass_nms3_0.tmp_2\"\r\n    data_type: TYPE_INT32\r\n    dims: [-1, 2 ]\r\n  }\r\n\r\n]\r\n\r\n通过grpcclient能够正常调用模型，调用代码：\r\n```python\r\n            inputs = []\r\n            infer_input0 = grpcclient.InferInput('image', model_inputs['image'].shape, 'FP32')\r\n            infer_input0.set_data_from_numpy(model_inputs['image'])  # 载入输入数据\r\n            infer_input1 = grpcclient.InferInput('scale_factor', model_inputs['scale_factor'].shape, 'FP32')\r\n            infer_input1.set_data_from_numpy(model_inputs['scale_factor'])\r\n            inputs.append(infer_input0)\r\n            inputs.append(infer_input1)\r\n\r\n            outputs = []\r\n            infer_output0 = grpcclient.InferRequestedOutput('multiclass_nms3_0.tmp_0')\r\n            infer_output1 = grpcclient.InferRequestedOutput('multiclass_nms3_0.tmp_2')\r\n            outputs.append(infer_output0)\r\n            outputs.append(infer_output1)```\r\n\r\n但是有一个问题，用grpc获取的图片无论配置文件的output的维度怎么设置，始终返回维度为[1,6]和[1]的两个输出，但是正常用Detector类预测，同一张图片，Detector会给出[64,6]维度的box结果和box个数[64]。换图片Detector都能给出正确的所有box（box数量大于1），而grpc客户端每次都只返回一个box，并且这个box的logits与Detector类返回的第一个box完全一致，两者只有box数量不同区别。\r\n\r\n请问如何解决？",
        "state": "closed",
        "user": "aixuedegege",
        "closed_by": "aixuedegege",
        "created_at": "2023-03-08T12:25:55+00:00",
        "updated_at": "2025-06-10T15:19:05+00:00",
        "closed_at": "2023-03-09T10:01:08+00:00",
        "comments_count": [
            "aixuedegege",
            "aixuedegege",
            "rainyfly",
            "aixuedegege",
            "rainyfly",
            "aixuedegege",
            "rainyfly",
            "rainyfly",
            "aixuedegege",
            "rainyfly",
            "aixuedegege",
            "impl1874"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1554,
        "title": "Jetson TX2 NX利用PPOCRv3GPU推理找不到PPOCRv3模块",
        "body": "## 环境\r\n- 【FastDeploy版本】： fastdeploy-gpu-python==0.2.1\r\n- 【编译命令】使用安装文档Jetson部署环境编译安装(编译安装成功)\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) Jetpack4.6\r\n- 【硬件】： NVIDIA Pascalm architecture with 256 NVIDIAR CUDAR cores\r\n- 【编译语言】： Python3.6.9\r\n\r\n------------------------------------------------------------------------------------------------------------------\r\n使用fastdeploy中的PPOCRv3的GPU推理找不到PPOCRv3的模块，但是使用CPU推理却又可以，推理代码如下：\r\nOPENBLAS_CORETYPE=ARMV8 python3 test.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu\r\n------------------------------------------------------------------------------------------------------------------\r\n运行之后出现以下的情况：\r\n[WARNING] fastdeploy/backends/ort/ort_backend.cc(57)::BuildOption\tCompiled fastdeploy with onnxruntime doesn't support GPU, the available providers are CPUExecutionProvider, will fallback to CPUExecutionProvider.\r\n[INFO] fastdeploy/runtime.cc(283)::Init\tRuntime initialized with Backend::ORT in Device::GPU.\r\n[WARNING] fastdeploy/backends/ort/ort_backend.cc(57)::BuildOption\tCompiled fastdeploy with onnxruntime doesn't support GPU, the available providers are CPUExecutionProvider, will fallback to CPUExecutionProvider.\r\n[INFO] fastdeploy/runtime.cc(283)::Init\tRuntime initialized with Backend::ORT in Device::GPU.\r\n[WARNING] fastdeploy/backends/ort/ort_backend.cc(57)::BuildOption\tCompiled fastdeploy with onnxruntime doesn't support GPU, the available providers are CPUExecutionProvider, will fallback to CPUExecutionProvider.\r\n[INFO] fastdeploy/runtime.cc(283)::Init\tRuntime initialized with Backend::ORT in Device::GPU.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 197, in <module>\r\n    ppocr_v3 = fd.vision.ocr.PPOCRv3(\r\nAttributeError: module 'fastdeploy.vision.ocr' has no attribute 'PPOCRv3'\r\n请问该如何解决？",
        "state": "closed",
        "user": "ChihoJack",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-08T08:51:07+00:00",
        "updated_at": "2024-04-16T09:03:17+00:00",
        "closed_at": "2024-04-16T09:03:17+00:00",
        "comments_count": [
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "ChihoJack",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "wang-xinyu",
            "ChihoJack",
            "jiangjiajun",
            "ChihoJack",
            "ChihoJack",
            "jiangjiajun",
            "ChihoJack",
            "ChihoJack",
            "jiangjiajun",
            "ChihoJack",
            "ChihoJack"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1562,
        "title": "fastdeploy' has no attribute 'RuntimeOption",
        "body": "## Environment\r\nrk3568\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n runtime_option = build_tinypose_option(args)\r\n  File \"examples/vision/keypointdetection/tiny_pose/rknpu2/python/pptinypose_infer.py\", line 21, in build_tinypose_option\r\n    option = fd.RuntimeOption()\r\nAttributeError: module 'fastdeploy' has no attribute 'RuntimeOption'\r\n",
        "state": "open",
        "user": "maybe2122",
        "closed_by": null,
        "created_at": "2023-03-09T02:06:23+00:00",
        "updated_at": "2024-08-24T10:23:10+00:00",
        "closed_at": null,
        "comments_count": [
            "leiqing1",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "hitzhu",
            "xireli",
            "Zheng-Bicheng",
            "jackyzzy"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1556,
        "title": "量化训练",
        "body": "自动压缩模型的工具是只支持Linux系统吗？量化训练时提示这个\r\n709-WARNING: post-quant-hpo is not support in system other than linux",
        "state": "closed",
        "user": "xiaotailang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-08T12:13:07+00:00",
        "updated_at": "2024-04-16T09:03:17+00:00",
        "closed_at": "2024-04-16T09:03:17+00:00",
        "comments_count": [
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "xiaotailang",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang",
            "xiaotailang",
            "yunyaoXYY",
            "xiaotailang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1565,
        "title": "paddleocr3在rk3588 npu上推理时间和cpu推理时间无明显提升",
        "body": "rk3588 cpu推理：\r\n![58d7c6de5bec6a5e138ef22561bb9c1](https://user-images.githubusercontent.com/26742943/223942402-fea1f242-d72e-4535-9589-4307cf8ae4c1.png)\r\nrk3588 npu推理：\r\n![908e390d9a08946c6bca0e37d8f734a](https://user-images.githubusercontent.com/26742943/223942481-7a036170-e8c4-42a1-b593-d49e1aee8805.png)\r\n\r\n图像输入是640x640；",
        "state": "closed",
        "user": "leokwu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-09T06:47:45+00:00",
        "updated_at": "2024-11-26T06:40:46+00:00",
        "closed_at": "2024-11-26T06:40:46+00:00",
        "comments_count": [
            "leokwu",
            "Zheng-Bicheng",
            "zhouweic36"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1567,
        "title": "FastDeploy部署mbart模型报错",
        "body": "![562d191db9e6ad37d84ddb898839d07](https://user-images.githubusercontent.com/107381937/223949939-97ecda19-9355-4c6d-88b5-f5f68dff223b.png)\r\n",
        "state": "closed",
        "user": "Amy234543",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-09T07:23:58+00:00",
        "updated_at": "2024-04-16T09:03:18+00:00",
        "closed_at": "2024-04-16T09:03:18+00:00",
        "comments_count": [
            "leiqing1",
            "Amy234543"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1568,
        "title": "在RK3588上跑paddledetection/rknpu2示例报错反馈",
        "body": "在RK3588上跑paddledetection/rknpu2示例 ，会报fastdeploy.libs.fastdeploy_main.vision.detection.P' object has no attribute 'apply_decode_and_nms'，这个是啥情况？没编译好？（操作系统 kylin）\r\n![image](https://user-images.githubusercontent.com/61813449/223953838-7ebbf036-eb72-4e6d-97c7-93075046d7a5.png)\r\n",
        "state": "closed",
        "user": "eyu11",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-09T07:43:35+00:00",
        "updated_at": "2023-03-12T17:37:24+00:00",
        "closed_at": "2023-03-12T17:37:24+00:00",
        "comments_count": [
            "wjj19950828",
            "Zheng-Bicheng",
            "eyu11"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1569,
        "title": "DLA 量化测速",
        "body": "环境\r\n【FastDeploy版本】： 如fastdeploy-linux-gpu-1.0.3\r\n【系统平台】: jetpack 5.1\r\n【硬件】： Orin AGX， CUDA 11.4 CUDNN 8.4\r\n\r\nINT8模型推理未采用DLA加速，同时未找到启动DLA接口",
        "state": "open",
        "user": "ShawnXsw",
        "closed_by": null,
        "created_at": "2023-03-09T09:24:08+00:00",
        "updated_at": "2023-05-15T01:54:06+00:00",
        "closed_at": null,
        "comments_count": [
            "ShawnXsw",
            "ShawnXsw"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1571,
        "title": "segmentation中的infer错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，最新的预编译包\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 2080TI， CUDA 11.2 CUDNN 8.4\r\n- 【编译语言】： Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n执行examples下的infer代码，具体位置为segmentation下的cpu-gpu----python------infer.py\r\n由于使用的tensorrt原始后端，因此将部分代码修改如下：\r\n\r\n    if args.use_trt:\r\n        option.use_trt_backend()\r\n        # If use original Tensorrt, not Paddle-TensorRT,\r\n        # comment the following two lines\r\n        # option.enable_paddle_to_trt()\r\n        # option.enable_paddle_trt_collect_shape()\r\n        # option.set_trt_input_shape(\"x\", [1, 3, 256, 256], [1, 3, 1024, 1024],\r\n        #                            [1, 3, 2048, 2048])\r\n        option.set_trt_input_shape(tensor_name=\"image\", min_shape=[1, 3, 512, 512], opt_shape=[1, 3, 512, 512],\r\n                                   max_shape=[1, 3, 512, 512])\r\n        # option.set_trt_input_shape(tensor_name=\"scale_factor\", min_shape=[1, 2], opt_shape=[1, 2], max_shape=[1, 2])\r\n        option.set_trt_cache_file(os.path.join(args.model, \"model.trt\"))\r\n\r\n运行错误提示：\r\n[INFO] fastdeploy/runtime.cc(585)::Init Runtime initialized with Backend::TRT in Device::GPU.\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(238)::log   3: [executionContext.cpp::setBindingDimensions::944] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::setBindingDimensions::944, condition: profileMinDims.d[i] <= dimensions.d[i]. Supplied binding dimension [1,3,512,512] for bindings[0] exceed min ~ max range at index 2, maximum dimension in profile is 1024, minimum dimension in profile is 1024, but supplied dimension is 512.\r\n)\r\n[ERROR] fastdeploy/backends/tensorrt/trt_backend.cc(345)::Infer [ERROR] Error occurs while copy memory from GPU to CPU.\r\n但能够正常输出结果：\r\n\r\n\r\n但如果将上述代码修改为：\r\n\r\n    if args.use_trt:\r\n…………\r\n        option.set_trt_input_shape(tensor_name=\"image\", min_shape=[1, 3, 256, 256], opt_shape=[1, 3, 1024, 1024],\r\n                                   max_shape=[1, 3, 2048, 2048])\r\n……\r\n则会报错，且不输出结果\r\n\r\n\r\n由于deploy.yaml文件中没有图像尺寸大小：\r\nDeploy:\r\n  model: model.pdmodel\r\n  params: model.pdiparams\r\n  transforms:\r\n  - type: Normalize\r\n\r\n所以这种情况该如何设置？\r\n\r\n模型下载地址：\r\n链接: https://pan.baidu.com/s/19YfrbrO5CIWTCd5NzeFV_Q  密码: e8ps\r\n",
        "state": "closed",
        "user": "qiyuezhiguang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-10T02:08:47+00:00",
        "updated_at": "2024-03-12T06:39:18+00:00",
        "closed_at": "2024-03-12T06:39:18+00:00",
        "comments_count": [
            "qiyuezhiguang",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1577,
        "title": "cpu_thread_num does't work with docker image python:3.9.16-slim-buster",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\n\r\nroot@1cfb49fa663e:/home/aiBox# pip list\r\nPackage            Version\r\n------------------ ---------\r\nanyio              3.6.2\r\nasgiref            3.6.0\r\ncertifi            2022.12.7\r\ncharset-normalizer 3.1.0\r\nclick              8.1.3\r\nfaiss-cpu          1.7.3\r\nfastapi            0.93.0\r\nfastdeploy-python  1.0.4\r\nfastdeploy-tools   0.0.5\r\nh11                0.14.0\r\nidna               3.4\r\nnumpy              1.24.2\r\nopencv-python      4.7.0.72\r\nPillow             9.4.0\r\npip                22.0.4\r\npydantic           1.10.6\r\nPyYAML             6.0\r\nrequests           2.28.2\r\nschedule           1.1.0\r\nsetuptools         58.1.0\r\nsniffio            1.3.0\r\nstarlette          0.25.0\r\ntqdm               4.65.0\r\ntyping_extensions  4.5.0\r\nurllib3            1.26.14\r\nuvicorn            0.16.0\r\nwheel              0.38.4\r\n\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\n\r\nLinux 1cfb49fa663e 5.10.104-linuxkit #1 SMP Wed Mar 9 19:05:23 UTC 2022 x86_64 GNU/Linux\r\n\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nmacbook pro 2019 i9  8 core\r\nProgram Language: e.g. Python 3.8\r\n\r\nPython 3.9.16 (main, Mar  1 2023, 15:52:45)\r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import sys\r\n>>> print(sys.version_info)\r\nsys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\r\n>>> print(sys.version)\r\n3.9.16 (main, Mar  1 2023, 15:52:45)\r\n[GCC 8.3.0]\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\nfastdeploy infer always use 1 cpu , with max 100%,  the  option.set_cpu_thread_num() does't work .\r\nBut without docker, just run on mac aconda python 3.9 , it works!\r\n",
        "state": "closed",
        "user": "shao77622",
        "closed_by": "shao77622",
        "created_at": "2023-03-11T02:38:49+00:00",
        "updated_at": "2023-03-13T02:09:09+00:00",
        "closed_at": "2023-03-12T14:04:35+00:00",
        "comments_count": [
            "shao77622",
            "shao77622",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1572,
        "title": "编译C#api过程报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\ncudn11.2,cudnn8.4,windows10, c++,cmake gui 编译，vs2019生成了fastdeploy.dll,未生成fastdeploy csharp.dll, 成功生成13，失败1，但没报错\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "zhinangubei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-10T02:51:14+00:00",
        "updated_at": "2024-04-16T09:03:19+00:00",
        "closed_at": "2024-04-16T09:03:19+00:00",
        "comments_count": [
            "rainyfly",
            "zhinangubei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1578,
        "title": "yolov8 实例分割 可以部署检测不",
        "body": "yolov8 实例分割 可以部署检测不\r\n\r\n没发现有 yolov8 实例分割",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-11T07:56:38+00:00",
        "updated_at": "2024-03-19T06:40:04+00:00",
        "closed_at": "2024-03-19T06:40:03+00:00",
        "comments_count": [
            "jiangjiajun",
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1582,
        "title": "rv1126,rk3568,rk3588 等用例子都需要修改异构文件subgraph.txt",
        "body": "rv1126,rk3568,rk3588 等用例子都需要修改异构文件subgraph.txt但是对于小白来说这个很难快速验证请增加一些快速验证例如人脸识别这种不需要修改异构文件的例子",
        "state": "closed",
        "user": "liruiyan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-13T00:55:18+00:00",
        "updated_at": "2024-03-19T06:40:04+00:00",
        "closed_at": "2024-03-19T06:40:04+00:00",
        "comments_count": [
            "yeliang2258",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1583,
        "title": "opencv裁剪后的图,直接预测检测不到",
        "body": "python_opencv\r\n裁剪后的图,\r\n预测检测不到\r\n\r\n```\r\n# cv2\r\n# fd.vision.detection.YOLOv8\r\n\r\nimgcj = img[ymin:ymax, xmin:xmax]\r\n# 要加上转换颜色代码 才能检测到有东西  颜色也不正常了\r\nimgcj= cv2.cvtColor(np.asarray(imgcj), cv2.COLOR_BGR2RGB)\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-03-13T01:44:13+00:00",
        "updated_at": "2023-03-13T09:06:37+00:00",
        "closed_at": "2023-03-13T09:06:37+00:00",
        "comments_count": [
            "jiangjiajun",
            "ChaoII",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1584,
        "title": "3d facealign",
        "body": "请问FaceAlign的相关模型都只支持2d landmarks是吗？有没有3d landmarks的模型？",
        "state": "closed",
        "user": "yxdydgithub",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-13T03:11:20+00:00",
        "updated_at": "2024-03-19T06:40:05+00:00",
        "closed_at": "2024-03-19T06:40:05+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1586,
        "title": "rv1126 跑 ppyoloe 报错 ",
        "body": "https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rv1126/cpp\r\n报这个错\r\n undefined symbol: _ZTVN10fastdeploy6vision9detection21PaddleDetPreprocessorE\r\n\r\n",
        "state": "closed",
        "user": "liruiyan",
        "closed_by": "liruiyan",
        "created_at": "2023-03-13T06:10:50+00:00",
        "updated_at": "2023-03-13T07:16:27+00:00",
        "closed_at": "2023-03-13T07:16:27+00:00",
        "comments_count": [
            "liruiyan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1585,
        "title": "rv1126 跑了 几个例子 都很慢 ",
        "body": "是不是 哪里有问题 \r\nPaddleClas 分类模型在 RV1126 上的 C++ 部署示例    这个识别同样的例子图片 跑了10次 大约 2秒一个图片\r\nYOLOv5 检测模型在 RV1126 上的 C++ 部署示例   这个识别同样的例子图片 跑了10次 大约 8秒一个图片",
        "state": "closed",
        "user": "liruiyan",
        "closed_by": "liruiyan",
        "created_at": "2023-03-13T05:58:54+00:00",
        "updated_at": "2023-03-15T03:21:22+00:00",
        "closed_at": "2023-03-15T03:21:22+00:00",
        "comments_count": [
            "leiqing1",
            "liruiyan",
            "liruiyan",
            "liruiyan",
            "liruiyan",
            "jiangjiajun",
            "liruiyan",
            "liruiyan",
            "liruiyan",
            "liruiyan",
            "yeliang2258",
            "liruiyan",
            "liruiyan",
            "yeliang2258",
            "liruiyan",
            "liruiyan",
            "yeliang2258",
            "liruiyan",
            "yeliang2258",
            "liruiyan",
            "liruiyan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1599,
        "title": "UIE经过数据蒸馏后的模型，如何使用fastdeploy方式部署",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.4-cpu-only-21.10\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【编译语言】： python3.8\r\n因直接使用UIE模型在CPU上推理较慢，所以按照下面的文档进行了数据蒸馏，推理速度有很大提升。\r\n[UIE数据蒸馏](https://github.com/PaddlePaddle/PaddleNLP/tree/a13e8776d2032f8e1fbac0958a7d2109fe61b10c/applications/information_extraction/text/data_distill)\r\n我的问题是如何把数据蒸馏后的模型通过fastdeploy方式部署？在UIE的fastdeploy部署示例中，都有传递schema参数，而数据蒸馏后的模型不需要传递schema参数\r\n",
        "state": "closed",
        "user": "adatwfx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-14T02:09:37+00:00",
        "updated_at": "2024-04-16T09:03:21+00:00",
        "closed_at": "2024-04-16T09:03:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1588,
        "title": "基于飞腾S2500/64，昆仑xpuR200编译报错",
        "body": "根据官方示例(https://github.com/PaddlePaddle/FastDeploy/tree/develop/docs/cn/build_and_install)/kunlunxin.md执行报错如下：\r\n\r\nWill force to set ENABLE_LITE_BACKEND when build with KunlunXin.\r\nCMake Error at cmake/kunlunxin.cmake:7 (message):\r\n  KunlunXin XPU is only supported on Linux x64 platform\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:132 (include)\r\n\r\n显示fastdeploy只支持x64架构，想确定一下fastdeploy是不是不支持arm架构呢？\r\n",
        "state": "closed",
        "user": "HustleOoo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-13T07:07:16+00:00",
        "updated_at": "2024-04-16T09:03:20+00:00",
        "closed_at": "2024-04-16T09:03:20+00:00",
        "comments_count": [
            "DefTruth",
            "HustleOoo",
            "jiangjiajun",
            "HustleOoo",
            "108851027",
            "HustleOoo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1591,
        "title": "RK3588使用提供的rknn模型运行infer.py报错segmentation fault",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 2023年3月13日最新git https://github.com/PaddlePaddle/FastDeploy.git\r\n- 【编译命令】官方提供“编译FastDeploy Python SDK”方法\r\n- 【系统平台】: debian11.6\r\n- 【硬件】： rk3588 4G+8G\r\n- 【编译语言】： Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - RK3588 debian 4G+8G \r\n先安装了python SDK，再安装了板端推理rknpu2_install_rk3588.sh\r\n下载fast deploy提供的yolov5s_relu_tk2_RK3588_i8.rknn\r\n运行/example/vision/detection/rkyolo/python中的infer.py，命令python3 infer.py --model_file ./model/ --image 6770.jpg\r\n显示信息Segmentation fault，无其他任何提示信息\r\n",
        "state": "closed",
        "user": "Shifiter",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-13T09:41:55+00:00",
        "updated_at": "2023-03-13T12:07:29+00:00",
        "closed_at": "2023-03-13T12:07:29+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Shifiter",
            "Zheng-Bicheng",
            "Shifiter",
            "Shifiter",
            "Zheng-Bicheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1602,
        "title": "请问这个rv1126 的例子 用fastdeploy 哪个版本 ",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/rv1126/cpp/README_CN.md",
        "state": "closed",
        "user": "liruiyan",
        "closed_by": "liruiyan",
        "created_at": "2023-03-14T06:32:59+00:00",
        "updated_at": "2023-03-15T03:20:54+00:00",
        "closed_at": "2023-03-15T03:20:54+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1597,
        "title": "海康SDK & FastDeploy SDK 结合在Linux部署 数据类型冲突",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.3\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n部署遇到 枚举**数据类型**`BOOL`和其他第三库冲突`#define  BOOL  int` 怎么解决？冲突代码和日志如下：\r\n\r\n\r\n![48b0c47f9566ba5a3b639f8a53124f7](https://user-images.githubusercontent.com/68001817/224866432-db836fd7-9610-4f7f-8343-fd040e93df61.png)\r\n\r\n\r\n```c++\r\n// 海康威视SDK \r\n#if (defined(_WIN32)) //windows\r\n    #define NET_DVR_API  extern \"C\" __declspec(dllimport)\r\n    typedef  unsigned __int64   UINT64;\r\n    typedef  signed   __int64   INT64;\r\n#elif defined(__linux__) || defined(__APPLE__) //linux\r\n    #define  BOOL  int\r\n    typedef  unsigned int       DWORD;\r\n    typedef  unsigned short     WORD;\r\n    typedef  unsigned short     USHORT;\r\n    typedef  short              SHORT;\r\n    typedef  int                LONG;\r\n    typedef  unsigned char      BYTE;\r\n    typedef  unsigned int       UINT;\r\n    typedef  void*              LPVOID;\r\n    typedef  void*              HANDLE;\r\n    typedef  unsigned int*      LPDWORD; \r\n    typedef  unsigned long long UINT64;\r\n    typedef  signed long long   INT64;\r\n\r\n// fastdeploy 冲突代码\r\nenum FASTDEPLOY_DECL FDDataType {\r\n  BOOL,\r\n  INT16,\r\n  INT32,\r\n  INT64,\r\n  FP16,\r\n  FP32,\r\n  FP64,\r\n  UNKNOWN1,\r\n  UNKNOWN2,\r\n  UNKNOWN3,\r\n  UNKNOWN4,\r\n  UNKNOWN5,\r\n  UNKNOWN6,\r\n  UNKNOWN7,\r\n  UNKNOWN8,\r\n  UNKNOWN9,\r\n  UNKNOWN10,\r\n  UNKNOWN11,\r\n  UNKNOWN12,\r\n  UNKNOWN13,\r\n  UINT8,\r\n  INT8\r\n};\r\n```\r\n```bash\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/hk/include/HCNetSDK.h:35:20: 错误： expected identifier before ‘int’\r\n     #define  BOOL  int\r\n                    ^~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:27:3: 附注： in expansion of macro ‘BOOL’\r\n   BOOL,\r\n   ^~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/hk/include/HCNetSDK.h:35:20: 错误： expected ‘}’ before ‘int’\r\n     #define  BOOL  int\r\n                    ^~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:27:3: 附注： in expansion of macro ‘BOOL’\r\n   BOOL,\r\n   ^~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:53:48: 错误： ‘FDDataType’不是一个类型名\r\n                                          const FDDataType& fdt);\r\n\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:26:33: 附注： to match this ‘{’\r\n enum FASTDEPLOY_DECL FDDataType {\r\n                                 ^\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:27:7: 错误： expected unqualified-id before ‘,’ token\r\n   BOOL,\r\n       ^\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:28:3: 错误： ‘INT16’的声明指定了两个以上的数据类型\r\n   INT16,\r\n   ^~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:29:3: 错误： ‘INT32’的声明指定了两个以上的数据类型\r\n   INT32,\r\n   ^~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:30:3: 错误： ‘INT64’的声明指定了两个以上的数据类型\r\n   INT64,\r\n   ^~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:31:3: 错误： ‘FP16’的声明指定了两个以上的数据类型\r\n   FP16,\r\n   ^~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:32:3: 错误： ‘FP32’的声明指定了两个以上的数据类型\r\n   FP32,\r\n   ^~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:33:3: 错误： ‘FP64’的声明指定了两个以上的数据类型\r\n   FP64,\r\n   ^~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:34:3: 错误： ‘UNKNOWN1’的声明指定了两个以上的数据类型\r\n   UNKNOWN1,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:35:3: 错误： ‘UNKNOWN2’的声明指定了两个以上的数据类型\r\n   UNKNOWN2,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:36:3: 错误： ‘UNKNOWN3’的声明指定了两个以上的数据类型\r\n   UNKNOWN3,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:37:3: 错误： ‘UNKNOWN4’的声明指定了两个以上的数据类型\r\n   UNKNOWN4,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:38:3: 错误： ‘UNKNOWN5’的声明指定了两个以上的数据类型\r\n   UNKNOWN5,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:39:3: 错误： ‘UNKNOWN6’的声明指定了两个以上的数据类型\r\n   UNKNOWN6,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:40:3: 错误： ‘UNKNOWN7’的声明指定了两个以上的数据类型\r\n   UNKNOWN7,\r\n~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:41:3: 错误： ‘UNKNOWN8’的声明指定了两个以上的数据类型\r\n   UNKNOWN8,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:42:3: 错误： ‘UNKNOWN9’的声明指定了两个以上的数据类型\r\n   UNKNOWN9,\r\n   ^~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:43:3: 错误： ‘UNKNOWN10’的声明指定了两个以上的数据类型\r\n   UNKNOWN10,\r\n   ^~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:44:3: 错误： ‘UNKNOWN11’的声明指定了两个以上的数据类型\r\n   UNKNOWN11,\r\n   ^~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:45:3: 错误： ‘UNKNOWN12’的声明指定了两个以上的数据类型\r\n   UNKNOWN12,\r\n   ^~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:46:3: 错误： ‘UNKNOWN13’的声明指定了两个以上的数据类型\r\n   UNKNOWN13,\r\n   ^~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:47:3: 错误： ‘UINT8’的声明指定了两个以上的数据类型\r\n   UINT8,\r\n   ^~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:49:1: 错误： expected initializer before ‘}’ token\r\n };\r\n ^\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:55:39: 错误： ‘FDDataType’不是一个类型名\r\n FASTDEPLOY_DECL std::string Str(const FDDataType& fdt);\r\n                                       ^~~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:57:46: 错误： ‘FDDataType’不是一个类型名\r\n FASTDEPLOY_DECL int32_t FDDataTypeSize(const FDDataType& data_dtype);\r\n                                              ^~~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:57:46: 错误： ‘FDDataType’不是一个类型名\r\n FASTDEPLOY_DECL int32_t FDDataTypeSize(const FDDataType& data_dtype);\r\n                                              ^~~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:53:48: 错误： ‘FDDataType’不是一个类型名\r\n                                          const FDDataType& fdt);\r\n                                                ^~~~~~~~~~\r\n/home/ubuntu/workspace/mtd_vision_fastdeploy/base/fastdeploy-linux-x64-1.0.3/include/fastdeploy/core/fd_type.h:57:46: 错误： ‘FDDataType’不是一个类型名\r\n FASTDEPLOY_DECL int32_t FDDataTypeSize(const FDDataType& data_dtype);\r\n\r\n\r\n```\r\n\r\n",
        "state": "closed",
        "user": "chccc1994",
        "closed_by": "chccc1994",
        "created_at": "2023-03-14T01:09:53+00:00",
        "updated_at": "2023-03-22T06:06:40+00:00",
        "closed_at": "2023-03-22T06:06:40+00:00",
        "comments_count": [
            "jiangjiajun",
            "chccc1994",
            "jiangjiajun",
            "chccc1994"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1601,
        "title": "我可以在昇腾310上使用fastdeploy 部署uie吗？或者使用lite后端？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 昇腾310\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n我可以在昇腾310上使用fastdeploy 部署uie吗？或者使用lite后端？",
        "state": "closed",
        "user": "apexg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-14T02:41:18+00:00",
        "updated_at": "2024-04-16T09:03:22+00:00",
        "closed_at": "2024-04-16T09:03:22+00:00",
        "comments_count": [
            "jiangjiajun",
            "apexg",
            "jccg",
            "apexg"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1603,
        "title": "RK3568平台开启FlyCV优化加速相关问题？",
        "body": "利用FastDeploy实现RK3568平台（Firefly-AIO3568J开发板）的部署过程中，如何实现FlyCV的优化加速？\r\n参考https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/boost_cv_by_flycv.md 文档中流程，在编译过程中打开了FlyCV编译选项，但是推理部署后通过测试时间发现并没有真正用到FlyCV的优化加速，能否提供一个RK平台使用FlyCV的简单流程？\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-14T06:39:23+00:00",
        "updated_at": "2023-03-24T16:05:21+00:00",
        "closed_at": "2023-03-24T16:05:21+00:00",
        "comments_count": [
            "jiangjiajun",
            "MrMzl",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1604,
        "title": "最新的 fastdeploy 使用下面的例子 跑的很慢 ",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/rv1126/cpp/README_CN.md\r\n\r\n是因为 程序没有 在npu 中启动 \r\n以下 代码 不去作用 请问是什么情况 \r\n\r\n  fastdeploy::RuntimeOption option;\r\n  option.UseTimVX();\r\n\r\n其他 启动 npu的 程序  Used :  不为零 下面的 信息 \r\nVIDEO MEMORY:\r\n  POOL SYSTEM:\r\n    Free :       4180566 B\r\n    Used :         13738 B\r\n    MinFree :     597654 B\r\n    MaxUsed :    3596650 B\r\n    Total :      4194304 B\r\n  POOL VIRTUAL:\r\n    Used :             0 B\r\n    MaxUsed :          0 B\r\n\r\n",
        "state": "closed",
        "user": "liruiyan",
        "closed_by": "liruiyan",
        "created_at": "2023-03-14T06:56:18+00:00",
        "updated_at": "2023-03-15T03:20:40+00:00",
        "closed_at": "2023-03-15T03:20:40+00:00",
        "comments_count": [
            "liruiyan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1606,
        "title": "自己训练的yolov5模型，检测segmentation fault问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 2023年3月13日最新git https://github.com/PaddlePaddle/FastDeploy.git\r\n- 【编译命令】官方提供“编译FastDeploy Python SDK”方法\r\n- 【系统平台】: debian11.6\r\n- 【硬件】： rk3588 4G+8G\r\n- 【编译语言】： Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n利用airockship提供的yolov5训练（https://github.com/airockchip/yolov5），利用其中的python export.py --rknpu RK3588转换得到onnx，利用（https://github.com/rockchip-linux/rknn-toolkit2/tree/master/examples/onnx/yolov5）中的test.py将onnx转换为rknn，成功转换且输出了检测结果，运行/example/vision/detection/rkyolo/python中的infer.py，命令python3 infer.py --model_file ./model/best.rknn --image 6770.jpg，输出如下：\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion\trknn_api/rknnrt version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39), driver version: 0.7.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output, n_dims=4, dims=[1, 36, 80, 80], n_elems=230400, size=230400, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003919, pass_through=0\r\nindex=1, name=272, n_dims=4, dims=[1, 36, 40, 40], n_elems=57600, size=57600, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003910, pass_through=0\r\nindex=2, name=274, n_dims=4, dims=[1, 36, 20, 20], n_elems=14400, size=14400, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(334)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(326)::Infer\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\nSegmentation fault\r\n",
        "state": "closed",
        "user": "Shifiter",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-14T08:44:12+00:00",
        "updated_at": "2023-03-20T07:50:22+00:00",
        "closed_at": "2023-03-20T07:50:22+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Shifiter"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1610,
        "title": "yolo字体问题",
        "body": "yolo检测框的标签字体看不清，接口里面职能调整大小。\r\n有地方能调整粗细，颜色之类的吗？",
        "state": "closed",
        "user": "beckhz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-14T10:01:51+00:00",
        "updated_at": "2024-03-19T06:40:07+00:00",
        "closed_at": "2024-03-19T06:40:07+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1611,
        "title": "自训练RKYOLOv5模型在RK3568平台部署过程中出现的目标检测结果标注框不显示问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-develop版本\r\n- 【系统平台】: Linux firefly 4.19.232 / Debian 10\r\n- 【硬件】： firefly AIO-3568J\r\n- 【编译语言】：C++ & Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\nc++部署正常运行后，能够在命令窗口打印推理结果，但是保存的结果图上无目标检测框，另外通过窗口输出的结果进行分析，输出结果个数与真实推理图中的目标数量不一致。命令窗口打印出来的推理结果中目标框很多，但实际所用的推理图中目标只有10个左右.\r\n- 命令窗口输出内容\r\nfirefly@firefly:~/FastDeploy_RK3568_FLYCV/FastDeploy-develop/examples/vision/detection/rkyolo/cpp/build/install$ ./infer_rkyolo model/yolov5s_3568_rgb_car_RK356X_i8.rknn images/rgb_car/0003.jpg \r\n[INFO] fastdeploy/vision/common/processors/base.cc(97)::EnableFlyCV\tWill change to use image processing library ProcLib::FLYCV\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion\trknn_api/rknnrt version: 1.4.0 (a10f100eb@2022-09-09T09:07:14), driver version: 0.7.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output, n_dims=4, dims=[1, 18, 80, 80], n_elems=115200, size=115200, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=1, name=272, n_dims=4, dims=[1, 18, 40, 40], n_elems=28800, size=28800, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=2, name=274, n_dims=4, dims=[1, 18, 20, 20], n_elems=7200, size=7200, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(334)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(326)::Infer\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[FastDeploy] RKYOLOV5 in RKNN duration = 1.70816s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n85.847931,585.966614, 118.152061, 614.033386, 0.999997, 1\r\n110.775131,585.224854, 141.224869, 614.775146, 0.999997, 1\r\n124.547310,581.789978, 151.452682, 618.210022, 0.999997, 1\r\n136.547302,582.614502, 163.452682, 617.385498, 0.999997, 1\r\n173.392273,576.441895, 198.607697, 623.558044, 0.999997, 1\r\n149.392273,581.789978, 174.607712, 618.210022, 0.999997, 1\r\n71.911331,560.494507, 108.088669, 615.505432, 0.999997, 1\r\n99.674927,565.380981, 128.325073, 610.618896, 0.999997, 1\r\n159.674927,570.614502, 188.325073, 605.385498, 0.999997, 1\r\n184.547302,569.789978, 211.452682, 606.210022, 0.999997, 1\r\n197.392288,571.684082, 222.607727, 604.315857, 0.999997, 1\r\n210.209854,571.684082, 233.790131, 604.315857, 0.999997, 1\r\n221.392288,572.464050, 246.607727, 603.535950, 0.999997, 1\r\n233.392288,572.464050, 258.607727, 603.535950, 0.999997, 1\r\n245.392288,572.464050, 270.607727, 603.535950, 0.999997, 1\r\n256.547302,571.684082, 283.452667, 604.315857, 0.999997, 1\r\n267.674927,569.789978, 296.325073, 606.210022, 0.999997, 1\r\n276.893341,567.201843, 311.106659, 608.798035, 0.999997, 1\r\n302.775116,571.684082, 333.224854, 604.315857, 0.999997, 1\r\n314.775116,573.966614, 345.224854, 602.033386, 0.999997, 1\r\n327.674927,575.392700, 356.325073, 600.607239, 0.999997, 1\r\n342.209839,577.388733, 365.790131, 598.611328, 0.999997, 1\r\n352.547302,574.689209, 379.452667, 601.310730, 0.999997, 1\r\n364.547302,577.388733, 391.452698, 598.611328, 0.999997, 1\r\n378.209839,573.966614, 401.790161, 602.033386, 0.999997, 1\r\n390.209839,571.684082, 413.790100, 604.315857, 0.999997, 1\r\n403.000031,576.742493, 424.999969, 599.257507, 0.999997, 1\r\n415.762787,577.388733, 436.237183, 598.611328, 0.999997, 1\r\n427.000031,574.689209, 448.999969, 601.310730, 0.999997, 1\r\n438.209839,573.224854, 461.790100, 602.775146, 0.999997, 1\r\n449.392273,571.684082, 474.607666, 604.315857, 0.999997, 1\r\n458.775146,568.083679, 489.224884, 607.916260, 0.999997, 1\r\n472.547272,562.506409, 499.452667, 613.493591, 0.999997, 1\r\n482.775146,570.614502, 513.224854, 605.385498, 0.999997, 1\r\n496.547272,568.083679, 523.452637, 607.916260, 0.999997, 1\r\n163.000031,580.083679, 184.999969, 619.916260, 0.999997, 1\r\n504.893311,573.224854, 539.106628, 602.775146, 0.999997, 1\r\n513.865112,568.946411, 554.134888, 607.053589, 0.999997, 1\r\n538.901917,568.083679, 577.098083, 607.916260, 0.999997, 1\r\n552.893311,571.684082, 587.106628, 604.315857, 0.999997, 1\r\n566.775146,576.742493, 597.224854, 599.257507, 0.999997, 1\r\n576.893311,579.212769, 611.106628, 596.787109, 0.999997, 1\r\n587.911316,576.742493, 624.088623, 599.257507, 0.999997, 1\r\n601.847900,574.689209, 634.152039, 601.310730, 0.999997, 1\r\n246.209854,588.742493, 269.790131, 611.257507, 0.999997, 1\r\n615.674927,571.684082, 644.325073, 604.315857, 0.999997, 1\r\n627.674927,573.224854, 656.325073, 602.775146, 0.999997, 1\r\n639.674927,574.689209, 668.325073, 601.310730, 0.999997, 1\r\n650.775146,574.689209, 681.224854, 601.310730, 0.999997, 1\r\n664.547241,573.966614, 691.452637, 602.033386, 0.999997, 1\r\n677.392273,571.684082, 702.607666, 604.315857, 0.999997, 1\r\n689.392273,571.684082, 714.607666, 604.315857, 0.999997, 1\r\n701.392273,573.224854, 726.607666, 602.775146, 0.999997, 1\r\n713.392273,573.224854, 738.607666, 602.775146, 0.999997, 1\r\n723.674927,571.684082, 752.325073, 604.315857, 0.999997, 1\r\n736.547241,571.684082, 763.452637, 604.315857, 0.999997, 1\r\n747.674927,569.789978, 776.325073, 606.210022, 0.999997, 1\r\n757.847900,572.464050, 790.152100, 603.535950, 0.999997, 1\r\n768.893372,576.742493, 803.106689, 599.257507, 0.999997, 1\r\n794.775146,573.966614, 825.224854, 602.033386, 0.999997, 1\r\n807.674927,570.614502, 836.325073, 605.385498, 0.999997, 1\r\n258.209839,585.966614, 281.790131, 614.033386, 0.999997, 1\r\n304.547302,593.708557, 331.452667, 606.291382, 0.999997, 1\r\n269.392273,588.742493, 294.607727, 611.257507, 0.999997, 1\r\n821.392273,568.946411, 846.607727, 607.053589, 0.999997, 1\r\n222.209854,587.392700, 245.790131, 612.607239, 0.999997, 1\r\n832.547302,568.946411, 859.452759, 607.053589, 0.999997, 1\r\n842.775146,568.946411, 873.224854, 607.053589, 0.999997, 1\r\n855.674927,570.614502, 884.325073, 605.385498, 0.999997, 1\r\n869.392273,570.614502, 894.607727, 605.385498, 0.999997, 1\r\n882.209839,573.224854, 905.790161, 602.775146, 0.999997, 1\r\n895.762756,568.083679, 916.237122, 607.916260, 0.999997, 1\r\n908.256042,570.614502, 927.743835, 605.385498, 0.999997, 1\r\n919.000000,570.614502, 940.999939, 605.385498, 0.999997, 1\r\n930.209839,570.614502, 953.790161, 605.385498, 0.999997, 1\r\n280.547302,591.782593, 307.452667, 608.217285, 0.999997, 1\r\n292.547302,594.189209, 319.452667, 605.810669, 0.999997, 1\r\n0.000000,578.300964, 10.152063, 621.698975, 0.999997, 1\r\n0.000000,574.506409, 23.106661, 625.493591, 0.999997, 1\r\n0.000000,580.946411, 37.098072, 619.053589, 0.999997, 1\r\n30.209862,579.201843, 53.790138, 620.798035, 0.999997, 1\r\n36.893337,582.614502, 71.106659, 617.385498, 0.999997, 1\r\n45.865112,589.388733, 86.134880, 610.611328, 0.999997, 1\r\n62.775135,631.684082, 93.224869, 664.315857, 0.999997, 1\r\n50.775135,633.966614, 81.224869, 662.033386, 0.999997, 1\r\n24.893337,635.392700, 59.106663, 660.607239, 0.999997, 1\r\n10.901925,632.464050, 49.098072, 663.535950, 0.999997, 1\r\n0.000000,624.441895, 37.098072, 671.558044, 0.999997, 1\r\n0.000000,623.483704, 10.152063, 672.516235, 0.999997, 1\r\n925.847900,616.083679, 958.151978, 655.916260, 0.999997, 1\r\n901.847900,620.464050, 934.151978, 651.535950, 0.999997, 1\r\n887.911377,622.689209, 924.088684, 649.310730, 0.999997, 1\r\n863.911377,630.189209, 900.088684, 641.810669, 0.999997, 1\r\n851.911377,635.616943, 888.088684, 636.383057, 0.999997, 1\r\n827.911377,629.208801, 864.088684, 642.791199, 0.999997, 1\r\n817.847900,623.392700, 850.151978, 648.607239, 0.999997, 1\r\n806.775146,621.224854, 837.224854, 650.775146, 0.999997, 1\r\n796.547302,617.789978, 823.452759, 654.210022, 0.999997, 1\r\n784.547302,621.224854, 811.452759, 650.775146, 0.999997, 1\r\n771.674927,623.392700, 800.325073, 648.607239, 0.999997, 1\r\n760.547241,624.742493, 787.452637, 647.257507, 0.999997, 1\r\n748.547241,624.742493, 775.452637, 647.257507, 0.999997, 1\r\n737.392273,629.208801, 762.607666, 642.791199, 0.999997, 1\r\n725.392273,630.189209, 750.607666, 641.810669, 0.999997, 1\r\n711.674927,628.333374, 740.325073, 643.666626, 0.999997, 1\r\n701.392273,622.689209, 726.607666, 649.310730, 0.999997, 1\r\n689.392273,621.224854, 714.607666, 650.775146, 0.999997, 1\r\n676.547241,618.614502, 703.452637, 653.385498, 0.999997, 1\r\n661.847900,617.789978, 694.152039, 654.210022, 0.999997, 1\r\n649.847900,615.201843, 682.152039, 656.798035, 0.999997, 1\r\n637.847900,616.946411, 670.152039, 655.053589, 0.999997, 1\r\n608.800842,620.464050, 651.199036, 651.535950, 0.999997, 1\r\n586.901917,623.392700, 625.098083, 648.607239, 0.999997, 1\r\n578.775146,620.464050, 609.224854, 651.535950, 0.999997, 1\r\n567.674927,620.464050, 596.325073, 651.535950, 0.999997, 1\r\n553.847900,625.388733, 586.152039, 646.611328, 0.999997, 1\r\n542.775146,624.742493, 573.224854, 647.257507, 0.999997, 1\r\n531.674927,622.689209, 560.325073, 649.310730, 0.999997, 1\r\n518.775146,621.224854, 549.224854, 650.775146, 0.999997, 1\r\n506.775146,624.742493, 537.224854, 647.257507, 0.999997, 1\r\n492.893311,614.300964, 527.106628, 657.698975, 0.999997, 1\r\n481.847931,617.789978, 514.152039, 654.210022, 0.999997, 1\r\n471.674927,616.083679, 500.325073, 655.916260, 0.999997, 1\r\n460.547272,616.946411, 487.452667, 655.053589, 0.999997, 1\r\n447.674927,616.946411, 476.325073, 655.053589, 0.999997, 1\r\n435.674927,616.946411, 464.325073, 655.053589, 0.999997, 1\r\n423.674927,616.083679, 452.325073, 655.916260, 0.999997, 1\r\n410.775146,614.300964, 441.224884, 657.698975, 0.999997, 1\r\n398.775146,619.684082, 429.224884, 652.315857, 0.999997, 1\r\n384.893311,615.201843, 419.106628, 656.798035, 0.999997, 1\r\n359.911316,613.380981, 396.088654, 658.618896, 0.999997, 1\r\n338.775116,620.464050, 369.224854, 651.535950, 0.999997, 1\r\n326.775116,616.083679, 357.224854, 655.916260, 0.999997, 1\r\n315.674927,615.201843, 344.325073, 656.798035, 0.999997, 1\r\n303.674927,615.201843, 332.325073, 656.798035, 0.999997, 1\r\n292.547302,615.201843, 319.452667, 656.798035, 0.999997, 1\r\n280.547302,614.300964, 307.452667, 657.698975, 0.999997, 1\r\n268.547302,616.946411, 295.452667, 655.053589, 0.999997, 1\r\n256.547302,626.623901, 283.452667, 645.376099, 0.999997, 1\r\n243.674927,633.851257, 272.325073, 638.148804, 0.999997, 1\r\n230.775116,635.693298, 261.224854, 636.306580, 0.999997, 1\r\n219.674927,635.575562, 248.325073, 636.424438, 0.999997, 1\r\n210.209854,627.782593, 233.790131, 644.217285, 0.999997, 1\r\n197.392288,621.966614, 222.607727, 650.033386, 0.999997, 1\r\n184.547302,625.388733, 211.452682, 646.611328, 0.999997, 1\r\n171.674927,621.966614, 200.325073, 650.033386, 0.999997, 1\r\n158.775131,614.300964, 189.224869, 657.698975, 0.999997, 1\r\n140.800888,600.463867, 183.199097, 671.536072, 0.999997, 1\r\n128.800888,572.305847, 171.199097, 699.694153, 0.999997, 1\r\n109.847931,612.441895, 142.152054, 659.558044, 0.999997, 1\r\n100.547310,617.789978, 127.452690, 654.210022, 0.999997, 1\r\n89.392281,617.789978, 114.607712, 654.210022, 0.999997, 1\r\n76.547310,619.684082, 103.452690, 652.315857, 0.999997, 1\r\n52.547310,618.614502, 79.452690, 653.385498, 0.999997, 1\r\n39.674919,624.077148, 68.325073, 647.922791, 0.999997, 1\r\n26.775131,633.851257, 57.224865, 638.148804, 0.999997, 1\r\n12.893338,624.077148, 47.106663, 647.922791, 0.999997, 1\r\n0.000000,619.684082, 21.224867, 652.315857, 0.999997, 1\r\n915.674927,606.614502, 944.325073, 641.385498, 0.999997, 1\r\n903.674927,607.684082, 932.325073, 640.315857, 0.999997, 1\r\n891.674927,605.789978, 920.325073, 642.210022, 0.999997, 1\r\n877.847900,598.506409, 910.151978, 649.493591, 0.999997, 1\r\n863.911377,586.495483, 900.088684, 661.504456, 0.999997, 1\r\n851.911377,557.679565, 888.088684, 690.320435, 0.999997, 1\r\n830.775146,588.463867, 861.224854, 659.536072, 0.999997, 1\r\n759.674927,606.614502, 788.325073, 641.385498, 0.999997, 1\r\n748.547241,610.689209, 775.452637, 637.310730, 0.999997, 1\r\n736.547241,615.212769, 763.452637, 632.787109, 0.999997, 1\r\n721.847900,600.441895, 754.152039, 647.558044, 0.999997, 1\r\n698.775146,602.300964, 729.224854, 645.698975, 0.999997, 1\r\n687.674927,604.946411, 716.325073, 643.053589, 0.999997, 1\r\n679.000000,612.742493, 700.999939, 635.257507, 0.999997, 1\r\n664.547241,622.980286, 691.452637, 625.019775, 0.999997, 1\r\n651.674927,623.486389, 680.325073, 624.513489, 0.999997, 1\r\n641.392273,614.623901, 666.607666, 633.376099, 0.999997, 1\r\n626.775146,607.684082, 657.224854, 640.315857, 0.999997, 1\r\n602.775146,606.614502, 633.224854, 641.385498, 0.999997, 1\r\n592.547241,604.946411, 619.452637, 643.053589, 0.999997, 1\r\n581.392273,607.684082, 606.607666, 640.315857, 0.999997, 1\r\n557.392273,604.946411, 582.607666, 643.053589, 0.999997, 1\r\n545.392273,604.946411, 570.607666, 643.053589, 0.999997, 1\r\n533.392273,605.789978, 558.607666, 642.210022, 0.999997, 1\r\n521.392273,618.499146, 546.607666, 629.500916, 0.999997, 1\r\n508.547272,621.945618, 535.452637, 626.054321, 0.999997, 1\r\n400.547272,607.684082, 427.452667, 640.315857, 0.999997, 1\r\n388.547272,605.789978, 415.452667, 642.210022, 0.999997, 1\r\n377.392273,597.510010, 402.607727, 650.489929, 0.999997, 1\r\n353.392273,604.946411, 378.607727, 643.053589, 0.999997, 1\r\n341.392273,605.789978, 366.607727, 642.210022, 0.999997, 1\r\n256.547302,616.333374, 283.452667, 631.666626, 0.999997, 1\r\n244.547302,615.782593, 271.452667, 632.217285, 0.999997, 1\r\n232.547302,616.864990, 259.452667, 631.135071, 0.999997, 1\r\n222.209854,584.474121, 245.790131, 663.525940, 0.999997, 1\r\n209.392288,604.946411, 234.607727, 643.053589, 0.999997, 1\r\n198.209854,618.189209, 221.790131, 629.810669, 0.999997, 1\r\n185.392273,615.782593, 210.607697, 632.217285, 0.999997, 1\r\n173.392273,615.782593, 198.607697, 632.217285, 0.999997, 1\r\n161.392273,613.388733, 186.607712, 634.611328, 0.999997, 1\r\n149.392273,605.789978, 174.607712, 642.210022, 0.999997, 1\r\n136.547302,606.614502, 163.452682, 641.385498, 0.999997, 1\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-14T10:11:24+00:00",
        "updated_at": "2023-03-28T09:05:51+00:00",
        "closed_at": "2023-03-28T09:05:51+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MrMzl"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1613,
        "title": "在华为昇腾上部署ocr，识别模型的批次数期待设置6，如何编译代码？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "apexg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-14T11:16:01+00:00",
        "updated_at": "2024-03-19T06:40:08+00:00",
        "closed_at": "2024-03-19T06:40:08+00:00",
        "comments_count": [
            "yunyaoXYY"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1617,
        "title": "使用C API编译PPYOLOE example make失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n在make -j example中的infer_ppyoloe.c文件时，出现了FD_C_CreateDetectionResult和FD_C_ModelFormat_PADDLE无法识别的问题。\r\n引入了fastdeploy_capi/runtime/enum_variables.h之后，FD_C_ModelFormat_PADDLE可以识别，这个example是否存在bug？\r\n\r\n![图片](https://user-images.githubusercontent.com/73475321/225190378-f6068484-ae65-4711-8a6d-3ef969c38940.png)\r\n\r\n",
        "state": "closed",
        "user": "wanziyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T02:38:43+00:00",
        "updated_at": "2024-04-16T09:03:24+00:00",
        "closed_at": "2024-04-16T09:03:24+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1609,
        "title": "c版本的yolov5文档没有编译demo的步骤",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/c/README_CN.md",
        "state": "closed",
        "user": "wangxu372848892",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-14T09:36:36+00:00",
        "updated_at": "2024-03-19T06:40:06+00:00",
        "closed_at": "2024-03-19T06:40:06+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1616,
        "title": "HTTP推理服务调用失败：MemoryError: std::bad_alloc  & Stub process is not healthy ",
        "body": "- 【FastDeploy版本】：1.0.4-cpu-only-21.10\r\n- 【系统平台】: Windows x64(Windows10) \r\n模型服务正常启动\r\n通过GTE  localhost:8000/v2/models/uie/versions/1  ，返回信息正常，如下：\r\n```\r\n{\r\n    \"name\": \"uie\",\r\n    \"versions\": [\r\n        \"1\"\r\n    ],\r\n    \"platform\": \"python\",\r\n    \"inputs\": [\r\n        {\r\n            \"name\": \"INPUT_0\",\r\n            \"datatype\": \"BYTES\",\r\n            \"shape\": [\r\n                -1,\r\n                1\r\n            ]\r\n        },\r\n        {\r\n            \"name\": \"INPUT_1\",\r\n            \"datatype\": \"BYTES\",\r\n            \"shape\": [\r\n                -1,\r\n                1\r\n            ]\r\n        }\r\n    ],\r\n    \"outputs\": [\r\n        {\r\n            \"name\": \"OUTPUT_0\",\r\n            \"datatype\": \"BYTES\",\r\n            \"shape\": [\r\n                -1,\r\n                1\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n接着调用推理服务时，POST localhost:8000/v2/models/uie/versions/1/infer  ，在body中传入JSON格式参数，响应信息如下：\r\n`{\"error\":\"Failed to process the request(s) for model instance 'uie_0', message: MemoryError: std::bad_alloc\\n\\nAt:\\n  /usr/local/lib/python3.8/dist-packages/fastdeploy/text/uie/__init__.py(84): predict\\n  /uie_serving/models/uie/1/model.py(142): execute\\n\"}`\r\n\r\n在docker控制台窗口中，日志信息如下：\r\n```\r\n0315 01:40:42.846612 202 pb_stub.cc:402] Failed to process the request(s) for model 'uie_0', message: MemoryError: std::bad_alloc\r\nAt:\r\n  /usr/local/lib/python3.8/dist-packages/fastdeploy/text/uie/__init__.py(84): predict\r\n  /uie_serving/models/uie/1/model.py(142): execute\r\n```\r\n\r\n继续调用时，又出现了不同的响应信息：\r\n`{\"error\":\"Failed to process the request(s) for model instance 'uie_0', message: Stub process is not healthy.\"}`\r\n对应的控制台信息如下：\r\n```\r\nE0315 01:42:58.534205 134 python.cc:1942] Stub process is unhealthy and it will be restarted.\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'uie', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'INPUT_1', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'execution_accelerators': {'gpu_execution_accelerator': [], 'cpu_execution_accelerator': [{'name': 'paddle', 'parameters': {'cpu_threads': '2'}}]}, 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'uie_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\ninput: ['INPUT_0', 'INPUT_1']\r\noutput: ['OUTPUT_0']\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0315 09:42:59.618898   219 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\n```",
        "state": "closed",
        "user": "adatwfx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T01:44:59+00:00",
        "updated_at": "2024-04-16T09:03:23+00:00",
        "closed_at": "2024-04-16T09:03:23+00:00",
        "comments_count": [
            "heliqi",
            "adatwfx",
            "heliqi",
            "adatwfx",
            "heliqi",
            "adatwfx",
            "adatwfx",
            "heliqi",
            "heliqi",
            "heliqi",
            "adatwfx",
            "adatwfx",
            "adatwfx",
            "joey12300",
            "adatwfx"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1618,
        "title": "Meet unsupported output dtype for Cast ",
        "body": "I converted pp_lite_seg_stdc1 and pp_lite_seg_stdc2 model to rknn (with target platform set to rk3588s) using rknn-toolkit2 on linux x86_64 PC, the conversion process is fine and the result rknn model can infer without error. But as I transfer the rknn model to rk3588s board, below error occured:\r\n```\r\n--> Load RKNN model\r\nload_rknn done\r\n--> Init runtime environment\r\nI RKNN: [14:47:19.457] RKNN Runtime Information: librknnrt version: 1.4.0 (a10f100eb@2022-09-09T09:07:14)\r\nI RKNN: [14:47:19.457] RKNN Driver Information: version: 0.8.2\r\nI RKNN: [14:47:19.457] RKNN Model Information: version: 1, toolkit version: 1.4.0-22dcfef4(compiler version: 1.4.0 (3b4520e4f@2022-09-05T12:50:09)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW\r\nInit runtime done\r\n--> Running model\r\nE RKNN: [14:47:19.564] Meet unsupported output dtype for all Cast nodes\r\nAborted (core dumped)\r\n```\r\n1, Tried using both py3.7 and py3.9 rknn_toolkit_lite2 environment\r\n2, Tried to change onnx cast datatype\r\nError persists.\r\n\r\n我在linux x86_64电脑上用rknn-toolkit2把pp_lite_seg_stdc1和pp_lite_seg_stdc2模型转换为rknn（目标平台设置为rk3588s），但当我把rknn模型转移到rk3588s板上时，发生了以下错误。\r\n```\r\n同上\r\n```\r\n\r\n1, 尝试了使用py3.7和py3.9 rknn_toolkit_lite2环境\r\n2, 尝试了改变onnx cast节点的数据类型\r\n错误持续存在。\r\n",
        "state": "closed",
        "user": "PinkPanther-ny",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T03:20:55+00:00",
        "updated_at": "2024-04-12T08:18:53+00:00",
        "closed_at": "2024-04-12T08:18:53+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "jiangq195",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1620,
        "title": "win 7 下运行fastdeploy的onnx推理缺失api-ms-win-core-libraryloader-l1-2-0.dll",
        "body": "【FastDeploy版本】：1.0.3-cpu-only-21.10\r\n【系统平台】: Windows x64(Windows7)\r\n运行vision_detection_paddledetection_infer_ppyoloe.exe会提示缺失api-ms-win-core-libraryloader-l1-2-0.dll。\r\n![CC729867-2488-4f75-9C7E-F45582FE2A40](https://user-images.githubusercontent.com/34128317/225225394-8dfb49ef-58d7-4cc5-b243-f84c1b9283eb.png)。\r\n从网上下载了缺失的依赖库后，最终在加入api-ms-win-core-heap-l2-1-0.dll时提示无法找到入口；测试了多个不同版本的api-ms-win-core-heap-l2-1-0.dll，都报了这个错误，可能是二进制版本不兼容。\r\n![2070730F-D35C-402d-BD13-C97A0DD9EB77](https://user-images.githubusercontent.com/34128317/225226253-9e8c08a1-96b9-4eb1-88d7-8a1fcc0ae98e.png)\r\n请问在win7下如果想要运行onnx runtime推理有什么方法？\r\n\r\n",
        "state": "closed",
        "user": "SchrodingerLLX",
        "closed_by": "SchrodingerLLX",
        "created_at": "2023-03-15T06:39:17+00:00",
        "updated_at": "2023-03-21T07:59:08+00:00",
        "closed_at": "2023-03-21T07:59:08+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1621,
        "title": "华为昇腾+鲲鹏环境，部署ocr，推理一张发票10几秒太慢了",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：如fastdeploy-linux-develop\r\n- 【编译命令】自行编译，按照昇腾手册，构建了docker，然后使用如下命令编译：\r\n-                           export WITH_ASCEND=ON\r\n                            export ENABLE_VISION=ON\r\n                            export ENABLE_PADDLE_BACKEND=OFF\r\n                            export ENABLE_ORT_BACKEND=ON\r\n                            export ENABLE_LITE_BACKEND=ON\r\n                            export ENABLE_TEXT=ON\r\n                            \r\n                            python setup.py build\r\n                            python setup.py bdist_wheel\r\n- 【系统平台】: Linux arch64(Ubuntu 18.04) \r\n- 【硬件】： 昇腾310\r\n- 【编译语言】： Python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型正常跑，但有性能问题，推理一张发票大概需要10秒以上】\r\n- - 先执行`examples`下的部署示例，FastDeploy/examples/vision/ocr/PP-OCRv2/python/infer_static_shape.py，修改了部分代码：\r\n- 在107行后面增加：目的是检测第二次推理耗时\r\n- \r\n                                              #预测并打印结果\r\n                                              result = ppocr_v2.predict(im)\r\n                                              \r\n                                              print(result)\r\n                                              \r\n                                              #如下是自己增加的代码：目的是检测第二次推理耗时：\r\n                                              import time\r\n                                              total_time = 0\r\n                                              \r\n                                              _st = time.time()\r\n                                              # 预测图片准备\r\n                                              im = cv2.imread('微信图片_20230314114908.png')\r\n                                              result = ppocr_v2.predict(im)\r\n                                              elapse = time.time() - _st\r\n                                              total_time += elapse\r\n                                              print('总时间：'+str(total_time))\r\n                                              # 可视化结果\r\n                                              vis_im = fd.vision.vis_ppocr(im, result)\r\n                                              cv2.imwrite(\"visualized_result.jpg\", vis_im)\r\n没有使用cls模型，det模型用3.0：\r\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\r\ntar xvf ch_PP-OCRv3_det_infer.tar\r\nrec模型用server 2.0\r\n如下是调用命令：\r\npython3 infer_static_shape.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_ppocr_server_v2.0_rec_infer --rec_label_file ppocr_keys_v1.txt --image test.png --device ascend\r\n\r\n推理图片：\r\n![微信图片_20230314114908](https://user-images.githubusercontent.com/6739219/225229437-d57ee326-14ae-4282-8498-68308932bff1.png)\r\n![test](https://user-images.githubusercontent.com/6739219/225229454-c882085e-abc4-4b17-a61b-c11d80aeafe0.png)\r\n运行日志：\r\n[log.log](https://github.com/PaddlePaddle/FastDeploy/files/10976788/log.log)",
        "state": "closed",
        "user": "apexg",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T06:53:11+00:00",
        "updated_at": "2024-10-19T01:05:26+00:00",
        "closed_at": "2024-03-26T06:39:47+00:00",
        "comments_count": [
            "yunyaoXYY",
            "apexg",
            "minboo",
            "Serenagirl"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1623,
        "title": "关于Serving加载openvino IR模型的问题",
        "body": "请问fastdeploy里面的Triton服务器怎么使用量化后的openvino IR格式模型呢？\r\n我放到模型目录里面加载会提示没有model文件。\r\n但我去看Triton的文档，里面提到是可以直接支持加载IR格式模型的\r\n<img width=\"437\" alt=\"1678866794017\" src=\"https://user-images.githubusercontent.com/18228342/225243001-f6a06fb8-39a2-4494-ad71-2cc5a26f7986.png\">\r\n\r\n",
        "state": "closed",
        "user": "flamebox",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T07:55:38+00:00",
        "updated_at": "2024-04-16T09:03:25+00:00",
        "closed_at": "2024-04-16T09:03:25+00:00",
        "comments_count": [
            "jiangjiajun",
            "flamebox"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1625,
        "title": "FastDeploy是否支持在Atlas 200 DK开发者套件上进行模型部署(工作方式为RC模式)？",
        "body": "测试平台:全爱科技QA200RC Atlas 200 DK\r\n工作方式为RC模式，非fastdeploy官方提供的搭配鲲鹏920CPU，EP模式下的晟腾部署",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T08:15:21+00:00",
        "updated_at": "2024-04-16T09:03:25+00:00",
        "closed_at": "2024-04-16T09:03:25+00:00",
        "comments_count": [
            "yunyaoXYY",
            "MrMzl"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1633,
        "title": "在jetson orin中怎么安装fastdeploy gpu版本",
        "body": "使用命令行装了之后，无法import fastdeploy",
        "state": "closed",
        "user": "kankanjiuzou123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-16T05:14:56+00:00",
        "updated_at": "2024-04-16T09:03:26+00:00",
        "closed_at": "2024-04-16T09:03:26+00:00",
        "comments_count": [
            "jiangjiajun",
            "kankanjiuzou123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1631,
        "title": "Windows11，docker环境部署UIE模型运行失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy1.0.4-cpu-only-21.10\r\n- 【系统平台】: Windows x64(Windows11) docker\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- - 在cpu环境下部署yolov5成功，但部署`uie`模型时，完全按照[文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/text/uie/serving/README_CN.md)操作，运行服务报错\r\n```shell\r\nE0316 02:18:58.105748 78 model_repository_manager.cc:1890] Poll failed for model directory 'uie': instance group uie_0 of model uie has kind KIND_GPU but server does not support GPUs\r\n```\r\n部署文档，包括uie模型的文档并没有说明必须在gpu环境下运行",
        "state": "closed",
        "user": "lucky0604",
        "closed_by": "lucky0604",
        "created_at": "2023-03-16T02:58:23+00:00",
        "updated_at": "2023-04-23T06:39:10+00:00",
        "closed_at": "2023-04-06T01:30:58+00:00",
        "comments_count": [
            "jiangjiajun",
            "lucky0604",
            "heliqi",
            "lucky0604",
            "TherChenYang",
            "heliqi",
            "TherChenYang",
            "heliqi",
            "heliqi"
        ],
        "labels": [
            "Question",
            "Windows x64",
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1627,
        "title": "安装paddlepaddle-xpu后，无法import fastdeploy",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： release/1.0.4\r\n- 【编译命令】\r\n```bash\r\nexport WITH_KUNLUNXIN=ON\r\nexport WITH_GPU=OFF\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_VISION=OFF\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n```\r\n\r\n- 【系统平台】: Linux x64 (CentOS 7.5)\r\n- 【硬件】： 昆仑R200\r\n- 【编译语言】： Python 3.7.16\r\n\r\n【问题描述】安装paddlepaddle-xpu以后，无法import fastdeploy。\r\n【复现步骤】\r\n(1) 安装[paddlepaddle-xpu](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/hardware_support/xpu_docs/paddle_install_xpu2_cn.html)\r\n```bash\r\n$wget https://paddle-inference-lib.bj.bcebos.com/2.3.0-rc0/python/Linux/XPU2/x86-64_gcc8.2_py36_avx_mkl/paddlepaddle_xpu-2.3.0rc0-cp37-cp37m-linux_x86_64.whl\r\n$ python -m pip install paddlepaddle_xpu-2.3.0rc0-cp37-cp37m-linux_x86_64.whl\r\n\r\n$ pip list | grep paddle\r\npaddle-bfloat      0.1.2\r\npaddlepaddle-xpu   2.3.0rc0\r\n```\r\n\r\n(2) 编译 + 安装 fastdeploy\r\n```\r\n$ export WITH_KUNLUNXIN=ON\r\n$ export WITH_GPU=OFF\r\n$ export ENABLE_ORT_BACKEND=ON\r\n$ export ENABLE_PADDLE_BACKEND=ON\r\n$ export ENABLE_VISION=OFF\r\n\r\n$ python setup.py build\r\n$ python setup.py bdist_wheel\r\n\r\n$ cd dist\r\n$ python -m pip install fastdeploy_python-1.0.4-cp37-cp37m-linux_x86_64.whl\r\n\r\n$ pip list | grep fastdeploy\r\nfastdeploy-python  1.0.4\r\nfastdeploy-tools   0.0.5\r\n```\r\n\r\n(3) 运行以下代码，复现问题\r\n```python\r\nimport paddle\r\nprint(f\"paddle.get_device()\")\r\n# xpu:0\r\n\r\nimport fastdeploy as fd # 这行会报错 ImportError\r\n```\r\n\r\n报错如下:\r\n```bash\r\n[INFO][BKCL][baidu/xpu/bkcl/src/globals.cpp:52] set BKCL timeout to 600 seconds\r\n[INFO][BKCL][baidu/xpu/bkcl/src/globals.cpp:53] set BKCL RING BUFFER SIZE to 1048576\r\nXPURT /home/huoke/miniconda3/envs/py3713/lib/python3.7/site-packages/paddle/fluid/../libs/libxpurt.so loaded\r\npaddle.get_device() = xpu:0\r\nTraceback (most recent call last):\r\n  File \"/home/huoke/miniconda3/envs/py3713/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /home/huoke/miniconda3/envs/py3713/lib/python3.7/site-packages/fastdeploy/libs/libfastdeploy.so.1.0.4: undefined symbol: _ZN6paddle8lite_api12MobileConfig16check_fp16_validEv\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 9, in <module>\r\n    import fastdeploy as fd\r\n  File \"/home/huoke/miniconda3/envs/py3713/lib/python3.7/site-packages/fastdeploy/__init__.py\", line 19, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/home/huoke/miniconda3/envs/py3713/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n```\r\n![image](https://user-images.githubusercontent.com/48248227/225264430-441dcde8-941c-4d4b-bca9-bf421315630d.png)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "kehuo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-15T09:21:11+00:00",
        "updated_at": "2025-02-04T06:41:27+00:00",
        "closed_at": "2025-02-04T06:41:27+00:00",
        "comments_count": [
            "DefTruth",
            "kehuo",
            "Morphaxzx"
        ],
        "labels": [
            "XPU",
            "stable_diffusion"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1634,
        "title": "window Python轻量服务化部署,如何放开ip访问限制",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】：\r\n-通过 pip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html 安装\r\n![image](https://user-images.githubusercontent.com/49382323/225521864-9b1c33bc-2cdf-411f-af2d-62449bdec1f6.png)\r\n\r\n- 【系统平台】: win10\r\n- 【硬件】：cpu\r\n- 【编译语言】： Python3.9\r\n\r\n通过执行python client.py可以正常推理(http url:http://127.0.0.1:8000/fd/ppocrv3)\r\n![image](https://user-images.githubusercontent.com/49382323/225522152-a7f6f4d0-6aac-4693-bf74-bbfa2b96fe77.png)\r\n但是作为http服务提供者，如何解除host访问限制供其他人使用此服务接口\r\n\r\n",
        "state": "closed",
        "user": "kerry-weic",
        "closed_by": "kerry-weic",
        "created_at": "2023-03-16T05:23:57+00:00",
        "updated_at": "2023-03-17T05:55:21+00:00",
        "closed_at": "2023-03-17T05:55:21+00:00",
        "comments_count": [
            "jiangjiajun",
            "kerry-weic"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1635,
        "title": "Linux FastDeploy docker启动服务失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy镜像包版本: 1.0.4-cpu-only-21.10\r\n- \r\n![image](https://user-images.githubusercontent.com/49382323/225523533-13bfe0de-2b49-46ad-9b3e-952494eec4d5.png)\r\n\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： Intel(R) Xeon(R) CPU E5-2620 v2\r\n- 【编译语言】：Python 3.8.5\r\n\r\n按照https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/fastdeploy_serving文档进行操作。\r\n执行fastdeployserver --model-repository=/ocr_serving/models 启动服务时出错 ，日志如下：\r\n\r\nr0316 06:33:56.232776 165 model_repository_manager.cc:1890] Poll failed for model directory 'cls_runtime': instance group cls_runtime_0 of model cls_runtime has kind KIND_GPU but server does not support GPUs\r\nE0316 06:33:56.233814 165 model_repository_manager.cc:1890] Poll failed for model directory 'det_runtime': instance group det_runtime_0 of model det_runtime has kind KIND_GPU but server does not support GPUs\r\nE0316 06:33:56.234847 165 model_repository_manager.cc:1890] Poll failed for model directory 'rec_runtime': instance group rec_runtime_0 of model rec_runtime has kind KIND_GPU but server does not support GPUs\r\nE0316 06:33:56.235118 165 model_repository_manager.cc:1375] Invalid argument: ensemble pp_ocr contains models that are not available: det_runtime\r\nE0316 06:33:56.235235 165 model_repository_manager.cc:1375] Invalid argument: ensemble rec_pp contains models that are not available: rec_runtime\r\nE0316 06:33:56.235302 165 model_repository_manager.cc:1375] Invalid argument: ensemble cls_pp contains models that are not available: cls_runtime\r\nI0316 06:33:56.235421 165 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0316 06:33:56.335945 165 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0316 06:33:56.343297 165 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nI0316 06:33:56.436638 165 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0316 06:33:56.537053 165 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0316 06:33:57.401222 165 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\nI0316 06:33:57.402776 165 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0316 06:33:58.523082 165 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0316 06:33:58.523395 165 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0316 06:33:59.523855 165 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0316 06:33:59.524233 165 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0316 06:34:00.546161 165 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\nI0316 06:34:00.546492 165 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0316 06:34:00.546641 165 server.cc:549] \r\n+---------+-------------------------------------------------------+--------+\r\n| Backend | Path                                                  | Config |\r\n+---------+-------------------------------------------------------+--------+\r\n| python  | /opt/tritonserver/backends/python/libtriton_python.so | {}     |\r\n+---------+-------------------------------------------------------+--------+\r\n\r\nI0316 06:34:00.546807 165 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0316 06:34:00.547184 165 tritonserver.cc:1920] \r\n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                        |\r\n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                       |\r\n| server_version                   | 2.15.0                                                                                                                                                                       |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data  |\r\n|                                  | statistics                                                                                                                                                                   |\r\n| model_repository_path[0]         | /ocr_serving/models                                                                                                                                                          |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                    |\r\n| strict_model_config              | 1                                                                                                                                                                            |\r\n| rate_limit                       | OFF                                                                                                                                                                          |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                    |\r\n| response_cache_byte_size         | 0                                                                                                                                                                            |\r\n| min_supported_compute_capability | 0.0                                                                                                                                                                          |\r\n| strict_readiness                 | 1                                                                                                                                                                            |\r\n| exit_timeout                     | 30                                                                                                                                                                           |\r\n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0316 06:34:00.547466 165 server.cc:252] Waiting for in-flight requests to complete.\r\nI0316 06:34:00.547507 165 model_repository_manager.cc:1055] unloading: rec_postprocess:1\r\nI0316 06:34:00.547588 165 model_repository_manager.cc:1055] unloading: det_preprocess:1\r\nI0316 06:34:00.547689 165 model_repository_manager.cc:1055] unloading: det_postprocess:1\r\nI0316 06:34:00.547770 165 model_repository_manager.cc:1055] unloading: cls_postprocess:1\r\nI0316 06:34:00.547871 165 server.cc:267] Timeout 30: Found 4 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nCleaning up...\r\nCleaning up...\r\nI0316 06:34:01.548107 165 server.cc:267] Timeout 29: Found 4 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nI0316 06:34:01.769178 165 model_repository_manager.cc:1166] successfully unloaded 'rec_postprocess' version 1\r\nI0316 06:34:01.777994 165 model_repository_manager.cc:1166] successfully unloaded 'cls_postprocess' version 1\r\nI0316 06:34:01.851643 165 model_repository_manager.cc:1166] successfully unloaded 'det_postprocess' version 1\r\nI0316 06:34:01.853469 165 model_repository_manager.cc:1166] successfully unloaded 'det_preprocess' version 1\r\nI0316 06:34:02.548376 165 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n\r\n\r\n",
        "state": "closed",
        "user": "kerry-weic",
        "closed_by": "heliqi",
        "created_at": "2023-03-16T05:33:20+00:00",
        "updated_at": "2024-05-17T07:02:29+00:00",
        "closed_at": "2023-03-29T05:24:19+00:00",
        "comments_count": [
            "heliqi",
            "kerry-weic",
            "heliqi",
            "kerry-weic",
            "heliqi",
            "xiaomi0922",
            "kerry-weic",
            "wuqiu-ai",
            "xiaomi0922",
            "kerry-weic",
            "pangdahua",
            "sheiy"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1639,
        "title": "FastDeploy/examples/audio/pp-tts/   C++ demo support?",
        "body": "Hi  ,we want to use the pp-tts demo ,but find it doesnot has C++  deployment .can you add this one ?\r\n \r\n\r\n",
        "state": "closed",
        "user": "roy699",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-16T10:31:09+00:00",
        "updated_at": "2024-05-21T06:40:42+00:00",
        "closed_at": "2024-05-21T06:40:42+00:00",
        "comments_count": [
            "943fansi"
        ],
        "labels": [
            "Enhancement"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1650,
        "title": "RKNPU2自训练模型labels无法更改",
        "body": "- 【FastDeploy版本】：fastdeploy-linux-arrch-1.0.2\r\n- 【编译命令】预编译库\r\n- 【系统平台】: Linux aarch64\r\n- 【硬件】：RK3588(RKNPU2)\r\n- 【编译语言】： C++\r\n\r\nRKNPU2自训练模型无labels_list调用接口，导致无法自定义识别类。\r\n",
        "state": "closed",
        "user": "MarKKwan27149",
        "closed_by": "MarKKwan27149",
        "created_at": "2023-03-19T15:09:42+00:00",
        "updated_at": "2023-03-21T12:16:09+00:00",
        "closed_at": "2023-03-21T12:16:09+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1641,
        "title": "jetson依赖库问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-dev\r\n- 【系统平台】: jetpack 5.1\r\n- 【硬件】： ORIN NX\r\n- 【编译语言】： C++\r\n\r\nhost编译正常，docker（nvcr.io/nvidia/l4t-jetpack:r35.2.1）中编译后，缺失部分so\r\n![image](https://user-images.githubusercontent.com/33308054/225653099-02df9f9d-265d-4b41-8c88-6f2b99a387b6.png)\r\n\r\n上述缺失so不在docker中编译时，链接如下：\r\n![image](https://user-images.githubusercontent.com/33308054/225653797-14521aea-5edd-4c29-9c62-560c902a300b.png)\r\n\r\n",
        "state": "closed",
        "user": "ShawnXsw",
        "closed_by": "ShawnXsw",
        "created_at": "2023-03-16T14:46:01+00:00",
        "updated_at": "2023-03-17T10:12:11+00:00",
        "closed_at": "2023-03-17T10:12:02+00:00",
        "comments_count": [
            "wang-xinyu",
            "ShawnXsw"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1642,
        "title": "为什么我在rk3588没法初始化cpu的环境，会报错，只能使用npu进行推理",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n如果使用npu推理就可以，使用cpu就环境初始化错误\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-aarch-\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【编译语言】： C++ \r\n\r\n代码：\r\nauto model=fastdeploy::vision::faceid::ArcFace(model_path,\"\");\r\n\r\n## 问题日志及出现问题的操作流程\r\n[ERROR] fastdeploy/fastdeploy_model.cc(229)::CreateCpuBackend   Found no valid backend for model: ArcFace\r\n[ERROR] fastdeploy/vision/faceid/contrib/insightface/base.cc(42)::Initialize    Failed to initialize fastdeploy backend.\r\n\r\n",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-16T14:52:25+00:00",
        "updated_at": "2023-03-20T09:58:01+00:00",
        "closed_at": "2023-03-20T09:57:44+00:00",
        "comments_count": [
            "jiangjiajun",
            "XiaBing992"
        ],
        "labels": [
            "rknpu2"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1656,
        "title": "fd-python三个模型串行怎么分别设置postprocessor的conf_threshold和nms_threshold?",
        "body": "`det_person = fd.vision.detection.YOLOv5(\r\n                model_file=os.path.join(ROOT, 'pretrained/det/person/weights/best.onnx'),\r\n                runtime_option=runtime_option_1)\r\ndet_person.preprocessor.size = [640, 640]\r\n\r\ndet_head = fd.vision.detection.YOLOv5(\r\n    model_file=os.path.join(ROOT, 'pretrained/det/head/weights/best.onnx'),\r\n    runtime_option=runtime_option_2)\r\ndet_head.preprocessor.size = [640, 640]\r\n\r\ndet_foot = fd.vision.detection.YOLOv5(\r\n    model_file=os.path.join(ROOT, 'pretrained/det/foot_s/weights/best.onnx'),\r\n    runtime_option=runtime_option_3)\r\ndet_foot.preprocessor.size = [640, 640]`",
        "state": "closed",
        "user": "WanderAlphonse",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-20T08:11:02+00:00",
        "updated_at": "2024-04-16T09:03:29+00:00",
        "closed_at": "2024-04-16T09:03:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1646,
        "title": "deploy RK3588 rknn model in runtime but get infer result as all zero",
        "body": ".rknn model is transform from rknn2 and it can run correctly in rktoolkit2 in PC.\r\nHowever, it got wrong when using 'runtime' to infer and got all zero result, while the shape of result is correct. The input has already checked.(there is no example in runtime usage, while my model is not used as any way in 'vision' example).\r\n![image](https://user-images.githubusercontent.com/87317372/225900541-7ef20eb8-16cb-4fca-b2a7-a2fe9e58494f.png)\r\nThe code is below:\r\n```python\r\nimport fastdeploy as fd\r\nfrom fastdeploy import ModelFormat\r\nimport numpy as np\r\nimport cv2\r\n\r\ncamera = cv2.VideoCapture(2)\r\noption = fd.RuntimeOption()\r\n\r\noption.set_model_path(\"mymodel.rknn\", model_format=ModelFormat.RKNN)\r\n\r\noption.use_cpu()\r\noption.use_rknpu2(rknpu2_name=fd.c_lib_wrap.CpuName.RK3588,rknpu2_core=fd.c_lib_wrap.CoreMask.RKNN_NPU_CORE_AUTO)\r\n\r\nruntime = fd.Runtime(option)\r\n\r\ninput_name = runtime.get_input_info(0).name\r\n\r\nwhile True:\r\n    ret,frame = camera.read()\r\n    cv2.imshow(\"test\",frame)\r\n    cv2.waitKey(1)\r\n    rgb_array = cv2.resize(frame,(320,176))\r\n    hsv_array = cv2.cvtColor(rgb_array,cv2.COLOR_BGR2HSV).reshape((1,176,320,3))\r\n    # print(hsv_array)\r\n    results = runtime.infer({\r\n        input_name: hsv_array\r\n    })\r\n    input_check = hsv_array\r\n    output_check = runtime.num_outputs()\r\n    print(input_check)\r\n    print(output_check)\r\n    print(results)\r\n```\r\n",
        "state": "closed",
        "user": "ken4647",
        "closed_by": "ken4647",
        "created_at": "2023-03-17T12:17:01+00:00",
        "updated_at": "2023-03-20T08:55:43+00:00",
        "closed_at": "2023-03-20T08:55:43+00:00",
        "comments_count": [
            "ken4647"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1644,
        "title": "【A311D部署PP-ShiTuV2】问题2：特征提取这一步，跑预处理时，报错：Segmentation fault core dumped",
        "body": "# 环境\r\n\r\n-【FastDeploy版本】： fastdeploy-linux-1.0.4 源码\r\n-【编译命令】cmake -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DWITH_TIMVX=ON -DTARGET_ABI=arm64 -DENABLE_FLYCV=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-timvx -DENABLE_VISION=ON -DENABLE_LITE_BACCKEND=ON -Wno-dev ..\r\n-【系统平台】: aarch64 linux kernel 4.9.113，gcc、g++=9.3.0，make=4.2.1，cmake=3.22.6，ubuntu 20.04 arm64\r\n-【硬件】： 荣品A311D，NPU驱动已经安装为 6.4.4.3\r\n-【编译语言】： C++\r\n\r\n# 问题日志及出现问题的操作流程\r\n## 【模型跑不通】\r\n-  已经按照FD官方文档跑通 [PaddleClas A311D 开发板 C++ 部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/classification/paddleclas/a311d/cpp/README_CN.md)。\r\n- 换成PaddleClas官方提供的特征提取模型：[general_PPLCNetV2_base_pretrained_v1.0.pdparams](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/PPShiTuV2/general_PPLCNetV2_base_pretrained_v1.0.pdparams) 也可以跑通分类任务。\r\n- 但是，因为PP-ShiTuV2的第二个步骤是需要提取图片的特征向量，所以需要先采用FD的预处理，然后使用FD的runtime.Infer获取Output中的特征向量，而不需要获取FD的后处理结果。\r\n\r\n按这个方式，在程序执行到 preprocessor.Apply(&fd_mats, &inputs); 这一句时报错：\r\n\r\n`./run_demo.sh: line 14: 10508 Segmentation fault      (core dumped) ./build/infer_demo ./models/feature_extract_convfalse ./images/Adboard_clas_test_img02.jpg`\r\n\r\n完整的Log信息如下：\r\n```\r\nrpdzkj@rpdzkj:~/Downloads/fastdeploy1.0.4/examples/vision/detection/paddledetection/a311d/cpp/Adboard$ ./run_demo.sh \r\n[INFO] fastdeploy/vision/common/processors/base.cc(92)::EnableFlyCV     Will change to use image processing library ProcLib::FLYCV\r\n[dhq info] runtime.Init(option);   Done \r\n[dhq info] cv::imread(image_file);   Done \r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[dhq info] auto preprocessor = fastdeploy::vision::classification::PaddleClasPreprocessor(config_file);   Done \r\n./run_demo.sh: line 14: 10508 Segmentation fault      (core dumped) ./build/infer_demo ./models/feature_extract_convfalse ./images/Adboard_clas_test_img02.jpg\r\n```\r\n\r\n用到的文件和模型已经打包上传到[source_files.zip](https://github.com/PaddlePaddle/FastDeploy/files/10998678/source_files.zip)，其中包含：\r\n\r\n推理代码：infer_ppshitu.cc，自己修改过。\r\n编译脚本：generate_demo.sh\r\n运行脚本：run_demo.sh\r\n模型文件：feature_extract_convfalse\r\n测试图片：Adboard_clas_test_img02.jpg\r\n",
        "state": "closed",
        "user": "Taichipeace",
        "closed_by": "Taichipeace",
        "created_at": "2023-03-17T07:08:16+00:00",
        "updated_at": "2023-03-25T11:26:01+00:00",
        "closed_at": "2023-03-25T11:26:01+00:00",
        "comments_count": [
            "Taichipeace"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1652,
        "title": "模型成功部署，客户端可以正常调用，但有时也会报输出参数名不存在，这是一个概率问题",
        "body": "调用部署的模型，有的时候是正确返回值，有的时候报下个这个错：\r\ntritonclient.utils.InferenceServerException: [StatusCode.INVALID_ARGUMENT] unexpected inference output 'argmax_0.tmp_0' for model \r\n请问这是为什么？",
        "state": "closed",
        "user": "aixuedegege",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-20T03:52:31+00:00",
        "updated_at": "2024-04-16T09:03:28+00:00",
        "closed_at": "2024-04-16T09:03:28+00:00",
        "comments_count": [
            "aixuedegege"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1657,
        "title": "docker环境内UIE服务化部署模型失败，报错 The FastDeploy didn't compile with Paddle Inference.",
        "body": "\r\n- 【FastDeploy版本】：fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： i9-13900K， Nvidia GPU 3090TI\r\n\r\n\r\n按照\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/text/uie/serving/README_CN.md\r\n在fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10的docker环境内，服务化部署UIE模型，启动失败\r\n\r\n输出：\r\nroot@xmdx:/# CUDA_VISIBLE_DEVICES=0 fastdeployserver --model-repository=/hdd/ljz/FastDeploy/examples/text/uie/serving/models/ --backend-config=python,shm-default-byte-size=10485760\r\nI0320 08:03:03.270487 274 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3090 Ti\r\nI0320 08:03:03.379255 274 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f4ace000000' with size 268435456\r\nI0320 08:03:03.379357 274 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0320 08:03:03.379930 274 model_repository_manager.cc:1022] loading: uie:1\r\nI0320 08:03:03.510236 274 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: uie_0 (GPU device 0)\r\nmodel_config: {'name': 'uie', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'INPUT_1', 'data_type': 'TYPE_STRING', 'format': 'FORMAT_NONE', 'dims': [1], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'execution_accelerators': {'gpu_execution_accelerator': [{'name': 'paddle', 'parameters': {'cpu_threads': '12'}}], 'cpu_execution_accelerator': []}, 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'uie_0', 'kind': 'KIND_GPU', 'count': 1, 'gpus': [0], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\ninput: ['INPUT_0', 'INPUT_1']\r\noutput: ['OUTPUT_0']\r\n[ERROR] fastdeploy/runtime/runtime_option.cc(133)::UsePaddleBackend     The FastDeploy didn't compile with Paddle Inference.\r\n\r\n提示The FastDeploy didn't compile with Paddle Inference.，但是我没有做修改，而且之前跑OCR的服务化部署跑通过，也是使用 Paddle Inference后端+paddle的模型，在UIE服务化就启动不起来，不知道是什么问题，求大佬们赐教",
        "state": "open",
        "user": "z5z56",
        "closed_by": null,
        "created_at": "2023-03-20T08:12:25+00:00",
        "updated_at": "2024-09-06T08:15:53+00:00",
        "closed_at": null,
        "comments_count": [
            "LouisHeck",
            "z5z56",
            "LouisHeck",
            "blakeliu",
            "blakeliu",
            "huangjun11",
            "neo502721",
            "neo502721",
            "DDUFlyme",
            "neoragex2002"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1661,
        "title": "昇腾NPU使用Docker编译安装FastDeploy部署OCR报错[ERROR] fastdeploy/runtime/runtime_option.cc(133)::UsePaddleBackend     The FastDeploy didn't compile with Paddle Inference.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n\r\n## 环境\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n```\r\n# Download the latest source code\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport WITH_ASCEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\n```\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n宿主机UOS v20  容器 Ubuntu 18.04\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n昇腾310\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n**请问我根据文档：[华为昇腾NPU 部署环境编译准备](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/huawei_ascend.md)的步骤编译安装完成后\r\n使用脚本：`python infer_static_shape.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device ascend`可以进行推理出结果。**\r\n![image](https://user-images.githubusercontent.com/69302396/226298730-d5df8a3a-419b-42d7-920b-4e6613c05e3d.png)\r\n\r\n**但是按照文档[PP-OCRv3 Python轻量服务化部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCRv3/python/serving/README_CN.md)中使用`fastdeploy simple_serving --app server:app`启动服务失败，报错显示**\r\n![image](https://user-images.githubusercontent.com/69302396/226298904-45e73688-1cab-4770-bb02-fbde7a8c0624.png)\r\n\r\n```\r\nroot@localhost:/home/FastDeploy/examples/vision/ocr/PP-OCRv3/python/serving# fastdeploy simple_serving --app server:app\r\n[ERROR] fastdeploy/runtime/runtime_option.cc(133)::UsePaddleBackend     The FastDeploy didn't compile with Paddle Inference.\r\nAborted (core dumped)\r\n```\r\n**请问我是漏了什么步骤没有做吗？求解。**",
        "state": "closed",
        "user": "minboo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-20T09:56:04+00:00",
        "updated_at": "2024-04-16T09:03:30+00:00",
        "closed_at": "2024-04-16T09:03:30+00:00",
        "comments_count": [
            "yunyaoXYY",
            "minboo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1669,
        "title": "在参照examples/vision/ocr/PP-OCRv3/rknpu2/执行，模型转换失败",
        "body": "![image](https://user-images.githubusercontent.com/50976808/226572107-0b949918-43ef-424f-bcd2-90d554cd9784.png)\r\n代码拉取最新的。ubuntu18.04 x86_64",
        "state": "closed",
        "user": "qingmengya",
        "closed_by": "qingmengya",
        "created_at": "2023-03-21T09:57:40+00:00",
        "updated_at": "2023-03-21T12:12:49+00:00",
        "closed_at": "2023-03-21T12:12:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1663,
        "title": "segmentation task in streamer[deepstream]?",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n\r\nhello,\r\nI'd like to run a semantic model supported by paddleseg in deepstream base, is there any restriction?\r\nOr can't I expect a big speed difference from what Deepstream itself runs on Paddle Trt backend from a different perspective?\r\n\r\n@wang-xinyu ",
        "state": "closed",
        "user": "YoungjaeDev",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-20T12:07:17+00:00",
        "updated_at": "2024-04-16T09:03:31+00:00",
        "closed_at": "2024-04-16T09:03:31+00:00",
        "comments_count": [
            "wang-xinyu",
            "YoungjaeDev"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1664,
        "title": "Cannot compile FastDeploy on Jetson with Paddle backend",
        "body": "## Environment\r\n\r\nFastDeploy version: latest code in develop branch\r\nOS Platform: l4t jetpack linux (arm)\r\nHardware: Jetson Xavier AGX (jetpack 5.0.2, CUDA 11.4, TRT 8.4)\r\nProgram Language: e.g. Python 3.8.10\r\n\r\n## Problem description\r\nI am trying to build a docker image with FastDeploy to serve a Solov2 model on a Jetson Xavier AGX with Jetpack 5.0.2. Solov2 uses operators that are currently not supported by paddle2ONNX and thus I need the paddle backend to serve a Solov2 model with FastDeploy. I can build FastDeploy with TRT and vision backend but when I enable the `ENABLE_PADDLE_BACKEND=ON` switch without specifying a prebuilt paddlepaddle GPU directory I get errors regarding the aarch64 linux and missing python installations. Is it currently possible to build FastDeploy with Paddle backend on this hardware with this OS? Additionally, is this the correct approach to serving Solov2 on a Jetson device or should I rather use PaddleDetection deploy for example? My current dockerfile consists of: \r\n\r\n```\r\nFROM nvcr.io/nvidia/l4t-jetpack:r35.1.0\r\n\r\nWORKDIR /opt/app\r\n\r\nARG BUILD_ON_JETSON=ON\r\nARG ENABLE_VISION=ON\r\n\r\nRUN apt-get update && \\\r\n    apt-get -y install python3-pip git cmake && \\\r\n    pip3 install wheel\r\n\r\n# Install prebuilt paddlepaddle GPU for Jetson\r\nRUN wget https://paddle-inference-lib.bj.bcebos.com/2.4.2/python/Jetson/jetpack5.0.2_gcc9.4/xavier/paddlepaddle_gpu-2.4.2-cp38-cp38-linux_aarch64.whl #prebuilt library\r\nRUN pip3 install paddlepaddle_gpu-2.4.2-cp38-cp38-linux_aarch64.whl\r\n\r\n# No prebuilt fastdeploy available for Jetson so manual build\r\nRUN git clone https://github.com/PaddlePaddle/FastDeploy.git\r\nWORKDIR /opt/app/FastDeploy/python\r\n\r\nARG ENABLE_PADDLE_BACKEND=ON\r\nARG PADDLEINFERENCE_DIRECTORY=/opt/app\r\nARG ENABLE_TRT_BACKEND=ON\r\n\r\nRUN python3 setup.py build\r\nRUN python3 setup.py bdist_wheel\r\n\r\nWORKDIR /opt/app/FastDeploy/python/dist\r\nRUN pip3 install fastdeploy_gpu_python-0.0.0-cp38-cp38-linux_aarch64.whl\r\n\r\n```\r\nThis image builds and works when the `ARG ENABLE_PADDLE_BACKEND=ON` and `ARG PADDLEINFERENCE_DIRECTORY=/opt/app` are removed. With only `ARG ENABLE_PADDLE_BACKEND=ON` set and the directory omitted, this error occurs:\r\n\r\n```\r\nStep 13/20 : RUN python3 setup.py build                                      \r\n ---> Running in 30d047f535f0                                                \r\nrunning build                                                                \r\nrunning build_py                                                             \r\nrunning create_version                                                       \r\nrunning cmake_build                                                          \r\n-- The C compiler identification is GNU 9.4.0                                \r\n-- The CXX compiler identification is GNU 9.4.0                              \r\n-- Check for working C compiler: /usr/bin/cc                                 \r\n-- Check for working C compiler: /usr/bin/cc -- works                        \r\n-- Detecting C compiler ABI info                                             \r\n-- Detecting C compiler ABI info - done                                      \r\n-- Detecting C compile features                                              \r\n-- Detecting C compile features - done                                       \r\n-- Check for working CXX compiler: /usr/bin/c++                              \r\n-- Check for working CXX compiler: /usr/bin/c++ -- works                     \r\n-- Detecting CXX compiler ABI info                                           \r\n-- Detecting CXX compiler ABI info - done                                    \r\n-- Detecting CXX compile features                                            \r\n-- Detecting CXX compile features - done                                     \r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.\r\n-- [download 1% complete]                                                    \r\n-- [download 6% complete]                                                    \r\n-- [download 11% complete]                                                   \r\n-- [download 16% complete]                                                   \r\n-- [download 21% complete]                                                   \r\n-- [download 27% complete]                                                   \r\n-- [download 32% complete]                                                   \r\n-- [download 37% complete]                                                   \r\n-- [download 42% complete]                                                   \r\n-- [download 47% complete]                                                   \r\n-- [download 52% complete]                                                   \r\n-- [download 57% complete]                                                   \r\n-- [download 63% complete]                                                   \r\n-- [download 68% complete]                                                   \r\n-- [download 73% complete]                                                   \r\n-- [download 78% complete]                                                   \r\n-- [download 83% complete]                                                   \r\n-- [download 88% complete]                                                   \r\n-- [download 93% complete]                                                   \r\n-- [download 99% complete]                                                   \r\n-- [download 100% complete]                                                  \r\nDecompress file /opt/app/FastDeploy/python/.setuptools-cmake-build/patchelf-0\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /opt/app/FastDeploy\r\nCMake Error at cmake/paddle_inference.cmake:99 (message):                    \r\n  Paddle Backend doesn't support linux aarch64 now.                          \r\nCall Stack (most recent call first):                                         \r\n  CMakeLists.txt:232 (include)                                               \r\n                                                                             \r\n                                                                             \r\n-- Configuring incomplete, errors occurred!                                  \r\nSee also \"/opt/app/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMake\r\nTraceback (most recent call last):                                           \r\n  File \"setup.py\", line 417, in <module>                                     \r\n    setuptools.setup(                                                        \r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 144, in \r\n    return distutils.core.setup(**attrs)                                     \r\n  File \"/usr/lib/python3.8/distutils/core.py\", line 148, in setup            \r\n    dist.run_commands()                                                      \r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 966, in run_commands     \r\n    self.run_command(cmd)                                                    \r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command      \r\n    cmd_obj.run()                                                            \r\n  File \"/usr/lib/python3.8/distutils/command/build.py\", line 135, in run     \r\n    self.run_command(cmd_name)                                               \r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command       \r\n    self.distribution.run_command(command)                                   \r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command      \r\n    cmd_obj.run()                                                            \r\n  File \"setup.py\", line 280, in run                                          \r\n    self.run_command('cmake_build')                                          \r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command       \r\n    self.distribution.run_command(command)                                   \r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command      \r\n    cmd_obj.run()                                                            \r\n  File \"setup.py\", line 266, in run                                          \r\n    subprocess.check_call(cmake_args)                                        \r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call           \r\n    raise CalledProcessError(retcode, cmd)                                   \r\nsubprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_\r\nN', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-38-aarch64-linu\r\nRT_BACKEND=OFF', '-DENABLE_OPENVINO_BACKEND=OFF', '-DENABLE_PADDLE_BACKEND=ON\r\n'-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DEN\r\nRECTORY=UNDEFINED', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DLIBRARY_NAME=fastd\r\n_SOC=', '/opt/app/FastDeploy']' returned non-zero exit status 1.             \r\nThe command '/bin/sh -c python3 setup.py build' returned a non-zero code: 1  \r\n```\r\n\r\nThis error is bypassed when I add the `PADDLEINFERENCE_DIRECTORY` flag and point that to either the paddle gpu python wheels or the installed package location, but then a new error occurs regarding which python version cmake invokes:\r\n\r\n```\r\nDecompress file /opt/app/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /opt/app/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\nCMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 (message):\r\n  Could NOT find Python (missing: Python_INCLUDE_DIRS Development) (found\r\n  version \"2.7.18\")\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:393 (_FPHSA_FAILURE_MESSAGE)\r\n  /usr/share/cmake-3.16/Modules/FindPython.cmake:347 (find_package_handle_standard_args)\r\n  cmake/paddle_inference.cmake:72 (find_package)\r\n  CMakeLists.txt:232 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/opt/app/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 417, in <module>\r\n    setuptools.setup(\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 144, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.8/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.8/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 280, in run\r\n    self.run_command('cmake_build')\r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 266, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.8', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-38-aarch64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DWITH_ASCEND=OFF', '-DENABLE_ORT_BACKEND=OFF', '-DENABLE_OPENVINO_BACKEND=OFF', '-DENABLE_PADDLE_BACKEND=ON', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=ON', '-DENABLE_LITE_BACKEND=OFF', '-DPADDLELITE_URL=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=OFF', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=OFF', '-DWITH_IPU=OFF', '-DWITH_KUNLUNXIN=OFF', '-DBUILD_ON_JETSON=ON', '-DTRT_DIRECTORY=UNDEFINED', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=/opt/app', '-DRKNN2_TARGET_SOC=', '/opt/app/FastDeploy']' returned non-zero exit status 1.\r\nThe command '/bin/sh -c python3 setup.py build' returned a non-zero code: 1\r\n```\r\n\r\nDeleting python2 and updating the path doesn't help this either as it will then just claim python is missing as a whole.",
        "state": "closed",
        "user": "JeroendenBoef",
        "closed_by": "JeroendenBoef",
        "created_at": "2023-03-20T19:48:07+00:00",
        "updated_at": "2023-03-22T02:34:33+00:00",
        "closed_at": "2023-03-21T12:52:02+00:00",
        "comments_count": [
            "wang-xinyu",
            "JeroendenBoef",
            "wang-xinyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1665,
        "title": "请问FastDeploy在服务端部署OCR只支持pp-ocrv2和pp-ocrv3两种模型吗，自己训练的模型是否能支持？",
        "body": "**请问FastDeploy在服务端部署OCR只支持pp-ocrv2和pp-ocrv3两种模型吗，自己训练的模型是否能支持？**",
        "state": "closed",
        "user": "minboo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-21T00:10:35+00:00",
        "updated_at": "2024-04-16T09:03:31+00:00",
        "closed_at": "2024-04-16T09:03:31+00:00",
        "comments_count": [
            "yunyaoXYY",
            "minboo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1671,
        "title": "Ubuntu 编译 CPU C++ 版本时失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=ON     -DENABLE_PADDLE_BACKEND=ON         \r\n                                      -DENABLE_OPENVINO_BACKEND=OFF          \r\n                                      -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk          \r\n                                      -DENABLE_VISION=ON          \r\n                                      -DOPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4         \r\n                                      -DENABLE_TEXT=ON\r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【硬件】： 想要编译CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n我想使用 Java 调用 FastDeploy 的接口，所以得先编译 C++ 版本的 FastDeploy ，然后想编译好后作为 库 引入到 JNI 里面，但是在编译 C++ 版本时报如下错误：\r\n```shell\r\n100%] Linking CXX shared library libfastdeploy.so\r\n[100%] Built target fastdeploy\r\nbash: line 1: python: command not found\r\nmake[2]: *** [CMakeFiles/patchelf_paddle_inference.dir/build.make:70: CMakeFiles/patchelf_paddle_inference] Error 127\r\nmake[1]: *** [CMakeFiles/Makefile2:273: CMakeFiles/patchelf_paddle_inference.dir/all] Error 2\r\nmake: *** [Makefile:156: all] Error 2\r\n\r\n```\r\n![1679405860745](https://user-images.githubusercontent.com/53164956/226624731-5fd3c47e-bb12-426f-ab62-4592186bec49.png)\r\n编译过程中还有一段这样的提示：\r\n```shell\r\nIn file included from /home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir_pb_converter.h:10,\r\n                 from /home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir_pb_converter.cc:8:\r\nIn constructor ‘paddle2onnx::Dimension::Dimension(paddle2onnx::Dimension&&)’,\r\n    inlined from ‘void __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = paddle2onnx::Dimension; _Args = {paddle2onnx::Dimension}; _Tp = paddle2onnx::Dimension]’ at /usr/include/c++/11/ext/new_allocator.h:162:4,\r\n    inlined from ‘static void std::allocator_traits<std::allocator<_CharT> >::construct(std::allocator_traits<std::allocator<_CharT> >::allocator_type&, _Up*, _Args&& ...) [with _Up = paddle2onnx::Dimension; _Args = {paddle2onnx::Dimension}; _Tp = paddle2onnx::Dimension]’ at /usr/include/c++/11/bits/alloc_traits.h:516:17,\r\n    inlined from ‘void std::vector<_Tp, _Alloc>::emplace_back(_Args&& ...) [with _Args = {paddle2onnx::Dimension}; _Tp = paddle2onnx::Dimension; _Alloc = std::allocator<paddle2onnx::Dimension>]’ at /usr/include/c++/11/bits/vector.tcc:115:30,\r\n    inlined from ‘void std::vector<_Tp, _Alloc>::push_back(std::vector<_Tp, _Alloc>::value_type&&) [with _Tp = paddle2onnx::Dimension; _Alloc = std::allocator<paddle2onnx::Dimension>]’ at /usr/include/c++/11/bits/stl_vector.h:1204:21,\r\n    inlined from ‘std::vector<paddle2onnx::Dimension> paddle2onnx::tensorShapeProtoToDimensions(const paddle2onnx::TensorShapeProto&)’ at /home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir_pb_converter.cc:201:21:\r\n/home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir.h:74:8: warning: ‘<unnamed>.paddle2onnx::Dimension::dim’ may be used uninitialized [-Wmaybe-uninitialized]\r\n   74 | struct Dimension final {\r\n      |        ^~~~~~~~~\r\n/home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir_pb_converter.cc: In function ‘std::vector<paddle2onnx::Dimension> paddle2onnx::tensorShapeProtoToDimensions(const paddle2onnx::TensorShapeProto&)’:\r\n/home/hjyp/code/DeepLearning/FastDeploy/third_party/onnx/onnx/common/ir_pb_converter.cc:201:32: note: ‘<anonymous>’ declared here\r\n  201 |       dims.push_back(Dimension());\r\n      |                                ^\r\n\r\n```\r\n![image](https://user-images.githubusercontent.com/53164956/226625120-4fef456d-686a-41a5-8bbd-bbe2097855cc.png)\r\n",
        "state": "closed",
        "user": "Fancy-hjyp",
        "closed_by": "Fancy-hjyp",
        "created_at": "2023-03-21T13:48:48+00:00",
        "updated_at": "2024-05-23T09:09:22+00:00",
        "closed_at": "2023-05-25T01:43:08+00:00",
        "comments_count": [
            "jiangjiajun",
            "Fancy-hjyp",
            "jiangjiajun",
            "FeatherZhong",
            "caicaicai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1673,
        "title": "FastDeploy部署PPOCRv3模型后，识别率大幅度下降",
        "body": "**npu下根据[文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md)编译安装，\r\n使用命令`python infer_static_shape.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 2a.jpg --device ascend`进行推理。推理官网给出的图片可以正确识别，换成其它图片则识别率大幅降低**\r\n\r\n**以下为图片内容**\r\n![image](https://user-images.githubusercontent.com/69302396/226786780-e74db27f-b0bc-406c-973d-a507f4106548.png)\r\n**推理结果如下**\r\n![image](https://user-images.githubusercontent.com/69302396/226787583-9b740b93-2d3f-4557-9433-8ff2ef94a8c8.png)\r\n\r\n**推理产生的结果图片如下**\r\n![image](https://user-images.githubusercontent.com/69302396/226790235-72bd3ba2-8f58-46b9-a5ca-ae6d20f2377d.png)\r\n\r\n**注：脚本文件均为默认，无修改**",
        "state": "closed",
        "user": "minboo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T02:38:22+00:00",
        "updated_at": "2024-04-16T09:03:33+00:00",
        "closed_at": "2024-04-16T09:03:33+00:00",
        "comments_count": [
            "yunyaoXYY",
            "minboo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1672,
        "title": "PPOCR fastdeployserver 解析莫名乱码和Failed to perform CUDA copy",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： docker镜像fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10，使用develop分支\r\n- 【编译命令】）官方部署步骤\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia GPU 3090\r\n- 【编译语言】：python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n按照[](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCRv3/serving/README_CN.md)的步骤部署进行服务化部署。示例图片解析成功，在解析几张图片之后，再解析就会偶尔出现如下错误：\r\n![image](https://user-images.githubusercontent.com/51997291/226783055-4124b991-eaf7-4046-9985-b107df3ae12f.png)\r\n但是重新启动fastdeployserver服务后，还是使用出现上述错误的同样的图片解析又成功了。\r\n\r\n另外，在出现CUDA_MEMORY警告后\r\n![image](https://user-images.githubusercontent.com/51997291/226785567-f3b2076b-d7ef-454e-bfeb-3e9ca2d9e506.png)\r\n再解析示例图片，会出现\r\n![image](https://user-images.githubusercontent.com/51997291/226785642-fdab129c-b252-4f67-ab13-c60e49471281.png)\r\n部分全英文图片解析会出现乱码\r\n![image](https://user-images.githubusercontent.com/51997291/226786110-fb65fb6e-a28a-4191-9feb-82b95228c665.png)\r\n",
        "state": "closed",
        "user": "xlvy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T02:27:49+00:00",
        "updated_at": "2024-04-16T09:03:32+00:00",
        "closed_at": "2024-04-16T09:03:32+00:00",
        "comments_count": [
            "yunyaoXYY",
            "xlvy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1674,
        "title": "rknpu2中rkyolov5部分格式图片及视频出现无法识别的情况",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 板端编译 fastdeploy-linux-aarch64-rk3588-0.0.0\r\n- 【编译命令】cmake ..  -DENABLE_ORT_BACKEND=ON \\\r\n\t-DENABLE_RKNPU2_BACKEND=ON \\\r\n\t-DENABLE_VISION=ON \\\r\n\t-DRKNN2_TARGET_SOC=RK3588 \\\r\n        -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy_sdk \\\r\n\t-DOPENCV_DIRECTORY=/usr/lib/aarch64-linux-gnu/cmake/opencv4\r\n- 【系统平台】: Linux aarch64(Ubuntu 20.04)\r\n- 【硬件】： rk3588-rknpu2\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\npng图片无法识别，jpeg可以识别，mp4视频无法识别",
        "state": "closed",
        "user": "MarKKwan27149",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T03:10:08+00:00",
        "updated_at": "2024-04-02T06:40:04+00:00",
        "closed_at": "2024-04-02T06:40:04+00:00",
        "comments_count": [
            "jiangjiajun",
            "MarKKwan27149",
            "Zheng-Bicheng",
            "MarKKwan27149",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MarKKwan27149",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1676,
        "title": "RK3588 python  RuntimeError: FastDeploy initalized failed!",
        "body": "cd FastDeploy/examples/vision/segmentation/paddleseg/semantic_segmentation/rockchip/rknpu2/python\r\n\r\npython3 infer.py --model_file ./Portrait_PP_HumanSegV2_Lite_256x144_infer/Portrait_PP_HumanSegV2_Lite_256x144_infer_rk3588.rknn                 --config_file ./Portrait_PP_HumanSegV2_Lite_256x144_infer/deploy.yaml                 --image images/portrait_heng.jpg\r\nTraceback (most recent call last):\r\n  File \"/home/firefly/.local/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /home/firefly/.local/lib/python3.9/site-packages/fastdeploy/libs/fastdeploy_main.cpython-39-aarch64-linux-gnu.so: undefined symbol: _ZTIN10fastdeploy6vision14classification6ResNetE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/firefly/projects/FastDeploy/examples/vision/segmentation/paddleseg/semantic_segmentation/rockchip/rknpu2/python/infer.py\", line 14, in <module>\r\n    import fastdeploy as fd\r\n  File \"/home/firefly/.local/lib/python3.9/site-packages/fastdeploy/__init__.py\", line 53, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/home/firefly/.local/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n",
        "state": "closed",
        "user": "jiangq195",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T06:17:20+00:00",
        "updated_at": "2024-03-26T06:39:48+00:00",
        "closed_at": "2024-03-26T06:39:48+00:00",
        "comments_count": [
            "DefTruth",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1681,
        "title": "使用镜像起容器，然后git代码后，跑示例出错",
        "body": "1、\r\ndocker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.4-21.10\r\n2、\r\ndocker run -it --net=host --name fastdeploy --gpus all --shm-size='10g' registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10 /bin/bash\r\n\r\n3、\r\n容器内git clone https://github.com/PaddlePaddle/FastDeploy.git\r\n\r\n4、之后跑yolov8示例代码，\r\n\r\ncpu报错如下：\r\nroot@b385c568c32b:/FastDeploy/examples/vision/detection/yolov8/python# python3 infer.py --model yolov8.onnx --image 000000014439.jpg --device cpu\r\n[ERROR] fastdeploy/fastdeploy_model.cc(191)::CreateCpuBackend   Found no valid backend for model: yolov8\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov8/yolov8.cc(40)::Initialize    Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 45, in <module>\r\n    model = fd.vision.detection.YOLOv8(args.model, runtime_option=runtime_option)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/vision/detection/contrib/yolov8.py\", line 181, in __init__\r\n    assert self.initialized, \"YOLOv8 initialize failed.\"\r\nAssertionError: YOLOv8 initialize failed.\r\n\r\ngpu报错如下：\r\nroot@b385c568c32b:/FastDeploy/examples/vision/detection/yolov8/python# python3 infer.py --model yolov8.onnx --image 000000014439.jpg --device gpu\r\n[ERROR] fastdeploy/fastdeploy_model.cc(214)::CreateGpuBackend   Cannot find an available gpu backend to load this model.\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov8/yolov8.cc(40)::Initialize    Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 45, in <module>\r\n    model = fd.vision.detection.YOLOv8(args.model, runtime_option=runtime_option)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/vision/detection/contrib/yolov8.py\", line 181, in __init__\r\n    assert self.initialized, \"YOLOv8 initialize failed.\"\r\nAssertionError: YOLOv8 initialize failed.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "liuyiche",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T09:48:21+00:00",
        "updated_at": "2024-12-03T06:42:49+00:00",
        "closed_at": "2024-12-03T06:42:49+00:00",
        "comments_count": [
            "DefTruth",
            "liuyiche",
            "liuyiche",
            "fanruifeng",
            "fanruifeng",
            "heliqi",
            "xzk-seu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1677,
        "title": "使用 cuda-python、tensorrt 推理FastDeploy生成的trt模型，结果为inf",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：1.0.4\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 3060TI， CUDA 11.7.1 CUDNN 8.4.1 TensorRT 8.4.2.4\r\n- 【编译语言】： Python 3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【将FastDeploy库中的tensorrt替换为8.4.2.4版本】\r\n- 【编写推理脚本】\r\n- - 使用TensorRT官方例子进行推理\r\n- - https://github.com/NVIDIA/TensorRT/blob/release/8.6/samples/python/efficientdet/infer.py\r\n- 【输出推理结果】\r\n- - 输出138行`outputs = self.infer(batch)`的值如下\r\n- - ```\r\n    ==>> outputs:  [array([300, 300, 300, 300, 300, 300]), array([[5.        , 0.9999976 ,        inf,        inf,        inf,\r\n                   inf],\r\n           [5.        , 0.9999738 ,        inf,        inf,        inf,\r\n                   inf],\r\n           [5.        , 0.9999561 ,        inf,        inf,        inf,\r\n                   inf],\r\n           ...,\r\n           [0.        , 0.04501155,        inf,       -inf,        inf,\r\n                   inf],\r\n           [0.        , 0.04495454,        inf,        inf,        inf,\r\n                   inf],\r\n           [0.        , 0.04481488,       -inf,       -inf,        inf,\r\n                   inf]], dtype=float32)]\r\n\r\n",
        "state": "closed",
        "user": "laugh12321",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-22T07:08:49+00:00",
        "updated_at": "2024-04-16T09:03:34+00:00",
        "closed_at": "2024-04-16T09:03:34+00:00",
        "comments_count": [
            "DefTruth",
            "laugh12321"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1682,
        "title": "FastDploy 运行遇到trt_backend的报错。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-windows-gpu-1.0.4\r\n- 【编译命令】通过官网的pip命令 pip install numpy opencv-python fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： Nvidia GPU 3060TI， CUDA 11.7 CUDNN 8.4\r\n- 【编译语言】：Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n在运行FastDeploy时遇到以下的报错。\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(232)::fastdeploy::FDTrtLogger::log  1: [stdArchiveReader.cpp::nvinfer1::rt::StdArchiveReader::StdArchiveReader::42] Error Code 1: Serialization (Serialization assertion stdVersionRead == serializationVersion failed.Version tag does not match. Note: Current Version: 232, Serialized Engine Version: 205)\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(232)::fastdeploy::FDTrtLogger::log  4: [runtime.cpp::nvinfer1::Runtime::deserializeCudaEngine::66] Error Code 4: Internal Error (Engine deserialization failed.)\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(82)::fastdeploy::TrtBackend::LoadTrtCache   Failed to call deserializeCudaEngine().\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(291)::fastdeploy::TrtBackend::InitFromOnnx  Failed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime/runtime.cc(349)::fastdeploy::Runtime::CreateTrtBackend       Failed to initialize TensorRT backend.\r\n",
        "state": "closed",
        "user": "xiaoyao123456789",
        "closed_by": "xiaoyao123456789",
        "created_at": "2023-03-22T11:00:51+00:00",
        "updated_at": "2023-04-13T06:51:09+00:00",
        "closed_at": "2023-03-23T05:24:33+00:00",
        "comments_count": [
            "DefTruth",
            "laugh12321",
            "xiaoyao123456789",
            "laugh12321",
            "zx-lhb",
            "laugh12321"
        ],
        "labels": [
            "Windows x64",
            "TensorRT"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1685,
        "title": "Fastdeploy二次开发推理报段错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-x64-gpu-1.0.4/\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】:CentOS Linux release 7.9.2009 (Core) x86\r\n- 【硬件】：Nvidia GPU Tesla P4， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ gcc 8.2\r\n\r\n## 问题日志及出现问题的操作流程\r\n1 操作步骤\r\n按照fastdeploy的sdk中ppocr的sample配置好cmakelists,正常运行没问题，后面自己修改代码，将代码封装成初始化，检测接口时候，在其他函数调用detect就会报段错误，麻烦工程师看看是哪里的问题\r\n[test_ocr.tar.gz](https://github.com/PaddlePaddle/FastDeploy/files/11046076/test_ocr.tar.gz)\r\n\r\n\r\n",
        "state": "closed",
        "user": "pangchao-git",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T01:38:12+00:00",
        "updated_at": "2024-04-16T09:03:35+00:00",
        "closed_at": "2024-04-16T09:03:35+00:00",
        "comments_count": [
            "DefTruth",
            "pangchao-git",
            "pangchao-git"
        ],
        "labels": [
            "OCR",
            "Linux x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1687,
        "title": "Linux FastDeploy docker OCR推理报错  ",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.4-cpu-only-21.10\r\n- 【系统平台】: Linux x64 (Ubuntu 18.04) \r\n- 【硬件】： 16  Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz\r\n- 【编译语言】： Python3.8\r\n\r\n按照https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/fastdeploy_serving文档进行操作。\r\n\r\nserver端，执行fastdeployserver --model-repository=/ocr_serving/models 启动服务成功 ，日志如下：\r\nI0323 02:13:45.461891 1362 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nI0323 02:13:45.562215 1362 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0323 02:13:45.662454 1362 model_repository_manager.cc:1022] loading: det_runtime:1\r\nI0323 02:13:45.699268 1362 fastdeploy_runtime.cc:1182] TRITONBACKEND_Initialize: fastdeploy\r\nI0323 02:13:45.699309 1362 fastdeploy_runtime.cc:1191] Triton TRITONBACKEND API version: 1.6\r\nI0323 02:13:45.699326 1362 fastdeploy_runtime.cc:1196] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0323 02:13:45.699348 1362 fastdeploy_runtime.cc:1225] backend configuration:\r\n{}\r\nI0323 02:13:45.699429 1362 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0323 02:13:45.702743 1362 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (CPU device 0)\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0323 10:13:45.716488  1363 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\nI0323 02:13:45.762764 1362 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0323 02:13:45.863142 1362 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0323 02:13:45.963409 1362 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0323 02:13:46.030319 1362 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\nI0323 02:13:46.030720 1362 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0323 02:13:46.032259 1362 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0323 02:13:46.063790 1362 model_repository_manager.cc:1022] loading: rec_runtime:1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0323 02:13:46.460128 1362 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (CPU device 0)\r\nI0323 02:13:46.460377 1362 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0323 02:13:46.933564 1362 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nI0323 02:13:46.933794 1362 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0323 02:13:47.301404 1362 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0323 02:13:47.301628 1362 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0323 02:13:47.691956 1362 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\nI0323 02:13:47.692282 1362 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0323 02:13:47.693223 1362 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0323 02:13:48.052532 1362 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (CPU device 0)\r\nI0323 02:13:48.052826 1362 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0323 02:13:48.440503 1362 model_repository_manager.cc:1183] successfully loaded 'rec_runtime' version 1\r\nI0323 02:13:48.440929 1362 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0323 02:13:48.541282 1362 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0323 02:13:48.641614 1362 model_repository_manager.cc:1022] loading: rec_pp:1\r\nI0323 02:13:48.741941 1362 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0323 02:13:48.741959 1362 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0323 02:13:48.742065 1362 model_repository_manager.cc:1183] successfully loaded 'rec_pp' version 1\r\nI0323 02:13:48.742231 1362 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0323 02:13:48.742308 1362 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0323 02:13:48.742378 1362 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| cls_pp          | 1       | READY  |\r\n| cls_runtime     | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| det_runtime     | 1       | READY  |\r\n| pp_ocr          | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n| rec_pp          | 1       | READY  |\r\n| rec_runtime     | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0323 02:13:48.742484 1362 tritonserver.cc:1920] \r\n+----------------------------------+------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                    |\r\n+----------------------------------+------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                   |\r\n| server_version                   | 2.15.0                                                                                   |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_po |\r\n|                                  | licy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data stat |\r\n|                                  | istics                                                                                   |\r\n| model_repository_path[0]         | /ocr_serving/models                                                                      |\r\n| model_control_mode               | MODE_NONE                                                                                |\r\n| strict_model_config              | 1                                                                                        |\r\n| rate_limit                       | OFF                                                                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                |\r\n| response_cache_byte_size         | 0                                                                                        |\r\n| min_supported_compute_capability | 0.0                                                                                      |\r\n| strict_readiness                 | 1                                                                                        |\r\n| exit_timeout                     | 30                                                                                       |\r\n+----------------------------------+------------------------------------------------------------------------------------------+\r\n\r\nI0323 02:13:48.743927 1362 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0323 02:13:48.744215 1362 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0323 02:13:48.785501 1362 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\nW0323 02:14:32.827865 1362 pinned_memory_manager.cc:133] failed to allocate pinned system memory: no pinned memory pool, falling back to non-pinned system memory\r\n\r\n----------------------------------------------------------------------------------\r\nclient端，执行python3 client.py报错：\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 112, in <module>\r\n    scores[i_box], '  bbox=', bboxes[i_box])\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n![image](https://user-images.githubusercontent.com/16531170/227084004-dce8a326-ba93-43d2-87e2-100cdd61ec32.png)\r\n\r\n将rec_texts、rec_scores、det_bboxes结果打印都为空\r\n![image](https://user-images.githubusercontent.com/16531170/227113712-3896b132-adfe-4664-9095-e7fcd4a6c14d.png)\r\n![image](https://user-images.githubusercontent.com/16531170/227113781-8e53589d-26ff-4b47-847c-8bde08dd0a02.png)\r\n",
        "state": "closed",
        "user": "xiaomi0922",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T02:25:00+00:00",
        "updated_at": "2025-06-24T06:46:15+00:00",
        "closed_at": "2025-06-24T06:46:15+00:00",
        "comments_count": [
            "yunyaoXYY",
            "yunyaoXYY",
            "kerry-weic",
            "yunyaoXYY",
            "kerry-weic",
            "polarisunny",
            "bltcn",
            "hanzhy-code",
            "tianv"
        ],
        "labels": [
            "OCR"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1688,
        "title": "关于使用Pyton多进程轻量服务化部署PP-OCRv3模型",
        "body": "**如题，我看到了文档，但是不知要如何修改代码https://github.com/PaddlePaddle/FastDeploy/blob/develop/tutorials/multi_thread/python/pipeline/README_CN.md**\r\n\r\n**之前我是根据[PP-OCRv3 Python轻量服务化部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCRv3/python/serving/README_CN.md)这个文档部署的，但是要用多线程的话不知如何修改使其能够服务化部署，可以给点修改建议吗，万分感谢！**",
        "state": "closed",
        "user": "minboo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T03:18:38+00:00",
        "updated_at": "2024-03-26T06:39:49+00:00",
        "closed_at": "2024-03-26T06:39:49+00:00",
        "comments_count": [
            "yunyaoXYY",
            "minboo",
            "yunyaoXYY"
        ],
        "labels": [
            "OCR"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1689,
        "title": "rv1126使用官方例子测试，跑的很慢",
        "body": "D [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"conv2d\"\r\nD [compute_node:377]Instance node[135] \"SWISH\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"swish\"\r\nD [compute_node:377]Instance node[136] \"CONV2D\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"conv2d\"\r\nD [compute_node:377]Instance node[137] \"SWISH\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"swish\"\r\nD [compute_node:377]Instance node[138] \"CONCAT\" ...\r\nD [compute_node:377]Instance node[139] \"CONV2D\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"conv2d\"\r\nD [compute_node:377]Instance node[140] \"SWISH\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"swish\"\r\nD [compute_node:377]Instance node[141] \"CONV2D\" ...\r\nD [vsi_nn_kernel_selector:1265]Instance OPENVX node with kernel \"conv2d\"\r\n[3  3/23  3: 8:50.156 ...r/src/driver/verisilicon_timvx/engine.cc:191 Build] Build the tim-vx graph success.\r\nD [_check_swapped_tensors:114]Check swapped tensors\r\n[3  3/23  3: 8:50.221 ...r/src/driver/verisilicon_timvx/engine.cc:258 Execute] Process cost 59198 us\r\n[3  3/23  3: 8:50.235 ...le-Lite/lite/kernels/nnadapter/engine.cc:248 Execute] Process cost 78457 us\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n103.668625,45.436493, 128.724014, 95.971954, 0.871636, 0\r\n265.735657,81.108414, 300.017883, 173.137909, 0.839909, 0\r\n156.409607,81.675583, 198.970245, 165.704285, 0.836028, 0\r\n379.059906,40.282700, 395.753448, 82.530655, 0.829662, 0\r\n361.590851,56.148071, 382.801788, 115.459198, 0.769370, 0\r\n503.740662,117.393051, 593.712830, 270.002625, 0.755905, 0\r\n328.305634,40.057297, 344.999176, 78.257736, 0.735399, 0\r\n415.107574,88.961212, 505.079742, 286.959686, 0.706305, 0\r\n580.929382,111.280991, 615.211609, 203.310486, 0.697555, 0\r\n3.109375,149.812500, 39.140625, 172.250000, 0.573872, 24\r\n351.475525,44.292389, 368.169067, 94.520966, 0.534139, 0\r\n186.300156,45.818710, 200.014877, 60.994644, 0.510922, 0\r\n54.859375,154.390625, 98.328125, 174.109375, 0.506969, 24\r\n167.062500,84.718750, 395.984375, 343.468750, 0.498233, 33\r\n168.894302,47.093414, 177.465988, 62.093231, 0.445955, 0\r\n25.062500,117.843750, 58.796875, 153.296875, 0.405319, 24\r\n68.062500,124.187500, 104.093750, 154.468750, 0.388880, 56\r\n3.796875,134.453125, 41.875000, 154.171875, 0.304446, 24\r\n65.421875,133.500000, 88.937500, 155.093750, 0.297584, 24\r\n\r\nPreprocess time: 21782.973000 ms\r\nPostprocess time: 44861.748000 ms\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n[I  3/23  3: 8:50.615 ...r/src/driver/verisilicon_timvx/driver.cc:89 DestroyProgram] Destroy program for verisilicon_timvx.\r\n\r\n测试log如上，加入了Postprocess  time 看了推理的运行时间非常慢，整个例子跑完整差不多1分钟才跑完，不知道是那一步没弄好，都是照着官方教程部署的",
        "state": "closed",
        "user": "16lwzheng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T03:19:10+00:00",
        "updated_at": "2024-09-24T06:41:11+00:00",
        "closed_at": "2024-09-24T06:41:11+00:00",
        "comments_count": [
            "DefTruth",
            "16lwzheng",
            "16lwzheng",
            "yeliang2258",
            "16lwzheng",
            "yeliang2258",
            "16lwzheng",
            "yeliang2258",
            "16lwzheng",
            "yeliang2258",
            "16lwzheng",
            "yeliang2258",
            "16lwzheng",
            "16lwzheng",
            "erroot"
        ],
        "labels": [
            "rv1126"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1691,
        "title": "我们先具体看下哈，方便贴一些具体的报错信息吗？",
        "body": "              我们先具体看下哈，方便贴一些具体的报错信息吗？\r\n\r\n_Originally posted by @DefTruth in https://github.com/PaddlePaddle/FastDeploy/issues/1685#issuecomment-1480616582_\r\n            \r\n![图片](https://user-images.githubusercontent.com/59760495/227120289-53f882ab-21a7-4784-82f3-a83ba991833a.png)\r\n [ERROR] fastdeploy/runtime/runtime.cc(197)::Infer\tDevice id of input tensor(32661) and runtime(0) are not same. 有这一句报错\r\n ",
        "state": "closed",
        "user": "pangchao-git",
        "closed_by": "pangchao-git",
        "created_at": "2023-03-23T06:21:11+00:00",
        "updated_at": "2023-03-23T06:24:48+00:00",
        "closed_at": "2023-03-23T06:24:48+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1690,
        "title": "相同环境下使用fastdeploy在c++部署官方yolov8和直接python原生运行官方yolov8推理速度相差较大",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-x64-gpu-1.0.4\r\n- 【编译命令】下载的官方预编译库\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia GPU 2060， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n【性能问题】相同环境下使用fastdeploy在c++部署官方yolov8和直接python原生运行官方yolov8推理速度相差较大\r\n- 数据集：图片尺寸是640*480\r\n- 模型：yolov8n.pt, yolov8n.onnx\r\n- 代码： 对数据集每张图片都进行推理，取后80%求平均时间\r\n```python\r\nfrom ultralytics import YOLO\r\nimport time\r\nimport os\r\n\r\n# Load a model\r\nmodel = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\r\n\r\n# read files in rgbd_dataset_freiburg1_xyz/rgb folder\r\n# and save them in a list\r\n\r\npath= \"rgbd_dataset_freiburg3_walking_xyz/rgb\"\r\nfiles = os.listdir(path)\r\n\r\ntotal=0\r\ncount=0\r\n\r\n# infer each image in the list\r\nfor file in files:\r\n    start = time.time()\r\n    results = model(path + \"/\" + file)    \r\n    end = time.time()\r\n\r\n    # sum the later 80% of the time\r\n    if count > len(files)*0.2:\r\n        total += end-start\r\n\r\n    count += 1\r\n\r\nprint(\"Average time: \", total/(len(files)*0.8))\r\n```\r\n计算出来Average time:  0.018936545871050727\r\n\r\n然后是fastdeploy端计算\r\n```c++\r\nvoid GpuInfer(const std::string& model_file, cv::Mat im) {\r\n  auto option = fastdeploy::RuntimeOption();\r\n  option.UseGpu();\r\n  auto model = fastdeploy::vision::detection::YOLOv8(model_file, \"\", option);\r\n  if (!model.Initialized()) {\r\n    std::cerr << \"Failed to initialize.\" << std::endl;\r\n    return;\r\n  }\r\n\r\n  fastdeploy::vision::DetectionResult res;\r\n  if (!model.Predict(im, &res)) {\r\n    std::cerr << \"Failed to predict.\" << std::endl;\r\n    return;\r\n  }\r\n\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc < 4) {\r\n    std::cout << \"Usage: infer_demo path/to/model path/to/image run_option, \"\r\n                 \"e.g ./infer_model ./yolov8s.onnx ./test.jpeg 0\"\r\n                 \"e.g ./infer_model ./yolov8s.onnx ./dataset 1\"\r\n              << std::endl;\r\n    std::cout << \"The data type of run_option is int, 0: run with cpu; 1: run \"\r\n                 \"with gpu; 2: run with gpu and use tensorrt backend.\"\r\n              << std::endl;\r\n    return -1;\r\n  }\r\n\r\n  // record file names in a directory\r\n  vector<string> filename;\r\n\r\n  // get file names in a directory\r\n  filename = GetFilesInDir(argv[2]);\r\n\r\n  cv::Mat im;\r\n\r\n  double total = 0;\r\n  for (int i = 2; i < filename.size(); i++) {\r\n    string image_file = argv[2] + filename[i];\r\n    im = cv::imread(image_file);\r\n\r\n    if (std::atoi(argv[3]) == 0) {\r\n        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();\r\n        CpuInfer(argv[1], im);\r\n        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();\r\n\r\n        double tdetect= std::chrono::duration_cast<std::chrono::duration<double> >(t2 - t1).count();\r\n        // sum infer time of later 80% images\r\n        if (i > filename.size() * 0.2) {\r\n            total += tdetect;\r\n        }\r\n  } else if (std::atoi(argv[3]) == 1) {\r\n\r\n            std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();\r\n            GpuInfer(argv[1], im);\r\n            std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();\r\n\r\n            double tdetect= std::chrono::duration_cast<std::chrono::duration<double> >(t2 - t1).count();\r\n            // sum infer time of later 80% images\r\n        if (i > filename.size() * 0.2) {\r\n            total += tdetect;\r\n        }\r\n\r\n    //GpuInfer(argv[1], argv[2]);\r\n  } else if (std::atoi(argv[3]) == 2) {\r\n            std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();\r\n\r\n            TrtInfer(argv[1], im);\r\n            std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();\r\n\r\n            double tdetect= std::chrono::duration_cast<std::chrono::duration<double> >(t2 - t1).count();\r\n\r\n            // sum infer time of later 80% images\r\n        if (i > filename.size() * 0.2) {\r\n            total += tdetect;\r\n        }\r\n\r\n    //TrtInfer(argv[1], argv[2]);\r\n    }\r\n\r\n  }\r\n\r\n  // calculate average infer time\r\n  double avg = total / (filename.size() * 0.8);\r\n  std::cout << \"Average infer time: \" << avg << \" s\" << std::endl;\r\n\r\n  return 0;\r\n}\r\n```\r\n计算出来Average infer time: 0.293134 s，足足慢了一个数量级\r\n我想问一下是我哪里弄错了吗？",
        "state": "closed",
        "user": "SimonWXW",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T05:23:07+00:00",
        "updated_at": "2024-03-26T06:39:50+00:00",
        "closed_at": "2024-03-26T06:39:50+00:00",
        "comments_count": [
            "yunyaoXYY",
            "DefTruth",
            "SimonWXW",
            "DefTruth"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1692,
        "title": "yolov8 rknn 部署",
        "body": "可以使用yolov8官网模型转rknn然后部署吗",
        "state": "closed",
        "user": "zhu709654396",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-23T08:34:22+00:00",
        "updated_at": "2024-01-24T13:42:00+00:00",
        "closed_at": "2023-03-25T13:14:27+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "zhu709654396",
            "Zheng-Bicheng",
            "zhu709654396",
            "Zheng-Bicheng",
            "zhu709654396",
            "ilbash"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1695,
        "title": "fastdeploy是否有人脸识别相关模型支持BM1684？",
        "body": "我看好像没有人脸相关的模型支持算丰",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T11:16:11+00:00",
        "updated_at": "2024-04-16T09:03:37+00:00",
        "closed_at": "2024-04-16T09:03:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1698,
        "title": "建议增加语音类:文字转语音,语音转文字",
        "body": "建议增加语音类:\r\n文字转语音\r\n语音转文字",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-24T02:27:22+00:00",
        "updated_at": "2024-04-16T09:03:38+00:00",
        "closed_at": "2024-04-16T09:03:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1694,
        "title": "jetson orin上如何部署fastdeploy?模型初始化失败",
        "body": "我现在用jetson orin agx, 已经刷上jetpack 5.1, 参考jetson 编译fd并安装。https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/jetson.md\r\n\r\n![image](https://user-images.githubusercontent.com/112528302/229712623-85e57089-ba6f-4a70-8cf4-ba45c50abc75.png)\r\n![image](https://user-images.githubusercontent.com/112528302/229712766-3962486b-408c-46e6-8d93-67688ebd4dc7.png)\r\n\r\n尝试部署失败\r\n![image](https://user-images.githubusercontent.com/112528302/229712846-25399438-42d4-4e6a-90f6-713f6815e6ce.png)\r\n\r\n\r\n模型调用代码如下：\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\nmodel_path = \"rice_frcnn\"\r\nmodel_file = os.path.join(model_path, \"model.pdmodel\")\r\nparams_file = os.path.join(model_path, \"model.pdiparams\")\r\ninfer_cfg_file = os.path.join(model_path, \"infer_cfg.yml\")\r\n\r\noption = fd.RuntimeOption()\r\noption.use_gpu()\r\n\r\nmodel = fd.vision.detection.FasterRCNN(model_file, params_file, infer_cfg_file, option)\r\n\r\nim = cv2.imread(\"image/uva0005.jpg\")\r\n\r\nresult = model.predict(im)\r\nprint(result)\r\n",
        "state": "closed",
        "user": "cunjing56",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-23T10:12:52+00:00",
        "updated_at": "2024-04-16T09:03:36+00:00",
        "closed_at": "2024-04-16T09:03:36+00:00",
        "comments_count": [
            "DefTruth",
            "cunjing56",
            "cunjing56",
            "leiqing1",
            "cunjing56"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1697,
        "title": "rk3588  PP_LiteSeg_T_STDC1_cityscapes_without_argmax_infer 没分割出效果",
        "body": "链接: https://pan.baidu.com/s/1_Vwb-zpgfOhVSmurRW4uxA?pwd=vkww 提取码: vkww \r\n--来自百度网盘超级会员v5的分享\r\n\r\n能帮我跑不跑吗，看看是不是我模型转换的问题，采用的官方pbmodel-onnx-rknn",
        "state": "closed",
        "user": "jiangq195",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-24T00:47:21+00:00",
        "updated_at": "2024-03-26T06:39:50+00:00",
        "closed_at": "2024-03-26T06:39:50+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1704,
        "title": "部署ernie模型报错",
        "body": "*********************************************\r\n按照ernie-3.0部署案例，下载提供的模型和参数，执行命令。fastdeplpyserver服务端可以启动。客户端调用报错。\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.4\r\n- 【编译命令】# 进入serving目录执行脚本编译fastdeploy和服务化的backend\r\n        cd serving\r\n        bash scripts/build_fd_cuda_11_2.sh\r\n        \r\n        # 退出到FastDeploy主目录，制作镜像\r\n        cd ../\r\n        docker build -t paddlepaddle/fastdeploy:1.0.3-gpu-cuda11.2-trt8.4-21.10 -f serving/Dockerfile_CUDA_11_2 .\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 按照ernie-3.0部署案例，下载提供的模型和参数，执行命令。fastdeplpyserver服务端可以启动。客户端调用报错。\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，服务端可以启动。客户端调用报错。\r\n- -服务端启动截图如下：\r\n-`I0325 00:58:57.080611 1493 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0325 00:58:57.080752 1493 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0325 00:58:57.121637 1493 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002`\r\n![1679706012207](https://user-images.githubusercontent.com/38588261/227671560-d2d72c20-c58f-4ba1-94cf-dda3c8ac193b.png)\r\n![1679706041246](https://user-images.githubusercontent.com/38588261/227671671-ed45c607-3256-4321-898b-7f61169fd387.png)\r\n\r\n- -客户端请求截图报错如下：\r\n`Traceback (most recent call last):\r\n  File \"seq_cls_grpc_client.py\", line 146, in <module>\r\n    result = runner.Run([text])\r\n  File \"seq_cls_grpc_client.py\", line 85, in Run\r\n    results = self._client.infer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 1446, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 76, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Socket closed\r\n`\r\n![1679706094760](https://user-images.githubusercontent.com/38588261/227671901-7b78ca4b-750d-4364-b15d-1d454838f7f3.png)\r\n\r\n- -客户端请求时，服务端截图报错如下：\r\n- `I0325 00:58:57.080611 1493 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0325 00:58:57.080752 1493 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0325 00:58:57.121637 1493 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\nterminate called after throwing an instance of 'ov::Exception'\r\n  what():  Using Blob on external nullptr memory\r\nSignal (6) received.\r\n 0# 0x000055D6A915E3A9 in fastdeployserver\r\n 1# 0x00007F32FEF13090 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# gsignal in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# abort in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 4# 0x00007F32FF2CC911 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 5# 0x00007F32FF2D838C in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 6# 0x00007F32FF2D83F7 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# 0x00007F32FF2D86A9 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 8# 0x00007F32D10CEF8F in /opt/fastdeploy/third_libs/install/openvino/runtime/lib/libopenvino.so\r\n 9# fastdeploy::OpenVINOBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.4\r\n10# fastdeploy::Runtime::Infer() in /opt/fastdeploy/lib/libfastdeploy_runtime.so.1.0.4\r\n11# 0x00007F32FC051BC4 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n12# 0x00007F32FC055456 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n13# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n14# 0x00007F32FFA0801A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n15# 0x00007F32FFA0882D in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n16# 0x00007F32FF8C2F91 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n17# 0x00007F32FFA025A7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n18# 0x00007F32FF304DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n19# 0x00007F32FF782609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n20# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\nAborted (core dumped)\r\n0325 01:00:59.617412 1551 pb_stub.cc:777] Non-graceful termination detected. 0325 01:00:59.617415 1505 pb_stub.cc:777] Non-graceful termination detected. root@2b3149713bf0:/home/FastDeploy/examples/text/ernie-3.0/serving# \r\n`\r\n![1679706164656](https://user-images.githubusercontent.com/38588261/227672218-b569017b-5ee9-4fd8-b286-be809f972139.png)\r\n这个是什么原因？该怎么解决",
        "state": "closed",
        "user": "LouisHeck",
        "closed_by": "LouisHeck",
        "created_at": "2023-03-25T01:03:20+00:00",
        "updated_at": "2023-11-21T08:37:16+00:00",
        "closed_at": "2023-03-27T09:10:51+00:00",
        "comments_count": [
            "ted8201",
            "huangjun11"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1705,
        "title": "PPOCRv3  不能重复调用问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：  Nvidia GPU 1070TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++\r\n\r\n\r\nauto det_model = fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, option); auto cls_model = fastdeploy::vision::ocr::Classifier(cls_model_file, cls_params_file, option); auto rec_model = fastdeploy::vision::ocr::Recognizer(rec_model_file, rec_params_file, rec_label_file, option);  auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);          每次运行PPOCRv3时，都需要新建det_model，cls_model，rec_model（而这3个变量在创建过程中，是读取对应的Model文件入内存，这个过程费时间啊）。否则，第2次调用PPOCRv3时就会崩。          我的本意是：程序在初始化时，对det_model，cls_model，rec_model初始化一次即可，识别时只需调用PPOCRv3。          不明白为什么这么设计？还是设计有问题，没考虑到这种情况？ \r\n\r\n",
        "state": "closed",
        "user": "zhenhuamo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-25T01:26:57+00:00",
        "updated_at": "2024-03-26T06:39:51+00:00",
        "closed_at": "2024-03-26T06:39:51+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1706,
        "title": "在AIStudio中报错undefined symbol: dnnl_layer_normalization_v2_forward_desc_init",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\nAIStudio， 新建一个项目， paddle是2.4.1\r\n\r\n## 问题日志及出现问题的操作流程\r\n新建AiStudio， 安装FastDeploy， import fastdeploy就报错\r\n```py\r\naistudio@jupyter-227854-5797577:~$ pip list | grep paddle\r\npaddle-bfloat                  0.1.7\r\npaddle2onnx                    1.0.0\r\npaddlefsl                      1.1.0\r\npaddlehub                      2.3.0\r\npaddlenlp                      2.4.2\r\npaddlepaddle-gpu               2.4.1.post112\r\ntb-paddle                      0.3.6\r\nx2paddle                       1.4.0\r\naistudio@jupyter-227854-5797577:~$ pip install numpy opencv-python fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nLooking in links: https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (1.23.5)\r\nRequirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (4.6.0.66)\r\nCollecting fastdeploy-gpu-python\r\n  Downloading https://bj.bcebos.com/fastdeploy/release/wheels/fastdeploy_gpu_python-1.0.5-cp39-cp39-manylinux1_x86_64.whl (1353.4 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 410.0 kB/s eta 0:00:00\r\nRequirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastdeploy-gpu-python) (5.1.2)\r\nRequirement already satisfied: wheel in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastdeploy-gpu-python) (0.36.2)\r\nRequirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastdeploy-gpu-python) (4.64.1)\r\nCollecting fastdeploy-tools>=0.0.5\r\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/27/527bb120f18b72d79b280bc7a810e67a00e0a0a4a273d4543feede0e812f/fastdeploy_tools-0.0.5-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: fastapi in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastdeploy-gpu-python) (0.90.1)\r\nRequirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastdeploy-gpu-python) (2.24.0)\r\nCollecting uvicorn==0.16.0\r\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a0/68/effcc7c39d79fbae86ad621ebcf28d63a3b9c111a885bc2b74c576ba8dc4/uvicorn-0.16.0-py3-none-any.whl (54 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.9/54.9 kB 1.0 MB/s eta 0:00:00\r\nRequirement already satisfied: h11>=0.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from uvicorn==0.16.0->fastdeploy-tools>=0.0.5->fastdeploy-gpu-python) (0.14.0)\r\nCollecting asgiref>=3.4.0\r\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/29/38d10a47b322a77b2d12c2b79c789f52956f733cb701d4d5157c76b5f238/asgiref-3.6.0-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: click>=7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from uvicorn==0.16.0->fastdeploy-tools>=0.0.5->fastdeploy-gpu-python) (8.0.4)\r\nRequirement already satisfied: starlette<0.24.0,>=0.22.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastapi->fastdeploy-gpu-python) (0.23.1)\r\nRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from fastapi->fastdeploy-gpu-python) (1.10.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from requests->fastdeploy-gpu-python) (1.25.11)\r\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from requests->fastdeploy-gpu-python) (2019.9.11)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from requests->fastdeploy-gpu-python) (3.0.4)\r\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from requests->fastdeploy-gpu-python) (2.8)\r\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi->fastdeploy-gpu-python) (4.3.0)\r\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from starlette<0.24.0,>=0.22.0->fastapi->fastdeploy-gpu-python) (3.6.1)\r\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.24.0,>=0.22.0->fastapi->fastdeploy-gpu-python) (1.3.0)\r\nInstalling collected packages: asgiref, uvicorn, fastdeploy-tools, fastdeploy-gpu-python\r\n  Attempting uninstall: uvicorn\r\n    Found existing installation: uvicorn 0.20.0\r\n    Uninstalling uvicorn-0.20.0:\r\n      Successfully uninstalled uvicorn-0.20.0\r\nSuccessfully installed asgiref-3.6.0 fastdeploy-gpu-python-1.0.5 fastdeploy-tools-0.0.5 uvicorn-0.16.0\r\n\r\n[notice] A new release of pip available: 22.1.2 -> 23.0.1\r\n[notice] To update, run: pip install --upgrade pip\r\naistudio@jupyter-227854-5797577:~$ python\r\nPython 3.9.16 (main, Jan 11 2023, 16:05:54) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import fastdeploy\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages/fastdeploy/libs/third_libs/paddle_inference/paddle/lib/libpaddle_inference.so: undefined symbol: dnnl_layer_normalization_v2_forward_desc_init\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages/fastdeploy/__init__.py\", line 53, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.9/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n>>> \r\naistudio@jupyter-227854-5797577:~$ conda --version\r\nconda 4.10.1\r\naistudio@jupyter-227854-5797577:~$ which python\r\n/opt/conda/envs/python35-paddle120-env/bin/python\r\naistudio@jupyter-227854-5797577:~$ which pip\r\n/opt/conda/envs/python35-paddle120-env/bin/pip\r\naistudio@jupyter-227854-5797577:~$ python --version\r\nPython 3.9.16\r\n```\r\n",
        "state": "closed",
        "user": "magicly",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-25T02:38:32+00:00",
        "updated_at": "2024-04-16T06:40:25+00:00",
        "closed_at": "2024-04-16T06:40:25+00:00",
        "comments_count": [
            "jiangjiajun",
            "clSpider",
            "jiangjiajun",
            "clSpider",
            "ncoll",
            "clSpider",
            "Toefinder",
            "CrazyStone6"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1707,
        "title": "不支持yolov5模型的openvino加速吗？",
        "body": "如题，trt加速没问题，想在cpu设备中部署，openvino是不能用么？",
        "state": "closed",
        "user": "kankanjiuzou123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-25T15:15:01+00:00",
        "updated_at": "2024-04-16T09:03:39+00:00",
        "closed_at": "2024-04-16T09:03:39+00:00",
        "comments_count": [
            "DefTruth",
            "kankanjiuzou123",
            "kankanjiuzou123",
            "DefTruth",
            "kankanjiuzou123",
            "kankanjiuzou123",
            "kankanjiuzou123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1710,
        "title": "VisualDL请求时报错",
        "body": "*********************************************\r\nVisualDL请求时报错\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.5\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.4 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- fastdeploy容器中模型服务启动正常，容器内调用正常\r\n- \r\n![1679881675701](https://user-images.githubusercontent.com/38588261/227821694-092907fa-b5e3-436b-b4fc-7ebb41fa12f5.png)\r\n![1679881832190](https://user-images.githubusercontent.com/38588261/227821934-f0a2d038-3aba-4a69-9357-7b15df914978.png)\r\n\r\n- 【VisualDL打开客户端时，提交服务请求报错】\r\n- - \r\n![1679881854891](https://user-images.githubusercontent.com/38588261/227821990-d920d694-4519-44a2-864f-27c6e77bad56.png)\r\n![1679881878438](https://user-images.githubusercontent.com/38588261/227822015-d527e4a3-06a5-4e14-ae8c-015471c9c4f6.png)\r\n\r\n",
        "state": "closed",
        "user": "LouisHeck",
        "closed_by": "LouisHeck",
        "created_at": "2023-03-27T01:51:25+00:00",
        "updated_at": "2023-03-27T09:10:28+00:00",
        "closed_at": "2023-03-27T09:10:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1709,
        "title": "多线程模式 VS2019 Release X64 编译  提示错误：std::invoke”: 未找到匹配的重载函数",
        "body": "## 环境\r\nWIN10 VS2019\r\n- 【FastDeploy版本】： 如fastdeploy-linux-gpu-1.0.4\r\n- 【编译命令】VS2019  Release X64\r\n- 【系统平台】:  Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： Nvidia GPU 1660TI， CUDA 11.2 CUDNN 8\r\n- 【编译语言】： C++ \r\n\r\n## 问题\r\n我参考c++ 多线程模式示例代码 如下所示   在 VS2019 Release X64 编译\r\n编译提示两个错误：\r\n C2672“std::invoke”: 未找到匹配的重载函数\r\n C2893\t未能使函数模板“unknown-type std::invoke(_Callable &&,_Types &&...) noexcept(<expr>)”专用化\r\n这两个错误 应该均与：语句  \r\n     void Predict(fastdeploy::vision::detection::YOLOv7End2EndTRT* model, int thread_id) \r\n和 threads.emplace_back(Predict, models[i].get(), i); 有关 ； model的类型不对么？\r\n我该怎么做呢？\r\n另外：auto model = fastdeploy::vision::detection::YOLOv7End2EndTRT(model_file, \"\", option);中\r\nmodel的具体类型是什么呢，实例代码不用auto这种 对我们用户而言会更好。\r\n\r\n谢谢   期待您的回答！\r\n\r\n代码：\r\n```c++\r\nvoid Predict(fastdeploy::vision::detection::YOLOv7End2EndTRT* model, int thread_id) {\r\n    auto im = cv::imread(\"D:\\\\images\\\\1.bmp\");\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model->Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n    // print res\r\n    std::cout << \"Thread Id: \" << thread_id << std::endl;\r\n    std::cout << res.Str() << std::endl;\r\n}\r\n\r\n\r\nvoid TrtInfer(const std::string& model_file, int thread_num) {\r\n    std::string cachefile = model_file.substr(0, model_file.find_last_of(\"\\\\/\")) + \"/yolov7x.trt\";\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n    option.SetTrtCacheFile(cachefile);\r\n    option.SetTrtInputShape(\"images\", { 1, 3, 640, 640 });\r\n    auto model = fastdeploy::vision::detection::YOLOv7End2EndTRT(model_file, \"\", option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n    std::vector<decltype(model.Clone())> models;\r\n    for (int i = 0; i < thread_num; ++i) {\r\n        models.emplace_back(std::move(model.Clone()));\r\n    }\r\n    std::vector<std::thread> threads;\r\n    for (int i = 0; i < thread_num; ++i) {\r\n        threads.emplace_back(Predict, models[i].get(), i); \r\n       //std::thread thread(Predict,  models[i].get(), i); \r\n    }\r\n    for (int i = 0; i < thread_num; ++i) {\r\n        threads[i].join();\r\n    }\r\n}\r\n\r\nint main(int argc, char** argv) {   \r\n    TrtInfer(\"D:\\\\best.onnx\", 2);        \r\n    return 0;\r\n}\r\n```\r\n\r\n",
        "state": "closed",
        "user": "xzxjysgd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-26T14:29:03+00:00",
        "updated_at": "2024-04-16T09:03:41+00:00",
        "closed_at": "2024-04-16T09:03:41+00:00",
        "comments_count": [
            "jiangjiajun",
            "xzxjysgd",
            "xzxjysgd"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1708,
        "title": "在cpu端部署目标检测模型，实时检测并且cpu占用不那么高，推荐哪个模型？",
        "body": "fastdeploy目前支持的目标检测模型，哪一个符合在cpu端占用率不那么高，并且能实时检测呢？支持trt的测试过了，效果不错，就是部署在cpu端占用率太高了。。",
        "state": "closed",
        "user": "kankanjiuzou123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-26T03:37:12+00:00",
        "updated_at": "2024-04-16T09:03:40+00:00",
        "closed_at": "2024-04-16T09:03:40+00:00",
        "comments_count": [
            "DefTruth",
            "kankanjiuzou123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1713,
        "title": "UIE服务化部署按照文档来，启动服务错报No server context available. Exiting immediately",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.4-cpu-only-21.10\r\n- 【系统平台】: Linux x64\r\n\r\n执行：CUDA_VISIBLE_DEVICES=0 fastdeployserver --model-repository=/uie_serving/models --backend-config=python,shm-default-byte-size=10485760\r\n\r\n配置文件：\r\n![image](https://user-images.githubusercontent.com/50578167/227849229-5f67bda2-331b-40b6-87d3-53a9832eddef.png)\r\n\r\n\r\n结果：\r\n![image](https://user-images.githubusercontent.com/50578167/227849140-cbdb92e1-4ce0-444d-9e9c-2108a11a2bc8.png)\r\n\r\n",
        "state": "closed",
        "user": "zongzijiayou",
        "closed_by": "zongzijiayou",
        "created_at": "2023-03-27T05:30:41+00:00",
        "updated_at": "2023-03-27T05:40:06+00:00",
        "closed_at": "2023-03-27T05:39:45+00:00",
        "comments_count": [
            "zongzijiayou"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1714,
        "title": "NLP UIE服务化按照例子报错，启动不起来",
        "body": "结果：![image](https://user-images.githubusercontent.com/50578167/227851303-493e900a-dab4-4c98-88ea-3d0c62eff671.png)",
        "state": "closed",
        "user": "zongzijiayou",
        "closed_by": "zongzijiayou",
        "created_at": "2023-03-27T05:46:20+00:00",
        "updated_at": "2023-05-16T09:44:30+00:00",
        "closed_at": "2023-05-16T09:44:30+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1715,
        "title": "分类模型得出的结果python是什么类型？",
        "body": "ClassifyResult(\r\nlabel_ids: 153, 204, \r\nscores: 0.686230, 0.095325, \r\n)\r\n如上，python需要怎么解析这个结果？字典还是数组？",
        "state": "closed",
        "user": "intjun",
        "closed_by": "intjun",
        "created_at": "2023-03-27T08:37:02+00:00",
        "updated_at": "2023-03-28T01:16:59+00:00",
        "closed_at": "2023-03-28T01:16:59+00:00",
        "comments_count": [
            "jiangjiajun",
            "intjun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1718,
        "title": "在armv7上执行yolov7的推理demo，推理速度很慢，设置option.SetCpuThreadNum(8)之后速度没有提升",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 最新下载\r\n- 【编译命令】\r\nANDROID_NDK=\"*/android-ndk-r22b\"\r\n# Setting up Android toolchanin\r\nANDROID_ABI=armeabi-v7a  # 'arm64-v8a', 'armeabi-v7a'\r\nANDROID_PLATFORM=\"android-21\"  # API >= 21\r\nANDROID_STL=c++_shared  # 'c++_shared', 'c++_static'\r\nANDROID_TOOLCHAIN=clang  # 'clang' only\r\nTOOLCHAIN_FILE=${ANDROID_NDK}/build/cmake/android.toolchain.cmake\r\n\r\n# Create build directory\r\nBUILD_ROOT=build/Android\r\nBUILD_DIR=${BUILD_ROOT}/${ANDROID_ABI}-api-21\r\nFASDEPLOY_INSTALL_DIR=\"./install\"\r\nmkdir build && mkdir ${BUILD_ROOT} && mkdir ${BUILD_DIR}\r\ncd ${BUILD_DIR}\r\n\r\n# CMake configuration with Android toolchain\r\ncmake -DCMAKE_TOOLCHAIN_FILE=${TOOLCHAIN_FILE} \\\r\n      -DCMAKE_BUILD_TYPE=Release \\\r\n      -DANDROID_ABI=${ANDROID_ABI} \\\r\n      -DANDROID_NDK=${ANDROID_NDK} \\\r\n      -DANDROID_STATIC_LIB=ON \\\r\n      -DANDROID_PLATFORM=${ANDROID_PLATFORM} \\\r\n      -DANDROID_STL=${ANDROID_STL} \\\r\n      -DANDROID_TOOLCHAIN=${ANDROID_TOOLCHAIN} \\\r\n      -DENABLE_LITE_BACKEND=ON \\\r\n      -DENABLE_VISION=ON \\\r\n      -DWITH_ANDROID_OPENMP=ON \\\r\n      -DWITH_LITE_STATIC=ON \\\r\n      -DCMAKE_INSTALL_PREFIX=${FASDEPLOY_INSTALL_DIR} \\\r\n      -Wno-dev ../../..\r\n\r\n# Build FastDeploy Android C++ SDK\r\nmake -j8\r\nmake install  \r\n- 【系统平台】: armv7\r\n- 【硬件】： SDM450\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n\r\n- 【性能问题】描述清楚对比的方式\r\n\r\n./infer_paddle_model_demo ./yolov7_infer/ ./images/test.jpg  0\r\nWARNING: linker: /data/mnntest/install/infer_paddle_model_demo: unsupported flags DT_FLAGS_1=0x8000001\r\n[I  1/ 1 12:18:23.854 ...oid/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: MODEL NAME     : ARMV7 PROCESSOR REV 4 (V7L)\r\nHARDWARE        : QUALCOMM TECHNOLOGIES, INC SDM450\r\n_QC_REFERENCE_PHONE_MSM8953_\r\n[I  1/ 1 12:18:23.854 ...oid/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 8\r\n[I  1/ 1 12:18:23.854 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.854 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.854 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 4, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 5, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 6, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 7, max freq: 1804, min freq: 1804, cluster ID: 0, CPU ARCH: A53\r\n[I  1/ 1 12:18:23.855 ...oid/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is:\r\n。。。。。。。。\r\n[FastDeploy][INFO] fastdeploy/runtime/runtime.cc(321)::CreateLiteBackend        Runtime initialized with Backend::PDLITE in Device::CPU.\r\nframes inference time is : 36341ms\r\nframes inference time is : 26378ms\r\nframes inference time is : 23049.7ms\r\nframes inference time is : 21373ms\r\nframes inference time is : 20375.8ms\r\nframes inference time is : 19711.3ms\r\nframes inference time is : 19237.3ms\r\nframes inference time is : 18874ms\r\nframes inference time is : 18595.9ms\r\nframes inference time is : 18371.2ms\r\nframes inference time is : 18190.5ms\r\nframes inference time is : 18036.7ms\r\nframes inference time is : 17907.8ms\r\nframes inference time is : 17798.1ms\r\nframes inference time is : 17702.5ms\r\nframes inference time is : 17617.5ms\r\nframes inference time is : 17543.6ms\r\n\r\n查看cpu及内存使用情况，看着好像没有使用多线程处理\r\nPID USER     PR  NI CPU% S  #THR     VSS     RSS PCY Name\r\n 4281 root     20   0  12% R     8 1495960K 1080856K  fg ./infer_paddle_model_demo\r\n\r\nc++程序应用paddlelite后端，设置：\r\noption.SetCpuThreadNum(8)，但是推理时没有生效。\r\n请问需要如何配置才能够使用多线程推理一张图片？",
        "state": "closed",
        "user": "yy2yy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-27T11:46:29+00:00",
        "updated_at": "2024-04-02T06:40:04+00:00",
        "closed_at": "2024-04-02T06:40:04+00:00",
        "comments_count": [
            "DefTruth",
            "yy2yy",
            "DefTruth",
            "yy2yy",
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1720,
        "title": "Jetson Xavier NX编译fastdeploy（C++方式）出现的Makefile:155: recipe for target 'all' failed错误？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-03-27T15:02:49+00:00",
        "updated_at": "2023-03-27T15:07:09+00:00",
        "closed_at": "2023-03-27T15:07:09+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1723,
        "title": "利用jetson xavier nx进行gpu推理时出现的标注框坐标信息不正确问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【编译命令】按照官方文档编译均正常通过\r\n- 【系统平台】: linux aarch64\r\n- 【硬件】： Jetson Xavier NX\r\n- 【编译语言】： C++ \r\n\r\n## 编译日志\r\n\r\n> racobit@ubuntu:/code/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build$ cmake .. -DFASTDEPLOY_INSTALL_DIR=/code/FastDeploy-develop/build/installed_fastdeploy\r\n-- The path of ONNXRuntime is /code/FastDeploy-develop/build/installed_fastdeploy/third_libs/install/onnxruntime/lib.\r\n-- The path of OpenCV is /usr/lib/aarch64-linux-gnu/cmake/opencv4/.\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.3\r\n--   CMake command             : /usr/local/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 7.5.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : \r\n--   Paddle Inference version  : \r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : \r\n--   DEPENDENCY_LIBS           : /code/FastDeploy-develop/build/installed_fastdeploy/lib/libfastdeploy.so;/code/FastDeploy-develop/build/installed_fastdeploy/third_libs/install/onnxruntime/lib/libonnxruntime.so;/code/FastDeploy-develop/build/installed_fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so;/usr/local/cuda/lib64/libcudart.so;/usr/lib/aarch64-linux-gnu/libnvinfer.so;/usr/lib/aarch64-linux-gnu/libnvonnxparser.so;/usr/lib/aarch64-linux-gnu/libnvinfer_plugin.so;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_gapi;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_stitching;opencv_video;opencv_videoio\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /code/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build\r\nracobit@ubuntu:/code/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build$ make -j6\r\n[ 11%] Built target infer_faster_rcnn_demo\r\n[ 15%] Built target infer_picodet_demo\r\n[ 11%] Built target infer_ppyolo_demo\r\n[ 18%] Built target infer_ppyoloe_demo\r\n[ 22%] Built target infer_yolov3_demo\r\n[ 27%] Built target infer_yolox_demo\r\n[ 40%] Built target infer_yolov6_demo\r\n[ 40%] Built target infer_mask_rcnn_demo\r\n[ 40%] Built target infer_yolov7_demo\r\n[ 45%] Built target infer_yolov5_demo\r\n[ 50%] Built target infer_ssd_demo\r\n[ 54%] Built target infer_yolov8_demo\r\n[ 59%] Built target infer_pssdet_demo\r\n[ 63%] Built target infer_retinanet_demo\r\n[ 68%] Built target infer_rtmdet_demo\r\n[ 72%] Built target infer_cascadercnn_demo\r\n[ 77%] Built target infer_ppyoloesod_demo\r\n[ 81%] Built target infer_fcos_demo\r\n[ 86%] Built target infer_ttfnet_demo\r\n[ 90%] Built target infer_gfl_demo\r\n[ 95%] Built target infer_tood_demo\r\n[100%] Built target infer_solov2_demo\r\n\r\n## 推理\r\n\r\n> racobit@ubuntu:/code/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build$ ./infer_ppyolo_demo ./ppyoloe_crn_l_300e_coco 000000014439.jpg 1\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n13898.647461,13700.485352, 13480.095703, 13656.628906, 193984.859375, 13\r\n15223.071289,14371.417969, 14240.288086, 14372.537109, 193389.593750, 13\r\n20341.392578,20145.931641, 19902.355469, 19841.552734, 193099.578125, 13\r\n19473.654297,18915.876953, 19870.466797, 19952.236328, 192906.562500, 13\r\n31814.914062,30404.287109, 29300.904297, 29522.035156, 191642.906250, 13\r\n26997.312500,25853.669922, 24952.451172, 25008.501953, 191502.937500, 13\r\n30498.230469,30924.185547, 30884.046875, 30803.703125, 191392.109375, 13\r\n27584.742188,27503.730469, 27396.683594, 27495.009766, 191079.734375, 13\r\n21547.396484,22370.921875, 25992.945312, 27877.283203, 190866.890625, 13\r\n30439.546875,29831.458984, 30313.572266, 28312.304688, 190423.406250, 13\r\n32198.195312,32421.955078, 32793.164062, 32994.964844, 190161.828125, 13\r\n21414.781250,20354.615234, 19666.558594, 19925.208984, 190006.812500, 13\r\n13625.150391,13021.408203, 12378.142578, 12331.798828, 189377.781250, 13\r\n25677.876953,26048.498047, 26115.388672, 26104.296875, 189002.796875, 13\r\n28794.261719,29354.263672, 29385.160156, 29601.931641, 188828.359375, 13\r\n22987.541016,24989.455078, 29988.046875, 32435.644531, 188248.000000, 13\r\n29778.185547,28432.160156, 27330.152344, 27598.195312, 187418.468750, 13\r\n29528.527344,28896.271484, 29468.162109, 28319.058594, 187280.390625, 13\r\n32171.158203,32080.980469, 32243.804688, 32294.431641, 186153.609375, 13\r\n14850.257812,14260.273438, 13478.571289, 13681.903320, 185442.515625, 13\r\n13920.094727,13955.944336, 14459.372070, 14743.318359, 185237.687500, 13\r\n32243.443359,30864.849609, 29808.902344, 30182.681641, 184386.765625, 13\r\n14321.480469,14394.779297, 14449.907227, 14910.231445, 183700.781250, 13\r\n22440.632812,21600.630859, 20924.839844, 21315.976562, 183693.843750, 13\r\n22244.919922,22103.265625, 21933.626953, 21810.199219, 183622.765625, 13\r\n29376.863281,29448.640625, 29908.953125, 30149.955078, 183144.250000, 13\r\n21697.945312,21948.792969, 22532.181641, 22842.246094, 182862.875000, 13\r\n25511.740234,23739.689453, 20872.332031, 14458.041992, 181975.578125, 13\r\n18012.617188,19265.476562, 21063.886719, 22227.515625, 181494.578125, 13\r\n15188.744141,14721.481445, 16305.268555, 18863.392578, 181483.437500, 13\r\n22199.544922,22649.578125, 22698.787109, 22992.330078, 179672.640625, 13\r\n8038.464355,17059.880859, 20867.835938, 22556.156250, 179134.187500, 13\r\n4714.025391,4741.491699, 4906.731934, 5296.706543, 177679.687500, 13\r\n9722.058594,9828.709961, 7532.137207, 5474.010254, 177620.281250, 13\r\n2601.830566,1974.712646, 3847.697021, 8294.219727, 177438.500000, 13\r\n9173.607422,20385.585938, 27213.267578, 29874.705078, 177264.531250, 13\r\n9689.443359,8432.454102, 5981.833984, 4633.600098, 176434.046875, 13\r\n7819.274902,7506.649902, 7316.188965, 7708.558105, 176411.968750, 13\r\n4780.824219,3718.646240, 3167.021240, 3029.998779, 176066.656250, 13\r\n4018.216064,3797.640137, 3731.134521, 4166.378906, 175985.421875, 13\r\n5968.611328,5579.180664, 7428.213867, 10920.863281, 175892.609375, 13\r\n7660.858398,6781.486816, 6361.066406, 6317.595215, 175044.140625, 13\r\n4607.604980,4638.335938, 4586.110840, 4971.461914, 174796.296875, 13\r\n22876.210938,20966.773438, 18211.232422, 12933.809570, 174757.140625, 13\r\n5112.660156,4374.933105, 3941.956543, 3688.937988, 173217.812500, 13\r\n3425.313232,3476.123291, 3871.743164, 4685.706055, 172414.078125, 13\r\n5645.903809,5626.342285, 5949.887207, 6859.405762, 171276.687500, 13\r\n7900.479980,7758.585449, 7624.594727, 7771.371094, 168931.203125, 13\r\n4580.891113,4593.938965, 4437.044922, 4349.182617, 167603.828125, 13\r\n3989.389893,3974.966064, 6673.581055, 8058.235840, 167265.656250, 13\r\n2824.516602,2720.520264, 2835.480469, 3175.279297, 167083.593750, 13\r\n3126.473633,1648.429077, 348.638336, -181.619766, 166723.171875, 13\r\n4191.312988,4409.237793, 4394.823242, 4279.939941, 166294.765625, 13\r\n4225.671387,-896.405945, 710.061401, 3214.664551, 165784.921875, 13\r\n4736.207031,4712.288086, 4659.015625, 4471.085938, 165230.578125, 13\r\n9991.475586,11837.979492, 10300.063477, 8374.154297, 163841.734375, 13\r\n4429.994141,4548.304199, 4507.631348, 4501.939941, 163710.671875, 13\r\n3500.218018,5890.133789, 4790.619629, 3440.960693, 163587.859375, 13\r\n7676.885742,7886.293945, 7885.084961, 8036.494629, 163015.703125, 13\r\n1393.687622,1310.825073, 1525.445068, 1644.343872, 162468.656250, 13\r\n3563.475098,2505.392334, 3612.191162, 7299.216309, 162072.218750, 13\r\n4476.540527,4385.434082, 4883.394531, 5849.995605, 161274.671875, 13\r\n3455.616455,3207.266602, 5897.636230, 7213.094238, 160904.343750, 13\r\n8272.943359,8134.401855, 8427.199219, 9346.506836, 160759.890625, 13\r\n7912.761230,9130.726562, 7568.887695, 6025.791504, 160490.000000, 13\r\n5956.010254,4737.101562, 4014.242188, 4033.297119, 160269.515625, 13\r\n4389.528809,4575.666016, 4437.704590, 4232.725098, 160072.562500, 13\r\n3699.661133,-1258.043823, -308.127716, 1759.021362, 159922.921875, 13\r\n5526.851074,5728.687500, 5601.500977, 5519.760742, 159887.062500, 13\r\n4952.292969,5020.840820, 5305.654297, 5576.982422, 159468.875000, 13\r\n3883.264160,4112.115723, 4003.116699, 3827.663330, 159436.843750, 13\r\n1393.166748,843.551208, 840.693420, 1181.263428, 158851.265625, 13\r\n3125.893066,3043.614258, 3001.035645, 2934.010742, 158815.546875, 13\r\n9483.300781,8228.990234, 7414.280762, 7417.906738, 158548.921875, 13\r\n4096.106445,4123.383301, 4122.114258, 4058.400635, 158532.296875, 13\r\n3979.698975,2703.444092, 3370.949219, 6089.224609, 158438.421875, 13\r\n11560.252930,10569.053711, 6741.463379, 5607.081055, 158370.578125, 13\r\n1300.080444,1460.765747, 1266.749268, 1462.301514, 158102.890625, 13\r\n1411.491943,1094.314453, 3314.490967, 8698.593750, 157850.140625, 13\r\n1985.436523,2090.101318, 2041.599121, 2087.573486, 157146.703125, 13\r\n12533.707031,12232.023438, 12027.949219, 11958.957031, 156564.859375, 13\r\n6424.125977,5471.043457, 4730.838379, 4578.938965, 156253.328125, 13\r\n571.276855,435.096527, 604.596436, 797.280334, 155804.296875, 13\r\n4132.422363,4176.175293, 4308.234375, 4405.379883, 155669.671875, 13\r\n3077.294434,3187.060791, 3155.485840, 2949.782227, 155579.046875, 13\r\n3846.345947,2926.302002, 3479.747803, 2778.665771, 155472.140625, 13\r\n4603.502930,4579.434570, 4350.989746, 4062.548340, 154564.281250, 13\r\n-1714.711792,-1147.932251, -1068.645020, -1003.182251, 154371.781250, 13\r\n4877.096191,4951.938965, 4912.200195, 4666.720703, 154305.171875, 13\r\n5814.209961,5917.836426, 5746.365234, 6107.844727, 154144.296875, 13\r\n5762.262207,5781.888672, 6022.663574, 6552.638672, 154113.750000, 13\r\n5691.342285,5530.376465, 5742.901367, 6374.024902, 153703.843750, 13\r\n4277.651855,5050.553223, 2535.943115, 1160.318970, 153664.671875, 13\r\n6875.808105,4807.097168, 2387.601807, 221.465866, 153456.437500, 13\r\n1620.881958,847.276917, 632.063538, 625.579224, 153246.750000, 13\r\n-2634.234863,-2767.277588, -2745.648682, -2420.542236, 152924.109375, 13\r\n-37396.105469,-35936.425781, -34531.648438, -34240.207031, 155231.593750, 14\r\n-38882.531250,-38530.734375, -38434.765625, -38006.929688, 153686.625000, 14\r\n-42642.613281,-41115.851562, -40493.796875, -40634.957031, 152535.890625, 14\r\n5831.250977,5953.833496, 7276.950684, 8275.652344, 152534.062500, 14\r\n-41379.535156,-41197.406250, -39487.917969, -33535.375000, 152421.515625, 14\r\n-62958.894531,-63352.546875, -65209.042969, -59523.847656, 227447.546875, 16\r\n-31338.445312,-32020.705078, -38220.886719, -41913.359375, 227196.171875, 16\r\n-36567.285156,-40480.027344, -50350.917969, -56405.562500, 227056.187500, 16\r\n-16513.328125,-13025.593750, -16370.399414, -20268.681641, 226806.359375, 16\r\n-56862.843750,-56186.632812, -55404.042969, -55276.839844, 226059.000000, 16\r\n-20407.585938,-19470.097656, -18829.689453, -18990.697266, 226005.328125, 16\r\n-61637.921875,-63505.828125, -63174.191406, -62785.402344, 225295.890625, 16\r\n-12451.416992,-13393.524414, -21429.353516, -30735.996094, 225172.671875, 16\r\n-12742.541016,-11865.483398, -14277.686523, -16886.443359, 224307.921875, 16\r\n-32092.902344,-32727.388672, -31843.878906, -31000.994141, 223841.828125, 16\r\n-16704.322266,-16769.267578, -16225.403320, -16410.978516, 223162.671875, 16\r\n-20935.681641,-21455.070312, -19739.384766, -18845.660156, 222325.890625, 16\r\n-14565.995117,-14828.361328, -13917.414062, -12885.697266, 222220.343750, 16\r\n-85159.960938,-84792.125000, -84419.484375, -84208.750000, 221647.625000, 16\r\n-67040.921875,-63412.511719, -60411.640625, -59730.042969, 221422.859375, 16\r\n-50996.023438,-60158.230469, -75976.890625, -84178.937500, 221405.171875, 16\r\n-85457.859375,-85316.070312, -84045.710938, -70864.500000, 220962.125000, 16\r\n-69118.984375,-68628.632812, -68320.109375, -67987.726562, 220677.281250, 16\r\n-12909.364258,-12454.557617, -12094.150391, -12408.454102, 219477.343750, 16\r\n-14271.840820,-15755.812500, -15306.893555, -14623.423828, 219186.203125, 16\r\n-19194.609375,-19478.462891, -18461.109375, -17226.326172, 218668.234375, 16\r\n-82559.898438,-78817.460938, -76401.007812, -76567.390625, 218504.343750, 16\r\n-83325.835938,-84839.343750, -85148.468750, -85015.656250, 218293.125000, 16\r\n-21901.384766,-44129.652344, -60805.703125, -68594.718750, 216822.187500, 16\r\n695.087463,-2979.242920, -9215.611328, -12858.825195, 216671.812500, 16\r\n-55810.578125,-53427.179688, -50230.390625, -48971.285156, 216563.156250, 16\r\n-79320.429688,-81430.695312, -81480.492188, -80984.492188, 215599.000000, 16\r\n-16556.083984,-15497.284180, -17396.796875, -17689.681641, 215415.703125, 16\r\n-80674.960938,-78641.390625, -71043.312500, -45449.542969, 214598.656250, 16\r\n-51544.683594,-50432.714844, -50266.187500, -42663.871094, 214495.328125, 16\r\n-27373.003906,-31863.630859, -42332.687500, -44371.542969, 214045.453125, 16\r\n-17855.812500,-19153.923828, -19583.361328, -19058.947266, 212957.281250, 16\r\n-52075.500000,-52050.394531, -51679.585938, -51272.539062, 212882.187500, 16\r\n-35551.269531,-34696.273438, -35861.167969, -32124.173828, 212769.078125, 16\r\n-44335.828125,-44069.089844, -43910.230469, -43668.976562, 212674.046875, 16\r\n-36985.804688,-36469.738281, -35951.878906, -35535.453125, 212287.640625, 16\r\n-20038.085938,-20906.455078, -26233.703125, -26572.982422, 211411.437500, 16\r\n-36471.160156,-35423.300781, -32872.136719, -31568.029297, 210347.046875, 16\r\n-25899.039062,-25557.376953, -25331.138672, -25364.162109, 209376.078125, 16\r\n-42723.652344,-41027.957031, -40322.187500, -40853.281250, 209339.687500, 16\r\n-36690.296875,-35361.773438, -34548.914062, -36094.871094, 208775.343750, 16\r\n-38756.289062,-38418.640625, -38410.039062, -37655.097656, 208063.875000, 16\r\n-26902.199219,-31195.476562, -36596.078125, -39328.589844, 207115.890625, 16\r\n-24651.367188,-25300.121094, -32060.078125, -40033.425781, 206334.750000, 16\r\n-39506.800781,-40298.433594, -39159.152344, -39004.480469, 205796.125000, 16\r\n-11051.914062,-25772.333984, -36111.976562, -38780.332031, 205711.703125, 16\r\n-41769.535156,-40767.328125, -39832.902344, -39676.550781, 204864.812500, 16\r\n-86672.929688,-82363.289062, -80388.171875, -81042.820312, 204262.500000, 16\r\n-52410.843750,-50364.667969, -49063.753906, -49986.765625, 203086.031250, 16\r\n-39905.601562,-38144.773438, -33911.585938, -22309.955078, 202719.656250, 16\r\n-26419.167969,-22979.419922, -25227.498047, -27852.900391, 202295.656250, 16\r\n-26790.197266,-26706.466797, -25911.226562, -25691.517578, 202196.781250, 16\r\n-32141.490234,-33136.812500, -39946.976562, -44367.835938, 201444.859375, 16\r\n-39357.464844,-38688.613281, -38830.062500, -39356.066406, 201076.265625, 16\r\n-28578.552734,-28301.933594, -27565.257812, -27671.542969, 201039.828125, 16\r\n-25046.531250,-23608.841797, -22476.812500, -23020.927734, 200033.000000, 16\r\n-34155.476562,-34544.832031, -33953.035156, -33268.632812, 197919.921875, 16\r\n-35880.531250,-34690.734375, -33484.390625, -33573.914062, 195823.140625, 16\r\n-39783.175781,-39316.792969, -38536.539062, -38631.960938, 195795.000000, 16\r\n-37006.148438,-36591.750000, -35233.246094, -35214.269531, 195018.640625, 16\r\n-10874.956055,-23068.755859, -32641.376953, -36473.285156, 192930.921875, 16\r\n-27973.750000,-27260.283203, -25841.474609, -25916.302734, 192234.078125, 16\r\n-37680.425781,-36842.792969, -36247.484375, -28610.818359, 191305.328125, 16\r\n-25161.613281,-25696.138672, -26808.966797, -26665.589844, 188898.953125, 16\r\n-54667.988281,-54534.062500, -54858.425781, -53972.925781, 187669.421875, 16\r\n-37074.664062,-36226.960938, -35534.410156, -35447.164062, 187481.500000, 16\r\n-27508.865234,-35260.652344, -48903.246094, -51823.933594, 183894.546875, 16\r\n-50730.191406,-50547.726562, -50814.191406, -43491.875000, 183196.968750, 16\r\n-18699.330078,-24318.474609, -33074.101562, -34112.898438, 182689.609375, 16\r\n-88045.296875,-88178.843750, -88600.781250, -88593.562500, 181825.781250, 16\r\n-52417.261719,-52349.175781, -52684.285156, -52452.238281, 181782.265625, 16\r\n-19390.646484,-19533.535156, -22056.126953, -22730.767578, 181374.937500, 16\r\n-49703.121094,-62417.753906, -76930.367188, -83367.726562, 181349.593750, 16\r\n-51204.417969,-51861.996094, -50977.917969, -50708.890625, 181337.421875, 16\r\n-84802.914062,-85669.328125, -85938.554688, -85546.859375, 181207.515625, 16\r\n-34239.675781,-34641.480469, -35327.585938, -35257.144531, 181125.296875, 16\r\n-67510.429688,-67343.257812, -67935.945312, -63433.085938, 181102.109375, 16\r\n-18912.000000,-18409.781250, -18263.140625, -18827.464844, 179588.343750, 16\r\n-65995.992188,-66808.500000, -67037.859375, -67791.140625, 179117.031250, 16\r\n-51187.375000,-52912.925781, -52319.320312, -51478.941406, 178746.953125, 16\r\n-12928.995117,57.529343, 4254.352051, 4445.583496, 178162.984375, 16\r\n-50782.968750,-48662.453125, -47478.558594, -48444.312500, 178124.296875, 16\r\n-10669.397461,-12672.573242, -20398.699219, -26306.847656, 177647.265625, 16\r\n-34008.246094,-33142.804688, -33295.945312, -34201.425781, 177546.781250, 16\r\n-51950.656250,-52155.355469, -52592.636719, -52050.460938, 177401.390625, 16\r\n3715.738770,3001.647705, 2556.478760, 2401.717773, 177067.890625, 16\r\n-12354.378906,-33724.699219, -49127.039062, -52447.773438, 177021.078125, 16\r\n-66838.289062,-68004.703125, -68506.492188, -68295.539062, 176593.671875, 16\r\n-66462.390625,-63097.687500, -62558.492188, -63299.011719, 176113.171875, 16\r\n-18736.482422,-18317.832031, -17308.525391, -16318.678711, 175634.000000, 16\r\n-35538.671875,-35606.558594, -35472.031250, -35759.644531, 174472.578125, 16\r\n1768.296021,1688.812988, 2282.854736, 2193.408691, 174417.203125, 16\r\n-22168.349609,-44992.937500, -61303.261719, -66697.601562, 174364.906250, 16\r\n2409.331787,2033.816406, -3561.429443, -10646.742188, 174164.109375, 16\r\n-36354.714844,-35129.664062, -31722.919922, -21625.871094, 173959.828125, 16\r\n-12304.208008,-11882.375000, -11283.951172, -10776.797852, 173619.593750, 16\r\n-98.545700,1509.029297, 2297.047852, 2464.842773, 173430.750000, 16\r\n-3813.152344,4539.787109, 6562.916016, 8864.694336, 173172.218750, 16\r\n-50844.363281,-49145.082031, -47856.917969, -49007.871094, 172739.671875, 16\r\n-83019.546875,-78963.968750, -77841.898438, -79420.445312, 172668.437500, 16\r\n10001.346680,10295.473633, 9928.929688, 9268.927734, 171446.828125, 16\r\n-49816.527344,-50680.906250, -51184.570312, -51435.605469, 171327.140625, 16\r\n-33246.554688,-39328.746094, -46882.382812, -50087.980469, 171187.953125, 16\r\n-33880.558594,-35326.910156, -39824.519531, -45108.382812, 171127.546875, 16\r\n-18863.558594,-18032.683594, -17757.085938, -18051.482422, 171019.671875, 16\r\n425.756165,12035.913086, 15605.632812, 15128.802734, 170801.828125, 16\r\n13872.338867,12511.910156, 3335.059082, -10524.774414, 170734.765625, 16\r\n13475.225586,12525.557617, 12909.450195, 13359.282227, 170121.656250, 16\r\n3162.743408,4405.751465, 4788.428223, 4139.049316, 170075.437500, 16\r\n-50924.695312,-48961.058594, -43959.460938, -29436.900391, 169900.296875, 16\r\n-31760.820312,-32039.460938, -32334.632812, -33189.777344, 169628.609375, 16\r\n-24895.867188,-25166.599609, -24494.634766, -24340.800781, 169144.640625, 16\r\n-50152.000000,-48134.968750, -48110.750000, -48920.687500, 168738.312500, 16\r\n8499.689453,8985.041992, 9099.479492, 8582.291992, 168503.656250, 16\r\n13920.448242,15987.977539, 17267.056641, 17817.876953, 168054.687500, 16\r\n11066.199219,12133.128906, 12588.825195, 13231.596680, 167849.000000, 16\r\n-50990.707031,-51520.925781, -51426.898438, -51681.781250, 167811.656250, 16\r\n17814.759766,16044.738281, 7020.042969, -2732.133789, 167550.062500, 16\r\n-1033.361084,-339.661987, 165.070068, -499.025513, 166326.609375, 16\r\n-1240.120728,5812.250488, 8634.354492, 11498.666016, 166121.843750, 16\r\n11879.488281,10752.283203, 10750.751953, 10471.467773, 165687.093750, 16\r\n13912.175781,14500.424805, 14031.956055, 13395.024414, 165658.812500, 16\r\n16871.494141,17317.490234, 16665.832031, 15173.971680, 165235.562500, 16\r\n3214.949219,2457.115723, 802.897888, -411.559021, 165205.125000, 16\r\n12347.451172,10760.443359, 10167.660156, 10386.623047, 165166.000000, 16\r\n13122.442383,13876.491211, 13189.822266, 11777.089844, 165068.140625, 16\r\n7719.108398,11726.846680, 12712.692383, 15410.641602, 164463.562500, 16\r\n14178.711914,13824.266602, 13733.377930, 13346.789062, 164251.937500, 16\r\n8763.514648,10465.135742, 11516.847656, 11740.496094, 164019.218750, 16\r\n8839.049805,13627.394531, 14521.436523, 14505.140625, 163626.171875, 16\r\n52806.664062,53701.046875, 53693.449219, 52286.886719, 161806.687500, 16\r\n12023.015625,12378.150391, 10875.066406, 5735.793457, 161785.703125, 16\r\n18664.458984,43314.378906, 49142.667969, 50879.167969, 161766.687500, 16\r\n55655.992188,54500.343750, 43491.859375, 18305.152344, 161262.750000, 16\r\n-13887.250977,-13170.622070, -12061.070312, -12369.545898, 161146.921875, 16\r\n49437.261719,48111.964844, 38808.621094, 16057.142578, 161041.250000, 16\r\n50551.324219,49767.242188, 48219.667969, 49842.527344, 160148.453125, 16\r\n54574.574219,55533.492188, 55031.593750, 53900.250000, 160134.781250, 16\r\n-33099.296875,-31628.298828, -30574.591797, -30624.109375, 159950.171875, 16\r\n51439.328125,52351.066406, 54220.660156, 55220.992188, 159817.328125, 16\r\n46041.777344,46842.320312, 47975.949219, 48668.699219, 159121.406250, 16\r\n-25820.908203,-25857.955078, -25361.005859, -25154.263672, 158846.640625, 16\r\n9203.021484,6149.204102, 175.537735, -5902.675781, 158716.718750, 16\r\n-23540.212891,-22344.537109, -21049.847656, -20638.986328, 158371.765625, 16\r\n52369.726562,51867.226562, 49959.515625, 50778.558594, 158062.578125, 16\r\n10110.877930,9947.259766, 9272.744141, 8484.130859, 157898.625000, 16\r\n55767.269531,55782.402344, 50123.531250, 32113.220703, 157853.609375, 16\r\n-1007.453308,18467.064453, 25876.248047, 27583.677734, 157600.890625, 16\r\n26992.115234,26047.056641, 25754.607422, 25523.525391, 157520.406250, 16\r\n13074.844727,13031.335938, 11683.628906, 10220.663086, 157340.781250, 16\r\n-1211.190063,-917.369385, -897.124268, -948.859436, 157036.515625, 16\r\n26161.042969,27112.994141, 28118.443359, 29813.466797, 155606.375000, 16\r\n53904.277344,54832.417969, 54854.335938, 53860.562500, 155044.406250, 16\r\n44718.601562,44404.863281, 43035.382812, 44677.148438, 154428.500000, 16\r\n32707.324219,33445.179688, 32797.695312, 32519.089844, 154172.484375, 16\r\n9546.742188,24644.265625, 27647.328125, 30337.091797, 154007.218750, 16\r\n52163.425781,51298.601562, 49765.343750, 49987.464844, 153996.734375, 16\r\n46514.046875,46143.035156, 36211.351562, 12531.580078, 153935.078125, 16\r\n-538.410767,44.158096, -59.543961, 187.332504, 153681.484375, 16\r\n-18503.359375,-19059.419922, -19972.712891, -19767.062500, 153514.687500, 16\r\n132.384506,4.972870, -835.542053, -1804.552124, 153240.328125, 16\r\n-2526.751465,-3015.286377, -1235.369263, 1194.799805, 152688.203125, 16\r\n10490.351562,11104.155273, 11638.371094, 12714.098633, 152531.125000, 16\r\n594.828064,13669.524414, 18087.425781, 21223.433594, 152450.390625, 16\r\n42690.550781,43688.472656, 45336.039062, 46208.460938, 152384.375000, 16\r\n46571.039062,46893.593750, 46836.476562, 45769.835938, 152352.937500, 16\r\n3100.065674,-20061.816406, -8741.670898, 4974.727539, 186900.187500, 18\r\n1397.250000,-10821.012695, -14539.028320, 14100.660156, 185942.781250, 18\r\n-3291.076904,47282.585938, -16356.460938, -11690.355469, 185505.640625, 18\r\n-7704.313477,-22768.658203, -9591.928711, 1112.631470, 184421.296875, 18\r\n2526.373535,-6116.527344, -26183.955078, -12242.720703, 183787.875000, 18\r\n42866.207031,17069.367188, 15599.785156, 4196.897949, 183229.890625, 18\r\n15300.466797,-28049.527344, -2923.721924, 9387.245117, 183224.671875, 18\r\n-12158.714844,-11039.902344, 8038.727539, -24073.169922, 182275.000000, 18\r\n6446.124512,15385.427734, -5283.701172, -17362.917969, 182093.359375, 18\r\n7191.021484,-3052.382324, 11897.769531, -19372.701172, 179191.562500, 18\r\n-180.240662,-234.515427, -20243.763672, -14471.463867, 178918.500000, 18\r\n27188.833984,22323.935547, 13372.831055, -16359.061523, 178831.765625, 18\r\n4310.295410,-8977.172852, -13737.405273, 15083.664062, 178262.640625, 18\r\n8753.743164,-1653.117554, 18029.007812, 4071.813232, 176730.531250, 18\r\n-25137.244141,38138.410156, 3609.559814, 6691.149902, 176476.687500, 18\r\n-5468.118652,7129.317383, -27590.917969, 10223.685547, 173735.421875, 18\r\n31347.382812,14541.092773, -5036.220215, 34080.585938, 173131.171875, 18\r\n19083.005859,18717.517578, 6633.358887, 12776.686523, 172602.031250, 18\r\n-5649.342773,23967.962891, -30241.076172, -15135.023438, 170311.984375, 18\r\n-13403.245117,4516.817383, 6541.751465, -14586.978516, 170243.640625, 18\r\n5490.452637,-22208.572266, -20344.269531, -29073.433594, 169946.531250, 18\r\n-4203.848145,9895.790039, -4919.003418, 25516.050781, 168205.500000, 18\r\n-3191.564941,-7184.510254, -15239.272461, -17717.453125, 165919.484375, 18\r\n-18992.849609,-18917.134766, -18867.576172, -18602.980469, 165645.062500, 18\r\n-28741.968750,-28105.816406, -35046.164062, -17892.767578, 165329.265625, 18\r\n3925.588623,-14279.032227, -25452.386719, -29669.003906, 165285.031250, 18\r\n-44788.167969,-43390.367188, -46313.082031, -31853.716797, 164029.203125, 18\r\n-20823.650391,1108.862549, 10560.770508, 16781.500000, 163991.546875, 18\r\n-45958.988281,-45489.140625, -45758.199219, -45389.058594, 163859.171875, 18\r\n-29524.236328,-29093.695312, -29246.488281, -29092.257812, 163745.593750, 18\r\n-31442.544922,-28657.917969, -27156.189453, -29422.613281, 163488.812500, 18\r\n-31706.718750,-31699.304688, -31375.990234, -31071.521484, 162102.921875, 18\r\n-18230.955078,-22024.947266, 4892.232910, -11758.918945, 161667.578125, 18\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n## 推理\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-03-28T03:37:21+00:00",
        "updated_at": "2023-03-28T04:01:30+00:00",
        "closed_at": "2023-03-28T04:01:30+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1722,
        "title": "MacOSX编译运行examples/vision/segmentation/paddleseg/semantic_segmentation/cpu-gpu/cpp失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 当前github主线 (rev. b672e32daedb2d539984a43ac55d16fb6dde6234)\r\n- 【编译命令】\r\n```\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/usr/local/Cellar/opencv/4.7.0_2\r\n\r\nmake -j12\r\nmake install\r\n```\r\n\r\n- 【系统平台】:  Mac OSX intel(12.0)\r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n编译运行 examples/vision/segmentation/paddleseg/semantic_segmentation/cpu-gpu/cpp 报 segmentation fault\r\n\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【运行示例报错】\r\n```\r\ncd ~/ff_work/FastDeploy/build/compiled_fastdeploy_sdk\r\nsource ./fastdeploy_init.sh\r\n\r\ncd ~/ff_work/FastDeploy/examples/vision/segmentation/paddleseg/semantic_segmentation/cpu-gpu/cpp\r\nmkdir build && cd build\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=~/ff_work/FastDeploy/build/compiled_fastdeploy_sdk\r\nmake\r\n\r\nwget https://bj.bcebos.com/paddlehub/fastdeploy/PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer.tgz\r\ntar xvf PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer.tgz\r\n\r\nwget https://paddleseg.bj.bcebos.com/dygraph/demo/cityscapes_demo.png\r\n\r\n./infer_demo PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer cityscapes_demo.png 0\r\n\r\n[1]    43238 segmentation fault  ./infer_demo PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer  0\r\n\r\n```",
        "state": "closed",
        "user": "edwardzhou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-28T02:25:04+00:00",
        "updated_at": "2024-04-30T06:42:42+00:00",
        "closed_at": "2024-04-30T06:42:42+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "DefTruth",
            "edwardzhou",
            "edwardzhou",
            "Zheng-Bicheng",
            "edwardzhou",
            "edwardzhou",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "edwardzhou",
            "DefTruth",
            "edwardzhou",
            "Zheng-Bicheng",
            "edwardzhou",
            "edwardzhou",
            "edwardzhou",
            "hhxdestiny"
        ],
        "labels": [
            "Bug",
            "Mac OSX x86_64",
            "Semantic Segmentation"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1721,
        "title": "Jetson Xavier NX编译fastdeploy（C++方式）出现的Makefile:155: recipe for target 'all' failed错误？",
        "body": "运行日志如下：\r\nracobit@ubuntu:/code/FastDeploy-develop/build$ cmake .. -DBUILD_ON_JETSON=ON -DENABLE_VISION=ON -DENABLE_PADDLE_BACKEND=ON -DPADDLEINFERENCE_DIRECTORY=/code/paddle_inference_install_dir -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\n-- The C compiler identification is GNU 7.5.0\r\n-- The CXX compiler identification is GNU 7.5.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /code/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 16% complete]\r\n-- [download 21% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 42% complete]\r\n-- [download 47% complete]\r\n-- [download 52% complete]\r\n-- [download 57% complete]\r\n-- [download 63% complete]\r\n-- [download 68% complete]\r\n-- [download 73% complete]\r\n-- [download 78% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /code/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /code/FastDeploy-develop/build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\nCMake Warning (dev) at /usr/local/share/cmake-3.25/Modules/ExternalProject.cmake:3074 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/share/cmake-3.25/Modules/ExternalProject.cmake:4184 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:104 (ExternalProject_Add)\r\n  CMakeLists.txt:217 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found Python: /usr/bin/python3.6 (found version \"3.6.9\") found components: Interpreter Development Development.Module Development.Embed \r\n-- Copying /code/paddle_inference_install_dir to /code/FastDeploy-develop/build/third_libs/install/paddle_inference ...\r\n-- The CUDA compiler identification is NVIDIA 10.2.300\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /usr/local/cuda-10.2/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- CUDA compiler: /usr/local/cuda-10.2/bin/nvcc, version: NVIDIA 10.2.300\r\n-- CUDA detected: 10.2.300\r\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_53,code=sm_53 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_72,code=sm_72\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/lib/aarch64-linux-gnu/cmake/opencv4/\r\n-- Found OpenCV: /usr (found version \"4.1.1\") \r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/protobuf-linux-aarch64-3.16.0.tgz to /code/FastDeploy-develop/build/protobuf-linux-3.16.0.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 2% complete]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 7% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 13% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\n-- [download 17% complete]\r\n-- [download 18% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 23% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 29% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 34% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- [download 40% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 45% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 51% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 56% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 61% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 67% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 72% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 78% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 89% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 94% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /code/FastDeploy-develop/build/protobuf-linux-3.16.0.tgz ...\r\n-- Build type not set - defaulting to Release\r\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.17\") \r\n-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython2.7.so (found version \"2.7.17\") \r\n-- Found Protobuf: /code/FastDeploy-develop/build/third_libs/protobuf/lib/libprotobuf.a (found version \"3.16.0\") \r\nGenerated: /code/FastDeploy-develop/build/third_party/onnx/onnx/onnx_paddle2onnx-ml.proto\r\nGenerated: /code/FastDeploy-develop/build/third_party/onnx/onnx/onnx-operators_paddle2onnx-ml.proto\r\nGenerated: /code/FastDeploy-develop/build/third_party/onnx/onnx/onnx-data_paddle2onnx.proto\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version             : 3.25.3\r\n--   CMake command             : /usr/local/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 7.5.0\r\n--   CXX flags                 : -Wno-format -g0 -O3 -Wnon-virtual-dtor\r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;WITH_GPU;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX;MAX_ONNX_OPSET_VERSION=16;PADDLE2ONNX_LIB;ONNX_NAMESPACE=paddle2onnx;__STDC_FORMAT_MACROS\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /code/FastDeploy-develop/build/installed_fastdeploy\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   ONNX version              : 1.11.0\r\n--   ONNX NAMESPACE            : paddle2onnx\r\n--   ONNX_USE_LITE_PROTO       : OFF\r\n--   USE_PROTOBUF_SHARED_LIBS  : OFF\r\n--   Protobuf_USE_STATIC_LIBS  : ON\r\n--   ONNX_DISABLE_EXCEPTIONS   : OFF\r\n--   ONNX_WERROR               : OFF\r\n--   ONNX_BUILD_TESTS          : OFF\r\n--   ONNX_BUILD_BENCHMARKS     : OFF\r\n--   ONNXIFI_DUMMY_BACKEND     : OFF\r\n--   ONNXIFI_ENABLE_EXT        : OFF\r\n-- \r\n--   Protobuf compiler         : /code/FastDeploy-develop/build/third_libs/protobuf/bin/protoc\r\n--   Protobuf includes         : /code/FastDeploy-develop/build/third_libs/protobuf/include\r\n--   Protobuf libraries        : /code/FastDeploy-develop/build/third_libs/protobuf/lib/libprotobuf.a\r\n--   BUILD_ONNX_PYTHON         : OFF\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.3\r\n--   CMake command             : /usr/local/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 7.5.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;WITH_GPU;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX;MAX_ONNX_OPSET_VERSION=16;PADDLE2ONNX_LIB;ONNX_NAMESPACE=paddle2onnx\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /code/FastDeploy-develop/build/installed_fastdeploy\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : \r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /code/FastDeploy-develop/build\r\nracobit@ubuntu:/code/FastDeploy-develop/build$ make -j6\r\n[  0%] Running cpp protocol buffer compiler on p2o_paddle.proto\r\n[  0%] Running gen_proto.py on onnx/onnx.in.proto\r\n[  0%] Creating directories for 'extern_onnxruntime'\r\n[  0%] Building C object third_party/onnx/CMakeFiles/onnxifi_loader.dir/onnx/onnxifi_loader.c.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  2%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o\r\n  File \"/code/FastDeploy-develop/third_party/onnx/onnx/gen_proto.py\", line 36\r\n    def process_ifs(lines: Iterable[Text], onnx_ml: bool) -> Iterable[Text]:\r\n                         ^\r\nSyntaxError: invalid syntax\r\n[  2%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\nthird_party/onnx/CMakeFiles/gen_onnx_proto.dir/build.make:80: recipe for target 'third_party/onnx/onnx/onnx_paddle2onnx-ml.proto' failed\r\nmake[2]: *** [third_party/onnx/onnx/onnx_paddle2onnx-ml.proto] Error 1\r\nCMakeFiles/Makefile2:402: recipe for target 'third_party/onnx/CMakeFiles/gen_onnx_proto.dir/all' failed\r\nmake[1]: *** [third_party/onnx/CMakeFiles/gen_onnx_proto.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n-- Downloading...\r\n   dst='/code/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Building CXX object paddle2onnx/proto/CMakeFiles/p2o_paddle_proto.dir/p2o_paddle.pb.cc.o\r\n[  2%] Linking C static library libonnxifi_loader.a\r\n[  2%] Built target onnxifi_loader\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  2%] Linking C shared library libonnxifi_dummy.so\r\n[  2%] Built target onnxifi_dummy\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/code/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n     dst='/code/FastDeploy-develop/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  3%] No update step for 'extern_onnxruntime'\r\n[  3%] No patch step for 'extern_onnxruntime'\r\n[  3%] No configure step for 'extern_onnxruntime'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  4%] No build step for 'extern_onnxruntime'\r\n[  5%] Performing install step for 'extern_onnxruntime'\r\n[  5%] Completed 'extern_onnxruntime'\r\n[  5%] Built target extern_onnxruntime\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[  9%] Linking CXX static library libp2o_paddle_proto.a\r\n[  9%] Built target p2o_paddle_proto\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 10%] Linking CXX static library libyaml-cpp.a\r\n[ 10%] Built target yaml-cpp\r\nMakefile:155: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-03-27T15:15:45+00:00",
        "updated_at": "2023-03-28T03:10:27+00:00",
        "closed_at": "2023-03-28T03:10:27+00:00",
        "comments_count": [
            "DefTruth",
            "MrMzl"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1724,
        "title": "咨询FD库的架构问题",
        "body": "请问一下FD中已经有了FDTensor了为什么还有做一个Mat呀，他们在功能上的区别主要是什么，是不是可以不需要Mat这种？",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-28T04:04:01+00:00",
        "updated_at": "2024-04-02T06:40:05+00:00",
        "closed_at": "2024-04-02T06:40:05+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1726,
        "title": "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is not signed",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n\r\n\r\n Built target triton-fastdeploy-backend\r\n[root@ecs-07571807-001 serving]# cd ../\r\n[root@ecs-07571807-001 FastDeploy]# docker build -t paddlepaddle/fastdeploy:1.0.3-gpu-cuda11.2-trt8.4-21.10 -f serving/Dockerfile_CUDA_11_2 .\r\n[+] Building 69.1s (6/18)\r\n => [internal] load .dockerignore                                                                                                 0.1s\r\n => => transferring context: 2B                                                                                                   0.0s\r\n => [internal] load build definition from Dockerfile_CUDA_11_2                                                                    0.1s\r\n => => transferring dockerfile: 3.49kB                                                                                            0.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:11.2.2-cudnn8-devel-ubuntu20.04                                            0.0s\r\n => [ 1/14] FROM docker.io/nvidia/cuda:11.2.2-cudnn8-devel-ubuntu20.04                                                            0.5s\r\n => [internal] load build context                                                                                                27.3s\r\n => => transferring context: 3.68GB                                                                                              17.0s\r\n => ERROR [ 2/14] RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recomme  68.6s\r\n------\r\n > [ 2/14] RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip &&     apt-get clean && rm -rf /var/lib/apt/lists/*:\r\n#0 1.010 Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\r\n#0 1.080 Get:1 https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\r\n#0 1.094 Get:4 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\r\n#0 1.198 Err:1 https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease\r\n#0 1.198   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\r\n#0 1.478 Ign:2 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\r\n#0 1.616 Get:5 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Release [564 B]\r\n#0 1.650 Get:6 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Release.gpg [833 B]\r\n#0 1.792 Get:7 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Packages [2445 B]\r\n#0 2.500 Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1025 kB]\r\n#0 3.148 Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\r\n#0 3.678 Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\r\n#0 4.869 Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\r\n#0 5.309 Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2587 kB]\r\n#0 11.40 Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2060 kB]\r\n#0 18.18 Get:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\r\n#0 39.37 Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\r\n#0 39.51 Get:16 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\r\n#0 39.95 Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\r\n#0 44.20 Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\r\n#0 44.28 Get:19 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2198 kB]\r\n#0 50.12 Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1320 kB]\r\n#0 54.30 Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3065 kB]\r\n#0 64.48 Get:22 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\r\n#0 66.98 Get:23 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\r\n#0 67.65 Reading package lists...\r\n#0 68.38 W: GPG error: https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\r\n#0 68.38 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is not signed.\r\n------\r\nDockerfile_CUDA_11_2:22\r\n--------------------\r\n  21 |     #Install the build dependencies\r\n  22 | >>> RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip \\\r\n  23 | >>>     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager \\\r\n  24 | >>>     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip && \\\r\n  25 | >>>     apt-get clean && rm -rf /var/lib/apt/lists/*\r\n  26 |\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip &&     apt-get clean && rm -rf /var/lib/apt/lists/*\" did not complete successfully: exit code: 100\r\n[root@ecs-07571807-001 FastDeploy]# docker build -t paddlepaddle/fastdeploy:1.0.3-gpu-cuda11.2-trt8.4-21.10 -f serving/Dockerfile_CUDA_11_2 .\r\n[+] Building 59.6s (6/18)\r\n => [internal] load .dockerignore                                                                                                 0.0s\r\n => => transferring context: 2B                                                                                                   0.0s\r\n => [internal] load build definition from Dockerfile_CUDA_11_2                                                                    0.0s\r\n => => transferring dockerfile: 3.49kB                                                                                            0.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:11.2.2-cudnn8-devel-ubuntu20.04                                            0.0s\r\n => [internal] load build context                                                                                                 0.4s\r\n => => transferring context: 511.59kB                                                                                             0.3s\r\n => CACHED [ 1/14] FROM docker.io/nvidia/cuda:11.2.2-cudnn8-devel-ubuntu20.04                                                     0.0s\r\n => ERROR [ 2/14] RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recomme  59.5s\r\n------\r\n > [ 2/14] RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip &&     apt-get clean && rm -rf /var/lib/apt/lists/*:\r\n#0 0.943 Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\r\n#0 1.062 Get:1 https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\r\n#0 1.157 Err:1 https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease\r\n#0 1.157   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\r\n#0 1.418 Ign:2 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\r\n#0 1.433 Get:4 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\r\n#0 1.556 Get:5 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Release [564 B]\r\n#0 1.592 Get:6 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Release.gpg [833 B]\r\n#0 1.714 Get:7 https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu2004/x86_64  Packages [2445 B]\r\n#0 2.621 Get:8 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\r\n#0 2.939 Get:9 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1025 kB]\r\n#0 5.114 Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2587 kB]\r\n#0 6.714 Get:11 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\r\n#0 9.169 Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2060 kB]\r\n#0 13.74 Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\r\n#0 16.32 Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\r\n#0 22.31 Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\r\n#0 38.76 Get:16 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\r\n#0 39.71 Get:17 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\r\n#0 41.19 Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\r\n#0 41.53 Get:19 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2198 kB]\r\n#0 45.88 Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1320 kB]\r\n#0 50.12 Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3065 kB]\r\n#0 56.11 Get:22 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\r\n#0 57.10 Get:23 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\r\n#0 58.64 Reading package lists...\r\n#0 59.37 W: GPG error: https://developer.download.nvidia.cn/compute/cuda/repos/ubuntu2004/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\r\n#0 59.37 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is not signed.\r\n------\r\nDockerfile_CUDA_11_2:22\r\n--------------------\r\n  21 |     #Install the build dependencies\r\n  22 | >>> RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip \\\r\n  23 | >>>     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager \\\r\n  24 | >>>     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip && \\\r\n  25 | >>>     apt-get clean && rm -rf /var/lib/apt/lists/*\r\n  26 |\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Asia/Shanghai apt-get install -y --no-install-recommends curl wget vim git patchelf python3-dev python3-pip     python3-setuptools build-essential libgl1-mesa-glx libglib2.0-dev ca-certificates libb64-dev datacenter-gpu-manager     libssl-dev zlib1g-dev rapidjson-dev libboost-dev libre2-dev librdmacm-dev libnuma-dev libarchive-dev unzip &&     apt-get clean && rm -rf /var/lib/apt/lists/*\" did not complete successfully: exit code: 100\r\n[\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "enjoyloveless",
        "closed_by": "enjoyloveless",
        "created_at": "2023-03-28T07:50:29+00:00",
        "updated_at": "2023-04-20T04:05:34+00:00",
        "closed_at": "2023-04-20T04:05:34+00:00",
        "comments_count": [
            "enjoyloveless"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1725,
        "title": "intel 显卡 能部署检测不",
        "body": "intel  Arc A380\r\nIntel  Arc A750 \r\n\r\nintel 显卡 能部署检测不\r\n想入手intel 显卡",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-28T06:19:51+00:00",
        "updated_at": "2024-05-31T05:53:49+00:00",
        "closed_at": "2024-04-02T06:40:06+00:00",
        "comments_count": [
            "DefTruth",
            "junruizh2021"
        ],
        "labels": [
            "Question",
            "Intel GPU"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1727,
        "title": "default cuda directory does not exists",
        "body": "--- FastDeploy was built with gpu,                     \r\n--- but the default cuda directory does not exists.                     \r\n--- Please setup one of ['CUDA_DIRECTORY', 'CUDA_HOME', 'CUDA_ROOT', 'CUDA_PATH'] manually,                     \r\n--- this path should look like: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2.                     \r\n--- Check FAQ: https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/FAQ.md(这个文件不存在)\r\n  logging.warnings.warn(f\"\\n--- FastDeploy was built with gpu, \\\r\n\r\nCUDA_HOME文件夹应该设置在哪个位置？\r\n\r\n\r\n",
        "state": "closed",
        "user": "WanderAlphonse",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-28T08:30:04+00:00",
        "updated_at": "2024-04-16T09:03:42+00:00",
        "closed_at": "2024-04-16T09:03:42+00:00",
        "comments_count": [
            "DefTruth",
            "WanderAlphonse"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1729,
        "title": "使用你们的demo 交叉编译（infer_ppyoloe.cc），目标平台bm1684，在目标平台运行报错 Illegal instruction",
        "body": "## 环境\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【编译语言】： C++\r\n- 【目标平台】：算丰bm1684\r\n",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-28T09:26:28+00:00",
        "updated_at": "2024-04-02T06:40:07+00:00",
        "closed_at": "2024-04-02T06:40:07+00:00",
        "comments_count": [
            "DefTruth",
            "XiaBing992",
            "XiaBing992",
            "DefTruth",
            "XiaBing992",
            "Yi-sir"
        ],
        "labels": [
            "Sophgo"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1730,
        "title": "Paddle2ONNX冲突问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【系统平台】: Linux x86(Ubuntu 22.04)\r\n\r\n## 问题日志及出现问题的操作流程\r\n安装paddle2onnx后由于FastDeploy根目录下存在paddle2onnx文件夹，会与目前存在的paddle2onnx发生冲突。例如执行以下代码:\r\n```bash\r\npython -m paddle2onnx.optimize --input_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --output_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --input_shape_dict \"{'x':[1,3,960,960]}\"\r\n```\r\n报错如下\r\n```\r\n(rknn) zbc@pop-os:~/FastDeploy$ python -m paddle2onnx.optimize --input_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --output_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --input_shape_dict \"{'x':[1,3,960,960]}\"\r\nTraceback (most recent call last):\r\n  File \"/home/zbc/miniconda3/envs/rknn/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/zbc/miniconda3/envs/rknn/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/zbc/FastDeploy/paddle2onnx/__init__.py\", line 19, in <module>\r\n    from .version import version\r\nModuleNotFoundError: No module named 'paddle2onnx.version'\r\n```\r\n\r\n删掉paddle2onnx后，转换模型正常\r\n```\r\n(rknn) zbc@pop-os:~/FastDeploy$ python -m paddle2onnx.optimize --input_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --output_model ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx                                --input_shape_dict \"{'x':[1,3,960,960]}\"\r\n2023-03-28 18:49:36 [INFO]      Model optmized, saved in ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx.\r\n```",
        "state": "closed",
        "user": "Zheng-Bicheng",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-03-28T10:56:12+00:00",
        "updated_at": "2023-03-29T02:38:05+00:00",
        "closed_at": "2023-03-29T02:38:05+00:00",
        "comments_count": [
            "DefTruth",
            "Zheng-Bicheng",
            "DefTruth",
            "Zheng-Bicheng"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1739,
        "title": "请问支持图片红色区域这样配置么",
        "body": "![image](https://user-images.githubusercontent.com/126228496/228457426-6eff88cd-3a40-4115-94f4-fee76c20b765.png)\r\n\r\n#请问支持图片红色区域这样配置么",
        "state": "closed",
        "user": "ChinaGPT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-29T07:23:44+00:00",
        "updated_at": "2024-04-16T09:03:43+00:00",
        "closed_at": "2024-04-16T09:03:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1735,
        "title": "FastDeploy无法正常部署",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： 说明具体硬件型号，8核16G \r\n- 【编译语言】：  Python 3.8\r\n- 根据文档https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving\r\n没有使用docker\r\n安装paddle\r\npip install paddlepaddle==2.4.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n安装fastdeploy\r\npip install fastdeploy-python==1.0.4\r\n获取FastDeploy仓库\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\n解决libgl1.so报错\r\napt-get install libgl1\r\n直接执行正常\r\n```\r\nroot@iZbp19wu4707e6xaoyzx3gZ:~/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python# python3 infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu --backend paddle\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0329 13:51:06.211759 11016 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\ndet boxes: [[42,413],[483,391],[484,428],[43,450]]rec text: 上海斯格威铂尔大酒店 rec score:0.980085 cls label: 0 cls score: 1.000000\r\ndet boxes: [[187,456],[399,448],[400,480],[188,488]]rec text: 打浦路15号 rec score:0.964993 cls label: 0 cls score: 1.000000\r\ndet boxes: [[23,507],[513,488],[515,529],[24,548]]rec text: 绿洲仕格维花园公寓 rec score:0.993727 cls label: 0 cls score: 1.000000\r\ndet boxes: [[74,553],[427,542],[428,571],[75,582]]rec text: 打浦路252935号 rec score:0.947723 cls label: 0 cls score: 1.000000\r\n```\r\n\r\n## 问题日志及出现问题的操作流程\r\n尝试启动服务执行```fastdeployserver --model-repository=/ocr_serving/models```\r\n会报错```fastdeployserver: command not found```\r\n\r\n如果使用visualdl配置，启动服务时会报错\r\n![image](https://user-images.githubusercontent.com/26214176/228442207-e1cf58bd-e22e-42ac-8376-3382e75dec19.png)\r\n",
        "state": "closed",
        "user": "bxjxxyy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-29T06:12:56+00:00",
        "updated_at": "2024-04-16T09:03:43+00:00",
        "closed_at": "2024-04-16T09:03:43+00:00",
        "comments_count": [
            "rainyfly",
            "bxjxxyy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1738,
        "title": "请问支持chatglm-6b模型么",
        "body": "如题~需要怎么处理",
        "state": "closed",
        "user": "ChinaGPT",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-29T07:18:28+00:00",
        "updated_at": "2024-04-02T06:40:08+00:00",
        "closed_at": "2024-04-02T06:40:08+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1740,
        "title": "Paddle2Onnx不支持转换量化后的模型",
        "body": "- 【FastDeploy版本】： fastdeploy-python 0.0.0\r\n- 【编译命令】瑞芯微 RK3588编译命令\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】：瑞芯微RK3588S\r\n- 【编译语言】：Python 3.8.16\r\n\r\n- 【模型无法转换】\r\n- 运行命令:\r\npaddle2onnx --model_dir ch_PP-OCRv3_det_infer \\\r\n                      --model_filename inference.pdmodel \\\r\n                      --params_filename inference.pdiparams \\\r\n                      --save_file ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx \\\r\n                      --enable_dev_version True\r\n-  报错信息：\r\n[Paddle2ONNX] Start to parse PaddlePaddle model...\r\n[Paddle2ONNX] Model file path: ch_PP-OCRv3_det_infer/inference.pdmodel\r\n[Paddle2ONNX] Paramters file path: ch_PP-OCRv3_det_infer/inference.pdiparams\r\n[Paddle2ONNX] Start to parsing Paddle model...\r\n[Paddle2ONNX] [Info] The Paddle model is a quantized model. \r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including fake_channel_wise_quantize_dequantize_abs_max,fake_quantize_dequantize_moving_average_abs_max,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.\r\nAborted (core dumped)\r\n",
        "state": "open",
        "user": "ChihoJack",
        "closed_by": null,
        "created_at": "2023-03-29T09:02:03+00:00",
        "updated_at": "2023-04-25T13:05:53+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "ChihoJack",
            "Zheng-Bicheng",
            "ChihoJack"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1743,
        "title": " 自己编译的时候报错：Paddle Backend doesn't support linux aarch64 now. ",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [说明具体的版本，拉的最新的代码]使用下面的命令进行c++编译：(https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/cpu.md)\r\n- 【编译命令】\r\n```bash\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \\\r\n         -DENABLE_TEXT=ON\r\nmake -j12\r\nmake install\r\n```\r\n- 【系统平台】:  Linux localhost.localdomain 4.19.90-24.4.v2101.ky10.aarch64 #1 SMP Mon May 24 14:45:37 CST 2021 aarch64 aarch64 aarch64 GNU/Linux\r\n- 【硬件】： 说明具体硬件型号，kunpeng 920\r\n- 【编译语言】： gcc(7.3.0) / Python(3.7.4）\r\n\r\n请问如何用fastdeploy安装在kunpeng920 cpu上，如果paddle backend不支持，可以支持onnx么？仅用fastdeploy的onnx backend如何自己编译？",
        "state": "closed",
        "user": "aixuedegege",
        "closed_by": "aixuedegege",
        "created_at": "2023-03-30T02:39:55+00:00",
        "updated_at": "2023-03-31T02:13:17+00:00",
        "closed_at": "2023-03-31T02:13:17+00:00",
        "comments_count": [
            "DefTruth",
            "aixuedegege"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1744,
        "title": "ocr案例GPU比CPU更耗时",
        "body": "CPU 0.25s\r\nGPU 3s多\r\nocr案例代码如下，测试的是0和4选项\r\n```c++\r\n#include \"fastdeploy/vision.h\"\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\n//全局变量\r\nfastdeploy::vision::ocr::DBDetector det_model;\r\nfastdeploy::vision::ocr::Classifier cls_model;\r\nfastdeploy::vision::ocr::Recognizer rec_model;\r\nauto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);\r\n\r\nvoid Init(const std::string& det_model_dir, const std::string& cls_model_dir, const std::string& rec_model_dir, const std::string& rec_label_file, const fastdeploy::RuntimeOption& option) \r\n{\r\n    auto det_model_file = det_model_dir + sep + \"inference.pdmodel\";\r\n    auto det_params_file = det_model_dir + sep + \"inference.pdiparams\";\r\n\r\n    auto cls_model_file = cls_model_dir + sep + \"inference.pdmodel\";\r\n    auto cls_params_file = cls_model_dir + sep + \"inference.pdiparams\";\r\n\r\n    auto rec_model_file = rec_model_dir + sep + \"inference.pdmodel\";\r\n    auto rec_params_file = rec_model_dir + sep + \"inference.pdiparams\";\r\n\r\n    auto det_option = option;\r\n    auto cls_option = option;\r\n    auto rec_option = option;\r\n\r\n    // The cls and rec model can inference a batch of images now.\r\n    // User could initialize the inference batch size and set them after create\r\n    // PP-OCR model.\r\n    int cls_batch_size = 1;\r\n    int rec_batch_size = 6;\r\n\r\n    // If use TRT backend, the dynamic shape will be set as follow.\r\n    // We recommend that users set the length and height of the detection model to\r\n    // a multiple of 32.\r\n    // We also recommend that users set the Trt input shape as follow.\r\n    det_option.SetTrtInputShape(\"x\", { 1, 3, 64, 64 }, { 1, 3, 640, 640 },\r\n        { 1, 3, 960, 960 });\r\n    cls_option.SetTrtInputShape(\"x\", { 1, 3, 48, 10 }, { cls_batch_size, 3, 48, 320 },\r\n        { cls_batch_size, 3, 48, 1024 });\r\n    rec_option.SetTrtInputShape(\"x\", { 1, 3, 48, 10 }, { rec_batch_size, 3, 48, 320 },\r\n        { rec_batch_size, 3, 48, 2304 });\r\n\r\n    // Users could save TRT cache file to disk as follow.\r\n    // det_option.SetTrtCacheFile(det_model_dir + sep + \"det_trt_cache.trt\");\r\n    // cls_option.SetTrtCacheFile(cls_model_dir + sep + \"cls_trt_cache.trt\");\r\n    // rec_option.SetTrtCacheFile(rec_model_dir + sep + \"rec_trt_cache.trt\");\r\n\r\n    det_model = fastdeploy::vision::ocr::DBDetector(\r\n        det_model_file, det_params_file, det_option);\r\n    cls_model = fastdeploy::vision::ocr::Classifier(\r\n        cls_model_file, cls_params_file, cls_option);\r\n    rec_model = fastdeploy::vision::ocr::Recognizer(\r\n        rec_model_file, rec_params_file, rec_label_file, rec_option);\r\n\r\n    assert(det_model.Initialized());\r\n    assert(cls_model.Initialized());\r\n    assert(rec_model.Initialized());\r\n\r\n    // Parameters settings for pre and post processing of Det/Cls/Rec Models.\r\n    // All parameters are set to default values.\r\n    det_model.GetPreprocessor().SetMaxSideLen(960);\r\n    det_model.GetPostprocessor().SetDetDBThresh(0.3);\r\n    det_model.GetPostprocessor().SetDetDBBoxThresh(0.6);\r\n    det_model.GetPostprocessor().SetDetDBUnclipRatio(1.5);\r\n    det_model.GetPostprocessor().SetDetDBScoreMode(\"slow\");\r\n    det_model.GetPostprocessor().SetUseDilation(0);\r\n    cls_model.GetPostprocessor().SetClsThresh(0.9);\r\n\r\n    // The classification model is optional, so the PP-OCR can also be connected\r\n    // in series as follows\r\n    // auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &rec_model);\r\n    //auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);\r\n\r\n    // Set inference batch size for cls model and rec model, the value could be -1\r\n    // and 1 to positive infinity.\r\n    // When inference batch size is set to -1, it means that the inference batch\r\n    // size\r\n    // of the cls and rec models will be the same as the number of boxes detected\r\n    // by the det model.\r\n    ppocr_v3.SetClsBatchSize(cls_batch_size);\r\n    ppocr_v3.SetRecBatchSize(rec_batch_size);\r\n\r\n    if (!ppocr_v3.Initialized()) {\r\n        std::cerr << \"Failed to initialize PP-OCR.\" << std::endl;\r\n        return;\r\n    }\r\n}\r\n\r\nvoid Infer(const std::string& image_file)\r\n{\r\n    auto im = cv::imread(image_file);\r\n    auto im_bak = im.clone();\r\n\r\n    fastdeploy::vision::OCRResult result;\r\n    if (!ppocr_v3.Predict(&im, &result)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto vis_im = fastdeploy::vision::VisOcr(im_bak, result);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\nint main() {\r\n    fastdeploy::RuntimeOption option;\r\n    int flag = 0;\r\n\r\n    if (flag == 0) {\r\n        option.UseCpu();\r\n        option.UsePaddleBackend();  // Paddle Inference\r\n    }\r\n    else if (flag == 1) {\r\n        option.UseCpu();\r\n        option.UseOpenVINOBackend();  // OpenVINO\r\n    }\r\n    else if (flag == 2) {\r\n        option.UseCpu();\r\n        option.UseOrtBackend();  // ONNX Runtime\r\n    }\r\n    else if (flag == 3) {\r\n        option.UseCpu();\r\n        option.UseLiteBackend();  // Paddle Lite\r\n    }\r\n    else if (flag == 4) {\r\n        option.UseGpu();\r\n        option.UsePaddleBackend();  // Paddle Inference\r\n    }\r\n    else if (flag == 5) {\r\n        option.UseGpu();\r\n        option.UsePaddleInferBackend();\r\n        option.paddle_infer_option.collect_trt_shape = true;\r\n        option.paddle_infer_option.enable_trt = true;  // Paddle-TensorRT\r\n    }\r\n    else if (flag == 6) {\r\n        option.UseGpu();\r\n        option.UseOrtBackend();  // ONNX Runtime\r\n    }\r\n    else if (flag == 7) {\r\n        option.UseGpu();\r\n        option.UseTrtBackend();  // TensorRT\r\n    }\r\n\r\n    std::string det_model_dir = \"./ch_PP-OCRv3_det_slim_infer\";\r\n    std::string cls_model_dir = \"./ch_ppocr_mobile_v2.0_cls_slim_infer\";\r\n    std::string rec_model_dir = \"./ch_PP-OCRv3_rec_slim_infer\";\r\n    std::string rec_label_file = \"ppocr_keys_v1.txt\";\r\n    std::string test_image = \"text.jpg\";\r\n\r\n    //初始化\r\n    Init(det_model_dir, cls_model_dir, rec_model_dir, rec_label_file, option);\r\n\r\n    //推理，并统计耗时\r\n    double start = static_cast<double>(cv::getTickCount());\r\n    Infer(test_image);\r\n    double time = ((double)cv::getTickCount() - start) / cv::getTickFrequency();\r\n    std::cout << \"所用时间为：\" << time << \"秒\" << std::endl;\r\n\r\n    return 0;\r\n}\r\n\r\n```\r\n",
        "state": "closed",
        "user": "zuoyizhongguo",
        "closed_by": "zuoyizhongguo",
        "created_at": "2023-03-30T04:38:36+00:00",
        "updated_at": "2023-09-14T12:42:19+00:00",
        "closed_at": "2023-04-04T02:56:43+00:00",
        "comments_count": [
            "DefTruth",
            "zuoyizhongguo",
            "zuoyizhongguo",
            "Y-huange"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1746,
        "title": "rv1126 使用自己训练的量化出现模型(ppyoloe)加载错误",
        "body": "[5  3/30  9:39:22.746 ...kernels/nnadapter/converter/converter.cc:96 Apply] Converting conv2d ...\r\n[5  3/30  9:39:22.747 ...te/kernels/nnadapter/converter/conv2d.cc:81 ConvertConv2D] padding_algorithm:EXPLICIT\r\n[5  3/30  9:39:22.747 ...te/kernels/nnadapter/converter/conv2d.cc:84 ConvertConv2D] depthwise mode(0).\r\n[F  3/30  9:39:22.747 ...te/kernels/nnadapter/converter/conv2d.cc:94 ConvertConv2D] Check failed: IsValidSymmPerLayerQuantParams(output_scales): Missing the quant params 'Output0_scale' for the output 'conv2d_113.tmp_0'\r\nterminate called after throwing an instance of 'paddle::lite::PaddleLiteException'\r\n  what():  Paddle-Lite C++ Exception:\r\n[F  3/30  9:39:22.747 ...te/kernels/nnadapter/converter/conv2d.cc:94 ConvertConv2D] Check failed: IsValidSymmPerLayerQuantParams(output_scales): Missing the quant params 'Output0_scale' for the output 'conv2d_113.tmp_0'\r\n\r\n出现上面的报错",
        "state": "closed",
        "user": "16lwzheng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-30T09:40:48+00:00",
        "updated_at": "2024-04-16T09:03:44+00:00",
        "closed_at": "2024-04-16T09:03:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1745,
        "title": "docker fastdeployserver 启动失败 CUDA driver version is insufficient for CUDA runtime version",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： docker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： NVIDIA RTX A4000，CUDA 11.7\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n参考文档启动服务失败:\r\nhttps://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/fastdeploy_serving#33-%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%9C%A8docker%E5%86%85\r\n\r\n报错原因是因为我使用镜像包cuda版本为11.4？\r\n\r\n\r\n报错日志片段如下，\r\nrror: Failed to initialize NVML\r\nW0330 07:24:49.642447 370 metrics.cc:221] DCGM unable to start: DCGM initialization error\r\nW0330 07:24:49.643144 370 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: **CUDA driver version is insufficient for CUDA runtime version**\r\nI0330 07:24:49.643227 370 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nE0330 07:24:49.645021 370 model_repository_manager.cc:1890] Poll failed for model directory 'cls_runtime': instance group cls_runtime_0 of model cls_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0330 07:24:49.645423 370 model_repository_manager.cc:1890] Poll failed for model directory 'det_runtime': instance group det_runtime_0 of model det_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0330 07:24:49.646024 370 model_repository_manager.cc:1890] Poll failed for model directory 'rec_runtime': instance group rec_runtime_0 of model rec_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0330 07:24:49.646119 370 model_repository_manager.cc:1375] Invalid argument: ensemble rec_pp contains models that are not available: rec_runtime\r\nE0330 07:24:49.646143 370 model_repository_manager.cc:1375] Invalid argument: ensemble pp_ocr contains models that are not available: det_runtime\r\nE0330 07:24:49.646156 370 model_repository_manager.cc:1375] Invalid argument: ensemble cls_pp contains models that are not available: cls_runtime\r\n",
        "state": "closed",
        "user": "kerry-weic",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-30T08:32:53+00:00",
        "updated_at": "2024-06-18T06:41:08+00:00",
        "closed_at": "2024-06-18T06:41:08+00:00",
        "comments_count": [
            "a0735a",
            "kerry-weic",
            "teymur-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1751,
        "title": "TritonModelException: Dimension Mismatch",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.4\r\nOS Platform: Ubuntu 18.04\r\nHardware: Tesla P4\r\n\r\n## Problem description\r\nI am trying to serve PaddleOCR models using fastdeploy with Dockerfile. Here is the Dockerfile I am using \r\n```\r\nFROM registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n\r\n## Configuring Dependencies\r\nRUN apt-get update && apt-get install -y wget && rm -rf /var/lib/apt/lists/*\r\nRUN ldconfig\r\nRUN apt-get install libgl1\r\n\r\n## Convertion Part\r\nCOPY models /models\r\n\r\nRUN wget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\r\nRUN tar xvf en_PP-OCRv3_det_infer.tar && mv en_PP-OCRv3_det_infer 1\r\nRUN mv 1/inference.pdiparams 1/model.pdiparams && mv 1/inference.pdmodel 1/model.pdmodel\r\nRUN mv 1 models/det_runtime/ && rm -rf en_PP-OCRv3_det_infer.tar\r\n\r\nRUN wget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar\r\nRUN tar xvf en_PP-OCRv3_rec_infer.tar && mv en_PP-OCRv3_rec_infer 1\r\nRUN mv 1/inference.pdiparams 1/model.pdiparams && mv 1/inference.pdmodel 1/model.pdmodel\r\nRUN mv 1 models/rec_runtime/ && rm -rf en_PP-OCRv3_rec_infer.tar\r\n\r\nRUN wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\r\nRUN tar xvf ch_ppocr_mobile_v2.0_cls_infer.tar && mv ch_ppocr_mobile_v2.0_cls_infer 1\r\nRUN mv 1/inference.pdiparams 1/model.pdiparams && mv 1/inference.pdmodel 1/model.pdmodel\r\nRUN mv 1 models/cls_runtime/ && rm -rf ch_ppocr_mobile_v2.0_cls_infer.tar\r\n\r\nRUN mkdir models/pp_ocr/1 && mkdir models/rec_pp/1 && mkdir models/cls_pp/1\r\nCOPY en_dict.txt ./\r\nRUN mv en_dict.txt models/rec_postprocess/1/\r\n\r\n## Expose Part\r\nEXPOSE 8000\r\nEXPOSE 8001\r\nEXPOSE 8002\r\nCMD [\"/bin/bash\", \"-c\", \"fastdeployserver --model-repository=/models\"]\r\n```\r\nI copy the models folder defined [here](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models).\r\n\r\nFor the English models I changed the key of the output_map in rec_pp/config.pbtxt to softmax_2.tmp_0, and the output in rec_runtime/config/pbtxt to softmax_2.tmp_0. \r\n\r\nAfter these changes the models are normally served, and the docker container is up just fine, but when I call with the client defined [here](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/client.py), I get the following error:\r\n\r\n<img width=\"1449\" alt=\"image\" src=\"https://user-images.githubusercontent.com/22133074/228902939-d8707343-9031-4f50-a424-fd21d41984b9.png\">\r\n \r\nI would like to know a way to solve this issue.",
        "state": "closed",
        "user": "arsenignatosyan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-30T16:33:55+00:00",
        "updated_at": "2024-04-16T09:03:46+00:00",
        "closed_at": "2024-04-16T09:03:46+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1750,
        "title": "UTF-8 Encoding Error for English Inference",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.5\r\nOS Platform: Mac OSX\r\nHardware: M1 Chip\r\nProgram Language: Python 3.7\r\n\r\n## Problem description\r\nI am trying to replicate the [this](https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/deploy/fastdeploy/cpu-gpu/python/infer.py) example of fastdeploy locally. I passed the English detection and recognition models, also English recognition label file to the argument parser in the example. Here are the links to models: [detection model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar), [recognition model](https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar), [recognition label file](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/ppocr/utils/en_dict.txt). \r\n\r\nThe code works for the image given in the example (with Chinese text), but for other images, such as the German image again in PaddleOCR's examples ([this image](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/imgs/ger_1.jpg)) the code gives unicode error. I removed the `print(result)` part and added \r\n`print(result.boxes)`\r\n`print(result.rec_scores)`\r\n`result_text = result.text ` \r\nthese parts to locate the issue. The error is the following:\r\n<img width=\"1332\" alt=\"image\" src=\"https://user-images.githubusercontent.com/22133074/228895712-94104ab0-0ec4-41d8-8c7f-e623dae744e5.png\">\r\n\r\n",
        "state": "closed",
        "user": "arsenignatosyan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-30T16:09:31+00:00",
        "updated_at": "2024-04-23T06:40:51+00:00",
        "closed_at": "2024-04-23T06:40:51+00:00",
        "comments_count": [
            "garikdza"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1747,
        "title": "GPU版本  FastDeploy simple_serving部署启动失败( initalized failed )",
        "body": "环境信息:centos7(64)、python3.8、CUDA 11.7、cuDNN 8.4.1、NVIDIA RTX A4000（gpu）、cmake 3.18.0\r\n目标为部署ocr的推理服务，总体参考流程:https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/simple_serving#paddleocr-python%E8%BD%BB%E9%87%8F%E6%9C%8D%E5%8A%A1%E5%8C%96%E9%83%A8%E7%BD%B2%E7%A4%BA%E4%BE%8B\r\n尝试使用自编译和官方提供预编译fastdeploy-gpu-python启动推理服务均失败。\r\n自编译参考文档流程:https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/gpu.md#python%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85\r\n编译后的版本信息: \r\n![image](https://user-images.githubusercontent.com/49382323/228808202-089d0751-f1e8-4a3e-aeb6-fd089aac9956.png)\r\n启动报错日志如下:\r\n![image](https://user-images.githubusercontent.com/49382323/228998227-ed6939c4-b4b6-4a60-ab88-abdfae87b71c.png)\r\n\r\n使用官方预编译版本:pip3 install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n编译后的版本信息: \r\n![image](https://user-images.githubusercontent.com/49382323/228809565-9c45c979-a7b4-4376-9180-dc7d782a1cd8.png)\r\n启动报错日志如下:\r\n![image](https://user-images.githubusercontent.com/49382323/228809202-aaf1fb68-5e89-40d1-937b-0d823f2e2326.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "kerry-weic",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-30T10:35:56+00:00",
        "updated_at": "2024-04-16T09:03:45+00:00",
        "closed_at": "2024-04-16T09:03:45+00:00",
        "comments_count": [
            "kerry-weic",
            "fanruifeng",
            "kerry-weic",
            "fanruifeng",
            "kerry-weic",
            "fanruifeng",
            "kerry-weic"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1752,
        "title": "picodet_l_640_coco_lcnet转rknn失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 与origin/develop一致\r\n- 【编译命令】python ../export.py --config_path=./config.yaml --target_platform rk3588\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：\r\n- 【编译语言】：Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【转换模型出错】\r\n- 自己的模型不能转成rknn \r\n- root@94cc3f5f3ccc:/home/paddleDets/FastDeploy/tools/rknpu2/picodet_l_640_coco_lcnet# python ../export.py --config_path=./config.yaml --target_platform rk3588\r\n{'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'model_path': './picodet_l_640_coco_lcnet.onnx', 'target_platform': 'RK3588', 'outputs_nodes': None, 'do_quantization': False, 'dataset': None, 'output_folder': './'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nI base_optimize ...\r\nI base_optimize done.\r\nI\r\nI fold_constant ...\r\nI fold_constant done.\r\nI\r\nI correct_ops ...\r\nI correct_ops done.\r\nI\r\nI fuse_ops ...\r\nI fuse_ops results:\r\nI     fuse_bn_into_conv: remove node = ['p2o.BatchNormalization.0', 'p2o.BatchNormalization.1', 'p2o.BatchNormalization.2', 'p2o.BatchNormalization.3', 'p2o.BatchNormalization.4', 'p2o.BatchNormalization.5', 'p2o.BatchNormalization.6', 'p2o.BatchNormalization.7', 'p2o.BatchNormalization.8', 'p2o.BatchNormalization.9', 'p2o.BatchNormalization.10', 'p2o.BatchNormalization.11', 'p2o.BatchNormalization.12', 'p2o.BatchNormalization.13', 'p2o.BatchNormalization.14', 'p2o.BatchNormalization.15', 'p2o.BatchNormalization.16', 'p2o.BatchNormalization.17', 'p2o.BatchNormalization.18', 'p2o.BatchNormalization.19', 'p2o.BatchNormalization.20', 'p2o.BatchNormalization.21', 'p2o.BatchNormalization.22', 'p2o.BatchNormalization.23', 'p2o.BatchNormalization.24', 'p2o.BatchNormalization.25', 'p2o.BatchNormalization.26', 'p2o.BatchNormalization.29', 'p2o.BatchNormalization.50', 'p2o.BatchNormalization.51', 'p2o.BatchNormalization.28', 'p2o.BatchNormalization.30', 'p2o.BatchNormalization.31', 'p2o.BatchNormalization.32', 'p2o.BatchNormalization.33', 'p2o.BatchNormalization.27', 'p2o.BatchNormalization.34', 'p2o.BatchNormalization.35', 'p2o.BatchNormalization.36', 'p2o.BatchNormalization.37', 'p2o.BatchNormalization.38', 'p2o.BatchNormalization.39', 'p2o.BatchNormalization.40', 'p2o.BatchNormalization.41', 'p2o.BatchNormalization.42', 'p2o.BatchNormalization.43', 'p2o.BatchNormalization.44', 'p2o.BatchNormalization.45', 'p2o.BatchNormalization.46', 'p2o.BatchNormalization.47', 'p2o.BatchNormalization.48', 'p2o.BatchNormalization.49', 'p2o.BatchNormalization.52', 'p2o.BatchNormalization.53', 'p2o.BatchNormalization.87', 'p2o.BatchNormalization.88', 'p2o.BatchNormalization.89', 'p2o.BatchNormalization.90', 'p2o.BatchNormalization.91', 'p2o.BatchNormalization.92', 'p2o.BatchNormalization.93', 'p2o.BatchNormalization.94', 'p2o.BatchNormalization.95', 'p2o.BatchNormalization.96', 'p2o.BatchNormalization.97', 'p2o.BatchNormalization.76', 'p2o.BatchNormalization.77', 'p2o.BatchNormalization.78', 'p2o.BatchNormalization.79', 'p2o.BatchNormalization.80', 'p2o.BatchNormalization.81', 'p2o.BatchNormalization.82', 'p2o.BatchNormalization.83', 'p2o.BatchNormalization.84', 'p2o.BatchNormalization.85', 'p2o.BatchNormalization.86', 'p2o.BatchNormalization.65', 'p2o.BatchNormalization.66', 'p2o.BatchNormalization.67', 'p2o.BatchNormalization.68', 'p2o.BatchNormalization.69', 'p2o.BatchNormalization.70', 'p2o.BatchNormalization.71', 'p2o.BatchNormalization.72', 'p2o.BatchNormalization.73', 'p2o.BatchNormalization.74', 'p2o.BatchNormalization.75', 'p2o.BatchNormalization.54', 'p2o.BatchNormalization.55', 'p2o.BatchNormalization.56', 'p2o.BatchNormalization.57', 'p2o.BatchNormalization.58', 'p2o.BatchNormalization.59', 'p2o.BatchNormalization.60', 'p2o.BatchNormalization.61', 'p2o.BatchNormalization.62', 'p2o.BatchNormalization.63', 'p2o.BatchNormalization.64']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.0'], add node = ['p2o.Div.0_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.1'], add node = ['p2o.Div.1_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.2'], add node = ['p2o.Div.2_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.3'], add node = ['p2o.Div.3_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.4'], add node = ['p2o.Div.4_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.5'], add node = ['p2o.Div.5_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.6'], add node = ['p2o.Div.6_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.7'], add node = ['p2o.Div.7_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.8'], add node = ['p2o.Div.8_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.9'], add node = ['p2o.Div.9_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.10'], add node = ['p2o.Div.10_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.11'], add node = ['p2o.Div.11_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.12'], add node = ['p2o.Div.12_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.13'], add node = ['p2o.Div.13_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.14'], add node = ['p2o.Div.14_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.15'], add node = ['p2o.Div.15_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.16'], add node = ['p2o.Div.16_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.17'], add node = ['p2o.Div.17_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.18'], add node = ['p2o.Div.18_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.19'], add node = ['p2o.Div.19_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.20'], add node = ['p2o.Div.20_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.21'], add node = ['p2o.Div.21_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.22'], add node = ['p2o.Div.22_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.23'], add node = ['p2o.Div.23_2mul']\r\nI     convert_global_avgpool_to_conv: remove node = ['p2o.GlobalAveragePool.0'], add node = ['p2o.GlobalAveragePool.0_2conv_0', 'p2o.GlobalAveragePool.1']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.24'], add node = ['p2o.Div.24_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.25'], add node = ['p2o.Div.25_2mul']\r\nI     convert_global_avgpool_to_conv: remove node = ['p2o.GlobalAveragePool.2'], add node = ['p2o.GlobalAveragePool.2_2conv_0', 'p2o.GlobalAveragePool.3']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.26'], add node = ['p2o.Div.26_2mul']\r\nI     convert_div_to_mul: remove node = ['p2o.Div.29'], add node = ['p2o.Div.29_2mul']\r\nE build: Catch exception when building RKNN model!\r\nE build: Traceback (most recent call last):\r\nE build:   File \"rknn/api/rknn_base.py\", line 1546, in rknn.api.rknn_base.RKNNBase.build\r\nE build:   File \"rknn/api/graph_optimizer.py\", line 1276, in rknn.api.graph_optimizer.GraphOptimizer.fuse_ops\r\nE build:   File \"rknn/api/fuse_rules.py\", line 2138, in rknn.api.fuse_rules._p_convert_resize_to_deconv\r\nE build: TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\nTraceback (most recent call last):\r\n  File \"../export.py\", line 64, in <module>\r\n    assert ret == 0, \"Build model failed!\"\r\nAssertionError: Build model failed!\r\n\r\n",
        "state": "closed",
        "user": "vamoslc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-31T00:28:18+00:00",
        "updated_at": "2024-04-12T08:18:54+00:00",
        "closed_at": "2024-04-12T08:18:54+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "vamoslc",
            "lizheng-1",
            "Zheng-Bicheng",
            "vamoslc",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1756,
        "title": "Serving SOLOv2 model in FastDeploy server",
        "body": "## Environment\r\n\r\nFastDeploy version: latest code in develop branch\r\nOS Platform: l4t jetpack linux (aarch64)\r\nHardware: Jetson Xavier AGX (jetpack 5.0.2, CUDA 11.4, TRT 8.4)\r\nProgram Language: e.g. Python 3.8.10\r\n\r\n## Problem description\r\nI am trying to serve the SOLOv2 model via fastdeployserver using the paddle backend. I tried to follow the logic used in [the YOLOv5 serving example](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/yolov5/serving) but as far as I could figure out, SOLOv2's preprocessor and postprocessor cannot be initialized without initializing the full model:\r\n```\r\npostprocessor = fd.vision.detection.SOLOv2.postprocessor()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: 'property' object is not callable\r\n```\r\nMy current workaround would be to initialize the full model in post and preprocessing modules in the fastdeployserver just to pre- and postprocess but this kind of defeats the purpose:\r\n```\r\nmodel_file = \"model.pdmodel\"\r\nparams_file = \"model.pdiparams\"\r\nconfig_file = \"infer_cfg.yml\"\r\n\r\nmodel = fd.vision.detection.SOLOv2(model_file, params_file, config_file)\r\npostprocess_out = model.postprocessor.run(model_output)\r\n\r\n```\r\n Is there another way to use the SOLOv2 pre and postprocessor detached from the model using python?",
        "state": "closed",
        "user": "JeroendenBoef",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-31T15:02:45+00:00",
        "updated_at": "2024-04-16T09:03:48+00:00",
        "closed_at": "2024-04-16T09:03:48+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1753,
        "title": "之前提的问题没有回复",
        "body": "这个是两周前提的问题:https://github.com/PaddlePaddle/FastDeploy/issues/1685\r\n一直没有下文,有工程师帮忙看过么\r\n\r\n",
        "state": "closed",
        "user": "pangchao-git",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-03-31T01:48:37+00:00",
        "updated_at": "2024-04-16T09:03:47+00:00",
        "closed_at": "2024-04-16T09:03:47+00:00",
        "comments_count": [
            "DefTruth",
            "pangchao-git",
            "pangchao-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1755,
        "title": "PPOCRV2 的 example执行报错",
        "body": "Hi FastDeploy Teams,\r\n\r\n我的环境是Mac M1 12.3\r\n我根据教程编译了fastdepoly，编译命令：\r\n```\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF \\\r\n         -DENABLE_OPENVINO_BACKEND=OFF \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=OFF\r\nmake -j8\r\nmake install\r\n```\r\n\r\n在compiled_fastdeploy_sdk/examples/vision/ocr/PP-OCRv2/cpp下，  编译了PP-OCRv2的demo后，执行\r\n```\r\n./infer_demo ./ch_PP-OCRv2_det_infer ./ch_ppocr_mobile_v2.0_cls_infer ./ch_ppocr_server_v2.0_rec_infer ./ch_en.txt ./all_font_color.jpg 0\r\n```\r\n\r\n报错：\r\n```\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape\t`RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape\t`RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape\t`RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(365)::Infer\tFailed to Infer: Got invalid dimensions for input: x for the following indices\r\n index: 2 Got: 800 Expected: 960\r\n index: 3 Got: 672 Expected: 960\r\n Please fix either the inputs or the model.\r\n[ERROR] fastdeploy/vision/ocr/ppocr/dbdetector.cc(107)::BatchPredict\tFailed to inference by runtime.\r\n[ERROR] fastdeploy/vision/ocr/ppocr/ppocr_v2.cc(115)::BatchPredict\tThere's error while detecting image in PPOCR.\r\nFailed to predict.\r\n```\r\n我想问是哪里出错了吗？",
        "state": "closed",
        "user": "ANDROIDTODO",
        "closed_by": "ANDROIDTODO",
        "created_at": "2023-03-31T02:26:09+00:00",
        "updated_at": "2023-03-31T03:36:46+00:00",
        "closed_at": "2023-03-31T03:36:46+00:00",
        "comments_count": [
            "yunyaoXYY",
            "ANDROIDTODO"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1757,
        "title": "Jetson Nano编译python版本报错：'CMakeFiles/copy_third_libraries' failed",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 拉取最新源码自行编译的python版本\r\n- 【编译命令】: sudo python3 setup.py build\r\n- 【系统平台】: Ubuntu 18.04, Jetpack 4.6\r\n- 【硬件】： Jetson Nano 4GB\r\n- 【编译语言】： Python 3.6\r\n- 【cmake版本】：3.25\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【编译失败】\r\n执行sudo python3 setup.py build进行编译，环境变量为：\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TRT_BACKEND=ON\r\n\r\n编译报错：\r\n(前面进度都正常，省略)\r\n[100%] Built target fastdeploy_main\r\nError copying directory from \"/home/xxx/FastDeploy/python/.setuptools-cmake-build/third_libs/install\" to \"/home/xxx/FastDeploy/python/fastdeploy/libs/third_libs\".\r\nCMakeFiles/copy_third_libraries.dir/build.make:70: recipe for target 'CMakeFiles/copy_third_libraries' failed\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "Yuening-Ma",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-01T09:11:42+00:00",
        "updated_at": "2024-07-16T06:40:54+00:00",
        "closed_at": "2024-07-16T06:40:54+00:00",
        "comments_count": [
            "tongxiaohua"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1760,
        "title": "FastDeploy在Jetson上现在支持ppyoloe+的量化模型了吗？",
        "body": "【平台】：Jetson NX\r\n【语言】: C++\r\n【硬件后端】:GPU\r\n【报错信息】:\r\n[Paddle2ONNX] [Info] The Paddle model is a quantized model. \r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including fake_channel_wise_quantize_dequantize_abs_max,fake_quantize_dequantize_moving_average_abs_max,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.",
        "state": "closed",
        "user": "wanghuqiang123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-03T06:24:23+00:00",
        "updated_at": "2024-04-16T09:03:49+00:00",
        "closed_at": "2024-04-16T09:03:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1759,
        "title": "PaddleOCR 检测模型部署问题",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】：  fastdeploy-gpu-python-1.0.1\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】：TITAN X ， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】：Python3.8\r\n\r\nPaddleOCR检测模型训练配置文件：\r\n```\r\nGlobal:\r\n  use_gpu: true\r\n  use_xpu: false\r\n  use_mlu: false\r\n  epoch_num: 150\r\n  log_smooth_window: 20\r\n  print_batch_step: 10\r\n  save_model_dir: ./output/db_mv3_0331/\r\n  save_epoch_step: 30\r\n  # evaluation is run every 2000 iterations\r\n  eval_batch_step: [0, 200]\r\n  cal_metric_during_train: False\r\n  pretrained_model: ./pretrain_models/MobileNetV3_large_x0_5_pretrained\r\n  checkpoints:\r\n  save_inference_dir:\r\n  use_visualdl: False\r\n  infer_img: doc/imgs_en/img_10.jpg\r\n  save_res_path: ./output/det_db/predicts_db.txt\r\n\r\nArchitecture:\r\n  model_type: det\r\n  algorithm: DB\r\n  Transform:\r\n  Backbone:\r\n    name: MobileNetV3\r\n    scale: 0.5\r\n    model_name: large\r\n  Neck:\r\n    name: DBFPN\r\n    out_channels: 256\r\n  Head:\r\n    name: DBHead\r\n    k: 50\r\n\r\nLoss:\r\n  name: DBLoss\r\n  balance_loss: true\r\n  main_loss_type: DiceLoss\r\n  alpha: 5\r\n  beta: 10\r\n  ohem_ratio: 3\r\n\r\nOptimizer:\r\n  name: Adam\r\n  beta1: 0.9\r\n  beta2: 0.999\r\n  lr:\r\n    learning_rate: 0.0001\r\n  regularizer:\r\n    name: 'L2'\r\n    factor: 0\r\n\r\nPostProcess:\r\n  name: DBPostProcess\r\n  thresh: 0.3\r\n  box_thresh: 0.6\r\n  max_candidates: 1000\r\n  unclip_ratio: 1.5\r\n\r\nMetric:\r\n  name: DetMetric\r\n  main_indicator: hmean\r\n\r\nTrain:\r\n  dataset:\r\n    name: SimpleDataSet\r\n    data_dir: D:/paddleocr/PaddleOCR-release-2.6/train_data/0330/\r\n    label_file_list:\r\n      - D:/paddleocr/PaddleOCR-release-2.6/train_data/0330/train_0330_Label.txt\r\n    ratio_list: [1.0]\r\n    transforms:\r\n      - DecodeImage: # load image\r\n          img_mode: BGR\r\n          channel_first: False\r\n      - DetLabelEncode: # Class handling label\r\n      - IaaAugment:\r\n          augmenter_args:\r\n            - { 'type': Fliplr, 'args': { 'p': 0.5 } }\r\n            - { 'type': Affine, 'args': { 'rotate': [-10, 10] } }\r\n            - { 'type': Resize, 'args': { 'size': [0.5, 3] } }\r\n      - EastRandomCropData:\r\n          size: [640, 640]\r\n          max_tries: 50\r\n          keep_ratio: true\r\n      - MakeBorderMap:\r\n          shrink_ratio: 0.4\r\n          thresh_min: 0.3\r\n          thresh_max: 0.7\r\n      - MakeShrinkMap:\r\n          shrink_ratio: 0.4\r\n          min_text_size: 8\r\n      - NormalizeImage:\r\n          scale: 1./255.\r\n          mean: [0.485, 0.456, 0.406]\r\n          std: [0.229, 0.224, 0.225]\r\n          order: 'hwc'\r\n      - ToCHWImage:\r\n      - KeepKeys:\r\n          keep_keys: ['image', 'threshold_map', 'threshold_mask', 'shrink_map', 'shrink_mask'] # the order of the dataloader list\r\n  loader:\r\n    shuffle: True\r\n    drop_last: False\r\n    batch_size_per_card: 4\r\n    num_workers: 1\r\n    use_shared_memory: True\r\n\r\nEval:\r\n  dataset:\r\n    name: SimpleDataSet\r\n    data_dir: D:/paddleocr/PaddleOCR-release-2.6/train_data/0330/\r\n    label_file_list:\r\n      - D:/paddleocr/PaddleOCR-release-2.6/train_data/0330/test_0330_Label.txt\r\n    transforms:\r\n      - DecodeImage: # load image\r\n          img_mode: BGR\r\n          channel_first: False\r\n      - DetLabelEncode: # Class handling label\r\n      - DetResizeForTest:\r\n          image_shape: [736, 1280]\r\n      - NormalizeImage:\r\n          scale: 1./255.\r\n          mean: [0.485, 0.456, 0.406]\r\n          std: [0.229, 0.224, 0.225]\r\n          order: 'hwc'\r\n      - ToCHWImage:\r\n      - KeepKeys:\r\n          keep_keys: ['image', 'shape', 'polys', 'ignore_tags']\r\n  loader:\r\n    shuffle: False\r\n    drop_last: False\r\n    batch_size_per_card: 1 # must be 1\r\n    num_workers: 1\r\n    use_shared_memory: True\r\n```\r\n\r\n使用训练模型预测可以正常检测出文字所在位置：\r\n```\r\npython3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/\" Global.pretrained_model=\"./output/det_db/best_accuracy\"\r\n```\r\n\r\n使用下面命令进行模型导出后，推理预测结果为空，无法检测到文字位置。\r\n导出命令：\r\n```\r\npython3 tools/export_model.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=\"./output/det_db/best_accuracy\" Global.save_inference_dir=\"./output/det_db_inference/\"\r\n```\r\n推理命令：\r\n```\r\npython3 tools/infer/predict_det.py --det_algorithm=\"DB\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True\r\n```\r\n\r\n在推理命令中添加`--det_limit_type=\"resize_long\"` 可以正常检测。\r\n```\r\npython3 tools/infer/predict_det.py --det_algorithm=\"DB\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True  --det_limit_type=\"resize_long\"\r\n```\r\n\r\n问题：如何在Fastdeploy中加载--det_limit_type=\"resize_long\"命令或者如何导出模型使Fast deploy能够进行推理？",
        "state": "closed",
        "user": "LiQiang0307",
        "closed_by": "yunyaoXYY",
        "created_at": "2023-04-02T03:34:40+00:00",
        "updated_at": "2023-04-05T15:02:33+00:00",
        "closed_at": "2023-04-05T15:02:33+00:00",
        "comments_count": [
            "LiQiang0307"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1758,
        "title": "请问官方有意让其他人参与算丰平台bm1684姿态检测相关模型或者其他模型的开源维护吗",
        "body": "我们实验室最近使用算丰硬件平台，发现fastdeploy目前在算丰平台上支持的模型很少，想问下能否参与相关维护",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-01T09:40:08+00:00",
        "updated_at": "2024-04-02T06:40:09+00:00",
        "closed_at": "2024-04-02T06:40:09+00:00",
        "comments_count": [
            "DefTruth",
            "XiaBing992",
            "leiqing1"
        ],
        "labels": [
            "BM1684"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1762,
        "title": "使用fastapi和websocket协议将推理的视频帧推送到客户端，运行报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 最新git https://github.com/PaddlePaddle/FastDeploy.git\r\n- 【编译命令】pyhton SDK\r\n- 【系统平台】: Linux arm(Ubuntu 20.04) \r\n- 【硬件】： rk3588\r\n- 【编译语言】：Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用fastapi和websocket协议将推理的视频帧推送到客户端，运行报错\r\n-INFO:     Started server process [3726]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:40102 - \"GET / HTTP/1.1\" 200 OK\r\nINFO:     ('127.0.0.1', 40110) - \"WebSocket /ws\" [accepted]\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion\trknn_api/rknnrt version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39), driver version: 0.7.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output, n_dims=4, dims=[1, 18, 80, 80], n_elems=115200, size=115200, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=1, name=272, n_dims=4, dims=[1, 18, 40, 40], n_elems=28800, size=28800, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=2, name=274, n_dims=4, dims=[1, 18, 20, 20], n_elems=7200, size=7200, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(334)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(57)::GetSDKAndDeviceVersion\trknn_api/rknnrt version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39), driver version: 0.7.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output, n_dims=4, dims=[1, 36, 80, 80], n_elems=230400, size=230400, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003919, pass_through=0\r\nindex=1, name=272, n_dims=4, dims=[1, 36, 40, 40], n_elems=57600, size=57600, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003910, pass_through=0\r\nindex=2, name=274, n_dims=4, dims=[1, 36, 20, 20], n_elems=14400, size=14400, fmt=NCHW, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(334)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(326)::Infer\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\nE RKNN: [19:47:04.881] can not find memory for freeing\r\nSegmentation fault (core dumped)\r\n\r\n",
        "state": "closed",
        "user": "Shifiter",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-03T12:13:59+00:00",
        "updated_at": "2024-04-16T09:03:50+00:00",
        "closed_at": "2024-04-16T09:03:50+00:00",
        "comments_count": [
            "Shifiter",
            "Zheng-Bicheng",
            "Shifiter",
            "Zheng-Bicheng",
            "Shifiter",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "Shifiter"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1765,
        "title": "部署分类模型时，如何输出top5？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-gpu-python-1.0.4\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(centos7) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia T4， CUDA 11.4\r\n- 【编译语言】：  Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n\r\n部署分类模型时，如何输出top5？\r\n部署命令： CUDA_VISIBLE_DEVICES=0 fastdeployserver --model-repository=/serving/models --backend-config=python,shm-default-byte-size=10485760\r\n通过python paddlecls_grpc_client.py  调用服务，输出只有1个分类。使用PPHNet模型，使用paddlecls模块训练推理没问题。\r\n\r\n按照某个相似issue，调整了config文件：\r\nFastDeploy/examples/vision/classification/paddleclas/serving/models/paddlecls/config.pbtxt：\r\n\r\nname: \"paddlecls\"\r\nplatform: \"ensemble\"\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1,-1, -1, 3 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"CLAS_RESULT\"\r\n    data_type: TYPE_STRING\r\n    dims: [ -1,5]\r\n  }\r\n]\r\nensemble_scheduling {\r\n  step [\r\n    {\r\n      model_name: \"preprocess\"\r\n      model_version: 1\r\n      input_map {\r\n        key: \"preprocess_input\"\r\n        value: \"INPUT\"\r\n      }\r\n      output_map {\r\n        key: \"preprocess_output\"\r\n        value: \"RUNTIME_INPUT\"\r\n      }\r\n    },\r\n    {\r\n      model_name: \"runtime\"\r\n      model_version: 1\r\n      input_map {\r\n        key: \"inputs\"\r\n        value: \"RUNTIME_INPUT\"\r\n      }\r\n      output_map {\r\n        key: \"save_infer_model/scale_0.tmp_1\"\r\n        value: \"RUNTIME_OUTPUT\"\r\n      }\r\n    },\r\n    {\r\n      model_name: \"postprocess\"\r\n      model_version: 1\r\n      input_map {\r\n        key: \"post_input\"\r\n        value: \"RUNTIME_OUTPUT\"\r\n      }\r\n      output_map {\r\n        key: \"post_output\"\r\n        value: \"CLAS_RESULT\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nFastDeploy/examples/vision/classification/paddleclas/serving/models/preprocess/config.pbtxt：\r\nname: \"preprocess\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"preprocess_input\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1, -1, -1, 3 ]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"preprocess_output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1,3, 224, 224 ]\r\n  }\r\n]\r\n\r\n但是仍然只输出了一个分类，请问是否有参数填写错误？\r\n",
        "state": "closed",
        "user": "chengshaoxi0",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-04T07:54:56+00:00",
        "updated_at": "2024-04-16T09:03:51+00:00",
        "closed_at": "2024-04-16T09:03:51+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1764,
        "title": "无法重现Stable Diffusion推理速度",
        "body": "## 环境\r\n\r\n- 【版本】：\r\nfastdeploy-gpu-python   1.0.5\r\npaddle-bfloat            0.1.7\r\npaddle2onnx              1.0.6\r\npaddlefsl                1.1.0\r\npaddlenlp                2.5.2\r\npaddlepaddle-gpu         0.0.0.post117\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-runtime-cu11 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\npython 3.10\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： A100-40GB\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n模型从https://bj.bcebos.com/fastdeploy/models/stable-diffusion/runwayml/stable-diffusion-v1-5.tgz 下载的\r\n\r\n···\r\n/FastDeploy/examples/multimodal/stable_diffusion$ python infer.py --model_dir stable-diffusion-v1-5/ --scheduler \"euler_ancestral\" --backend paddle --inference_steps 50\r\n[2023-04-04 07:24:35,868] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/vocab.json\r\n[2023-04-04 07:24:35,869] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/merges.txt\r\n[2023-04-04 07:24:35,869] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/openai/clip-vit-large-patch14/added_tokens.json and saved to /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14\r\n[2023-04-04 07:24:36,849] [ WARNING] - file<https://bj.bcebos.com/paddlenlp/models/community/openai/clip-vit-large-patch14/added_tokens.json> not exist\r\n[2023-04-04 07:24:36,849] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/special_tokens_map.json\r\n[2023-04-04 07:24:36,850] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/tokenizer_config.json\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackendRuntime initialized with Backend::ORT in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackendRuntime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackendRuntime initialized with Backend::PDINFER in Device::GPU.\r\nSpend  11.65 s to load unet model.\r\nRun the stable diffusion pipeline 1 times to test the performance.\r\nNo   0 time cost: 4.535359 s\r\nMean latency: 4.535359 s, p50 latency: 4.535359 s, p90 latency: 4.535359 s, p95 latency: 4.535359 s.\r\n\r\n\r\nwget https://bj.bcebos.com/fastdeploy/models/stable-diffusion/CompVis/stable-diffusion-v1-4.tgz\r\n/FastDeploy/examples/multimodal/stable_diffusion$tar -xvzf stable-diffusion-v1-4.tgz\r\n/FastDeploy/examples/multimodal/stable_diffusion$python infer.py --model_dir stable-diffusion-v1-4/ --backend paddle --inference_steps 50 --use_fp16 1 --scheduler pndm --benchmark_steps 10\r\n[2023-04-04 07:56:04,939] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/vocab.json\r\n[2023-04-04 07:56:04,939] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/merges.txt\r\n[2023-04-04 07:56:04,939] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/openai/clip-vit-large-patch14/added_tokens.json and saved to /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14\r\n[2023-04-04 07:56:05,907] [ WARNING] - file<https://bj.bcebos.com/paddlenlp/models/community/openai/clip-vit-large-patch14/added_tokens.json> not exist\r\n[2023-04-04 07:56:05,908] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/special_tokens_map.json\r\n[2023-04-04 07:56:05,908] [    INFO] - Already cached /home/turinguser/.paddlenlp/models/openai/clip-vit-large-patch14/tokenizer_config.json\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackendRuntime initialized with Backend::ORT in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackendRuntime initialized with Backend::PDINFER in Device::GPU.\r\nwget https://bj.bcebos.com/fastdeploy/models/stable-diffusion/CompVis/stable-diffusion-v1-4.tgz[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackendRuntime initialized with Backend::PDINFER in Device::GPU.\r\nSpend  11.83 s to load unet model.\r\nRun the stable diffusion pipeline 5 times to test the performance.\r\nNo   0 time cost: 4.607649 s\r\nNo   1 time cost: 4.613580 s\r\nNo   2 time cost: 4.601662 s\r\nNo   3 time cost: 4.602602 s\r\nNo   4 time cost: 4.606175 s\r\nMean latency: 4.606334 s, p50 latency: 4.606175 s, p90 latency: 4.611208 s, p95 latency: 4.612394 s.\r\nImage saved in fd_astronaut_rides_horse.png!\r\n···\r\n\r\n速度大概是4.5~4.6秒，比Blog (https://blog.csdn.net/PaddlePaddle/article/details/129426638) 上的0.76秒差太远。",
        "state": "closed",
        "user": "tianleiwu",
        "closed_by": "tianleiwu",
        "created_at": "2023-04-04T07:39:52+00:00",
        "updated_at": "2023-05-24T20:03:43+00:00",
        "closed_at": "2023-05-24T20:03:43+00:00",
        "comments_count": [
            "wwbitejotunn"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1763,
        "title": "RKNPU2 板端推理报错: Invalid RKNN format!",
        "body": "- 【模型跑不通】\r\n环境：py38   rknn-toolkit2  ----- 1.4.2b0-df08fb60，在RK3588板端运行的时候报错\r\n模型：用的你们给的resent50，先paddle2onnx（1.0.6），再用export.py转成rknn模型\r\n最终在板端运行出现如下错误\r\n./rknpu_test ./demo_model ./demo_model/ILSVRC2012_val_00000010.jpeg\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nE RKNN: [13:57:54.317] parseRKNN from path: Invalid RKNN format!\r\nE RKNN: [13:57:54.317] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(157)::LoadModel    rknn_init fail! ret=-1\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(100)::Init load model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(328)::CreateRKNPU2Backend Failed to initialize RKNPU2 backend.\r\nAborted\r\n",
        "state": "closed",
        "user": "ZHR1997",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-04-03T12:26:08+00:00",
        "updated_at": "2023-04-07T02:26:19+00:00",
        "closed_at": "2023-04-07T02:20:30+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997",
            "ZHR1997",
            "Zheng-Bicheng",
            "ZHR1997"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1769,
        "title": "识别效果不一样",
        "body": null,
        "state": "closed",
        "user": "Steve662024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T01:49:27+00:00",
        "updated_at": "2024-04-16T09:03:52+00:00",
        "closed_at": "2024-04-16T09:03:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1773,
        "title": "是否支持在昇腾910的CPU（aarch64）上使用fastdeployserver部署erine3.0模型？",
        "body": "是否支持在昇腾910的CPU（aarch64）上使用fastdeployserver部署erine3.0模型？如果可以的话要怎么做呢？",
        "state": "closed",
        "user": "996-icu-FuJian",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T06:21:23+00:00",
        "updated_at": "2024-04-16T09:03:53+00:00",
        "closed_at": "2024-04-16T09:03:53+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1768,
        "title": "package_data must be a dictionary mapping package names to lists of wildcard patterns",
        "body": "我根据源码流程安装在altas 300I 显卡上按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md\r\n方式安装python包时候遇到了标题问题，请问如何解决？\r\n\r\n\r\n[root@localhost python]# export ENABLE_VISION=ON\r\n[root@localhost python]# export WITH_ASCEND=ON\r\n[root@localhost python]# python setup.py build\r\nerror in fastdeploy-python setup command: package_data must be a dictionary mapping package names to lists of wildcard patterns\r\n\r\n",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "jia0511",
        "created_at": "2023-04-05T02:18:30+00:00",
        "updated_at": "2023-04-12T14:28:57+00:00",
        "closed_at": "2023-04-12T14:28:56+00:00",
        "comments_count": [
            "DefTruth",
            "jia0511",
            "jia0511",
            "jia0511"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1771,
        "title": "容器内部连不了外网",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nsudo docker run -it  --name trt1 --net=host  --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --dns 8.8.8.8 --dns 114.114.114.114  -v `pwd`/:/FastDeploy registry.baidubce.com/paddlepaddle/fastdeploy:1.0.5-gpu-cuda11.4-trt8.5-21.10  bash\r\n\r\n容器内部连不了外网\r\n\r\n",
        "state": "closed",
        "user": "zyl75391",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T03:41:34+00:00",
        "updated_at": "2024-04-16T09:03:53+00:00",
        "closed_at": "2024-04-16T09:03:53+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1770,
        "title": "RK3568平台上进行PP-OCR模型部署出现推理失败的问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-develop\r\n- 【系统平台】: Linux firefly 4.19.232 / Debian 10\r\n- 【硬件】：  firefly AIO-3568J\r\n- 【编译语言】： C++\r\n\r\n## 报错内容\r\nfirefly@firefly:~/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ ./infer_demo ./ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx ./ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.onnx ./ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer.onnx ./ppocr_keys_v1.txt ./12.jpg 1\r\nE RKNN: [02:14:42.649] parseRKNN: invalid RKNN_MAGIC!\r\nE RKNN: [02:14:42.649] parseRKNN from path: Invalid RKNN format!\r\nE RKNN: [02:14:42.649] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(157)::LoadModel rknn_init fail! ret=-6\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(100)::Init load model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(328)::CreateRKNPU2Backend Failed to initialize RKNPU2 backend.\r\nAborted\r\n\r\n## PC端onnx转rknn模型环境\r\n【Package 】【 Version】\r\nabsl-py                 0.15.0\r\nastunparse              1.6.3\r\ncached-property         1.5.2\r\ncachetools              4.2.4\r\ncertifi                 2020.6.20\r\ncharset-normalizer      2.0.12\r\nclang                   5.0\r\ndataclasses             0.8\r\nfast-histogram          0.11\r\nflatbuffers             1.12\r\ngast                    0.4.0\r\ngoogle-auth             1.35.0\r\ngoogle-auth-oauthlib    0.4.6\r\ngoogle-pasta            0.2.0\r\ngrpcio                  1.48.2\r\nh5py                    3.1.0\r\nidna                    3.4\r\nimportlib-metadata      4.8.3\r\nimportlib-resources     5.4.0\r\nkeras                   2.6.0\r\nKeras-Preprocessing     1.1.2\r\nMarkdown                3.3.7\r\nnumpy                   1.19.5\r\noauthlib                3.2.2\r\nonnx                    1.10.0\r\nonnxoptimizer           0.2.7\r\nonnxruntime             1.10.0\r\nopencv-python           4.5.5.64\r\nopt-einsum              3.3.0\r\npaddle2onnx             1.0.6\r\nPillow                  8.4.0\r\npip                     21.3.1\r\nprotobuf                3.12.2\r\npsutil                  5.9.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\nPyYAML                  6.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrknn-toolkit2           1.4.3b4+48391b8f\r\nrsa                     4.9\r\nruamel.yaml             0.17.4\r\nruamel.yaml.clib        0.2.7\r\nscipy                   1.5.4\r\nsetuptools              59.6.0\r\nsix                     1.15.0\r\ntensorboard             2.6.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\ntensorflow              2.6.2\r\ntensorflow-estimator    2.6.0\r\ntermcolor               1.1.0\r\ntorch                   1.10.1\r\ntorchvision             0.11.2\r\ntqdm                    4.64.0\r\ntyping-extensions       3.7.4.3\r\nurllib3                 1.26.15\r\nWerkzeug                2.0.3\r\nwheel                   0.37.1\r\nwrapt                   1.12.1\r\nzipp                    3.6.0\r\n\r\n## 模型转换操作流程\r\n[export_log.txt](https://github.com/PaddlePaddle/FastDeploy/files/11165915/export_log.txt)\r\n\r\n## 板端环境编译流程\r\n在PC端完成交叉编译后在板端进入下面路径下进行编译\r\n> firefly@firefly:~/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ cmake .. -DFASTDEPLOY_INSTALL_DIR=~/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0\r\n-- The C compiler identification is GNU 8.3.0\r\n-- The CXX compiler identification is GNU 8.3.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- The path of RKNPU2 is /home/firefly/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0/third_libs/install/rknpu2_runtime/lib/librknnrt.so.\r\n-- The path of OpenCV is /home/firefly/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0/third_libs/install/opencv.\r\n-- Found OpenCV: /home/firefly/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0/third_libs/install/opencv (found version \"4.6.0\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.13.4\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 8.3.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : ON\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   DEPENDENCY_LIBS           : /home/firefly/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0/lib/libfastdeploy.so;/home/firefly/FastDeploy_RK3568/FastDeploy/build/fastdeploy-0.0.0/third_libs/install/rknpu2_runtime/lib/librknnrt.so;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_gapi;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_stitching;opencv_video;opencv_videoio\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/firefly/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build\r\nfirefly@firefly:~/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ make -j\r\nScanning dependencies of target infer_demo\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer.cc.o\r\n[100%] Linking CXX executable infer_demo\r\n[100%] Built target infer_demo\r\nfirefly@firefly:~/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ ls\r\nCMakeCache.txt  CMakeFiles  Makefile  cmake_install.cmake  infer_demo\r\n\r\n\r\n\r\n## 板端推理操作流程\r\nfirefly@firefly:~/FastDeploy_RK3568/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ ./infer_demo ./ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx ./ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.onnx ./ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer.onnx ./ppocr_keys_v1.txt ./12.jpg 1\r\nE RKNN: [02:14:42.649] parseRKNN: invalid RKNN_MAGIC!\r\nE RKNN: [02:14:42.649] parseRKNN from path: Invalid RKNN format!\r\nE RKNN: [02:14:42.649] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(157)::LoadModel\trknn_init fail! ret=-6\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(100)::Init\tload model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(328)::CreateRKNPU2Backend\tFailed to initialize RKNPU2 backend.\r\nAborted\r\n## 另外通过netron查看转换后的rknn模型，无法可视化，会提示Unsupported container version ‘4’\r\n![image](https://user-images.githubusercontent.com/85120075/230259316-43b10f47-3bcf-499a-9c2a-52e0b15e1d65.png)\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-04-06T02:59:01+00:00",
        "updated_at": "2023-04-06T03:04:58+00:00",
        "closed_at": "2023-04-06T03:04:58+00:00",
        "comments_count": [
            "MrMzl"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1774,
        "title": "ocr fastdeploy部署gpu服务端报错：failed to load all models",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： docker image: registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：  Nvidia GPU ， CUDA 12.0\r\n参考https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving\r\n\r\n## 问题日志及出现问题的操作流程\r\n执行fastdeployserver --model-repository=/ocr_serving/models 启动服务时出错 ，日志如下：\r\nError: Failed to initialize NVML\r\nW0406 06:06:25.695839 97 metrics.cc:221] DCGM unable to start: DCGM initialization error\r\nW0406 06:06:25.696071 97 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0406 06:06:25.696095 97 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nE0406 06:06:25.697896 97 model_repository_manager.cc:1890] Poll failed for model directory 'cls_runtime': instance group cls_runtime_0 of model cls_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0406 06:06:25.699168 97 model_repository_manager.cc:1890] Poll failed for model directory 'det_runtime': instance group det_runtime_0 of model det_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0406 06:06:25.700564 97 model_repository_manager.cc:1890] Poll failed for model directory 'rec_runtime': instance group rec_runtime_0 of model rec_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0406 06:06:25.700604 97 model_repository_manager.cc:1375] Invalid argument: ensemble rec_pp contains models that are not available: rec_runtime\r\nE0406 06:06:25.700613 97 model_repository_manager.cc:1375] Invalid argument: ensemble pp_ocr contains models that are not available: det_runtime\r\nE0406 06:06:25.700619 97 model_repository_manager.cc:1375] Invalid argument: ensemble cls_pp contains models that are not available: cls_runtime\r\nI0406 06:06:25.700740 97 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\nI0406 06:06:25.802928 97 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0406 06:06:25.812457 97 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0406 06:06:25.903565 97 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0406 06:06:26.004031 97 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0406 06:06:26.082822 97 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\nI0406 06:06:26.083348 97 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0406 06:06:26.329773 97 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0406 06:06:26.329891 97 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0406 06:06:26.601789 97 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\nI0406 06:06:26.602037 97 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0406 06:06:26.882847 97 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\nI0406 06:06:26.882963 97 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0406 06:06:26.883003 97 server.cc:549] \r\n+---------+-------------------------------------------------------+--------+\r\n| Backend | Path                                                  | Config |\r\n+---------+-------------------------------------------------------+--------+\r\n| python  | /opt/tritonserver/backends/python/libtriton_python.so | {}     |\r\n+---------+-------------------------------------------------------+--------+\r\n\r\nI0406 06:06:26.883037 97 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0406 06:06:26.883137 97 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                 |\r\n| server_version                   | 2.15.0                                                                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\r\n| model_repository_path[0]         | /ocr_serving/models                                                                                                                                                                    |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                              |\r\n| strict_model_config              | 1                                                                                                                                                                                      |\r\n| rate_limit                       | OFF                                                                                                                                                                                    |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\r\n| response_cache_byte_size         | 0                                                                                                                                                                                      |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0406 06:06:26.883193 97 server.cc:252] Waiting for in-flight requests to complete.\r\nI0406 06:06:26.883202 97 model_repository_manager.cc:1055] unloading: rec_postprocess:1\r\nI0406 06:06:26.883252 97 model_repository_manager.cc:1055] unloading: det_preprocess:1\r\nI0406 06:06:26.883298 97 model_repository_manager.cc:1055] unloading: det_postprocess:1\r\nI0406 06:06:26.883367 97 model_repository_manager.cc:1055] unloading: cls_postprocess:1\r\nI0406 06:06:26.883393 97 server.cc:267] Timeout 30: Found 4 live models and 0 in-flight non-inference requests\r\nI0406 06:06:27.883475 97 server.cc:267] Timeout 29: Found 4 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nCleaning up...\r\nCleaning up...\r\nCleaning up...\r\nI0406 06:06:27.944011 97 model_repository_manager.cc:1166] successfully unloaded 'det_postprocess' version 1\r\nI0406 06:06:27.945687 97 model_repository_manager.cc:1166] successfully unloaded 'cls_postprocess' version 1\r\nI0406 06:06:27.945913 97 model_repository_manager.cc:1166] successfully unloaded 'rec_postprocess' version 1\r\nI0406 06:06:27.946568 97 model_repository_manager.cc:1166] successfully unloaded 'det_preprocess' version 1\r\nI0406 06:06:28.883892 97 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n\r\n\r\n",
        "state": "closed",
        "user": "Rayna0122",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T06:27:44+00:00",
        "updated_at": "2025-02-25T06:45:09+00:00",
        "closed_at": "2025-02-25T06:45:09+00:00",
        "comments_count": [
            "DefTruth",
            "heliqi",
            "duyanfang123",
            "neo502721"
        ],
        "labels": [
            "Serving"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1775,
        "title": "fastdeploy ocr推理精度丢失",
        "body": "通过PaddleOCR.ocr python脚本运行的推理结果和fastDeploy ocr推理结果不一致。\r\nPaddleCOR.ocr推理脚本参考:https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_ch/quickstart.md#221-%E4%B8%AD%E8%8B%B1%E6%96%87%E4%B8%8E%E5%A4%9A%E8%AF%AD%E8%A8%80%E4%BD%BF%E7%94%A8 \r\nfastDeploy ocr推理参考文档:https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/cpu-gpu/python#paddleocr-cpu-gpu-python%E9%83%A8%E7%BD%B2%E7%A4%BA%E4%BE%8B \r\n\r\nPaddleCOR ocr推理结果日志输出如下:\r\n> [2023/04/06 14:34:37] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Administrator/.paddleocr/whl\\\\det\\\\ch\\\\ch_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, **use_dilation=True**, **det_db_score_mode='slow'**, det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Administrator/.paddleocr/whl\\\\rec\\\\ch\\\\ch_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\Administrator/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=True, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, lang='ch', det=True, rec=True, type='ocr', ocr_version='PP-OCRv3', structure_version='PP-StructureV2')\r\n[2023/04/06 14:36:18] ppocr DEBUG: dt_boxes num : 5, elapse : 0.31116414070129395\r\n[2023/04/06 14:36:18] ppocr DEBUG: cls num  : 5, elapse : 0.052858829498291016\r\n[2023/04/06 14:36:18] ppocr DEBUG: rec_res num  : 5, elapse : 0.2293851375579834\r\n[[[137.0, 119.0], [732.0, 119.0], [732.0, 162.0], [137.0, 162.0]], ('哈哈哈哈哈哈哈哈哈哈或', 0.9856414198875427)]\r\n[[[137.0, 189.0], [732.0, 189.0], [732.0, 229.0], [137.0, 229.0]], ('或或或或或或或或或或或', 0.9473259449005127)]\r\n**[[[137.0, 257.0], [731.0, 257.0], [731.0, 297.0], [137.0, 297.0]], ('或或或或或或或或或或或', 0.9495126605033875)]**\r\n[[[133.0, 322.0], [288.0, 322.0], [288.0, 368.0], [133.0, 368.0]], ('或或或', 0.8740523457527161)]\r\n\r\nfastDeploy ocr推理结果日志如下:\r\n> WARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0406 14:36:30.014596   944 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(309)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::CPU.\r\ndet boxes: [[136,118],[732,118],[732,162],[136,162]]rec text: 哈哈哈哈哈哈哈哈哈哈或 rec score:0.952564 cls label: 0 cls score: 1.000000\r\ndet boxes: [[136,189],[732,189],[732,228],[136,228]]rec text: 或或或或或或或或或或或 rec score:0.834911 cls label: 1 cls score: 0.764378\r\n**det boxes: [[136,256],[731,256],[731,296],[136,296]]rec text: 尚验指指务指验指 rec score:0.065504 cls label: 1 cls score: 0.958775**\r\ndet boxes: [[133,322],[288,322],[288,368],[133,368]]rec text: 或或或 rec score:0.869063 cls label: 0 cls score: 1.000000\r\ndet boxes: [[158,382],[344,382],[344,613],[158,613]]rec text: 車 rec score:0.358460 cls label: 0 cls score: 0.925917\r\n\r\nfastDeploy ocr推理执行脚本:\r\n\r\n> python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image bbbb.jpg --device cpu --backend paddle --rec_bs 10 --cls_bs 6 \r\n\r\n对应推理所使用参数如下:\r\n> rec_model:{\r\n\tis_scale: True\r\n\tmean: [0.5, 0.5, 0.5]\r\n\trec_image_shape: [3, 48, 320]\r\n\tscale: [0.5, 0.5, 0.5]\r\n\tstatic_shape_infer: False\r\n}\r\ndet_model:{\r\n\tdet_db_box_thresh: 0.6\r\n\t**det_db_score_mode: 'slow'**\r\n\tdet_db_thresh: 0.3\r\n\tdet_db_unclip_ratio: 1.5\r\n\tis_scale: True\r\n\tmax_side_len: 960\r\n\tscale: [0.2290000021457672, 0.2240000069141388, 0.22499999403953552]\r\n\t**use_dilation: 1**\r\n}\r\ncls_model:{\r\n\tcls_image_shape: [3, 48, 192]\r\n\tcls_thresh: 0.8999999761581421\r\n\tis_scale: True\r\n\tmean: [0.5, 0.5, 0.5]\r\n}\r\nppocr:{\r\n\tcls_batch_size: 6\r\n\trec_batch_size: 10\r\n}\r\n\r\npaddleocr本机版本为:2.6.1.2 ,fastDeploy ocr推理把第三行文本\"或或或或或或或或或或或\"识别成了“尚验指指务指验指”。对比过两者所使用的推理参数，在fastdeploy直接暴露可设置的参数均和前者一致。附件为ocr对应的原始图片\r\n![bbbb](https://user-images.githubusercontent.com/49382323/230296314-a68ff408-68d0-4d2d-8fcb-8708bc4a5f58.jpg)\r\n\r\n",
        "state": "closed",
        "user": "kerry-weic",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T07:07:17+00:00",
        "updated_at": "2024-12-24T09:40:37+00:00",
        "closed_at": "2024-04-12T08:18:55+00:00",
        "comments_count": [
            "yunyaoXYY",
            "kerry-weic",
            "kerry-weic",
            "yunyaoXYY",
            "xujiang1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1777,
        "title": "请问使用runtime的时候，如何将实际输入的cv::Mat类型转为对应的tensor呢?",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： jetson\r\n- 【编译语言】： C++\r\n\r\n请问使用runtime的时候，如何将实际输入的cv::Mat类型转为对应的tensor呢?\r\n\r\n<img width=\"716\" alt=\"fd66ee085ad632f345f55d414886adc\" src=\"https://user-images.githubusercontent.com/54393886/230326705-0c612a7d-7190-44de-98d1-f3bd190befad.png\">\r\n",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T08:54:32+00:00",
        "updated_at": "2024-04-12T08:18:57+00:00",
        "closed_at": "2024-04-12T08:18:56+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1776,
        "title": "量化训练",
        "body": "\r\nWARNING:root:cannot import name 'layers' from 'parl' (G:\\anaconda\\envs\\paddle_slim\\lib\\site-packages\\parl\\__init__.py)\r\nG:\\anaconda\\envs\\paddle_slim\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n2023-04-06 15:23:01,526-WARNING: post-quant-hpo is not support in system other than linux\r\nWelcome to use FastDeploy Auto Compression Toolkit!\r\nFinish Compression, total time used is :  0.0 seconds.\r\n",
        "state": "closed",
        "user": "zerobest",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T07:26:41+00:00",
        "updated_at": "2024-04-12T08:18:56+00:00",
        "closed_at": "2024-04-12T08:18:56+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1778,
        "title": "ppyoloe_plus转换为RKNN模型，运行结果异常",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-1.0.0\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 瑞芯微 RK3588\r\n- 【编译语言】： Python3.9.16\r\n\r\n## 【模型结果异常】\r\n\r\n根据教程将ppyoloe_plus模型量化后执行，报Segmentation fault (core dumped)。若是不量化模型，结果如下：\r\n\r\n```\r\n-inf,6132.000000, -inf, 2654.000000, 0.976562, 0\r\n-63360.000000,3370.000000, inf, 3654.000000, 0.971191, 0\r\ninf,-886.000000, inf, 2204.000000, 0.831055, 0\r\n-inf,-3836.000000, inf, -4360.000000, 0.771973, 0\r\n-57984.000000,8560.000000, -inf, 5464.000000, 0.548828, 0\r\ninf,2314.000000, inf, 1123.000000, 0.525879, 0\r\ninf,5076.000000, inf, -237.250000, 0.398193, 0\r\n```\r\n\r\n请问Fastdeploy是否支持将ppyoloe_plus模型转换为RKNN，因为即使[rknn_model_zoo](https://github.com/airockchip/rknn_model_zoo/blob/main/models/CV/object_detection/yolo/RKNN_model_convert/README.md)明确提到了支持ppyoloe_plus，但我却没有在官方仓库找到能够运行paddle模型的配置项。",
        "state": "closed",
        "user": "a31c8j",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T14:25:19+00:00",
        "updated_at": "2024-04-12T08:18:57+00:00",
        "closed_at": "2024-04-12T08:18:57+00:00",
        "comments_count": [
            "Zheng-Bicheng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1781,
        "title": "about multi-thread, Will ORT be supported in the future?",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/0e67d95d57eb572ff9c57181b60c31ce4172bdd1/fastdeploy/runtime/runtime.cc#L356\r\n\r\n![image](https://user-images.githubusercontent.com/56470579/230640432-385ff5cb-60c5-4978-af16-438ee5a2af9c.png)\r\n\r\n",
        "state": "closed",
        "user": "heiyiyayoo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-07T16:06:17+00:00",
        "updated_at": "2024-04-16T09:03:54+00:00",
        "closed_at": "2024-04-16T09:03:54+00:00",
        "comments_count": [
            "heiyiyayoo"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1783,
        "title": "[windows] fastdeploy PPOCRv3 按照示例代码运行后有glog的WARNING",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [fastdeploy-win-x64-1.0.5.zip](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-win-x64-1.0.5.zip)\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： cpu\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n代码是用的PPOCRv3的这份示例代码 [infer_rec.cc](https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/deploy/fastdeploy/cpu-gpu/cpp/infer_rec.cc)\r\n模型是用 中英文超轻量PP-OCRv3模型的  [识别模型](https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar)\r\n字典文件 [字典](https://gitee.com/paddlepaddle/PaddleOCR/raw/release/2.6/ppocr/utils/ppocr_keys_v1.txt)\r\nvs2022 编译\r\n\r\n代码可以正常运行，但是会出现下面这个glog的WARNING\r\n\r\n![image](https://user-images.githubusercontent.com/97545700/230734837-91423bb5-36bf-43fc-b79d-841f3b6250eb.png)\r\n",
        "state": "closed",
        "user": "dongwlin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-08T17:46:02+00:00",
        "updated_at": "2024-04-12T08:18:58+00:00",
        "closed_at": "2024-04-12T08:18:58+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1779,
        "title": "Illegal instruction (core dumped)/RuntimeError: FastDeploy initalized failed!  Jetson Nano",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】官方历程编译\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Jetson Nano Developer Kit / JetPack4.6.1 / CUDA10.2 \r\n- 【编译语言】： Python3.6.9\r\n\r\n\r\n- 【模型跑不通】\r\n官方例程中\r\nppyoloe模型无法正常编译，报错\r\nIllegal instruction (core dumped)\r\n尝试import fastdeploy\r\n报错\r\nRuntimeError: FastDeploy initalized failed!\r\n通过nano ~/.bashrc并在最后一行加入export OPENBLAS_CORETYPE=ARMV8后再次运行出现如下错误\r\n  File \"infer_picodet.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\nModuleNotFoundError: No module named 'fastdeploy'\r\n\r\n",
        "state": "closed",
        "user": "ZnSn399379",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-06T23:49:51+00:00",
        "updated_at": "2024-04-16T06:40:25+00:00",
        "closed_at": "2024-04-16T06:40:25+00:00",
        "comments_count": [
            "ZnSn399379",
            "DefTruth",
            "juddvinet"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1784,
        "title": "fastdeploy能部署fastdepth吗，部署pcwnet深度估计",
        "body": null,
        "state": "closed",
        "user": "jiangq195",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T02:17:59+00:00",
        "updated_at": "2024-04-12T08:18:59+00:00",
        "closed_at": "2024-04-12T08:18:59+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1785,
        "title": "VisualDL: fastdeploy is required for visualizing results",
        "body": "## Environment\r\n\r\nFastDeploy version: fastdeploy-gpu-python 1.0.5\r\nOS Platform:  Linux x64\r\nHardware:  Nvidia GPU 3090  CUDA 11.7\r\nProgram Language: Python 3.8\r\n\r\nVisualDL: visualdl              2.5.1\r\n\r\n## Problem description\r\nOn \"Download pre-trained model\" dialogue, error encountered:\r\n```fastdeploy is required for visualizing results，please refer to https://github.com/PaddlePaddle/FastDeploy to install fastdeploy```\r\n",
        "state": "closed",
        "user": "baudzhou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T02:46:13+00:00",
        "updated_at": "2024-04-16T09:03:55+00:00",
        "closed_at": "2024-04-16T09:03:55+00:00",
        "comments_count": [],
        "labels": [
            "VisualDL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1788,
        "title": "【FastDeploy问卷调研】飞桨AI套件产品试用意愿",
        "body": "飞桨诚邀您参与AI套件产品试用意愿调研～我们期待您的实际需求反馈，您的每一条建议都是飞桨不断前行的动力！本次调研将占用您2分钟时间进行填写，我们将对您的信息保密，另外在填写问卷用户中随机抽奖和送出福利，期待各位的反馈！\r\n问卷地址：https://iwenjuan.baidu.com/?code=f6vrji",
        "state": "closed",
        "user": "leiqing1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T06:00:39+00:00",
        "updated_at": "2024-04-16T09:03:56+00:00",
        "closed_at": "2024-04-16T09:03:56+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1787,
        "title": "请问Release和Develop有什么区别",
        "body": "![image](https://user-images.githubusercontent.com/48303408/230836290-b1d262a5-6af9-438a-8df1-b70dcc20f850.png)\r\n",
        "state": "closed",
        "user": "Dandelion111",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T05:56:50+00:00",
        "updated_at": "2024-04-12T08:19:00+00:00",
        "closed_at": "2024-04-12T08:19:00+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1789,
        "title": "rk3568运行目标检测时想用系统的opencv",
        "body": "## Environment\r\n\r\nFastDeploy version: the latest code in develop branch\r\nOS Platform: e.g. Linux x64 arm \r\nHardware: 3568\r\nProgram Language:c\r\n\r\n## Problem description\r\n我现在应用要用到opencv读取图像后用imshow显示，但提示报错，查了下，opencv是用的fastdeploy里面自带的第三方库，估计是编译时候什么开关没有打开，于是我改了cmake后，用自己系统安装的opencv，在log里可以看到opencv的路径是/usr/local，编译也是成功的，但在运行时为什么还是找的fastdeploy里面自带的第三方opencv？\r\n\r\n请问如果何才能用系统里面的opencv?\r\n\r\nlog如下：\r\nroot@localhost:/opt/FastDeploy-develop/examples/vision/detection/paddledetection/rknpu2/cpp/build# make\r\n-- The path of RKNPU2 is /opt/fastdeploy-0.0.0/third_libs/install/rknpu2_runtime/lib/librknnrt.so.\r\n-- The path of OpenCV is /usr/local.\r\n-- Found OpenCV: /usr/local (found version \"4.5.1\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : ON\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   DEPENDENCY_LIBS           : /opt/fastdeploy-0.0.0/lib/libfastdeploy.so;/opt/fastdeploy-0.0.0/third_libs/install/rknpu2_runtime/lib/librknnrt.so;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui\r\n\r\nroot@localhost:/opt/FastDeploy-develop/examples/vision/detection/paddledetection/rknpu2/cpp/build# ./infer_picodet_demo me.jpg 1\r\n./infer_picodet_demo: error while loading shared libraries: libopencv_video.so.406: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "LiangYongAI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T06:39:50+00:00",
        "updated_at": "2024-04-30T06:42:43+00:00",
        "closed_at": "2024-04-30T06:42:43+00:00",
        "comments_count": [
            "DefTruth",
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK356X"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1790,
        "title": "编译FastDeploy Python SDK",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n编译FastDeploy Python SDK过程中\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\n\r\n# 如果您使用的是develop分支输入以下命令\r\ngit checkout develop\r\n\r\ncd python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_RKNPU2_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\n# 请根据你的开发版的不同，选择RK3588和RK356X\r\nexport RKNN2_TARGET_SOC=RK3588\r\n\r\n# 如果你的核心板的运行内存大于等于8G，我们建议您执行以下命令进行编译。\r\npython3 setup.py build\r\n# 值得注意的是，如果你的核心板的运行内存小于8G，我们建议您执行以下命令进行编译。\r\npython3 setup.py build -j1\r\n\r\npython3 setup.py bdist_wheel\r\ncd dist\r\npip3 install fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl\r\n当执行python3 setup.py build -j1时报出如下错误Error copying directory from \"/home/orangepi/workspace/code/FastDeploy-develop/python/.setuptools-cmake-build/third_libs/install\" to \"/home/orangepi/workspace/code/FastDeploy-develop/python/fastdeploy/libs/third_libs\".\r\nmake[2]: *** [CMakeFiles/copy_third_libraries.dir/build.make:57: CMakeFiles/copy_third_libraries] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:103: CMakeFiles/copy_third_libraries.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 437, in <module>\r\n    license='Apache 2.0')\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/__init__.py\", line 87, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n    super().run_command(command)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/command/build.py\", line 132, in run\r\n    self.run_command(cmd_name)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n    super().run_command(command)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 280, in run\r\n    self.run_command('cmake_build')\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n    super().run_command(command)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 274, in run\r\n    subprocess.check_call(build_args)\r\n  File \"/home/anaconda/envs/paddle/lib/python3.7/subprocess.py\", line 363, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/cmake', '--build', '.', '--', '-j', '1']' returned non-zero exit status 2.",
        "state": "closed",
        "user": "zerobest",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T07:50:21+00:00",
        "updated_at": "2024-04-12T08:19:01+00:00",
        "closed_at": "2024-04-12T08:19:01+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1791,
        "title": "segment-anything 能否支持",
        "body": "https://github.com/facebookresearch/segment-anything\r\n![微信截图_20230410112431](https://user-images.githubusercontent.com/6490927/230864550-7d241b7b-621c-4ab1-ab19-9a0b1a7a8779.png)\r\n\r\n\r\n一切都能分割\r\n\r\n分割任何模型 （SAM） 根据输入提示（如点或框）生成高质量的对象蒙版，并可用于为图像中的所有对象生成蒙版。它已经在 11 万张图像和 1 亿个掩码的[数据集](https://segment-anything.com/dataset/index.html)上进行了训练，并且在各种分割任务上具有强大的零镜头性能。\r\n\r\n这个已经转换为onnx\r\n怎么用？",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T08:30:07+00:00",
        "updated_at": "2024-11-05T06:41:27+00:00",
        "closed_at": "2024-11-05T06:41:27+00:00",
        "comments_count": [
            "DefTruth",
            "umie0128"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1792,
        "title": "自训练PPYOLOE模型转RKNN格式模型过程中遇到的问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【模型转换环境】: rknn-toolkit2版本为1.4.3b4+48391b8f\r\n- 【转换环境详细包】:\r\n[requirements.txt](https://github.com/PaddlePaddle/FastDeploy/files/11189801/requirements.txt)\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【所用的自训练模型文件】\r\n- - 参考[ai studio中的项目](https://aistudio.baidu.com/aistudio/projectdetail/5921868) ，使用项目训练好的模型动转静导出Paddle模型文件，导出时设置export.nms=True，运行代码如下所示:`python tools/export_model.py -c configs-hazedet/ppyoloe/ppyoloe_crn_m_100e_hazedet.yml --output_dir=./inference_model -o weights=output/ppyoloe_crn_m_100e_hazedet/best_model export.nms=True`\r\n- 【Paddle模型转ONNX模型】\r\n- - 导出Paddle模型后，按照[Paddle模型转换为ONNX模型文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md),静态图转ONNX、固定Shape操作，运行代码如下所示：\r\n- - 静态图转ONNX：` paddle2onnx --model_dir ppyoloe_crn_m_100e_hazedet --model_filename model.pdmodel --params_filename model.pdiparams --save_file ppyoloe_crn_m_100e_hazedet/ppyoloe_crn_m_100e_hazedet.onnx --enable_dev_version True`\r\n- - 固定shape：` python -m paddle2onnx.optimize --input_model ./ppyoloe_crn_m_100e_hazedet/ppyoloe_crn_m_100e_hazedet.onnx --output_model ./ppyoloe_crn_m_100e_hazedet/ppyoloe_crn_m_100e_hazedet.onnx --input_shape_dict \"{'image':[1,3,640,640],'scale_factor':[1,2]}\"`\r\n- - 上述过程均能正常进行.\r\n- 【ONNX转RKNN】\r\n- - 转换前准备：将模型文件拷贝到FastDeploy/tools/rknpu2下，根据模型netron可视化结构图、模型配置文件infer_cfg.yml，修改config文件，按照按照[编写yaml文件文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md)修改、normaliz参数和output参数.\r\n- - 设置do_quantization为False并执行下面命令转换模型后出现错误：\r\n- - `python export.py --config_path=./config/ppyoloe_crn_m_100e_hazedet_quantized.yaml --target_platform rk3568`\r\n- - 错误信息提示：E RKNN: [18:58:46.015] REGTASK: The bit width of field value exceeds the limit, target: lite, offset: 0x403c, shift = 0, limit: 0x1fff, value: 0x20cf\r\nAborted (core dumped)\r\n- - 设置do_quantization为True并执行下面命令转换模型后出现错误：\r\n- - `python export.py --config_path=./config/ppyoloe_crn_m_100e_hazedet_quantized.yaml --target_platform rk3568`\r\n- - 错误信息提示：E build: ValueError: The input num: 1 in ./ppyoloe_crn_m_100e_hazedet/dataset.txt not match input num: 2 in model !\r\n- 模型转换运行日志：\r\n[export_rknn.txt](https://github.com/PaddlePaddle/FastDeploy/files/11190138/export_rknn.txt)\r\n- 【用到的模型文件】\r\n[模型文件](https://pan.baidu.com/s/1U3TLZhxWqZOcAlTkaA5NNg)提取码: n2x3",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-10T11:30:14+00:00",
        "updated_at": "2025-02-04T06:41:27+00:00",
        "closed_at": "2025-02-04T06:41:27+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "MrMzl",
            "Zheng-Bicheng",
            "TsingWei",
            "haozizhuimao"
        ],
        "labels": [
            "RKNN"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1793,
        "title": "FD onnx推理 报错",
        "body": "我直接用onnxruntime跑这个模型，可以跑通\r\n然后我自己指定这个跑通的onnxruntime目录，从新编的fd\r\n数据也是一样的\r\n不理解为啥，fd就跑不通了\r\n`\r\n#include \"fastdeploy/runtime.h\"\r\n#include \"/mnt/masimeng/workspace/FastDeploy/build/Linux/x86_64/install/third_libs/install/onnxruntime/include/onnxruntime_cxx_api.h\"  // NOLINT\r\n\r\nint main(){\r\n    //onnxruntime\r\n    {\r\n    Ort::Env env_ = Ort::Env(ORT_LOGGING_LEVEL_WARNING, \"\");\r\n    Ort::SessionOptions session_options_ = Ort::SessionOptions();\r\n    std::shared_ptr<Ort::Session> rescore_session_ = std::make_shared<Ort::Session>(\r\n            env_, \"asr-test/onnx_model_cpu/decoder.onnx\", session_options_);\r\n\r\n    Ort::MemoryInfo memory_info =\r\n        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);\r\n\r\n    std::vector<int64_t> hyps_pad(10*23, 5537);\r\n\r\n    std::vector<int64_t> hyps_lens(10, 0);\r\n\r\n    std::vector<float> rescore_input(223*512, 0);\r\n\r\n    const int64_t decode_input_shape[] = {1, 223, 512};\r\n\r\n    const int64_t hyps_pad_shape[] = {10, 23};\r\n\r\n    const int64_t hyps_lens_shape[] = {10};\r\n\r\n    Ort::Value decode_input_tensor_ = Ort::Value::CreateTensor<float>(\r\n        memory_info, rescore_input.data(), rescore_input.size(),\r\n        decode_input_shape, 3);\r\n    Ort::Value hyps_pad_tensor_ = Ort::Value::CreateTensor<int64_t>(\r\n        memory_info, hyps_pad.data(), hyps_pad.size(), hyps_pad_shape, 2);\r\n    Ort::Value hyps_lens_tensor_ = Ort::Value::CreateTensor<int64_t>(\r\n        memory_info, hyps_lens.data(), hyps_lens.size(), hyps_lens_shape, 1);\r\n\r\n    std::vector<Ort::Value> rescore_inputs;\r\n\r\n    rescore_inputs.emplace_back(std::move(hyps_pad_tensor_));\r\n    rescore_inputs.emplace_back(std::move(hyps_lens_tensor_));\r\n    rescore_inputs.emplace_back(std::move(decode_input_tensor_));\r\n\r\n    std::vector<const char*> rescore_in_names_ = {\"hyps\",\"hyps_lens\",\"encoder_out\"};\r\n    std::vector<const char*> rescore_out_names_ = {\"score\",\"r_score\"};\r\n    std::vector<Ort::Value> rescore_outputs = rescore_session_->Run(\r\n        Ort::RunOptions{nullptr}, rescore_in_names_.data(), rescore_inputs.data(),\r\n        rescore_inputs.size(), rescore_out_names_.data(),\r\n        rescore_out_names_.size());\r\n    printf(\"done\\n\");\r\n    }\r\n    {\r\n    //fd\r\n    fastdeploy::RuntimeOption runtime_option;\r\n    runtime_option.UseOrtBackend();\r\n    runtime_option.UseCpu();\r\n    runtime_option.SetCpuThreadNum(1);\r\n    runtime_option.SetModelPath(\"asr-test/onnx_model_cpu/decoder.onnx\", \"\", fastdeploy::ModelFormat::ONNX);\r\n    std::shared_ptr<fastdeploy::Runtime> rescore_ = std::make_shared<fastdeploy::Runtime>();\r\n    rescore_->Init(runtime_option);\r\n\r\n    std::vector<int64_t> hyps_pad(10*23, 5537);\r\n\r\n    std::vector<int64_t> hyps_lens(10, 0);\r\n\r\n    std::vector<float> rescore_input(223*512, 0);\r\n\r\n    const std::vector<int64_t> hyps_pad_shape = {10, 23};\r\n\r\n    const std::vector<int64_t> hyps_lens_shape = {10};\r\n\r\n    const std::vector<int64_t> decode_input_shape = {1, 223, 512};\r\n\r\n    fastdeploy::FDTensor hyps_pad_tensor_;\r\n    hyps_pad_tensor_.SetData({10, 23}, fastdeploy::FDDataType::INT64, &hyps_pad);\r\n    fastdeploy::FDTensor hyps_lens_tensor_;\r\n    hyps_lens_tensor_.SetData({10}, fastdeploy::FDDataType::INT64, &hyps_lens);\r\n    fastdeploy::FDTensor decode_input_tensor_;\r\n    decode_input_tensor_.SetData({1, 223, 512}, fastdeploy::FDDataType::FP32, &rescore_input);\r\n\r\n    std::vector<fastdeploy::FDTensor> rescore_inputs(3);\r\n    rescore_inputs[0] = std::move(hyps_pad_tensor_);\r\n    rescore_inputs[0].name = \"hyps\";\r\n    rescore_inputs[1] = std::move(hyps_lens_tensor_);\r\n    rescore_inputs[1].name = \"hyps_lens\";\r\n    rescore_inputs[2] = std::move(decode_input_tensor_);\r\n    rescore_inputs[2].name = \"encoder_out\";\r\n\r\n    std::vector<fastdeploy::FDTensor> rescore_outputs;\r\n    rescore_->Infer(rescore_inputs, &rescore_outputs);\r\n    }\r\n    return 0;\r\n}\r\n`\r\n![d071ce228e25665d4a29b13953980ad9](https://user-images.githubusercontent.com/24523981/230894158-01990501-7866-4683-b23e-d699f4243e54.png)\r\n![16842e7835f77968c015f354918be953](https://user-images.githubusercontent.com/24523981/230894186-57b652c8-3b7d-43b3-a6c9-550ac949b040.png)\r\n\r\n",
        "state": "closed",
        "user": "MarsMeng1994",
        "closed_by": "DefTruth",
        "created_at": "2023-04-10T11:35:40+00:00",
        "updated_at": "2023-04-12T02:26:58+00:00",
        "closed_at": "2023-04-12T02:26:58+00:00",
        "comments_count": [
            "DefTruth",
            "GodIsBoom"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1794,
        "title": "rk3568板内编译fastdeploy库时报错",
        "body": "root@localhost:/opt/FastDeploy-develop/build# cmake ..  -DENABLE_ORT_BACKEND=ON \\\r\n>       -DENABLE_RKNPU2_BACKEND=ON \\\r\n>       -DENABLE_VISION=ON \\\r\n>       -DRKNN2_TARGET_SOC=RK356X \\\r\n>           -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.0 \\\r\n>   -DOPENCV_DIRECTORY=/usr/local\r\n-- The C compiler identification is GNU 9.4.0\r\n-- The CXX compiler identification is GNU 9.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /opt/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 5% complete]\r\n-- [download 10% complete]\r\n-- [download 15% complete]\r\n-- [download 20% complete]\r\n-- [download 26% complete]\r\n-- [download 31% complete]\r\n-- [download 36% complete]\r\n-- [download 41% complete]\r\n-- [download 46% complete]\r\n-- [download 51% complete]\r\n-- [download 56% complete]\r\n-- [download 61% complete]\r\n-- [download 67% complete]\r\n-- [download 72% complete]\r\n-- [download 77% complete]\r\n-- [download 82% complete]\r\n-- [download 87% complete]\r\n-- [download 92% complete]\r\n-- [download 97% complete]\r\n-- [download 100% complete]\r\nDecompress file /opt/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /opt/FastDeploy-develop/build/third_libs/install/onnxruntime\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/rknpu2_runtime-linux-aarch64-1.4.2b0-RK356X.tgz to /opt/FastDeploy-develop/build/rknpu2_runtime-linux-aarch64-1.4.2b0-RK356X.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\n-- [download 17% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 100% complete]\r\nDecompress file /opt/FastDeploy-develop/build/rknpu2_runtime-linux-aarch64-1.4.2b0-RK356X.tgz ...\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/local\r\n-- Found OpenCV: /usr/local (found version \"4.5.1\") \r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/protobuf-linux-aarch64-3.16.0.tgz to /opt/FastDeploy-develop/build/protobuf-linux-3.16.0.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 2% complete]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 7% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 13% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\n-- [download 17% complete]\r\n-- [download 18% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 23% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 29% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 34% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- [download 40% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 45% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 51% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 56% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 61% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 67% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 72% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 78% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 89% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 94% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /opt/FastDeploy-develop/build/protobuf-linux-3.16.0.tgz ...\r\n-- Build type not set - defaulting to Release\r\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.18\") \r\n-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython2.7.so (found version \"2.7.18\") \r\n-- Found Protobuf: /opt/FastDeploy-develop/build/third_libs/protobuf/lib/libprotobuf.a (found version \"3.16.0\") \r\nGenerated: /opt/FastDeploy-develop/build/third_party/onnx/onnx/onnx_paddle2onnx-ml.proto\r\nGenerated: /opt/FastDeploy-develop/build/third_party/onnx/onnx/onnx-operators_paddle2onnx-ml.proto\r\nGenerated: /opt/FastDeploy-develop/build/third_party/onnx/onnx/onnx-data_paddle2onnx.proto\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format -g0 -O3 -Wnon-virtual-dtor\r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_RKNPU2_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX;MAX_ONNX_OPSET_VERSION=16;PADDLE2ONNX_LIB;ONNX_NAMESPACE=paddle2onnx;__STDC_FORMAT_MACROS\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /opt/FastDeploy-develop/build/fastdeploy-0.0.0\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   ONNX version              : 1.11.0\r\n--   ONNX NAMESPACE            : paddle2onnx\r\n--   ONNX_USE_LITE_PROTO       : OFF\r\n--   USE_PROTOBUF_SHARED_LIBS  : OFF\r\n--   Protobuf_USE_STATIC_LIBS  : ON\r\n--   ONNX_DISABLE_EXCEPTIONS   : OFF\r\n--   ONNX_WERROR               : OFF\r\n--   ONNX_BUILD_TESTS          : OFF\r\n--   ONNX_BUILD_BENCHMARKS     : OFF\r\n--   ONNXIFI_DUMMY_BACKEND     : OFF\r\n--   ONNXIFI_ENABLE_EXT        : OFF\r\n-- \r\n--   Protobuf compiler         : /opt/FastDeploy-develop/build/third_libs/protobuf/bin/protoc\r\n--   Protobuf includes         : /opt/FastDeploy-develop/build/third_libs/protobuf/include\r\n--   Protobuf libraries        : /opt/FastDeploy-develop/build/third_libs/protobuf/lib/libprotobuf.a\r\n--   BUILD_ONNX_PYTHON         : OFF\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_RKNPU2_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX;MAX_ONNX_OPSET_VERSION=16;PADDLE2ONNX_LIB;ONNX_NAMESPACE=paddle2onnx\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /opt/FastDeploy-develop/build/fastdeploy-0.0.0\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : ON\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /opt/FastDeploy-develop/build\r\nroot@localhost:/opt/FastDeploy-develop/build# make -j4\r\n[  0%] Running cpp protocol buffer compiler on p2o_paddle.proto\r\nScanning dependencies of target extern_onnxruntime\r\nScanning dependencies of target gen_onnx_proto\r\nScanning dependencies of target yaml-cpp\r\n[  0%] Running gen_proto.py on onnx/onnx.in.proto\r\n[  0%] Creating directories for 'extern_onnxruntime'\r\n  File \"/opt/FastDeploy-develop/third_party/onnx/onnx/gen_proto.py\", line 36\r\n    def process_ifs(lines: Iterable[Text], onnx_ml: bool) -> Iterable[Text]:\r\n                         ^\r\nSyntaxError: invalid syntax\r\nmake[2]: *** [third_party/onnx/CMakeFiles/gen_onnx_proto.dir/build.make:69: third_party/onnx/onnx/onnx_paddle2onnx-ml.proto] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:581: third_party/onnx/CMakeFiles/gen_onnx_proto.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[  0%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\nScanning dependencies of target p2o_paddle_proto\r\n[  1%] Building CXX object paddle2onnx/proto/CMakeFiles/p2o_paddle_proto.dir/p2o_paddle.pb.cc.o\r\n[  1%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n-- Downloading...\r\n   dst='/opt/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/opt/FastDeploy-develop/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n     dst='/opt/FastDeploy-develop/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  1%] No patch step for 'extern_onnxruntime'\r\n[  2%] No update step for 'extern_onnxruntime'\r\n[  2%] No configure step for 'extern_onnxruntime'\r\n[  2%] No build step for 'extern_onnxruntime'\r\n[  2%] Performing install step for 'extern_onnxruntime'\r\n[  2%] Completed 'extern_onnxruntime'\r\n[  2%] Built target extern_onnxruntime\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[  6%] Linking CXX static library libp2o_paddle_proto.a\r\n[  6%] Built target p2o_paddle_proto\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[  9%] Linking CXX static library libyaml-cpp.a\r\n[  9%] Built target yaml-cpp\r\nmake: *** [Makefile:152: all] Error 2\r\n",
        "state": "closed",
        "user": "LiangYongAI",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-11T00:54:27+00:00",
        "updated_at": "2024-04-30T06:42:44+00:00",
        "closed_at": "2024-04-30T06:42:44+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK356X"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1795,
        "title": "yolo5修改",
        "body": "我想为yolo5添加小目标检测头，在yolo5官方版本只用修改yaml文件就可以了，请问在fastdeploy里面要如何操作呢？",
        "state": "closed",
        "user": "beckhz",
        "closed_by": "beckhz",
        "created_at": "2023-04-11T01:54:37+00:00",
        "updated_at": "2023-04-24T03:09:55+00:00",
        "closed_at": "2023-04-24T03:09:55+00:00",
        "comments_count": [
            "wjj19950828"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1797,
        "title": "db++通过fastdeploy 无法正常部署",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-'1.0.5'\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: / Windows x64(Windows10) \r\n\r\n- 【编译语言】： Python(3.10）\r\n\r\n## 问题日志及出现问题的操作流程\r\n在部署pp_ocr_v3的过程中，我训练了一个基于det_mv3_db的检测模型，但由于精度有要求，因此我基于r50_db++重新训练了检测模型，但在使用fd部署的时候，基于det_mv3_db的检测模型可以正确的检测出我需要的内容，而db++模型无法正常检测。\r\n`\r\nimport fastdeploy.vision as vision\r\n\r\nmv3_db\r\ndet_model = vision.ocr.DBDetector(r\"inference_model/db/inference.pdmodel\",r\"inference_model/db/inference.pdiparams\") \r\n\r\ndb++\r\ndet_model = vision.ocr.DBDetector(r\"inference_model/det_db/inference_.pdmodel\",r\"inference_model/det_db/inference_.pdiparams\")\r\n\r\nrec_model = vision.ocr.Recognizer(\"inference_model/rec_ocrv3/inference.pdmodel\",\r\n                                  \"inference_model/rec_ocrv3/inference.pdiparams\",\r\n                                  \"inference_model/rec_ocrv3/num.txt\",\r\n                                  )\r\n\r\nppocr_v3 = fd.vision.ocr.PPOCRv3(det_model=det_model,cls_model=None,rec_model=rec_model)\r\n`\r\n\r\ndb++检测结果：\r\n`\r\nprint(result)\r\nrec text: 4 rec score:0.999919 \r\n`\r\nmv3_db检测结果\r\n`\r\nprint(result)\r\ndet boxes: [[99,32],[309,30],[310,118],[100,119]]rec text: 4 rec score:0.999997 \r\n`\r\n但是我通过命令行直接推理的能够得出正常结果\r\n`\r\npython3 tools/infer/predict_det.py --image_dir=\"/home/aistudio/data/images_split_paddle/ae90bb62-1185-470d-87d8-341c7c5c8ee8__33.jpg\" --det_model_dir=\"./inference/det_db++/\" --det_algorithm=\"DB++\"\r\n`",
        "state": "open",
        "user": "super-tian",
        "closed_by": null,
        "created_at": "2023-04-11T07:56:00+00:00",
        "updated_at": "2023-04-24T06:28:19+00:00",
        "closed_at": null,
        "comments_count": [
            "Jaccica",
            "super-tian"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1798,
        "title": "fastdeploy import 导入期 提示 初始化失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.5\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 3060 \r\n- 【编译语言】：python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n安装fd结束后，如果不安装paddle可以正常import，\r\n如果装了paddle（无论是2.4.2还是2.4.0，无论是gpu还是cpu），就会import期出错：（有可能遇到这两种错误）\r\n![8db913d4a2280d9ebaa436219022d64](https://user-images.githubusercontent.com/96160062/231103243-9dab3a5c-e481-49c1-8f73-52674aaa85ab.png)\r\n\r\n![727be60d0389da08e91d710fcf17edf](https://user-images.githubusercontent.com/96160062/231103317-73cf1019-dab7-46f4-b1b5-bc340e988ef4.png)\r\n\r\n怀疑是paddleinfer相关出现冲突，**不过1.0.4版本是可以正常使用的**",
        "state": "closed",
        "user": "sanbuphy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-11T08:34:00+00:00",
        "updated_at": "2024-04-16T09:03:57+00:00",
        "closed_at": "2024-04-16T09:03:57+00:00",
        "comments_count": [
            "DefTruth",
            "sanbuphy",
            "sanbuphy",
            "DefTruth",
            "DefTruth",
            "sanbuphy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1799,
        "title": "AttributeError: 'DBDetector' object has no attribute 'preprocessor'",
        "body": "jetson tx2平台，出现如下错误\r\n\r\nroot@ubuntu:/home/aimee/Downloads/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python# python3 infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu\r\n[INFO] fastdeploy/runtime.cc(283)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/runtime.cc(283)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/runtime.cc(283)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 189, in <module>\r\n    det_model.preprocessor.max_side_len = 960\r\nAttributeError: 'DBDetector' object has no attribute 'preprocessor'\r\nroot@ubuntu:/home/aimee/Downloads/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python#\r\n",
        "state": "closed",
        "user": "woodstiger-byte",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-11T09:34:51+00:00",
        "updated_at": "2024-04-16T09:03:58+00:00",
        "closed_at": "2024-04-16T09:03:58+00:00",
        "comments_count": [
            "woodstiger-byte"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1801,
        "title": "YOLOV8 ONNX 分类 ,怎么获取标签名",
        "body": "YOLOV8 ONNX 分类 ,怎么获取标签名\r\n\r\n我看都是 0 1  2  3  4  5\r\n\r\n怎么看 对应的标签名称\r\n\r\nyolov8中的数据集 \r\n是分类的数据集\r\n\r\n那么是按照什么排序的名称？\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-11T14:18:36+00:00",
        "updated_at": "2024-06-03T08:39:39+00:00",
        "closed_at": "2024-04-16T09:03:59+00:00",
        "comments_count": [
            "smalie2222"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1806,
        "title": "Serving中对python backend 修改报错",
        "body": "serving 模式下，对paddleseg（估计任何模型都是这样）的preprocess修改后报错。\r\n\r\n- 插入的代码实际无关，不影响下文\r\n\r\n```\r\n//位于def execute(self, requests):\r\npaddle.version.show()\r\noutputs1 = paddle.zeros(shape=[1,3,640,640],dtype='float32')\r\ndlpack_tensor1 = paddle.utils.dlpack.to_dlpack(outputs1)\r\n```\r\n\r\n- 注释dlpack_tensor1 = paddle.utils.dlpack.to_dlpack(outputs1)后正常\r\n- 报错内容：\r\n![image](https://user-images.githubusercontent.com/33308054/231401845-842a1303-60d6-49a7-b973-8ad8a7a225ab.png)\r\n\r\n\r\n\r\n```\r\n\r\nimport json\r\nimport numpy as np\r\nimport os\r\n\r\nimport fastdeploy as fd\r\nimport paddle\r\nimport triton_python_backend_utils as pb_utils\r\n\r\n\r\nclass TritonPythonModel:\r\n\r\n    def initialize(self, args):\r\n        # You must parse model_config. JSON string is not parsed here\r\n        self.model_config = json.loads(args['model_config'])\r\n        print(\"model_config:\", self.model_config)\r\n\r\n        self.input_names = []\r\n        for input_config in self.model_config[\"input\"]:\r\n            self.input_names.append(input_config[\"name\"])\r\n        print(\"preprocess input names:\", self.input_names)\r\n\r\n        self.output_names = []\r\n        self.output_dtype = []\r\n        for output_config in self.model_config[\"output\"]:\r\n            self.output_names.append(output_config[\"name\"])\r\n            # dtype = pb_utils.triton_string_to_numpy(output_config[\"data_type\"])\r\n            # self.output_dtype.append(dtype)\r\n            self.output_dtype.append(output_config[\"data_type\"])\r\n        print(\"preprocess output names:\", self.output_names)\r\n\r\n        # init PaddleSegPreprocess class\r\n        yaml_path = os.path.abspath(os.path.dirname(__file__)) + \"/deploy.yaml\"\r\n        self.preprocess_ = fd.vision.segmentation.PaddleSegPreprocessor(\r\n            yaml_path)\r\n        #if args['model_instance_kind'] == 'GPU':\r\n        #    device_id = int(args['model_instance_device_id'])\r\n        #    self.preprocess_.use_gpu(device_id)\r\n\r\n    def execute(self, requests):\r\n        responses = []\r\n        for request in requests:\r\n            data = pb_utils.get_input_tensor_by_name(request,\r\n                                                     self.input_names[0])\r\n            data = data.as_numpy()\r\n            outputs, im_info = self.preprocess_.run(data)\r\n            \r\n#!!!------HERE\r\n            paddle.version.show()\r\n            outputs1 = paddle.zeros(shape=[1,3,640,640],dtype='float32')\r\n            dlpack_tensor1 = paddle.utils.dlpack.to_dlpack(outputs1)\r\n#!!!------HERE END\r\n\r\n            # PaddleSeg preprocess has two outputs\r\n            dlpack_tensor = outputs[0].to_dlpack()\r\n            output_tensor_0 = pb_utils.Tensor.from_dlpack(self.output_names[0],\r\n                                                          dlpack_tensor)\r\n            output_tensor_1 = pb_utils.Tensor(\r\n                self.output_names[1], np.array(\r\n                    [im_info], dtype=np.object_))\r\n            inference_response = pb_utils.InferenceResponse(\r\n                output_tensors=[output_tensor_0, output_tensor_1])\r\n            responses.append(inference_response)\r\n        return responses\r\n\r\n    def finalize(self):\r\n        print('Cleaning up...')\r\n\r\n```\r\n",
        "state": "closed",
        "user": "ShawnXsw",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-12T08:40:54+00:00",
        "updated_at": "2024-04-16T09:04:00+00:00",
        "closed_at": "2024-04-16T09:03:59+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1802,
        "title": "Jetson Xavier NX部署PP-OCRv3模型过程中出现的问题?",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-develop最新版\r\n- 【硬件】： Jetson Xavier NX（Linux aarch64）\r\n- 【编译语言】： Python\r\n- 【其他环境包】：Jetpack 4.6.3、CUDA：10.2.300、cuDNN：8.2.1.32、TensorRT：8.2、OpenCV：4.1.1\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- - 按照[Jetson环境部署文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/jetson.md)正常实现环境编译与安装。\r\n- - [按照官方提供的部署文档](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/cpu-gpu/python)，执行`examples`下的部署示例，CPU下用Paddle Inference、ONNX Runtime；GPU下用ONNX Runtime 可以正常推理。但是利用其他推理后端进行推理有问题，具体如下：\r\n- - 错误一：GPU上利用Paddle Inference推理，识别结果是乱码，图片上检测框错误；\r\n- - 推理代码：`python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend paddle`\r\n- - 推理结果：`[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nPredict Time: 5268.916845321655\r\ndet boxes: [[0,22],[719,21],[719,33],[0,34]]rec text: 愛榈丫榈梦跆榈’榈語榈恢術榈协榈’榈租榈撕榈甜榈é＃榈±石甜榈é rec score:0.000509 cls label: 0 cls score: 0.000000\r\ndet boxes: [[0,107],[719,106],[719,118],[0,119]]rec text: 榈恢術榈恢梵榈俭榈俭榈泌榈榈撑飛榈俭榈跆榈é榈恢榈團榈俭榈俭榈語榈恢榈恢榈丫榈△榈俭榈恢榈紊榈網榈鉴榈纵榈榈#闯榈俭榈甜榈é洞榈丫榈甜榈é榈恢榈’洞榈俪榈 rec score:0.000292 cls label: 0 cls score: 0.000000\r\ndet boxes: [[0,111],[719,111],[719,642],[0,642]]rec text: 榈榈'丫汪藤榈μｅ次梵榈鏡榈撑俭榈鏡榈忌榈éμe单β楼旗交愛榈丫榈粱榈闯榈纵榈=榈汪榈紊榈次梵榈丫榈語榈恢術榈协跆榈撑榈庐榈紊榈#丫紊榈俭鏡ｌ榈纵榈租榈撑甜榈é洞榈俭榈丫宋焗榈甜榈é丫紊榈汪鏡榈协##網榈 rec score:0.000370 cls label: 0 cls score: 0.000000\r\ndet boxes: [[0,123],[719,123],[719,131],[0,131]]rec text: 榈恢術榈恢梵榈俭榈俭榈泌榈榈撑飛榈俭榈跆榈é榈恢榈團榈俭榈俭榈語榈恢榈恢榈丫榈△榈俭榈恢榈紊榈網榈鉴榈纵榈榈#闯榈俭榈甜榈é洞榈丫榈甜榈é榈恢榈’洞榈俪榈 rec score:0.000292 cls label: 0 cls score: 0.000000\r\ndet boxes: [[0,162],[719,162],[719,249],[0,249]]rec text: 榈榈'榈次梵榈鏡榈撑榈忌榈0A拉交愛榈丫榈粱榈闯榈纵榈=榈次梵榈丫榈語榈恢術榈协跆榈撑榈庐榈#榈ｌ榈纵榈租榈撑甜榈é洞榈俭榈丫宋焗榈甜榈é榈协網榈 rec score:0.000457 cls label: 0 cls score: 0.000000 `\r\n- - 推理结果图：\r\n![ffa028abe6b12b8ca08ad7a7ae0ff6b](https://user-images.githubusercontent.com/85120075/231191864-8a3d11f7-92f2-4a04-a02e-578dffd02e0a.jpg)\r\n\r\n- - 错误二：GPU上使用Paddle TensorRT进行推理，出现推理错误：Segmentation fault (core dumped) \r\n- - 推理代码：`python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend pptrt`\r\n- - 推理结果：`WARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_PP-OCRv3_det_infer` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_ppocr_mobile_v2.0_cls_infer` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_PP-OCRv3_rec_infer` instead.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(37)::BuildOption\tWill try to use tensorrt inference with Paddle Backend.\r\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(49)::BuildOption\tDetect that tensorrt cache file has been set to ch_PP-OCRv3_det_infer, but while enable paddle2trt, please notice that the cache file will save to the directory where paddle model saved.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(395)::GetDynamicShapeFromOption\tx: the max shape = [1, 3, 960, 960], the min shape = [1, 3, 64, 64], the opt shape = [1, 3, 640, 640]\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(358)::SetTRTDynamicShapeToConfig\tStart setting trt dynamic shape.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(360)::SetTRTDynamicShapeToConfig\tFinish setting trt dynamic shape.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(235)::InitFromPaddle\tStart loading shape range info file ch_PP-OCRv3_det_infer/shape_range_info.pbtxt to set TensorRT dynamic shape.\r\n6_gpu_infer_ptrt.sh: line 1:   565 Segmentation fault      (core dumped) python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend pptrt`\r\n\r\n- - 错误三：GPU上使用TensorRT进行推理，出现TRTBackend SetInputs not find name:x的错误\r\n- - 推理代码：`python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend trt`\r\n- - 推理结果：`WARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_PP-OCRv3_det_infer/det_trt_cache.trt` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_ppocr_mobile_v2.0_cls_infer/cls_trt_cache.trt` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = ch_PP-OCRv3_rec_infer/rec_trt_cache.trt` instead.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(702)::CreateTrtEngineFromOnnx\tDetect serialized TensorRT Engine file in ch_PP-OCRv3_det_infer/det_trt_cache.trt, will load it directly.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache\tBuild TensorRT Engine from cache file: ch_PP-OCRv3_det_infer/det_trt_cache.trt with shape range information as below,\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache\tInput name: x, shape=[-1, 3, -1, -1], min=[1, 3, 64, 64], max=[1, 3, 960, 960]\r\n\r\n[INFO] fastdeploy/runtime/runtime.cc(306)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(702)::CreateTrtEngineFromOnnx\tDetect serialized TensorRT Engine file in ch_ppocr_mobile_v2.0_cls_infer/cls_trt_cache.trt, will load it directly.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache\tBuild TensorRT Engine from cache file: ch_ppocr_mobile_v2.0_cls_infer/cls_trt_cache.trt with shape range information as below,\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache\tInput name: x, shape=[-1, 3, -1, -1], min=[1, 3, 48, 10], max=[1, 3, 48, 1024]\r\n\r\n`[INFO] fastdeploy/runtime/runtime.cc(306)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(556)::BuildTrtEngine\tStart to building TensorRT Engine...\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log\t10: [optimizer.cpp::computeCosts::2011] Error Code 10: Internal Error (Could not find any implementation for node {ForeignNode[linear_13.w_0 + (Unnamed Layer* 321) [Shuffle]...p2o.Reshape.23 + p2o.Transpose.7]}.)\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log\t2: [builder.cpp::buildSerializedNetwork::609] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed. )\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(619)::BuildTrtEngine\tFailed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(735)::CreateTrtEngineFromOnnx\tFailed to build tensorrt engine.\r\n[INFO] fastdeploy/runtime/runtime.cc(306)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(447)::SetInputs\tTRTBackend SetInputs not find name:x\r\n5_gpu_infer_trt.sh: line 1:   974 Aborted                 (core dumped) python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend trt\r\n\r\nC++方式下的部署，上述三种推理后端进行推理也存在相同问题。\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-11T14:34:20+00:00",
        "updated_at": "2024-04-23T06:40:52+00:00",
        "closed_at": "2024-04-23T06:40:52+00:00",
        "comments_count": [
            "MrMzl",
            "DefTruth",
            "MrMzl",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1805,
        "title": "[BUG] fastdeploy部署stable diffusion - tensorrt 出现内存溢出的现象",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： dev最新\r\n- 【系统平台】: ubuntu 20.04\r\n- 【硬件】： 3060\r\n- 【编译语言】： Python 3.8 \r\n\r\n## 问题日志及出现问题的操作流程\r\n关联issue：https://github.com/PaddlePaddle/PaddleNLP/issues/5617\r\n\r\n参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/deploy\r\n运行命令：\r\n```bash\r\npython text_to_img_infer.py --model_dir stable-diffusion-v1-5/ --scheduler \"euler_ancestral\" --backend paddle_tensorrt --use_fp16 True --device gpu\r\n```\r\n\r\n现象：\r\n![aa9caa5b0caa4493eddc1c15bccdb38](https://user-images.githubusercontent.com/96160062/231381225-ba56171c-93ae-48d0-9325-9dc450163a73.png)\r\n\r\n",
        "state": "closed",
        "user": "sanbuphy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-12T07:18:04+00:00",
        "updated_at": "2024-07-23T06:41:01+00:00",
        "closed_at": "2024-07-23T06:41:01+00:00",
        "comments_count": [
            "sanbuphy",
            "sanbuphy",
            "DefTruth",
            "sanbuphy",
            "lxp521125"
        ],
        "labels": [
            "stable_diffusion"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1803,
        "title": "请问自己编译的x86 cpu版本怎么用不了呢",
        "body": "![image](https://user-images.githubusercontent.com/48303408/231319988-2473bae3-1716-4214-a149-56c7c6d6ce56.png)\r\n分别用x64 Native Tools Command Prompt for VS 2019和cmake-gui的方式编译了，都报这个错误，而且把编译完的所有dll都拷贝到了程序的根目录\r\n![image](https://user-images.githubusercontent.com/48303408/231320222-2a382444-d467-4fb3-be55-03a5230cec91.png)\r\n编译过程都是按照文档一步一步走的，请大佬帮忙分析一下\r\n我试了下预编译的库也是这个问题，代码中只要包含fastdeploy::RuntimeOption option;就不行",
        "state": "closed",
        "user": "Dandelion111",
        "closed_by": "DefTruth",
        "created_at": "2023-04-12T01:00:18+00:00",
        "updated_at": "2023-04-12T05:15:26+00:00",
        "closed_at": "2023-04-12T05:15:25+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "Dandelion111"
        ],
        "labels": [
            "Windows x64"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1807,
        "title": " Could not load library libcublasLt.so.12,  我的是11.7",
        "body": "## 环境\r\n\r\n- 【编译命令】sh tests/install/check_predict.sh\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】：  Nvidia GPU 4080， CUDA 11.7  cuDNN  8.8.\r\n- 【编译语言】：Python(3.10）\r\n\r\nPlease NOTE: device: 0, GPU Compute Capability: 8.9, Driver API Version: 12.0, Runtime API Version: 11.7\r\n我是driver 12.0 ，但我是跑11.7 cuda\r\n\r\n彈出錯誤:  Could not load library libcublasLt.so.12. Error: libcublasLt.so.12: cannot open shared object file: No such file or directory\r\n我查過11.7 底下只有libcublasLt.so.11的文件，要怎麼才能把他要求的12版本換到11裡?\r\n\r\n我沒辦法安裝cuda12因為pytorch也只支援到11.8，有其他方法可以改善嗎?\r\n",
        "state": "closed",
        "user": "alantseone",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-12T14:09:52+00:00",
        "updated_at": "2025-01-07T06:40:28+00:00",
        "closed_at": "2025-01-07T06:40:28+00:00",
        "comments_count": [
            "DefTruth",
            "alantseone",
            "ArminLee",
            "xinliqingpi",
            "Ghy1209",
            "Ghy1209",
            "Ghy1209"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1808,
        "title": "libascendcl.so: 运行华为华为昇腾推理报错",
        "body": "参考：\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/yolov5/cpp\r\n运行命令：\r\n./infer_paddle_demo yolov5s_infer 000000014439.jpg 4\r\n报错关键信息：\r\n[I  4/12 22:16:54.127 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: argument_type_display_pass\r\n[I  4/12 22:16:54.127 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: variable_place_inference_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: variable_place_inference_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: control_flow_op_shared_inputs_and_outputs_place_sync_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:107 ApplyPasses]    - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: argument_type_display_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: argument_type_display_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: runtime_context_assign_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: runtime_context_assign_pass\r\n[I  4/12 22:16:54.132 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: argument_type_display_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: argument_type_display_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: lite_inplace_fuse_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: lite_inplace_fuse_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: memory_optimize_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:107 ApplyPasses]    - Skip memory_optimize_pass because the target or kernel does not match.\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: xpu_memory_optimize_pass\r\n[I  4/12 22:16:54.133 ...le-Lite/lite/core/optimizer/optimizer.cc:107 ApplyPasses]    - Skip xpu_memory_optimize_pass because the target or kernel does not match.\r\n[I  4/12 22:16:54.137 ...re/optimizer/mir/generate_program_pass.h:41 GenProgram] insts.size: 1\r\n[INFO] fastdeploy/runtime/runtime.cc(321)::CreateLiteBackend\tRuntime initialized with Backend::PDLITE in Device::ASCEND.\r\n[F  4/12 22:16:54.161 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libascendcl.so: cannot open shared object file: No such file or directory\r\nterminate called after throwing an instance of 'nnadapter::logging::Exception'\r\n  what():  NNAdapter C++ Exception: \r\n[F  4/12 22:16:54.161 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libascendcl.so: cannot open shared object file: No such file or directory\r\n\r\nAborted (core dumped)\r\n\r\n上面的库在哪里可以找到，如何解决？",
        "state": "open",
        "user": "jia0511",
        "closed_by": null,
        "created_at": "2023-04-12T14:25:12+00:00",
        "updated_at": "2024-03-19T05:12:49+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "jia0511",
            "DefTruth",
            "jia0511",
            "jia0511",
            "cocaer",
            "jia0511"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1810,
        "title": "ubuntu18.04 cpu版本fastdeploy提示yolov5s.onnx不支持",
        "body": "测试环境：\r\nubuntu18.04 x84_64位\r\nfastdeploy为源码编译最新版本，Paddleocr可以正常运行，但是用yolov5s.onnx就不行了，yolov5是6.0训练的模型导出onnx后用netron查看没有问题，但是fastdeploy报错\r\nterminate called after throwing an instance of 'ov::Exception'  what():  [ NETWORK_NOT_READ ] Unable to read the model: ../weights/best.onnx Please check that model format: onnx is supported and the model is correct. Available frontends: \r\n先是怀疑onnx没上传完整于是重新上传测试，结果还是一样，后面又用netron打开查看没有发现问题",
        "state": "closed",
        "user": "futureflsl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-13T05:53:50+00:00",
        "updated_at": "2024-04-16T06:40:26+00:00",
        "closed_at": "2024-04-16T06:40:26+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": [
            "Bug"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1817,
        "title": "为什么相同的模型在gpu上可以正常推理，但是在cpu上却不行呢？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： registry.baidubce.com/paddlepaddle/fastdeploy:1.0.5-cpu-only-21.10\r\n- 【系统平台】: docker \r\n- 【硬件】：如 Nvidia GPU RTX3090\r\n- 【编译语言】：Python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【相同模型在gpu上可以推理，切换到cpu却不行，得到的结果为空】\r\n简述环境，目前由于我们自身服务器架构的问题，需要将ocr模型中的检测模型放到cpu上进行运行，下面是我模型的配置\r\n[structure_s_det.zip](https://github.com/PaddlePaddle/FastDeploy/files/11227610/structure_s_det.zip)\r\njava文件夹是java的推理代码，调用grpc进行推理，ppdet_bin是压缩的numpy array，执行examples下面的TestKSERVEOcr_v4代码既可复现\r\n1.ppocr_det_pp是一个集成模型，它接收det preprocess后的压缩numpy array，然后进行解压，放入runtime进行预测\r\n2.修改runtime的config.pbtxt,将kind修改为KIND_GPU或者KINF_CPU发现GPU可以推理出来结果，而cpu不行\r\n",
        "state": "closed",
        "user": "liuwqiang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-14T00:35:38+00:00",
        "updated_at": "2024-04-16T09:04:01+00:00",
        "closed_at": "2024-04-16T09:04:01+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1824,
        "title": "Problems are encountered using the ascend reasoning library In the Atlas 300I ",
        "body": "## Environment\r\n![image](https://user-images.githubusercontent.com/24822418/232180284-1adcc841-a98e-4bf4-beb8-db9c2359ac3e.png)\r\nThe configuration is based on the following [environment](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/huawei_ascend.md)：\r\nMeanwhile, after compiling [yolov5cpp](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/cpp/README.md), the following problem was encountered：\r\n\r\nAccording to the environment configuration after how to reason encountered the following problems：\r\n\r\n Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libmsprofiler.so: cannot open shared object file: No such file or directory\r\n\r\n",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-15T03:21:14+00:00",
        "updated_at": "2024-04-16T09:04:02+00:00",
        "closed_at": "2024-04-16T09:04:02+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1818,
        "title": "FastDeploy推理PPocrV3模型问题",
        "body": "在使用FastDeploy推理PPocrV3模型时,我使用paddle模型转换onnx模型再转成rknn模型在rk3588上进行推理,发现推理结果与在windows上结果差异很大,开始我怀疑是模型在转换过程中出现问题,于是我在pc上对转换后的onnx及paddle原始模型进行推理,发现推理结果受到SetStaticShapeInfer()设置的影响,在pc平台使用paddle模型,onnx模型,在rk3588平台使用rknn模型预测,个模型间预测结果一致,由于我需要在rk3588上部署,所以只能使用固定尺寸的input,请问有何改善的方案\r\n\r\n这是我测试用的图片\r\n\r\n![dark](https://user-images.githubusercontent.com/50346534/231914544-d2fc5631-02e3-44fa-b03d-2d9ca077019c.png)\r\n\r\n推理代码使用example中的infer.cc的代码\r\n\r\n以下是推理结果\r\n\r\n设置:\r\n  det_model.GetPreprocessor().SetStaticShapeInfer(false);\r\n  rec_model.GetPreprocessor().SetStaticShapeInfer(false);\r\n\r\n结果:\r\ndet boxes: [[21,4],[278,4],[278,17],[21,17]]rec text: classProgressBar(t.GenericLvj): rec score:0.804982 \r\ndet boxes: [[54,22],[162,23],[162,40],[54,39]]rec text: def --init--( rec score:0.940800 \r\ndet boxes: [[88,46],[126,46],[126,62],[88,62]]rec text: self, rec score:0.881771 \r\ndet boxes: [[90,69],[368,69],[368,82],[90,82]]rec text: iterable: t.optionaz[t.Iterable[vs rec score:0.820816 \r\ndet boxes: [[87,88],[332,89],[332,106],[87,105]]rec text: length: t.optionazint] = None rec score:0.899627 \r\ndet boxes: [[89,113],[253,113],[253,127],[89,127]]rec text: fizl_char: str = \"#\" rec score:0.884660 \r\ndet boxes: [[88,135],[244,133],[244,147],[88,149]]rec text: empty_char: str = \" rec score:0.940556 \r\ndet boxes: [[88,156],[323,156],[323,171],[88,171]]rec text: bar_template: str = \"%(bar)s\" rec score:0.921710 \r\ndet boxes: [[87,177],[231,176],[231,193],[87,194]]rec text: info_sep: str = \" rec score:0.912973 \r\ndet boxes: [[89,200],[258,200],[258,214],[89,214]]rec text: show_eta: bool = True rec score:0.908067 \r\ndet boxes: [[87,221],[390,222],[390,239],[87,238]]rec text: show_percent: t.optionat[booz] = None, rec score:0.900560 \r\ndet boxes: [[89,245],[267,245],[267,259],[89,259]]rec text: show_pos: bool = False rec score:0.914557 \r\ndet boxes: [[87,264],[724,265],[724,283],[87,282]]rec text: item_show_func: t.Optional[t.Callable[[t.Optionat[v]], t.Optional[str]]] = None rec score:0.902894 \r\ndet boxes: [[88,288],[322,289],[322,303],[88,302]]rec text: label: t.optionaz[str] = None rec score:0.851348 \r\ndet boxes: [[87,308],[355,309],[355,327],[87,326]]rec text: file: t.optionat[t.TextIo] = None rec score:0.886582 \r\ndet boxes: [[86,330],[331,332],[331,349],[86,347]]rec text: color: t.optional[booz] : None rec score:0.912155 \r\ndet boxes: [[89,355],[293,355],[293,369],[89,369]]rec text: update_min_steps: int = 1 rec score:0.904869 \r\ndet boxes: [[87,376],[213,376],[213,390],[87,390]]rec text: width: int = 30 rec score:0.907129 \r\ndet boxes: [[67,399],[135,399],[135,413],[67,413]]rec text: -> None: rec score:0.963876 \r\ndet boxes: [[89,420],[296,420],[296,433],[89,433]]rec text: self.fizl char = fil char rec score:0.815899 \r\ndet boxes: [[89,444],[312,444],[312,457],[89,457]]rec text: self.empty_char = empty_char rec score:0.906245 \r\ndet boxes: [[89,465],[344,465],[344,479],[89,479]]rec text: self.bar_template = bar_template rec score:0.943523 \r\n\r\n\r\n设置:\r\n  det_model.GetPreprocessor().SetStaticShapeInfer(true);\r\n  rec_model.GetPreprocessor().SetStaticShapeInfer(true);\r\n\r\n结果:\r\ndet boxes: [[19,4],[277,3],[277,15],[19,16]]rec text: classroressarGeneric rec score:0.676173 \r\ndet boxes: [[56,25],[161,25],[161,39],[56,39]]rec text: def_-int.. rec score:0.552126 \r\ndet boxes: [[86,46],[128,46],[128,62],[86,62]]rec text: self rec score:0.823751 \r\ndet boxes: [[88,69],[369,68],[369,82],[88,83]]rec text: itbbe rec score:0.706840 \r\ndet boxes: [[88,91],[332,91],[332,105],[88,104]]rec text: Lengtht.optionalint]=Nne rec score:0.808381 \r\ndet boxes: [[88,113],[250,112],[250,126],[88,127]]rec text: fil.char tr \"#\" rec score:0.608173 \r\ndet boxes: [[89,136],[256,133],[257,147],[90,150]]rec text: empty.char: str = \" n rec score:0.743116 \r\ndet boxes: [[87,156],[323,157],[323,171],[87,170]]rec text: bartempate t=\"%bar)s\" rec score:0.723324 \r\ndet boxes: [[87,179],[232,178],[232,192],[87,194]]rec text: info_sp: t \" rec score:0.704647 \r\ndet boxes: [[88,201],[260,202],[260,214],[88,213]]rec text: showeta:bol rue rec score:0.759768 \r\ndet boxes: [[88,222],[389,222],[389,237],[88,237]]rec text: sherinab rec score:0.615717 \r\ndet boxes: [[90,246],[266,246],[266,258],[90,258]]rec text: showpo:bol False rec score:0.683770 \r\ndet boxes: [[87,265],[724,266],[724,282],[87,281]]rec text:  rec score:0.000000 \r\ndet boxes: [[88,288],[321,289],[321,303],[88,302]]rec text: Label:t.optional[st]=None rec score:0.792424 \r\ndet boxes: [[88,309],[355,311],[355,325],[88,323]]rec text: filenax rec score:0.642089 \r\ndet boxes: [[88,333],[331,333],[331,347],[88,346]]rec text: coooptionalbNne rec score:0.756318 \r\ndet boxes: [[90,356],[290,356],[290,368],[90,368]]rec text: updateminsteps: int=1 rec score:0.754334 \r\ndet boxes: [[87,377],[212,377],[212,391],[87,391]]rec text: with int - 30 rec score:0.665494 \r\ndet boxes: [[57,399],[136,399],[136,412],[57,412]]rec text: ) > None: rec score:0.822747 \r\ndet boxes: [[90,422],[292,422],[292,433],[90,433]]rec text: selfill har fill hal rec score:0.656918 \r\ndet boxes: [[90,445],[310,445],[310,457],[90,457]]rec text: sefemtychremptychr rec score:0.699114 \r\ndet boxes: [[90,467],[343,467],[343,477],[90,477]]rec text: selfbartemplatebartemplate rec score:0.767005 ",
        "state": "open",
        "user": "tank1530532",
        "closed_by": null,
        "created_at": "2023-04-14T01:11:41+00:00",
        "updated_at": "2023-04-26T08:44:35+00:00",
        "closed_at": null,
        "comments_count": [
            "Zheng-Bicheng",
            "tank1530532"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1815,
        "title": "Get \"https://registry-1.docker.io/v2/\": context deadline exceeded",
        "body": "按照这个流程走的时候使用docker流程：\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md\r\n在输入命令\r\nroot@ubuntu-18-04-5:~/jack# docker build --network=host -f Ascend_ubuntu18.04_aarch64_5.1.rc2.Dockerfile -t paddlelite/ascend_aarch64:cann_5.1.rc2 .\r\nSending build context to Docker daemon  4.957GB\r\nStep 1/18 : FROM ubuntu:18.04\r\nGet \"https://registry-1.docker.io/v2/\": context deadline exceeded\r\n后遇到错误",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "jia0511",
        "created_at": "2023-04-13T14:32:13+00:00",
        "updated_at": "2023-04-15T01:51:36+00:00",
        "closed_at": "2023-04-15T01:51:35+00:00",
        "comments_count": [
            "jia0511"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1814,
        "title": "用paddlelite做为后端编译的fastdeploy推理时遇到问题。",
        "body": "使用fastdeployserver命令运行在ARMV8处理器的设备上（aarch64 ubuntu18.04 triton22.05 无gpu）时产生了这个报错“UNAVAILABLE: Invalid argument: unknown cpu_execution_accelerator name 'paddle-lite' is provided. Available choices are [onnxruntime, paddle, openvino]” 我的模型无法转为onnxruntime，paddle不支持aarch。要怎么解决使用paddle-lite后端的问题呢？",
        "state": "closed",
        "user": "996-icu-FuJian",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-13T11:55:31+00:00",
        "updated_at": "2024-04-16T09:04:00+00:00",
        "closed_at": "2024-04-16T09:04:00+00:00",
        "comments_count": [
            "DefTruth",
            "996-icu-FuJian"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1821,
        "title": "训练picodet_s_416目标检测模型，只检测一类数据，在RK3588上使用Python推理，报错 Segmentation fault",
        "body": "按照[PaddleDetection RKNPU2部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md)\r\n进行onnx和rknn模型的转换，修改了infer_cfg.yml文件中的label_list，将coco的80类别换成自己一类，其他没改\r\n![image](https://user-images.githubusercontent.com/63158425/231963616-b4e57571-7cee-42fe-84e7-5ec9be259d41.png)\r\n\r\n开发板使用Python运行报错如下：\r\n![image](https://user-images.githubusercontent.com/63158425/231962524-0d0cb1bd-f271-4d55-b938-faee5c576505.png)\r\n\r\n他输出predict image time了，应该检测成功了吧？但输出目标时为什么报错 Segmentation fault？\r\n-----\r\n我在使用瑞芯微官方给的c++代码部署时，也出现 Segmentation fault 错误了，是修改了postprocess.h中的 OBJ_CLASS_NUM参数，将80改为1就成功了。\r\n这里我是不是也要修改类别参数，应该从哪里修改呢？",
        "state": "closed",
        "user": "lizheng-1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-14T06:45:21+00:00",
        "updated_at": "2024-04-30T06:42:44+00:00",
        "closed_at": "2024-04-30T06:42:44+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "Zheng-Bicheng"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1825,
        "title": "FastDeploy Streamer如何在Edgeboard上使用？",
        "body": "您好，我是广东工业大学的老师，我们采购了30套贵公司的Edgeboard，请问FastDeploy Streamer这种视频流方案可以部署到Edgeboard FZ3上吗？请问有没教程，主要为了开设实验课【智慧工厂、智慧工地、智慧校园、等视频监测实验】，锻炼本科学生的动手实践能力。谢谢",
        "state": "closed",
        "user": "wpfnlp",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-17T02:29:11+00:00",
        "updated_at": "2024-04-23T06:40:53+00:00",
        "closed_at": "2024-04-23T06:40:53+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1826,
        "title": "编译失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 最新版本\r\n- 【编译命令】F:\\ai_workspace\\venv\\Scripts\\activate.bat\r\nset ENABLE_ORT_BACKEND=ON\r\nset ENABLE_PADDLE_BACKEND=ON\r\nset ENABLE_OPENVINO_BACKEND=ON\r\nset ENABLE_VISION=ON\r\nset ENABLE_TEXT=ON\r\nset ENABLE_TRT_BACKEND=ON\r\nset WITH_GPU=ON\r\nset TRT_DIRECTORY=F:\\Tensorrt8.5\\TensorRT-8.5.3.1\r\nset CUDA_DIRECTORY=D:\\CUDA11.6\r\nset CudaToolkitDir=D:\\CUDA11.6\r\nset OpenCV_DIR=D:\\opencv\\build\\x64\\vc15\\bin\r\nset OPENCV_DIRECTORY=D:\\opencv\\build\r\nset ORT_DIRECTORY=D:\\onnxruntime-win-arm-1.14.1\r\n\r\npython F:\\ai_workspace\\FastDeploy\\python\\setup.py build\r\npython F:\\ai_workspace\\FastDeploy\\python\\setup.py bdist_wheel\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 2060， CUDA 11.6 CUDNN 8.5\r\n- 【编译语言】： C++ / Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n编译过程中\r\n`\r\n  Checking File Globs\r\n  Performing download step (download, verify and extract) for 'extern_paddle_inference'\r\n  -- File already exists but no hash specified (use URL_HASH):\r\n    file='F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_libs/paddle_inference/src/paddle_inference-wi\r\n  n-x64-gpu-trt-2.4-dev-20230410.zip'\r\n  Old file will be removed and new file downloaded from URL.\r\n  -- Downloading...\r\n     dst='F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_libs/paddle_inference/src/paddle_inference-wi\r\n  n-x64-gpu-trt-2.4-dev-20230410.zip'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle_inference-win-x64-gpu-trt-2.4-dev-20230410.zip'\r\n  onnxifi_loader.vcxproj -> F:\\ai_workspace\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\onnx\\Release\\onnxifi_\r\n  loader.lib\r\n  onnxifi_wrapper.vcxproj -> F:\\ai_workspace\\FastDeploy\\python\\.setuptools-cmake-build\\third_party\\onnx\\Release\\onnxifi\r\n  .dll\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_libs/paddle_inference/src/paddle_inference-\r\n  win-x64-gpu-trt-2.4-dev-20230410.zip'\r\n       dst='F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_libs/paddle_inference/src/extern_paddle_inf\r\n  erence'\r\n  -- extracting... [tar xfz]\r\nCUSTOMBUILD : CMake error : Problem with archive_write_finish_entry(): Can't restore time [F:\\ai_workspace\\FastDeploy\\p\r\nython\\.setuptools-cmake-build\\extern_paddle_inference.vcxproj]\r\nCUSTOMBUILD : CMake error : Problem extracting tar: F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_lib\r\ns/paddle_inference/src/paddle_inference-win-x64-gpu-trt-2.4-dev-20230410.zip [F:\\ai_workspace\\FastDeploy\\python\\.setupt\r\nools-cmake-build\\extern_paddle_inference.vcxproj]\r\n  -- extracting... [error clean up]\r\n  CMake Error at extern_paddle_inference-stamp/extract-extern_paddle_inference.cmake:40 (message):\r\n    Extract of\r\n    'F:/ai_workspace/FastDeploy/python/.setuptools-cmake-build/third_libs/paddle_inference/src/paddle_inference-win-x64\r\n  -gpu-trt-2.4-dev-20230410.zip'\r\n    failed`\r\n我看了那个extern_paddle_inference-urlinfo.txt是自动生成的他使用tar命令解压 我想修改他我该怎么做，我自己使用tar命令是可以解压的，不知道为什么在cmake命令里就不行，我已经更换了download_and_decompress的解压方式，但是这个我不知道怎么改。",
        "state": "closed",
        "user": "Wuhy5",
        "closed_by": "Wuhy5",
        "created_at": "2023-04-17T08:40:06+00:00",
        "updated_at": "2023-04-17T11:33:00+00:00",
        "closed_at": "2023-04-17T11:33:00+00:00",
        "comments_count": [
            "DefTruth",
            "Wuhy5",
            "Wuhy5"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1828,
        "title": "自己编译完成的fastdeploy python安装包无法正常安装",
        "body": "![image](https://user-images.githubusercontent.com/15075323/232718323-e8c5e107-416f-4abc-a17d-128e10adba4b.png)\r\n提示fastdeploy-tools>=0.0.5，但是最高只有0.0.2版的",
        "state": "closed",
        "user": "Caoxiaocao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-18T08:28:43+00:00",
        "updated_at": "2024-05-14T06:42:09+00:00",
        "closed_at": "2024-05-14T06:42:09+00:00",
        "comments_count": [
            "liuyingtao941212"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1832,
        "title": "请问FastDeploy支持鲲鹏920+Tesla T4部署吗",
        "body": "请问FastDeploy支持鲲鹏920+Tesla T4部署吗，是否有参考文档\r\n如题，希望官方解答",
        "state": "open",
        "user": "minboo",
        "closed_by": null,
        "created_at": "2023-04-18T11:06:26+00:00",
        "updated_at": "2023-04-18T11:06:26+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1829,
        "title": "error: creating server: Internal - failed to load all models",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy:1.0.4-cpu-only-21.10\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: centos 8 \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n   按照官方的步骤，用VisualDL可视化界面运行模型跑不起来\r\n- 【模型跑不通】\r\n   \r\n![ecde359c5365c2ce795efd4763d8e10](https://user-images.githubusercontent.com/59717369/232722952-591165ac-1d77-4716-befd-87468c10042e.png)\r\n\r\n",
        "state": "closed",
        "user": "Cheng-x-c",
        "closed_by": "Cheng-x-c",
        "created_at": "2023-04-18T08:43:55+00:00",
        "updated_at": "2024-03-07T03:02:00+00:00",
        "closed_at": "2023-04-20T07:59:06+00:00",
        "comments_count": [
            "Cheng-x-c",
            "Jaccica",
            "Cheng-x-c",
            "Jaccica",
            "sheiy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1830,
        "title": "RK3588长时间运行rkyolov5后推理耗时严重增加",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】： 板端编译 fastdeploy-linux-aarch64-rk3588-0.0.0\r\n【编译命令】cmake .. -DENABLE_ORT_BACKEND=OFF\r\n-DENABLE_RKNPU2_BACKEND=ON\r\n-DENABLE_VISION=ON\r\n-DRKNN2_TARGET_SOC=RK3588\r\n-DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy_sdk\r\n-DOPENCV_DIRECTORY=/usr/lib/aarch64-linux-gnu/cmake/opencv4\r\n【系统平台】: Linux aarch64(Ubuntu 20.04)\r\n【硬件】： rk3588-rknpu2\r\n【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【性能问题】\r\n- 推理耗时随着运行时间增大，运行5小时后由原本的平均0.025s增大至平均0.095秒。\r\n",
        "state": "closed",
        "user": "MarKKwan27149",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-18T08:51:21+00:00",
        "updated_at": "2025-05-06T06:45:21+00:00",
        "closed_at": "2025-05-06T06:45:21+00:00",
        "comments_count": [
            "zg651413411",
            "zj5200",
            "zhouweic36"
        ],
        "labels": [
            "rknpu2",
            "RK3588",
            "RKNN"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1835,
        "title": "使用yolov5的c++ example加载自己训练的yolov5_s模型报错，提示输入数量不对，该如何修改代码？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 自己编译的GPU版本\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4090， CUDA 12.0 CUDNN 8.8\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，可以正确执行\r\n- - `examples`下的代码可以运行，但自己的模型能运行\r\n- - - 提供复现问题的 代码+模型+错误log 如下：\r\n- \r\n- ./infer_paddle_demo yolov5_s_300e_voc_tianan 2.jpg 1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[ERROR] fastdeploy/runtime/backends/paddle/paddle_backend.cc(283)::Infer        [PaddleBackend] Size of inputs(1) should keep same with the inputs of this model(2).\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov5/yolov5.cc(80)::BatchPredict  Failed to inference by runtime.\r\nFailed to predict.\r\n\r\n![image](https://user-images.githubusercontent.com/34960316/232941394-f2c001e9-2f0d-4e1d-8c5a-c131521782f5.png)\r\n模型是使用paddleyolo训练的yolov5s,除了类别和数据是自定义的，其它参数配置是都是默认值，其中的reader配置如下图：\r\n![image](https://user-images.githubusercontent.com/34960316/232941707-af8a91ab-6ab8-4eea-aeb9-a313d5b5d08a.png)\r\n\r\n训练出来的模型使用python推理是没问题的，现在需要使用c++推理，我也参考其它issue https://github.com/PaddlePaddle/PaddleDetection/issues/4306修改了input_spec的输入，但是训练和推理都不正确。\r\n\r\n请问各位大佬给个解决思路，感谢～",
        "state": "open",
        "user": "hubhup",
        "closed_by": null,
        "created_at": "2023-04-19T01:26:56+00:00",
        "updated_at": "2023-04-19T01:26:56+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1834,
        "title": "fastdeploy  使用lite后端报Illegal instruction",
        "body": "问题描述：使用树莓派4B编译安装，带有vision，以及ort和lite后端，设置了opencv路径，并成功安装python版本的fastdeploy。\r\n但在执行提供的参考代码时报错，能够使用ort后端，但是使用lite后端出现Illegal instruction。\r\n\r\n\r\ntest.py:\r\n_____________________________________________________________________________________________________________________\r\nimport fastdeploy as fd\r\nimport numpy as np\r\noption = fd.RuntimeOption()\r\nmodel_url = \"https://bj.bcebos.com/fastdeploy/models/mobilenetv2.tgz\"\r\nfd.download_and_decompress(model_url, path=\".\")\r\noption.set_model_path(\"mobilenetv2/inference.pdmodel\",\r\n                      \"mobilenetv2/inference.pdiparams\")\r\n\r\noption.use_lite_backend() \r\n#option.use_ort_backend() \r\n\r\n#Initialise runtime\r\nruntime = fd.Runtime(option)\r\n\r\n#Get model input name\r\ninput_name = runtime.get_input_info(0).name\r\n\r\n#Constructing random data for inference\r\nresults = runtime.infer({\r\n    input_name: np.random.rand(1, 3, 224, 224).astype(\"float32\")\r\n})\r\n\r\nprint(results[0].shape)\r\n_____________________________________________________________________________________________________________________\r\n\r\nlite后端运行结果：\r\nFile is donwloaded, now extracting...\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: HARDWARE : BCM2835\r\n\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 4\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 1500, min freq: 1500, cluster ID: 0, CPU ARCH: A72\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 1500, min freq: 1500, cluster ID: 0, CPU ARCH: A72\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 1500, min freq: 1500, cluster ID: 0, CPU ARCH: A72\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 1500, min freq: 1500, cluster ID: 0, CPU ARCH: A72\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 3885396KB\r\n[I  4/18 12:41:50. 68 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  4/18 12:41:50. 69 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  4/18 12:41:50. 69 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  4/18 12:41:50. 76 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  4/18 12:41:50. 79 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from mobilenetv2/inference.pdmodel\r\n[I  4/18 12:41:50.143 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from mobilenetv2/inference.pdiparams\r\n[I  4/18 12:41:50.470 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!\r\nIllegal instruction\r\n_____________________________________________________________________________________________________________________\r\nort后端运行结果\r\nFile is donwloaded, now extracting...\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackend Runtime initialized with Backend::ORT in Device::CPU.\r\n(1, 1000)",
        "state": "closed",
        "user": "liushuai35",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-18T13:00:11+00:00",
        "updated_at": "2024-12-17T06:42:03+00:00",
        "closed_at": "2024-12-17T06:42:03+00:00",
        "comments_count": [
            "DefTruth",
            "woodrex83",
            "qianbin1989228",
            "mahesh11T"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1831,
        "title": "请求 fastdeploy_serving 服务器，不同的请求方式性能差距很大，这是什么原因导致的？",
        "body": "复现文档：https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN/client.md\r\n对比：1、使用tritonclient （grpc/http）客户端请求服务    ，耗时 0.4s\r\n           2、使用requests请求服务 ，耗时 4s\r\n           \r\n![40cf44154f44a9906c11464e504fc04](https://user-images.githubusercontent.com/60127735/232728849-dfbffaab-59a5-4ea5-baca-60d0acd7e20c.png)\r\n",
        "state": "open",
        "user": "teymur-git",
        "closed_by": null,
        "created_at": "2023-04-18T09:04:46+00:00",
        "updated_at": "2023-04-18T09:09:42+00:00",
        "closed_at": null,
        "comments_count": [
            "teymur-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1839,
        "title": "请问部署后比如deeplabv3切割模型部署后，输入数据还需要前处理么?我看输入数据是numpy格式的，是不是意思是不需要前处理了",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "kkpssr",
        "closed_by": "kkpssr",
        "created_at": "2023-04-19T09:15:46+00:00",
        "updated_at": "2023-04-20T01:57:50+00:00",
        "closed_at": "2023-04-20T01:57:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1838,
        "title": "自己训练的yolov5检测模型在jetson设备上检测异常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 最新git\r\n- 【编译命令】python SDK\r\n- 【系统平台】: Linux aarch64(Ubuntu 18.04) \r\n- 【硬件】： nvidia xavier nx， CUDA 10.2 CUDNN 8.2.1.32\r\n- 【编译语言】： Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用自己训练的yolov5三分类别 onnx模型检测图片，能够检测，但检测结果异常\r\nmodels/mask_yolov5.onnx --device gpu --image bus.jpg \r\n[WARNING] fastdeploy/runtime/backends/ort/ort_backend.cc(107)::BuildOption      Compiled fastdeploy with onnxruntime doesn't support GPU, the available providers are CPUExecutionProvider, will fallback to CPUExecutionProvider.\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::CreateOrtBackend     Runtime initialized with Backend::ORT in Device::GPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n0.000000,1.001957, 0.000000, 0.395512, 181.322754, 31\r\n0.000000,1.001957, 0.000000, 0.369145, 180.566071, 39\r\n0.000000,0.210942, 0.000000, 0.000000, 177.169708, 47\r\n0.000000,1.001957, 0.000000, 0.395512, 174.942490, 15\r\n0.000000,1.001957, 0.000000, 0.395512, 174.739410, 23\r\n0.000000,0.210942, 0.000000, 0.000000, 165.407379, 39\r\n0.000000,0.949223, 0.000000, 0.421879, 162.828674, 71\r\n0.000000,0.210942, 0.000000, 0.000000, 161.926346, 55\r\n0.000000,1.001957, 0.000000, 0.369145, 161.294617, 47\r\n0.000000,1.001957, 0.000000, 0.369145, 160.054245, 63\r\n0.000000,0.210942, 0.000000, 0.000000, 156.500076, 31\r\n0.000000,0.995365, 0.000000, 0.388920, 155.943481, 7\r\n0.000000,1.001957, 0.000000, 0.369145, 153.643463, 55\r\n0.000000,0.210942, 0.000000, 0.000000, 151.573547, 23\r\n0.000000,0.210942, 0.000000, 0.000000, 149.818756, 63\r\n0.000000,0.210942, 0.000000, 0.000000, 145.734756, 15\r\n0.000000,0.000000, 0.000000, 0.000000, 138.204865, 55\r\n0.000000,0.000000, 0.000000, 0.000000, 133.943787, 39\r\n0.000000,0.210942, 0.000000, 0.000000, 132.908005, 71\r\n0.000000,0.000000, 0.000000, 0.000000, 132.323013, 47\r\n0.000000,0.210942, 0.000000, 0.000000, 130.758591, 7\r\n0.000000,0.000000, 0.000000, 0.000000, 127.183723, 31\r\n0.000000,0.000000, 0.000000, 0.000000, 123.453979, 15\r\n0.000000,0.000000, 0.000000, 0.000000, 121.312378, 23\r\n0.000000,0.000000, 0.000000, 0.000000, 112.023926, 7\r\n0.000000,0.000000, 0.000000, 0.000000, 108.697189, 63\r\n0.000000,0.000000, 0.000000, 0.000000, 107.040901, 71\r\n0.000000,1.001957, 0.000000, 0.395512, 61.301853, 10\r\n0.000000,0.997013, 0.000000, 0.392216, 58.003975, 2\r\n0.000000,1.001957, 0.000000, 0.395512, 56.212009, 26\r\n0.000000,1.001957, 0.000000, 0.369145, 54.793217, 66\r\n0.000000,1.001957, 0.000000, 0.395512, 54.544479, 18\r\n0.000000,0.000000, 0.000000, 0.000000, 54.527737, 58\r\n0.000000,0.000000, 0.000000, 0.000000, 54.483116, 42\r\n0.000000,0.000000, 0.000000, 0.000000, 54.324497, 34\r\n0.000000,0.949223, 0.000000, 0.421879, 52.989067, 74\r\n0.000000,0.000000, 0.000000, 0.000000, 52.651043, 50\r\n0.000000,0.210942, 0.000000, 0.000000, 52.004429, 10\r\n0.000000,1.001957, 0.000000, 0.369145, 51.813637, 42\r\n0.000000,1.001957, 0.000000, 0.369145, 51.416630, 58\r\n0.000000,0.210942, 0.000000, 0.000000, 51.095356, 74\r\n0.000000,0.210942, 0.000000, 0.000000, 51.050442, 26\r\n0.000000,0.000000, 0.000000, 0.000000, 50.362961, 26\r\n0.000000,0.210942, 0.000000, 0.000000, 50.056538, 50\r\n0.000000,0.210942, 0.000000, 0.000000, 49.944679, 34\r\n0.000000,1.001957, 0.000000, 0.369145, 49.737259, 50\r\n0.000000,0.210942, 0.000000, 0.000000, 49.661171, 66\r\n0.000000,1.001957, 0.000000, 0.395512, 49.424870, 34\r\n0.000000,0.000000, 0.000000, 0.000000, 49.267784, 10\r\n0.000000,0.000000, 0.000000, 0.000000, 49.138210, 18\r\n0.000000,0.210942, 0.000000, 0.000000, 48.413609, 18\r\n0.000000,0.210942, 0.000000, 0.000000, 48.377003, 42\r\n0.000000,0.000000, 0.000000, 0.000000, 48.204887, 66\r\n0.000000,0.000000, 0.000000, 0.000000, 47.516205, 74\r\n0.000000,0.210942, 0.000000, 0.000000, 47.361111, 58\r\n0.000000,0.210942, 0.000000, 0.000000, 47.045578, 2\r\n0.000000,0.000000, 0.000000, 0.000000, 45.907986, 2\r\n0.000000,0.000000, 0.000000, 0.000000, 28.570461, 33\r\n0.000000,0.000000, 0.000000, 0.000000, 25.377211, 41\r\n0.000000,0.000000, 0.000000, 0.000000, 22.179728, 25\r\n0.000000,0.997013, 0.000000, 0.391392, 21.287197, 1\r\n0.000000,0.000000, 0.000000, 0.000000, 21.066339, 49\r\n0.000000,1.001957, 0.000000, 0.395512, 20.681120, 9\r\n0.000000,0.000000, 0.000000, 0.000000, 19.010233, 57\r\n0.000000,1.001957, 0.000000, 0.395512, 18.785971, 17\r\n0.000000,0.000000, 0.000000, 0.000000, 18.241829, 17\r\n0.000000,0.210942, 0.000000, 0.000000, 18.024366, 73\r\n0.000000,0.000000, 0.000000, 0.000000, 17.314758, 9\r\n0.000000,1.001957, 0.000000, 0.395512, 16.818521, 25\r\n0.000000,0.000000, 0.000000, 0.000000, 16.553030, 1\r\n0.000000,0.210942, 0.000000, 0.000000, 14.562191, 65\r\n0.000000,0.210809, 0.000000, 0.000000, 13.337114, 0\r\n0.000000,1.001957, 0.000000, 0.369145, 12.343892, 48\r\n0.000000,1.001957, 0.000000, 0.369145, 11.737338, 38\r\n0.000000,1.001957, 0.000000, 0.369145, 11.295923, 54\r\n0.000000,1.001957, 0.000000, 0.369145, 11.191858, 62\r\n0.000000,1.001957, 0.000000, 0.395512, 11.157640, 33\r\n0.000000,1.001957, 0.000000, 0.369145, 11.002785, 46\r\n0.000000,0.949223, 0.000000, 0.421879, 10.822205, 73\r\n0.000000,1.001957, 0.000000, 0.395512, 10.116310, 30\r\n0.000000,0.000000, 0.000000, 0.000000, 9.958363, 35\r\n0.000000,0.949223, 0.000000, 0.421879, 9.836544, 70\r\n0.000000,0.210942, 0.000000, 0.000000, 9.671453, 30\r\n0.000000,0.210942, 0.000000, 0.000000, 9.537181, 57\r\n0.000000,0.210942, 0.000000, 0.000000, 9.462279, 38\r\n0.000000,0.000000, 0.000000, 0.000000, 9.003942, 27\r\n0.000000,0.000000, 0.000000, 0.000000, 8.871261, 65\r\n0.000000,1.001957, 0.000000, 0.369145, 8.599919, 56\r\n0.000000,0.000000, 0.000000, 0.000000, 8.447394, 59\r\n0.000000,0.210942, 0.000000, 0.000000, 8.353404, 22\r\n0.000000,0.000000, 0.000000, 0.000000, 8.254053, 51\r\n0.000000,0.210942, 0.000000, 0.000000, 7.749981, 46\r\n0.000000,0.995365, 0.000000, 0.392216, 7.351247, 3\r\n0.000000,1.001957, 0.000000, 0.369145, 7.257781, 65\r\n0.000000,0.000000, 0.000000, 0.000000, 7.189124, 43\r\n0.000000,0.000000, 0.000000, 0.000000, 6.932538, 11\r\n0.000000,0.000000, 0.000000, 0.000000, 6.846417, 3\r\n0.000000,0.210942, 0.000000, 0.000000, 6.743724, 14\r\n0.000000,1.001957, 0.000000, 0.395512, 6.532372, 22\r\n0.000000,1.001957, 0.000000, 0.369145, 6.260764, 40\r\n0.000000,0.210942, 0.000000, 0.000000, 6.181995, 54\r\n0.000000,1.001957, 0.000000, 0.395512, 5.979812, 14\r\n0.000000,0.995365, 0.000000, 0.388920, 5.630559, 6\r\n0.000000,1.001957, 0.000000, 0.369145, 5.358659, 57\r\n0.000000,0.210942, 0.000000, 0.000000, 5.334750, 32\r\n0.000000,0.000000, 0.000000, 0.000000, 5.129237, 19\r\n0.000000,0.210942, 0.000000, 0.000000, 4.887820, 16\r\n0.000000,1.001957, 0.000000, 0.369145, 4.824608, 41\r\n0.000000,0.210942, 0.000000, 0.000000, 4.670094, 49\r\n0.000000,1.001957, 0.000000, 0.369145, 4.294595, 60\r\n0.000000,0.210942, 0.000000, 0.000000, 4.256571, 40\r\n0.000000,1.001957, 0.000000, 0.369145, 4.202176, 68\r\n0.000000,1.001957, 0.000000, 0.369145, 4.157883, 52\r\n0.000000,0.210942, 0.000000, 0.000000, 4.008450, 6\r\n0.000000,0.210942, 0.000000, 0.000000, 3.982243, 24\r\n0.000000,1.001957, 0.000000, 0.369145, 3.618586, 64\r\n0.000000,1.001957, 0.000000, 0.369145, 3.575584, 59\r\n0.000000,0.210942, 0.000000, 0.000000, 3.435827, 70\r\n0.000000,0.000000, 0.000000, 0.000000, 3.199871, 73\r\n0.000000,0.210942, 0.000000, 0.000000, 3.196651, 62\r\n0.000000,0.210942, 0.000000, 0.000000, 3.173521, 19\r\n0.000000,1.001957, 0.000000, 0.369145, 3.079395, 44\r\n0.000000,1.001957, 0.000000, 0.369145, 3.053259, 36\r\n0.000000,0.210942, 0.000000, 0.000000, 3.002864, 28\r\n0.000000,1.001957, 0.000000, 0.395512, 2.926658, 28\r\n0.000000,1.001957, 0.000000, 0.395512, 2.648625, 19\r\n0.000000,0.210942, 0.000000, 0.000000, 2.593024, 3\r\n0.000000,0.000000, 0.000000, 0.000000, 2.463940, 67\r\n0.000000,0.210942, 0.000000, 0.000000, 2.335432, 48\r\n0.000000,0.210942, 0.000000, 0.000000, 2.278639, 36\r\n0.000000,1.001957, 0.000000, 0.395512, 1.891616, 32\r\n0.000000,0.210942, 0.000000, 0.000000, 1.872728, 8\r\n0.000000,0.210942, 0.000000, 0.000000, 1.858283, 44\r\n0.000000,0.210942, 0.000000, 0.000000, 1.785972, 20\r\n0.000000,0.210942, 0.000000, 0.000000, 1.775231, 41\r\n0.000000,0.210942, 0.000000, 0.000000, 1.504135, 27\r\n0.000000,0.949223, 0.000000, 0.421879, 1.249904, 72\r\n0.000000,1.001957, 0.000000, 0.369145, 1.006238, 49\r\n0.000000,0.000000, 0.000000, 0.000000, 0.963787, 6\r\n0.000000,0.210942, 0.000000, 0.000000, 0.919920, 17\r\n0.000000,0.000000, 0.000000, 0.000000, 0.746780, 70\r\n0.000000,0.210942, 0.000000, 0.000000, 0.726820, 59\r\n0.000000,0.000000, 0.000000, 0.000000, 0.387675, 4\r\n\r\nVisualized result save in ./visualized_result.jpg\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "Shifiter",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-19T07:22:13+00:00",
        "updated_at": "2025-04-01T06:44:01+00:00",
        "closed_at": "2025-04-01T06:44:01+00:00",
        "comments_count": [
            "Thunderwisking"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1836,
        "title": "国产操作系统：openEuler20.03 中，编译c版本 compiled_fastdeploy_sdk 时,make -j12报错",
        "body": "## 环境\r\n- 【系统平台】: openEuler20.03，gcc版本7.3.0\r\n       现在大型国企央要求的国产化操作系统麒麟等操作系统都是基于openEuler的，所以感觉解决这个问题很有必要性。\r\n- 【FastDeploy版本】： fastdeploy-release-1.0.5 、 1.0.6分支代码\r\n- 【编译命令】\r\n\r\n     cd FastDeploy\r\n     mkdir build && cd build\r\n     \r\n     cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON \\\r\n         -DWITH_CAPI=ON \r\n         \r\n        make -j12\r\n        make install\r\n\r\n\r\n【问题】：compiled_fastdeploy_sdk，cmake没问题，make -j12报错，1.0.4代码没有问题\r\n\r\n![image](https://user-images.githubusercontent.com/59901551/232983611-23dfb8d0-67de-49ee-95dc-7a70207ea30d.png)\r\n",
        "state": "closed",
        "user": "wangxu372848892",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-19T06:18:52+00:00",
        "updated_at": "2024-04-30T06:42:45+00:00",
        "closed_at": "2024-04-30T06:42:45+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1842,
        "title": "deeplabv3似乎argmax输出不支持tensorrt部署?",
        "body": "当output_op是none的时候没有任何问题，换成argmax时加载模型出现以下报错\r\n[graphShapeAnalyzer.cpp::analyzeShapes::1872] Error Code 4: Miscellaneous (IShuffleLayer arg_max (Output: argmax_0.tmp_01928): reshape dimension with more than one -1 wildcard. Reshaping [1,1,(# 2 (SHAPE x)),(# 3 (SHAPE x))] to [1,-1,-1].)\r\n以下是加载模型代码\r\noption=fd.RuntimeOption()\r\nption.use_gpu(0)\r\n option.use_trt_backend() \r\noption.enable_paddle_to_trt()\r\noption.enable_paddle_trt_collect_shape()\r\noption.set_trt_input_shape(\"x\", [1, 3, 256, 256], [1,3,512,512],[1, 3, 1024, 1024])\r\noption.enable_trt_fp16()\r\nmodel_path=os.path.join(path,\"model.pdmodel\") \r\nparams_file = os.path.join(path, \"model.pdiparams\")\r\nconfig_file = os.path.join(path, \"deploy.yaml\")\r\nmodel=fd.vision.segmentation.PaddleSegModel(model_path,params_file,config_file，runtime_option=option)",
        "state": "open",
        "user": "kkpssr",
        "closed_by": null,
        "created_at": "2023-04-19T10:33:48+00:00",
        "updated_at": "2023-04-19T10:34:29+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1843,
        "title": "PaddleLite 示例程序执行时出错",
        "body": "文档位于：\r\nhttps://github.com/PaddlePaddle/Paddle-Lite/blob/develop/docs/demo_guides/opencl.md\r\n\r\n## 环境\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 16.04)\r\n\r\n## 问题一，脚本无法复制库文件\r\n\r\n```\r\nPaddleLite-generic-demo/image_classification_demo/shell$ ./run_with_adb.sh mobilenet_v1_fp32_224 imagenet_224.txt test android arm64-v8a opencl ZY22DVWSK3\r\nModel mobilenet_v1_fp32_224 not found! Try to download it from http://paddlelite-demo.bj.bcebos.com/devices/generic/models/mobilenet_v1_fp32_224.tar.gz ...\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 15.1M  100 15.1M    0     0  6145k      0  0:00:02  0:00:02 --:--:-- 6146k\r\nfailed to copy '../../libs/PaddleLite/android/arm64-v8a/lib/libpaddle_full_api_shared.so' to '../../libs/PaddleLite/android/arm64-v8a/lib/libpaddle_light_api_shared.so': secure_mkdirs failed: No such file or directory\r\n```\r\n\r\n经过查找发现文件存在，所以手动重命名 `PaddleLite-generic-demo/libs/PaddleLite/android/arm64-v8a/lib/libpaddle_light_api_shared.so.backup` 之后，这个问题不再发生\r\n\r\n## 问题二，报错 Unsupported model format!\r\n\r\n执行下面命令时报错。\r\n\r\n```\r\nPaddleLite-generic-demo/image_classification_demo/shell$ ./run_with_adb.sh mobilenet_v1_fp32_224 imagenet_224.txt test android arm64-v8a opencl ZY22DVWSK3\r\n```\r\n\r\n具体输出如下。\r\n\r\n```\r\n3984 KB/s (32647424 bytes in 8.001s)\r\npush: ../../libs/PaddleLite/android/arm64-v8a/lib/opencl/libpaddle_light_api_shared.so -> /data/local/tmp/test/libpaddle_light_api_shared.so\r\npush: ../../libs/PaddleLite/android/arm64-v8a/lib/opencl/libpaddle_full_api_shared.so -> /data/local/tmp/test/libpaddle_full_api_shared.so\r\npush: ../../libs/PaddleLite/android/arm64-v8a/lib/opencl/.gitignore -> /data/local/tmp/test/.gitignore\r\n3 files pushed. 0 files skipped.\r\n4125 KB/s (43953042 bytes in 10.404s)\r\npush: ../../libs/PaddleLite/android/arm64-v8a/lib/cpu/libc++_shared.so -> /data/local/tmp/test/libc++_shared.so\r\npush: ../../libs/PaddleLite/android/arm64-v8a/lib/cpu/.gitignore -> /data/local/tmp/test/.gitignore\r\n2 files pushed. 0 files skipped.\r\n3481 KB/s (1055034 bytes in 0.295s)\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_dw_bn_mean -> /data/local/tmp/test/conv3_1_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_sep_bn_mean -> /data/local/tmp/test/conv6_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_sep_bn_offset -> /data/local/tmp/test/conv2_1_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_sep_bn_variance -> /data/local/tmp/test/conv5_5_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv1_bn_scale -> /data/local/tmp/test/conv1_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_sep_bn_mean -> /data/local/tmp/test/conv5_6_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_sep_bn_variance -> /data/local/tmp/test/conv5_1_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_dw_weights -> /data/local/tmp/test/conv5_4_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_dw_bn_offset -> /data/local/tmp/test/conv5_1_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_sep_weights -> /data/local/tmp/test/conv2_2_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_dw_bn_scale -> /data/local/tmp/test/conv5_6_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_dw_weights -> /data/local/tmp/test/conv5_2_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_dw_bn_offset -> /data/local/tmp/test/conv5_5_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_sep_bn_scale -> /data/local/tmp/test/conv5_3_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_dw_bn_variance -> /data/local/tmp/test/conv5_4_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_dw_bn_variance -> /data/local/tmp/test/conv6_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_sep_weights -> /data/local/tmp/test/conv5_1_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_dw_bn_mean -> /data/local/tmp/test/conv3_2_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv1_bn_mean -> /data/local/tmp/test/conv1_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_sep_weights -> /data/local/tmp/test/conv2_1_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_dw_bn_offset -> /data/local/tmp/test/conv3_2_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_dw_bn_variance -> /data/local/tmp/test/conv3_1_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/fc7_weights -> /data/local/tmp/test/fc7_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_dw_weights -> /data/local/tmp/test/conv5_3_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_dw_bn_scale -> /data/local/tmp/test/conv5_4_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_dw_bn_mean -> /data/local/tmp/test/conv4_1_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_sep_bn_variance -> /data/local/tmp/test/conv3_2_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_sep_bn_mean -> /data/local/tmp/test/conv3_1_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_sep_bn_mean -> /data/local/tmp/test/conv5_5_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_dw_bn_variance -> /data/local/tmp/test/conv2_1_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_sep_bn_scale -> /data/local/tmp/test/conv5_5_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_dw_bn_offset -> /data/local/tmp/test/conv2_1_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_sep_bn_scale -> /data/local/tmp/test/conv2_2_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_sep_bn_scale -> /data/local/tmp/test/conv5_4_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_sep_weights -> /data/local/tmp/test/conv4_2_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_sep_bn_mean -> /data/local/tmp/test/conv5_1_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_sep_bn_offset -> /data/local/tmp/test/conv6_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_sep_bn_variance -> /data/local/tmp/test/conv5_6_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_sep_bn_scale -> /data/local/tmp/test/conv5_2_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_dw_bn_mean -> /data/local/tmp/test/conv5_6_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_dw_bn_variance -> /data/local/tmp/test/conv5_5_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_sep_bn_mean -> /data/local/tmp/test/conv2_2_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_dw_bn_offset -> /data/local/tmp/test/conv5_3_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_sep_bn_variance -> /data/local/tmp/test/conv6_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_sep_bn_scale -> /data/local/tmp/test/conv5_1_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_sep_bn_scale -> /data/local/tmp/test/conv4_2_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_sep_weights -> /data/local/tmp/test/conv5_2_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_dw_bn_mean -> /data/local/tmp/test/conv5_3_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_dw_bn_scale -> /data/local/tmp/test/conv2_1_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_sep_bn_mean -> /data/local/tmp/test/conv4_1_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_sep_bn_mean -> /data/local/tmp/test/conv5_3_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_dw_bn_mean -> /data/local/tmp/test/conv5_4_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_dw_weights -> /data/local/tmp/test/conv5_5_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_dw_bn_scale -> /data/local/tmp/test/conv3_2_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_sep_bn_offset -> /data/local/tmp/test/conv2_2_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv1_bn_variance -> /data/local/tmp/test/conv1_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_sep_bn_offset -> /data/local/tmp/test/conv4_2_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_sep_weights -> /data/local/tmp/test/conv6_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_sep_bn_scale -> /data/local/tmp/test/conv3_1_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_sep_bn_variance -> /data/local/tmp/test/conv4_1_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_sep_weights -> /data/local/tmp/test/conv3_1_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_sep_bn_offset -> /data/local/tmp/test/conv3_1_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_dw_bn_mean -> /data/local/tmp/test/conv5_2_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_dw_bn_scale -> /data/local/tmp/test/conv5_3_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_sep_bn_offset -> /data/local/tmp/test/conv3_2_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv1_bn_offset -> /data/local/tmp/test/conv1_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_sep_bn_offset -> /data/local/tmp/test/conv5_1_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_sep_bn_offset -> /data/local/tmp/test/conv5_4_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_dw_bn_offset -> /data/local/tmp/test/conv5_2_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_dw_bn_scale -> /data/local/tmp/test/conv4_2_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_sep_bn_variance -> /data/local/tmp/test/conv2_2_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_sep_bn_scale -> /data/local/tmp/test/conv2_1_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_dw_bn_scale -> /data/local/tmp/test/conv3_1_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_dw_bn_scale -> /data/local/tmp/test/conv5_2_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_sep_bn_scale -> /data/local/tmp/test/conv3_2_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_sep_bn_offset -> /data/local/tmp/test/conv4_1_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_sep_bn_mean -> /data/local/tmp/test/conv3_2_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_dw_bn_mean -> /data/local/tmp/test/conv4_2_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_dw_weights -> /data/local/tmp/test/conv4_2_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_dw_bn_offset -> /data/local/tmp/test/conv2_2_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_sep_weights -> /data/local/tmp/test/conv5_5_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_dw_weights -> /data/local/tmp/test/conv3_2_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_sep_bn_variance -> /data/local/tmp/test/conv3_1_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_dw_bn_variance -> /data/local/tmp/test/conv2_2_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_sep_bn_variance -> /data/local/tmp/test/conv5_4_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_dw_bn_offset -> /data/local/tmp/test/conv5_4_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_dw_bn_offset -> /data/local/tmp/test/conv4_2_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_dw_bn_offset -> /data/local/tmp/test/conv4_1_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_dw_weights -> /data/local/tmp/test/conv2_2_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_dw_bn_offset -> /data/local/tmp/test/conv6_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_sep_weights -> /data/local/tmp/test/conv4_1_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_dw_bn_variance -> /data/local/tmp/test/conv5_6_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_sep_bn_variance -> /data/local/tmp/test/conv5_3_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_dw_bn_variance -> /data/local/tmp/test/conv3_2_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_sep_bn_mean -> /data/local/tmp/test/conv5_2_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_sep_bn_variance -> /data/local/tmp/test/conv2_1_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_sep_bn_mean -> /data/local/tmp/test/conv5_4_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_dw_weights -> /data/local/tmp/test/conv3_1_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_2_sep_weights -> /data/local/tmp/test/conv3_2_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_sep_bn_offset -> /data/local/tmp/test/conv5_2_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv1_weights -> /data/local/tmp/test/conv1_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_sep_bn_scale -> /data/local/tmp/test/conv4_1_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_sep_bn_variance -> /data/local/tmp/test/conv4_2_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_dw_bn_offset -> /data/local/tmp/test/conv5_6_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/fc7_offset -> /data/local/tmp/test/fc7_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv3_1_dw_bn_offset -> /data/local/tmp/test/conv3_1_dw_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_sep_bn_scale -> /data/local/tmp/test/conv6_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_dw_bn_scale -> /data/local/tmp/test/conv6_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_dw_weights -> /data/local/tmp/test/conv2_1_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_sep_weights -> /data/local/tmp/test/conv5_6_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_dw_bn_scale -> /data/local/tmp/test/conv5_5_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_dw_bn_scale -> /data/local/tmp/test/conv5_1_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_dw_bn_mean -> /data/local/tmp/test/conv2_2_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_dw_bn_scale -> /data/local/tmp/test/conv4_1_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_sep_weights -> /data/local/tmp/test/conv5_3_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_dw_weights -> /data/local/tmp/test/conv6_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_dw_bn_mean -> /data/local/tmp/test/conv2_1_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/__model__ -> /data/local/tmp/test/__model__\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_sep_bn_variance -> /data/local/tmp/test/conv5_2_sep_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_sep_bn_offset -> /data/local/tmp/test/conv5_3_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_dw_bn_mean -> /data/local/tmp/test/conv5_5_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_dw_bn_variance -> /data/local/tmp/test/conv4_2_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_dw_bn_variance -> /data/local/tmp/test/conv4_1_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_dw_bn_variance -> /data/local/tmp/test/conv5_1_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_2_sep_bn_mean -> /data/local/tmp/test/conv4_2_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_dw_weights -> /data/local/tmp/test/conv5_6_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_2_dw_bn_variance -> /data/local/tmp/test/conv5_2_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv4_1_dw_weights -> /data/local/tmp/test/conv4_1_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_5_sep_bn_offset -> /data/local/tmp/test/conv5_5_sep_bn_offset\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_1_sep_bn_mean -> /data/local/tmp/test/conv2_1_sep_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv2_2_dw_bn_scale -> /data/local/tmp/test/conv2_2_dw_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_dw_bn_mean -> /data/local/tmp/test/conv5_1_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv6_dw_bn_mean -> /data/local/tmp/test/conv6_dw_bn_mean\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_1_dw_weights -> /data/local/tmp/test/conv5_1_dw_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_4_sep_weights -> /data/local/tmp/test/conv5_4_sep_weights\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_sep_bn_scale -> /data/local/tmp/test/conv5_6_sep_bn_scale\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_3_dw_bn_variance -> /data/local/tmp/test/conv5_3_dw_bn_variance\r\npush: ../assets/models/mobilenet_v1_fp32_224/conv5_6_sep_bn_offset -> /data/local/tmp/test/conv5_6_sep_bn_offset\r\n138 files pushed. 0 files skipped.\r\n1702 KB/s (17164678 bytes in 9.846s)\r\npush: ../assets/configs/./synset_words.txt -> /data/local/tmp/test/synset_words.txt\r\npush: ../assets/configs/./imagenet_224.txt -> /data/local/tmp/test/imagenet_224.txt\r\n2 files pushed. 0 files skipped.\r\n316 KB/s (31769 bytes in 0.097s)\r\n3919 KB/s (23147464 bytes in 5.767s)\r\ncannot stat '../assets/models/mobilenet_v1_fp32_224.nb': No such file or directory\r\n0 + :  + tabby_cat.jpg\r\n468 KB/s (24859 bytes in 0.051s)\r\n0 KB/s (14 bytes in 0.051s)\r\nWARNING: linker: Warning: unable to normalize \"./opencl\" (ignoring)\r\nWARNING: linker: Warning: unable to normalize \"./cpu\" (ignoring)\r\nWARNING: linker: Warning: unable to normalize \"./opencl\" (ignoring)\r\nWARNING: linker: Warning: unable to normalize \"./cpu\" (ignoring)\r\nWARNING: linker: Warning: \"/data/local/tmp/test/demo\" unused DT entry: DT_RPATH (type 0xf arg 0x1cfa) (ignoring)\r\nWARNING: linker: Warning: \"/data/local/tmp/test/libpaddle_full_api_shared.so\" unused DT entry: DT_RPATH (type 0xf arg 0x102699) (ignoring)\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: HARDWARE\t: QUALCOMM TECHNOLOGIES, INC KONA\r\n_NIO_KONA_\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 8\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 4, max freq: 2419, min freq: 2419, cluster ID: 0, CPU ARCH: A77\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 5, max freq: 2419, min freq: 2419, cluster ID: 0, CPU ARCH: A77\r\n[I  4/16  9:28:38.180 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 6, max freq: 2419, min freq: 2419, cluster ID: 0, CPU ARCH: A77\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 7, max freq: 3187, min freq: 3187, cluster ID: 0, CPU ARCH: A77\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 256 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 256 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 256 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 256 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 192 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 192 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 192 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 192 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 768 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 768 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 768 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 768 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 7852856KB\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[1  4/16  9:28:38.181 ...ace/Paddle-Lite/lite/api/cxx_api_impl.cc:97 Init] use_layout_preprocess_pass:18446744073709551615\r\n[I  4/16  9:28:38.181 ...orkspace/Paddle-Lite/lite/api/cxx_api.cc:371 Build] Load model from file.\r\n[F  4/16  9:28:38.181 ...e-Lite/lite/model_parser/model_parser.cc:132 PrintPbModelErrorMessage] \r\n Error, Unsupported model format!\r\n      1. contents in model directory should be in one of these formats:\r\n          (1) __model__ + var1 + var2 + etc.\r\n          (2) model + var1 + var2 + etc.\r\n          (3) model.pdmodel + model.pdiparams\r\n          (4) model + params\r\n          (5) model + weights\r\n      2. You can also appoint the model and params file in custom format:\r\n          eg. |-- set_model_file('custom_model_name')\r\n              |-- set_param_file('custom_params_name')'\r\n[F  4/16  9:28:38.181 ...e-Lite/lite/model_parser/model_parser.cc:132 PrintPbModelErrorMessage] \r\n Error, Unsupported model format!\r\n      1. contents in model directory should be in one of these formats:\r\n          (1) __model__ + var1 + var2 + etc.\r\n          (2) model + var1 + var2 + etc.\r\n          (3) model.pdmodel + model.pdiparams\r\n          (4) model + params\r\n          (5) model + weights\r\n      2. You can also appoint the model and params file in custom format:\r\n          eg. |-- set_model_file('custom_model_name')\r\n              |-- set_param_file('custom_params_name')'\r\n\r\nAn internal error occurred in PaddleLite(cxx config).\r\npull: building file list...\r\n0 files pulled. 0 files skipped.\r\nremote object '/data/local/tmp/test/mobilenet_v1_fp32_224.nb' does not exist\r\n```\r\n\r\n确认了当前模型文件夹中，是 `(1) __model__ + var1 + var2 + etc.` 的格式。\r\n\r\n又尝试了推送nb文件，推送model.pdmodel + model.pdiparams等方法，始终报错Unsupported model format!",
        "state": "open",
        "user": "unseenme",
        "closed_by": null,
        "created_at": "2023-04-19T14:49:43+00:00",
        "updated_at": "2023-04-23T14:03:29+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "unseenme"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1848,
        "title": "fastdeploy前处理问题",
        "body": "请问deeplabv3导出后，输入图片我自己resize和直接指定inputshape导出，输入原图有区别么\r\n",
        "state": "open",
        "user": "kkpssr",
        "closed_by": null,
        "created_at": "2023-04-20T06:41:55+00:00",
        "updated_at": "2023-04-20T06:41:55+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1845,
        "title": "按Paddle-Lite文档操作，出现编译错误与执行错误",
        "body": "文档：\r\nhttps://paddlepaddle.github.io/Paddle-Lite/v2.2.0/opencl/\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： Paddle-Lite v2.2.0\r\n- 【编译命令】\r\n\r\n```\r\n$ git clone -b release/v2.2.0 https://github.com/PaddlePaddle/Paddle-Lite.git\r\n$ docker run -it --name paddlelite_docker -v /home/unseenme/Paddle-Lite:/Paddle-Lite --net=host paddlepaddle/paddle-lite /bin/bash\r\n\r\nroot@PC:/# export NDK_ROOT=/opt/android-ndk-r17c\r\nroot@PC:/Paddle-Lite# ./lite/tools/ci_build.sh --arm_os=android --arm_abi=armv8 --arm_lang=gcc build_test_arm_opencl\r\n```\r\n\r\n## 问题一，示例1编译出错\r\n\r\n想要【在产物目录`demo/cxx/mobile_full`下编译`mobile_full`的demo】\r\n\r\n```\r\nroot@PC:/Paddle-Lite/build.lite.android.armv8.gcc.opencl/inference_lite_lib.android.armv8.opencl/demo/cxx/mobile_full# make\r\n```\r\n\r\n出错\r\n\r\n```\r\n../../..//cxx/lib//libpaddle_full_api_shared.so: undefined reference to `testing::UnitTest::GetInstance()'\r\n../../..//cxx/lib//libpaddle_full_api_shared.so: undefined reference to `testing::UnitTest::Run()'\r\n../../..//cxx/lib//libpaddle_full_api_shared.so: undefined reference to `testing::InitGoogleTest(int*, char**)'\r\n```\r\n\r\n\r\n## 问题二，示例二执行出错\r\n\r\n执行下面命令时，出错。\r\n\r\n```\r\n$ adb -s DEVICEID shell /data/local/tmp/opencl/test_mobilenetv1 --cl_path=/data/local/tmp/opencl --model_dir=/data/local/tmp/opencl/mobilenet_v1 --warmup=1 --repeats=1\r\n```\r\n\r\n输出如下。\r\n\r\n```\r\n[==========] Running 2 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 2 tests from MobileNetV1\r\n[ RUN      ] MobileNetV1.test_arm\r\n[I  4/20  8:43:12.846 /Paddle-Lite/lite/core/device_info.cc get_cpu_arch:210] Unknow cpu arch: 3341\r\n[I  4/20  8:43:12.846 /Paddle-Lite/lite/core/device_info.cc get_cpu_arch:210] Unknow cpu arch: 3341\r\n[I  4/20  8:43:12.846 /Paddle-Lite/lite/core/device_info.cc get_cpu_arch:210] Unknow cpu arch: 3341\r\n[I  4/20  8:43:12.847 /Paddle-Lite/lite/core/device_info.cc get_cpu_arch:210] Unknow cpu arch: 3341\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1035] ARM multiprocessors name: HARDWARE\t: QUALCOMM TECHNOLOGIES, INC KONA\r\n_NIO_KONA_\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1036] ARM multiprocessors number: 8\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 0, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 1, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 2, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 3, max freq: 1804, min freq: 1804, cluster ID: 1, CPU ARCH: A55\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 4, max freq: 2419, min freq: 2419, cluster ID: 1, CPU ARCH: A-1\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 5, max freq: 2419, min freq: 2419, cluster ID: 1, CPU ARCH: A-1\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 6, max freq: 2419, min freq: 2419, cluster ID: 1, CPU ARCH: A-1\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1038] ARM multiprocessors ID: 7, max freq: 3187, min freq: 3187, cluster ID: 0, CPU ARCH: A-1\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1044] L1 DataCache size is: \r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1046] 32 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1048] L2 Cache size is: \r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1050] 512 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1052] L3 Cache size is: \r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1054] 0 KB\r\n[I  4/20  8:43:12.853 /Paddle-Lite/lite/core/device_info.cc Setup:1056] Total memory: 7852856KB\r\n[I  4/20  8:43:12.878 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_quant_dequant_fuse_pass\r\n[I  4/20  8:43:12.881 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_quant_dequant_fuse_pass\r\n[I  4/20  8:43:12.881 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:12.882 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:12.882 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_bn_fuse_pass\r\n[I  4/20  8:43:12.894 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 14 subgraph\r\n[I  4/20  8:43:12.900 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 13 subgraph\r\n[I  4/20  8:43:12.901 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_bn_fuse_pass\r\n[I  4/20  8:43:12.901 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:12.902 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:12.902 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_activation_fuse_pass\r\n[I  4/20  8:43:12.904 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 14 subgraph\r\n[I  4/20  8:43:12.905 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 13 subgraph\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_activation_fuse_pass\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_var_conv_2d_activation_fuse_pass\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_fc_fuse_pass\r\n[I  4/20  8:43:12.906 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 1 subgraph\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_fc_fuse_pass\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_shuffle_channel_fuse_pass\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_shuffle_channel_fuse_pass\r\n[I  4/20  8:43:12.906 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_transpose_softmax_transpose_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_transpose_softmax_transpose_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_interpolate_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_interpolate_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: identity_scale_eliminate_pass\r\n[I  4/20  8:43:12.907 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 1 subgraph\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: identity_scale_eliminate_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: elementwise_mul_constant_eliminate_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: elementwise_mul_constant_eliminate_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_sequence_pool_concat_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_elementwise_add_activation_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_elementwise_add_activation_fuse_pass\r\n[I  4/20  8:43:12.907 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: static_kernel_pick_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: static_kernel_pick_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_target_cast_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_target_cast_pass\r\n[I  4/20  8:43:12.908 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: io_copy_kernel_pick_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: io_copy_kernel_pick_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.909 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_precision_cast_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_precision_cast_pass\r\n[I  4/20  8:43:12.910 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:12.911 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:12.911 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.911 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.911 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_layout_cast_pass\r\ndot:\r\ndigraph G {\r\n   node_478[label=\"fc7_offset\"]\r\n   node_475[label=\"conv2_1_dw_bn_offset\"]\r\n   node_473[label=\"depthwise_conv2d30\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n\r\n...\r\n   node_359->node_358\r\n   node_358->node_360\r\n   node_362->node_361\r\n\r\n...\r\n\r\n   node_363->node_476\r\n   node_478->node_476\r\n   node_476->node_365\r\n} // end G\r\ndot:\r\ndigraph G {\r\n   node_599[label=\"fc7_offset\"]\r\n   node_596[label=\"conv2_1_dw_bn_offset\"]\r\n...\r\n   node_572[label=\"conv5_3_dw_bn_offset\"]\r\n   node_488[label=\"fetch3\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_482[label=\"pool2d1\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_495[label=\"conv2d5\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_510[label=\"conv2d8\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_546[label=\"conv3_1_sep_weights\"]\r\n   node_480[label=\"feed\"]\r\n   node_515[label=\"conv2d9\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_481[label=\"image\"]\r\n   node_480->node_479\r\n   node_479->node_481\r\n   node_483->node_482\r\n...\r\n   node_484->node_597\r\n   node_599->node_597\r\n   node_597->node_486\r\n} // end G\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_layout_cast_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.912 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: runtime_context_assign_pass\r\n[I  4/20  8:43:12.947 ...-Lite/lite/backends/opencl/cl_runtime.cc:147 InitializeDevice] Using device: QUALCOMM Adreno(TM)\r\n[I  4/20  8:43:12.947 ...-Lite/lite/backends/opencl/cl_runtime.cc:150 InitializeDevice] The chosen device supports image processing.\r\n[I  4/20  8:43:12.947 ...-Lite/lite/backends/opencl/cl_runtime.cc:158 InitializeDevice] The chosen device supports the half data type.\r\n[I  4/20  8:43:12.947 ...-Lite/lite/backends/opencl/cl_runtime.cc:163 InitializeDevice] The chosen device has 3 compute units.\r\n[I  4/20  8:43:12.947 ...-Lite/lite/backends/opencl/cl_runtime.cc:165 InitializeDevice] The local memory size of the chosen device is 32 KB.\r\n[I  4/20  8:43:12.947 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: runtime_context_assign_pass\r\n[I  4/20  8:43:12.947 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:12.947 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:12.947 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: memory_optimize_pass\r\n[I  4/20  8:43:12.948 ...te/lite/core/mir/memory_optimize_pass.cc:103 CollectLifeCycleByDevice] There are 1 types device var.\r\n[I  4/20  8:43:12.948 ...te/lite/core/mir/memory_optimize_pass.cc:152 MakeReusePlan] cluster: fc_0.tmp_1\r\n[I  4/20  8:43:12.948 ...te/lite/core/mir/memory_optimize_pass.cc:152 MakeReusePlan] cluster: pool2d_0.tmp_0\r\n[I  4/20  8:43:12.948 ...te/lite/core/mir/memory_optimize_pass.cc:152 MakeReusePlan] cluster: batch_norm_24.tmp_3\r\n[I  4/20  8:43:12.953 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: memory_optimize_pass\r\n[I  4/20  8:43:12.953 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: npu_subgraph_pass\r\n[I  4/20  8:43:12.953 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip npu_subgraph_pass because the target or kernel does not match.\r\n[I  4/20  8:43:12.953 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: xpu_subgraph_pass\r\n[I  4/20  8:43:12.953 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip xpu_subgraph_pass because the target or kernel does not match.\r\ndot:\r\ndigraph G {\r\n   node_720[label=\"fc7_offset\"]\r\n   node_717[label=\"conv2_1_dw_bn_offset\"]\r\n   node_715[label=\"depthwise_conv2d30\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_714[label=\"conv2_2_dw_bn_offset\"]\r\n   node_713[label=\"conv2_2_dw_weights\"]\r\n...\r\n   node_631[label=\"conv2d8\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_667[label=\"conv3_1_sep_weights\"]\r\n   node_601[label=\"feed\"]\r\n   node_636[label=\"conv2d9\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_601->node_600\r\n   node_600->node_602\r\n   node_604->node_603\r\n...\r\n   node_605->node_718\r\n   node_720->node_718\r\n   node_718->node_607\r\n} // end G\r\n[I  4/20  8:43:12.954 ...te/lite/core/mir/generate_program_pass.h:37 GenProgram] insts.size 32\r\n[I  4/20  8:43:13. 25 ...Paddle-Lite/lite/api/mobilenetv1_test.cc:61 TestModel] ================== Speed Report ===================\r\n[I  4/20  8:43:13. 25 ...Paddle-Lite/lite/api/mobilenetv1_test.cc:62 TestModel] Model: /data/local/tmp/opencl/mobilenet_v1, threads num 1, warmup: 1, repeats: 1, spend 27.535 ms in average.\r\n[       OK ] MobileNetV1.test_arm (186 ms)\r\n[ RUN      ] MobileNetV1.test_opencl\r\n[I  4/20  8:43:13. 50 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_quant_dequant_fuse_pass\r\n[I  4/20  8:43:13. 52 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_quant_dequant_fuse_pass\r\n[I  4/20  8:43:13. 52 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:13. 53 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:13. 53 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_bn_fuse_pass\r\n[I  4/20  8:43:13. 61 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 14 subgraph\r\n[I  4/20  8:43:13. 67 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 13 subgraph\r\n[I  4/20  8:43:13. 68 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_bn_fuse_pass\r\n[I  4/20  8:43:13. 68 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:13. 69 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_elementwise_fuse_pass\r\n[I  4/20  8:43:13. 69 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_conv_activation_fuse_pass\r\n[I  4/20  8:43:13. 71 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 14 subgraph\r\n[I  4/20  8:43:13. 72 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 13 subgraph\r\n[I  4/20  8:43:13. 73 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_conv_activation_fuse_pass\r\n[I  4/20  8:43:13. 73 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_var_conv_2d_activation_fuse_pass\r\n[I  4/20  8:43:13. 73 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.\r\n[I  4/20  8:43:13. 73 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_fc_fuse_pass\r\n[I  4/20  8:43:13. 73 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 1 subgraph\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_fc_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_shuffle_channel_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_shuffle_channel_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_transpose_softmax_transpose_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_transpose_softmax_transpose_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_interpolate_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_interpolate_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: identity_scale_eliminate_pass\r\n[I  4/20  8:43:13. 74 ...le-Lite/lite/core/mir/pattern_matcher.cc:108 operator()] detected 1 subgraph\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: identity_scale_eliminate_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: elementwise_mul_constant_eliminate_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: elementwise_mul_constant_eliminate_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_sequence_pool_concat_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: lite_elementwise_add_activation_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: lite_elementwise_add_activation_fuse_pass\r\n[I  4/20  8:43:13. 74 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: static_kernel_pick_pass\r\n[I  4/20  8:43:13. 76 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: static_kernel_pick_pass\r\n[I  4/20  8:43:13. 76 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:13. 76 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:13. 76 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 77 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 77 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_target_cast_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_target_cast_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 84 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: io_copy_kernel_pick_pass\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:30 Apply] ....> picking a IO COPY kernel\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:36 Apply] input type Tensor<host,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:37 Apply] output type Tensor<opencl,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:40 Apply] kernels size 1\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:47 Apply] checking kernel candidate Tensor<host,float,NCHW,0>->Tensor<opencl,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:53 Apply] get a IOCopy kernel\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:30 Apply] ....> picking a IO COPY kernel\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:36 Apply] input type Tensor<opencl,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:37 Apply] output type Tensor<arm,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:40 Apply] kernels size 1\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:47 Apply] checking kernel candidate Tensor<opencl,float,NCHW,0>->Tensor<host,float,NCHW,0>\r\n[I  4/20  8:43:13. 85 ...ite/core/mir/io_copy_kernel_pick_pass.cc:53 Apply] get a IOCopy kernel\r\n[I  4/20  8:43:13. 85 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: io_copy_kernel_pick_pass\r\n[I  4/20  8:43:13. 85 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 85 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 85 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_precision_cast_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_precision_cast_pass\r\n[I  4/20  8:43:13. 86 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:13. 87 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:13. 87 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 87 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 87 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: type_layout_cast_pass\r\ndot:\r\ndigraph G {\r\n   node_1313[label=\"io_copy_once88\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1312[label=\"fc7_weights\"]\r\n   node_1309[label=\"io_copy_once86\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1300[label=\"conv5_6_sep_weights\"]\r\n   node_1295[label=\"io_copy_once79\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n...\r\n   node_1079[label=\"feed0\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1080->node_1079\r\n   node_1079->node_1081\r\n   node_1083->node_1082\r\n...\r\n   node_1313->node_1200\r\n   node_1201->node_1315\r\n   node_1315->node_1086\r\n} // end G\r\ndot:\r\ndigraph G {\r\n   node_1550[label=\"io_copy_once88\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1549[label=\"fc7_weights\"]\r\n   node_1546[label=\"io_copy_once86\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1537[label=\"conv5_6_sep_weights\"]\r\n   node_1532[label=\"io_copy_once79\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1531[label=\"conv5_5_sep_bn_offset\"]\r\n...\r\n   node_1317->node_1316\r\n   node_1316->node_1318\r\n   node_1320->node_1319\r\n...\r\n   node_1550->node_1437\r\n   node_1438->node_1552\r\n   node_1552->node_1323\r\n} // end G\r\n[I  4/20  8:43:13. 95 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: type_layout_cast_pass\r\n[I  4/20  8:43:13. 95 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 95 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 95 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: variable_place_inference_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: variable_place_inference_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: runtime_context_assign_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: runtime_context_assign_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: argument_type_display_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:180] == Finished running: argument_type_display_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: memory_optimize_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip memory_optimize_pass because the target or kernel does not match.\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: npu_subgraph_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip npu_subgraph_pass because the target or kernel does not match.\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:163] == Running pass: xpu_subgraph_pass\r\n[I  4/20  8:43:13. 96 /Paddle-Lite/lite/core/optimizer.h RunPasses:176]    - Skip xpu_subgraph_pass because the target or kernel does not match.\r\ndot:\r\ndigraph G {\r\n   node_1787[label=\"io_copy_once88\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n   node_1786[label=\"fc7_weights\"]\r\n   node_1783[label=\"io_copy_once86\" shape=\"box\" style=\"filled\" color=\"black\" fillcolor=\"yellow\"]\r\n...\r\n   node_1554->node_1553\r\n   node_1553->node_1555\r\n   node_1557->node_1556\r\n...\r\n   node_1787->node_1674\r\n   node_1675->node_1789\r\n   node_1789->node_1560\r\n} // end G\r\n[I  4/20  8:43:13.100 ...te/lite/core/mir/generate_program_pass.h:37 GenProgram] insts.size 90\r\n[I  4/20  8:43:13.233 ...-Lite/lite/backends/opencl/cl_runtime.cc:108 BuildProgram] OpenCL error with code CL_OUT_OF_HOST_MEMORY happened in file /Paddle-Lite/lite/backends/opencl/cl_runtime.cc at line 108. Exiting.\r\n\r\n[F  4/20  8:43:13.233 ...-Lite/lite/backends/opencl/cl_runtime.cc:114 BuildProgram] Program build error: Pass\r\nAborted \r\n```\r\n",
        "state": "closed",
        "user": "unseenme",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-19T23:44:11+00:00",
        "updated_at": "2024-04-23T06:40:54+00:00",
        "closed_at": "2024-04-23T06:40:54+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1844,
        "title": "fastdeploy_serving部署了OCR服务，但是识别不出内容",
        "body": "https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/fastdeploy_serving\r\n按照上面这个github文档用fastdeploy_serving部署了OCR服务(拉取项目这块我的虚拟机连不上github，我只取了其中的models文件夹和测试脚本还有测试图片放进虚拟机)，好像是部署成功了，调用官方给的测试脚本client.py报下标溢出什么的，我自己输出了下result对象，好像识别为空。能看出来我哪里没做对吗……\r\n\r\n- 系统环境/System Environment：centos7\r\n- 版本号/Version： fastdeploy:1.0.4-cpu-only-21.10\r\n- 运行指令/Command Code：python3 testclient.py \r\n- 完整报错/Complete Error Message：\r\n\r\n执行显示结果，因为下面报下标溢出，所以加了点料看看情况应该没啥影响吧。\r\nprint(\"1111\")\r\nprint(str(result))\r\nprint(\"222\")\r\n\r\n```\r\n1111\r\n{'rec_texts': array([[b'']], dtype=object), 'rec_scores': array([[0.]]), 'det_bboxes': array([], shape=(1, 0), dtype=float64)}\r\n222\r\nTraceback (most recent call last):\r\n  File \"testclient.py\", line 112, in <module>\r\n    scores[i_box], '  bbox=', bboxes[i_box])\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```\r\n这个是服务启动的信息\r\n```\r\nI0419 19:09:53.622293 26 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0419 19:09:53.723604 26 model_repository_manager.cc:1022] loading: det_runtime:1\r\nI0419 19:09:53.824584 26 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0419 19:09:53.892782 26 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0419 19:09:53.925668 26 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nI0419 19:09:54.026652 26 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0419 19:09:54.127385 26 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\nI0419 19:09:54.227914 26 model_repository_manager.cc:1022] loading: rec_runtime:1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0419 19:09:55.839864 26 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\nI0419 19:09:57.571733 26 fastdeploy_runtime.cc:1182] TRITONBACKEND_Initialize: fastdeploy\r\nI0419 19:09:57.571763 26 fastdeploy_runtime.cc:1191] Triton TRITONBACKEND API version: 1.6\r\nI0419 19:09:57.571769 26 fastdeploy_runtime.cc:1196] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0419 19:09:57.571775 26 fastdeploy_runtime.cc:1225] backend configuration:\r\n{}\r\nI0419 19:09:57.571825 26 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\nI0419 19:09:57.572325 26 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (CPU device 0)\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0420 03:09:57.694269    28 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0419 19:09:58.286728 26 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nI0419 19:09:58.310525 26 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0419 19:09:58.718479 26 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0419 19:09:58.736522 26 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0419 19:09:58.997529 26 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\nI0419 19:09:58.998007 26 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (CPU device 0)\r\nI0419 19:09:58.999130 26 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0419 19:09:59.608192 26 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0419 19:09:59.639647 26 model_repository_manager.cc:1183] successfully loaded 'rec_runtime' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0419 19:10:00.135574 26 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\nI0419 19:10:00.137279 26 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (CPU device 0)\r\n[INFO] fastdeploy/runtime/runtime.cc(91)::AutoSelectBackend\tFastDeploy will choose Backend::PDINFER to inference this model.\r\nI0419 19:10:00.142692 26 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nI0419 19:10:00.576136 26 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\nI0419 19:10:00.577625 26 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0419 19:10:00.685666 26 model_repository_manager.cc:1022] loading: rec_pp:1\r\nI0419 19:10:00.790667 26 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0419 19:10:00.897512 26 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0419 19:10:00.897642 26 model_repository_manager.cc:1183] successfully loaded 'rec_pp' version 1\r\nI0419 19:10:00.897751 26 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0419 19:10:00.898096 26 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0419 19:10:00.898122 26 server.cc:549] \r\n+------------+-------------------------------------------------------+--------+\r\n| Backend    | Path                                                  | Config |\r\n+------------+-------------------------------------------------------+--------+\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so | {}     |\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastd | {}     |\r\n|            | eploy.so                                              |        |\r\n+------------+-------------------------------------------------------+--------+\r\n\r\nI0419 19:10:00.898165 26 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| cls_pp          | 1       | READY  |\r\n| cls_runtime     | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| det_runtime     | 1       | READY  |\r\n| pp_ocr          | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n| rec_pp          | 1       | READY  |\r\n| rec_runtime     | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0419 19:10:00.898219 26 tritonserver.cc:1920] \r\n+----------------------------------+------------------------------------------+\r\n| Option                           | Value                                    |\r\n+----------------------------------+------------------------------------------+\r\n| server_id                        | triton                                   |\r\n| server_version                   | 2.15.0                                   |\r\n| server_extensions                | classification sequence model_repository |\r\n|                                  |  model_repository(unload_dependents) sch |\r\n|                                  | edule_policy model_configuration system_ |\r\n|                                  | shared_memory cuda_shared_memory binary_ |\r\n|                                  | tensor_data statistics                   |\r\n| model_repository_path[0]         | /opt/tritonserver/model/models           |\r\n| model_control_mode               | MODE_NONE                                |\r\n| strict_model_config              | 1                                        |\r\n| rate_limit                       | OFF                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                |\r\n| response_cache_byte_size         | 0                                        |\r\n| min_supported_compute_capability | 0.0                                      |\r\n| strict_readiness                 | 1                                        |\r\n| exit_timeout                     | 30                                       |\r\n+----------------------------------+------------------------------------------+\r\n\r\nI0419 19:10:00.903085 26 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0419 19:10:00.903272 26 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0419 19:10:01.038786 26 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\nW0419 19:13:01.920857 26 pinned_memory_manager.cc:133] failed to allocate pinned system memory: no pinned memory pool, falling back to non-pinned system memory\r\n```\r\n",
        "state": "closed",
        "user": "zhouyiminga",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-19T23:18:03+00:00",
        "updated_at": "2024-04-30T06:42:46+00:00",
        "closed_at": "2024-04-30T06:42:46+00:00",
        "comments_count": [
            "Jaccica",
            "pangdahua",
            "zhouyiminga",
            "zhouyiminga",
            "Jaccica"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1847,
        "title": "编译paddleseg_cpu&gpu_csharp的infer.cs失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.4\r\n- 【编译命令】通过cmake .. -G \"Visual Studio 16 2019\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_VISION=ON -DWITH_GPU=ON -DENABLE_TEXT=ON -DWITH_CAPI=ON -DWITH_CSHARPAPI=ON -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy_CUDA11_2\" -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\" -DTRT_DIRECTORY=\"E:\\work\\tools\\TensorRT-8.5.2.2.Windows10.x86_64.cuda-11.8.cudnn8.6\\TensorRT-8.5.2.2\"编译好了C#API\r\n- 通过cmake .. -G \"Visual Studio 16 2019\" -A x64 -DFASTDEPLOY_INSTALL_DIR=E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4 -DCUDA_DIRECTORY=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2\"编译infer.cs,生成了infer_demo.sln,\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060， CUDA 11.2 CUDNN 8.1\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\ncmakelist.txt 修改为\r\n\r\nPROJECT(infer_demo CSharp)\r\nCMAKE_MINIMUM_REQUIRED (VERSION 3.10)\r\n# Set the C# language version (defaults to 3.0 if not set).\r\nset(CMAKE_CSharp_FLAGS \"/langversion:10\")\r\nset(CMAKE_DOTNET_TARGET_FRAMEWORK \"net6.0\")\r\nset(CMAKE_DOTNET_SDK \"Microsoft.NET.Sdk\")\r\n# 指定下载解压后的fastdeploy库路径\r\n#option(FASTDEPLOY_INSTALL_DIR \"Path of downloaded fastdeploy sdk.\")\r\noption(FASTDEPLOY_INSTALL_DIR \"E:\\\\work\\\\tools\\\\fastdeploy-win-x64-gpu-1.0.4\\\\\")\r\ninclude(${FASTDEPLOY_INSTALL_DIR}/FastDeployCSharp.cmake)\r\nadd_executable(infer_demo ${PROJECT_SOURCE_DIR}/infer.cs)\r\nset_property(TARGET infer_demo PROPERTY VS_DOTNET_REFERENCES ${FASTDEPLOY_DOTNET_REFERENCES})\r\nset_property(TARGET infer_demo PROPERTY VS_PACKAGE_REFERENCES ${FASTDEPLOY_PACKAGE_REFERENCES})\r\n\r\n我是VS2019环境，net6.0，不支持，改net5.0也不行\r\n\r\n报错内容：\r\nE:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112>nuget restore\r\nMSBuild 自动检测: 使用来自 \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\MSBuild\\Current\\Bin\" 的 msbuild 版本 \"16.11.2.50704\"。\r\npackages.config 中列出的所有包均已安装。\r\nRestoring packages for E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112\\infer_demo.csproj...\r\nNU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0). Package OpenCvSharp4 4.7.0.20230115 supports:\r\n  - net48 (.NETFramework,Version=v4.8)\r\n  - net6.0 (.NETCoreApp,Version=v6.0)\r\n  - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n  - netstandard2.0 (.NETStandard,Version=v2.0)\r\n  - netstandard2.1 (.NETStandard,Version=v2.1)\r\nNU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n  - net48 (.NETFramework,Version=v4.8)\r\n  - net6.0 (.NETCoreApp,Version=v6.0)\r\n  - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n  - netstandard2.0 (.NETStandard,Version=v2.0)\r\n  - netstandard2.1 (.NETStandard,Version=v2.1)\r\nNU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win-x64. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n  - net48 (.NETFramework,Version=v4.8)\r\n  - net6.0 (.NETCoreApp,Version=v6.0)\r\n  - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n  - netstandard2.0 (.NETStandard,Version=v2.0)\r\n  - netstandard2.1 (.NETStandard,Version=v2.1)\r\nNU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win-x86. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n  - net48 (.NETFramework,Version=v4.8)\r\n  - net6.0 (.NETCoreApp,Version=v6.0)\r\n  - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n  - netstandard2.0 (.NETStandard,Version=v2.0)\r\n  - netstandard2.1 (.NETStandard,Version=v2.1)\r\nGenerating MSBuild file E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112\\obj\\infer_demo.csproj.nuget.g.props.\r\nWriting assets file to disk. Path: E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112\\obj\\project.assets.json\r\nFailed to restore E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112\\infer_demo.csproj (in 311 ms).\r\n\r\nErrors in E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp\\buildcuda112\\infer_demo.csproj\r\n    NU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0). Package OpenCvSharp4 4.7.0.20230115 supports:\r\n      - net48 (.NETFramework,Version=v4.8)\r\n      - net6.0 (.NETCoreApp,Version=v6.0)\r\n      - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n      - netstandard2.0 (.NETStandard,Version=v2.0)\r\n      - netstandard2.1 (.NETStandard,Version=v2.1)\r\n    NU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n      - net48 (.NETFramework,Version=v4.8)\r\n      - net6.0 (.NETCoreApp,Version=v6.0)\r\n      - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n      - netstandard2.0 (.NETStandard,Version=v2.0)\r\n      - netstandard2.1 (.NETStandard,Version=v2.1)\r\n    NU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win-x64. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n      - net48 (.NETFramework,Version=v4.8)\r\n      - net6.0 (.NETCoreApp,Version=v6.0)\r\n      - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n      - netstandard2.0 (.NETStandard,Version=v2.0)\r\n      - netstandard2.1 (.NETStandard,Version=v2.1)\r\n    NU1202: Package OpenCvSharp4 4.7.0.20230115 is not compatible with net40 (.NETFramework,Version=v4.0) / win-x86. Package OpenCvSharp4 4.7.0.20230115 supports:\r\n      - net48 (.NETFramework,Version=v4.8)\r\n      - net6.0 (.NETCoreApp,Version=v6.0)\r\n      - netcoreapp3.1 (.NETCoreApp,Version=v3.1)\r\n      - netstandard2.0 (.NETStandard,Version=v2.0)\r\n      - netstandard2.1 (.NETStandard,Version=v2.1)\r\n\r\nNuGet Config files used:\r\n    C:\\Users\\Administrator\\AppData\\Roaming\\NuGet\\NuGet.Config\r\n    C:\\Program Files (x86)\\NuGet\\Config\\Microsoft.VisualStudio.Offline.config\r\n\r\nFeeds used:\r\n    C:\\Program Files (x86)\\Microsoft SDKs\\NuGetPackages\\\r\n    https://api.nuget.org/v3/index.json",
        "state": "open",
        "user": "zhinangubei",
        "closed_by": null,
        "created_at": "2023-04-20T03:10:27+00:00",
        "updated_at": "2023-04-21T02:38:24+00:00",
        "closed_at": null,
        "comments_count": [
            "zhinangubei",
            "zhinangubei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1851,
        "title": "FastDeploy 服务转换http",
        "body": "您好 请问下 通过FastDeploy部署的服务 如何通过http去请求呢 有相关的文档嘛 大佬 找了一圈没找到",
        "state": "closed",
        "user": "TherChenYang",
        "closed_by": "TherChenYang",
        "created_at": "2023-04-21T07:05:21+00:00",
        "updated_at": "2023-04-21T09:17:25+00:00",
        "closed_at": "2023-04-21T09:17:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1849,
        "title": "RuntimeError: FastDeploy initalized failed!,ImportError: DLL load failed: 找不到指定的模块。",
        "body": "python-3.7.9-amd64.exe\r\nwin11  \r\n\r\n`pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html`\r\n\r\n```\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\r\nTraceback (most recent call last):\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed: 找不到指定的模块。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\__init__.py\", line 49, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"D:\\Python37\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(\"FastDeploy initalized failed!\")\r\nRuntimeError: FastDeploy initalized failed!\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-04-20T08:38:45+00:00",
        "updated_at": "2023-04-27T04:09:54+00:00",
        "closed_at": "2023-04-20T09:12:03+00:00",
        "comments_count": [
            "monkeycc",
            "LateLinux"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1850,
        "title": "segment直接输出是列表，转成矩阵花费时间太多了",
        "body": "使用fd部署后输出是列表，为了对应切割位置使用numpy.reshape转换，速度太慢了，一张3000x2000的图，转换耗时70ms，模型才10ms",
        "state": "closed",
        "user": "kkpssr",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-20T09:26:54+00:00",
        "updated_at": "2024-04-30T06:42:48+00:00",
        "closed_at": "2024-04-30T06:42:48+00:00",
        "comments_count": [
            "kkpssr",
            "Dandelion111"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1852,
        "title": "RK3568交叉编译过程中设置ENABLE_ORT_BACKEND=ON后编译报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-release1.0.6\r\n- 【系统平台】: Linux firefly 4.19.232 / Debian 10\r\n- 【硬件】： firefly AIO-3568J\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n按照[交叉编译文档](https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.6/docs/cn/faq/rknpu2/build.md)进行Fasteploy C++ SDK交叉编译，设置ENABLE_ORT_BACKEND=ON后，cmake成功，执行make -j8，执行到100%后不成功，报下面错误：\r\n`[ 99%] Building CXX object CMakeFiles/fastdeploy.dir/third_party/optimizer/onnxoptimizer/pass_registry.cc.o\r\n[100%] Linking CXX shared library libfastdeploy.so\r\nthird_libs/install/onnxruntime/lib/libonnxruntime.so: error adding symbols: File in wrong format\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:4466: libfastdeploy.so.1.0.6] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:201: CMakeFiles/fastdeploy.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\n`\r\n- 【运行日志】\r\n[交叉编译.txt](https://github.com/PaddlePaddle/FastDeploy/files/11293251/default.txt)\r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-21T07:24:26+00:00",
        "updated_at": "2024-12-17T06:42:04+00:00",
        "closed_at": "2024-12-17T06:42:04+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "MrMzl",
            "danny-zhu"
        ],
        "labels": [
            "Bug",
            "RK356X"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1857,
        "title": "使用C++进行端到端多线程推理时会出现偶发性错误：Mats shapes are not consistent.",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 预编译版：fastdeploy-win-x64-gpu-1.0.5\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 3070， CUDA 11.2.2，CUDNN 8.1.1.33\r\n- 【软件】：vs2019\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n```txt\r\n可执行文件ocr_infer_end2end.exe参数说明：\r\nargv[1] - 检测模型目录\r\nargv[2] - 识别模型目录\r\nargv[3] - 识别模型字典\r\nargv[4] - 测试图片目录\r\nargv[5] - 图片batchsize  # 调用BatchPredict时输入的图片数\r\nargv[6] - 重复次数 # 使用多线程预测所重复的次数，目的是为了测性能，取单图预测时间的平均值\r\nargv[7] - backend    # 此处使用的是paddleinference推理库，开启GPU\r\nargv[8] - thread_num # 开启的线程数\r\n```\r\n- - 报错信息\r\n```bash\r\nD:\\OCR\\build\\Release>ocr_infer_end2end.exe D:/OCR/model/det D:/OCR/model/rec D:/OCR/model/rec/en_dict.txt D:/OCR/model/imgs/ 2 1000 1 8\r\n --- load images file done ---\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::fastdeploy::RuntimeOption::SetTrtInputShape        `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::fastdeploy::RuntimeOption::SetTrtInputShape        `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0423 10:37:05.709136 11184 executor.cc:186] Old Executor is Running.\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [map_op_to_another_pass]e[0m\r\nI0423 10:37:05.730129 11184 fuse_pass_base.cc:59] ---  detected 15 subgraphs\r\ne[32m--- Running IR pass [identity_scale_op_clean_pass]e[0m\r\ne[32m--- Running IR pass [is_test_pass]e[0m\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [delete_quant_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_weight_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [constant_folding_pass]e[0m\r\ne[32m--- Running IR pass [silu_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_bn_fuse_pass]e[0m\r\nI0423 10:37:05.942971 11184 fuse_pass_base.cc:59] ---  detected 48 subgraphs\r\ne[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]e[0m\r\ne[32m--- Running IR pass [vit_attention_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fuse_multi_transformer_layer_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]e[0m\r\ne[32m--- Running IR pass [matmul_scale_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v3]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m\r\nI0423 10:37:08.334635 11184 fuse_pass_base.cc:59] ---  detected 43 subgraphs\r\ne[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0m\r\ne[32m--- Running IR pass [auto_mixed_precision_pass]e[0m\r\ne[32m--- Running IR pass [inplace_op_var_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\nI0423 10:37:08.344597 11184 ir_params_sync_among_devices_pass.cc:94] Sync params from CPU to GPU\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0423 10:37:08.369583 11184 memory_optimize_pass.cc:220] Cluster name : hardswish_17.tmp_0  size: 1920\r\nI0423 10:37:08.369583 11184 memory_optimize_pass.cc:220] Cluster name : tmp_7  size: 384\r\nI0423 10:37:08.370584 11184 memory_optimize_pass.cc:220] Cluster name : batch_norm_46.tmp_3  size: 1920\r\nI0423 10:37:08.371582 11184 memory_optimize_pass.cc:220] Cluster name : tmp_10  size: 384\r\nI0423 10:37:08.372582 11184 memory_optimize_pass.cc:220] Cluster name : tmp_1  size: 384\r\nI0423 10:37:08.373581 11184 memory_optimize_pass.cc:220] Cluster name : tmp_9  size: 384\r\nI0423 10:37:08.373581 11184 memory_optimize_pass.cc:220] Cluster name : elementwise_add_3  size: 96\r\nI0423 10:37:08.374614 11184 memory_optimize_pass.cc:220] Cluster name : elementwise_add_1  size: 64\r\nI0423 10:37:08.374614 11184 memory_optimize_pass.cc:220] Cluster name : x  size: 12\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0423 10:37:08.420553 11184 analysis_predictor.cc:1404] ======= optimize end =======\r\nI0423 10:37:08.420553 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:08.424551 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::GPU.\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [map_op_to_another_pass]e[0m\r\nI0423 10:37:08.468526 11184 fuse_pass_base.cc:59] ---  detected 14 subgraphs\r\ne[32m--- Running IR pass [identity_scale_op_clean_pass]e[0m\r\ne[32m--- Running IR pass [is_test_pass]e[0m\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [delete_quant_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_weight_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [constant_folding_pass]e[0m\r\ne[32m--- Running IR pass [silu_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_bn_fuse_pass]e[0m\r\nI0423 10:37:08.594483 11184 fuse_pass_base.cc:59] ---  detected 32 subgraphs\r\ne[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]e[0m\r\ne[32m--- Running IR pass [vit_attention_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fuse_multi_transformer_layer_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]e[0m\r\nI0423 10:37:10.817183 11184 fuse_pass_base.cc:59] ---  detected 9 subgraphs\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]e[0m\r\nI0423 10:37:10.820181 11184 fuse_pass_base.cc:59] ---  detected 4 subgraphs\r\ne[32m--- Running IR pass [matmul_scale_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v3]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\nI0423 10:37:10.901134 11184 fuse_pass_base.cc:59] ---  detected 9 subgraphs\r\ne[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]e[0m\r\nI0423 10:37:10.917125 11184 fuse_pass_base.cc:59] ---  detected 1 subgraphs\r\ne[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m\r\nI0423 10:37:10.982112 11184 fuse_pass_base.cc:59] ---  detected 34 subgraphs\r\ne[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0m\r\ne[32m--- Running IR pass [auto_mixed_precision_pass]e[0m\r\ne[32m--- Running IR pass [inplace_op_var_pass]e[0m\r\nI0423 10:37:11.008074 11184 fuse_pass_base.cc:59] ---  detected 5 subgraphs\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\nI0423 10:37:11.010123 11184 ir_params_sync_among_devices_pass.cc:94] Sync params from CPU to GPU\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0423 10:37:11.036058 11184 memory_optimize_pass.cc:220] Cluster name : batch_norm_14.tmp_3  size: 6144\r\nI0423 10:37:11.037086 11184 memory_optimize_pass.cc:220] Cluster name : transpose_0.tmp_0  size: 480\r\nI0423 10:37:11.038058 11184 memory_optimize_pass.cc:220] Cluster name : x  size: 576\r\nI0423 10:37:11.039085 11184 memory_optimize_pass.cc:220] Cluster name : batch_norm_15.tmp_4  size: 6144\r\nI0423 10:37:11.039085 11184 memory_optimize_pass.cc:220] Cluster name : tmp_0  size: 480\r\nI0423 10:37:11.040056 11184 memory_optimize_pass.cc:220] Cluster name : batch_norm_17.tmp_4  size: 6144\r\nI0423 10:37:11.041085 11184 memory_optimize_pass.cc:220] Cluster name : tmp_2  size: 480\r\nI0423 10:37:11.041085 11184 memory_optimize_pass.cc:220] Cluster name : matmul_v2_0.tmp_0  size: 32\r\nI0423 10:37:11.042055 11184 memory_optimize_pass.cc:220] Cluster name : shape_1.tmp_0_slice_1  size: 4\r\nI0423 10:37:11.043054 11184 memory_optimize_pass.cc:220] Cluster name : shape_0.tmp_0_slice_1  size: 4\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0423 10:37:11.076035 11184 analysis_predictor.cc:1404] ======= optimize end =======\r\nI0423 10:37:11.077035 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.080062 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::GPU.\r\nmodel initialized done\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.084030 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.086053 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.088033 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.090057 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.092041 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.096055 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.098023 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.100021 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.103058 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.106048 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.108031 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.110045 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.112058 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.114042 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.117012 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.119035 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.121042 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.123034 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.125018 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.129006 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.131006 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.134027 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.137001 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.139024 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.140998 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.144021 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.145995 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.148018 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.150993 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.153017 11184 naive_executor.cc:151] ---  skip [sigmoid_0.tmp_0], fetch -> fetch\r\n[INFO] fastdeploy/runtime/runtime.cc(364)::fastdeploy::Runtime::Clone   Runtime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\nI0423 10:37:11.155010 11184 naive_executor.cc:151] ---  skip [feed], feed -> x\r\nI0423 10:37:11.158012 11184 naive_executor.cc:151] ---  skip [softmax_2.tmp_0], fetch -> fetch\r\nassignThreadImg done\r\nW0423 10:37:11.203963 11184 gpu_resources.cc:85] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.4, Runtime API Version: 11.2\r\nW0423 10:37:11.211957 11184 gpu_resources.cc:115] device: 0, cuDNN Version: 8.1.\r\nW0423 10:37:11.218953 11184 gpu_resources.cc:241] WARNING: device:  . The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\r\nwarmup 1 times done\r\nwarmup 2 times done\r\nwarmup 3 times done\r\nwarmup 4 times done\r\nwarmup 5 times done\r\nwarmup 6 times done\r\nwarmup 7 times done\r\nwarmup 8 times done\r\nwarmup 9 times done\r\nwarmup 10 times done\r\n  ***** infer time per image = 24.1037ms *****\r\n  ***** repeat 1 times done *****\r\n  ***** infer time per image = 23.5881ms *****\r\n  ***** repeat 2 times done *****\r\n  ***** infer time per image = 23.9566ms *****\r\n  ***** repeat 3 times done *****\r\n  ***** infer time per image = 23.2061ms *****\r\n  ***** repeat 4 times done *****\r\n[ERROR] fastdeploy/vision/common/processors/mat_batch.cc(33)::fastdeploy::vision::FDMatBatch::Tensor    Mats shapes are not consistent.\r\n```\r\n\r\n- - 源码\r\n```C++\r\n// Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\r\n//\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n//     http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\n#include <thread>\r\n#include \"fastdeploy/vision.h\"\r\n#include <direct.h>\r\n#include <sys/stat.h>\r\n#include <iostream>\r\n#include <fstream>\r\n#include <io.h>\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\n\r\nvoid loadImgFiles(const std::string& imgPath, const std::string& imgFormat, std::vector<std::string>& imgs)\r\n{\r\n  intptr_t hFile = 0;\r\n  struct _finddata_t fileInfo;\r\n  std::string str, fileFormatName;\r\n  if (0 != strcmp(imgFormat.c_str(), \"\"))\r\n  {\r\n    fileFormatName = \"\\\\*.\" + imgFormat;\r\n  }\r\n  else\r\n  {\r\n    fileFormatName = \"\\\\*\";\r\n  }\r\n  if ((hFile = _findfirst(str.assign(imgPath).append(fileFormatName).c_str(), &fileInfo)) != -1)\r\n  {\r\n    do\r\n    {\r\n      imgs.push_back(str.assign(imgPath).append(\"\\\\\").append(fileInfo.name));\r\n    } while (_findnext(hFile, &fileInfo) == 0);\r\n    _findclose(hFile);\r\n  }\r\n}\r\n\r\n\r\nvoid assignThreadImg(std::vector<std::vector<cv::Mat>>* thread_images, std::vector<cv::Mat> all_images, int thread_num)\r\n{\r\n  // number of image files in images folder\r\n  size_t count = all_images.size();\r\n  size_t num = count / thread_num;\r\n  for (int i = 0; i < thread_num; i++)\r\n  {\r\n    std::vector<cv::Mat> temp_list;\r\n    if (i == thread_num - 1) {\r\n      for (size_t j = i * num; j < count; j++)\r\n      {\r\n        temp_list.push_back(all_images[j]);\r\n      }\r\n    }\r\n    else\r\n    {\r\n      for (size_t j = 0; j < num; j++)\r\n      {\r\n        temp_list.push_back(all_images[i * num + j]);\r\n      }\r\n    }\r\n    thread_images->push_back(temp_list);\r\n  }\r\n}\r\n\r\n\r\nvoid assignBatchImg(const std::vector<cv::Mat>& imgPerGpu, std::vector<std::vector<cv::Mat>>& imgByBatch, int max_batch_size)\r\n{\r\n  imgByBatch.clear();\r\n  int imgNum = imgPerGpu.size();\r\n  const int maxBatchTimes = imgNum / max_batch_size;\r\n  if (maxBatchTimes == 0)//如果待处理图片总数不足一个batch，则一次性处理所有图片\r\n  {\r\n    std::vector<cv::Mat> images;\r\n    for (int i = 0; i < imgNum; i++)\r\n    {\r\n      images.push_back(imgPerGpu[i]);\r\n    }\r\n    if (!images.empty()) {\r\n      imgByBatch.push_back(images);\r\n    }\r\n  }\r\n  else\r\n  {\r\n    const int rest = imgNum - maxBatchTimes * max_batch_size;\r\n    for (int i = 0; i < maxBatchTimes; i++)\r\n    {\r\n      std::vector<cv::Mat> images;\r\n      for (int j = 0; j < max_batch_size; j++)\r\n      {\r\n        images.push_back(imgPerGpu[i * max_batch_size + j]);\r\n      }\r\n      if (!images.empty()) {\r\n        imgByBatch.push_back(images);\r\n      }\r\n    }\r\n    if (rest != 0)\r\n    {\r\n      std::vector<cv::Mat> images;\r\n      for (int i = maxBatchTimes * max_batch_size; i < maxBatchTimes * max_batch_size + rest; i++)\r\n      {\r\n        images.push_back(imgPerGpu[i]);\r\n      }\r\n      if (!images.empty()) {\r\n        imgByBatch.push_back(images);\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ndouble getTime() {\r\n  return (double)cv::getTickCount();\r\n}\r\ndouble getTimeDiff(double t2, double t1) {\r\n  double diff = t2 - t1;\r\n  return diff * 1000.0 / cv::getTickFrequency(); // ms\r\n}\r\n\r\n\r\nvoid Predict(fastdeploy::pipeline::PPOCRv3* model, int batchsize, std::vector<cv::Mat>& images) {\r\n  std::vector<std::vector<cv::Mat>> m_batchImg;\r\n  assignBatchImg(images, m_batchImg, batchsize);\r\n\r\n  for (auto const& image_file : m_batchImg) {\r\n    std::vector<fastdeploy::vision::OCRResult> res;\r\n    if (!model->BatchPredict(image_file, &res)) {\r\n      std::cerr << \"Failed to predict.\" << std::endl;\r\n      return;\r\n    }\r\n    /*for (auto &r : res)\r\n    {\r\n      std::cout << r.Str() << std::endl;\r\n    }*/\r\n  }\r\n  \r\n}\r\n\r\n\r\nvoid InitAndInfer(const std::string& det_model_dir, const std::string& rec_model_dir, \r\n                  const std::string& rec_label_file, std::vector<cv::Mat> imgs, const fastdeploy::RuntimeOption& option, \r\n                  int thread_num, int batchsize, int repeatTimes) {\r\n  auto det_model_file = det_model_dir + sep + \"inference.pdmodel\";\r\n  auto det_params_file = det_model_dir + sep + \"inference.pdiparams\";\r\n\r\n  auto rec_model_file = rec_model_dir + sep + \"inference.pdmodel\";\r\n  auto rec_params_file = rec_model_dir + sep + \"inference.pdiparams\";\r\n\r\n  auto det_option = option;\r\n  auto rec_option = option;\r\n\r\n  // The cls and rec model can inference a batch of images now.\r\n  // User could initialize the inference batch size and set them after create PP-OCR model.\r\n  int rec_batch_size = 6;\r\n\r\n  // If use TRT backend, the dynamic shape will be set as follow.\r\n  // We recommend that users set the length and height of the detection model to a multiple of 32.\r\n  // We also recommend that users set the Trt input shape as follow.\r\n  det_option.SetTrtInputShape(\"x\", { 1, 3, 64,64 }, { 1, 3, 640, 640 },\r\n    { 1, 3, 960, 960 });\r\n  rec_option.SetTrtInputShape(\"x\", { 1, 3, 48, 10 }, { rec_batch_size, 3, 48, 320 },\r\n    { rec_batch_size, 3, 48, 2304 });\r\n\r\n  // Users could save TRT cache file to disk as follow. \r\n  // det_option.SetTrtCacheFile(det_model_dir + sep + \"det_trt_cache.trt\");\r\n  // cls_option.SetTrtCacheFile(cls_model_dir + sep + \"cls_trt_cache.trt\");\r\n  // rec_option.SetTrtCacheFile(rec_model_dir + sep + \"rec_trt_cache.trt\");\r\n\r\n  auto det_model = fastdeploy::vision::ocr::DBDetector(det_model_file, det_params_file, det_option);\r\n  auto rec_model = fastdeploy::vision::ocr::Recognizer(rec_model_file, rec_params_file, rec_label_file, rec_option);\r\n\r\n  assert(det_model.Initialized());\r\n  assert(rec_model.Initialized());\r\n\r\n  // The classification model is optional, so the PP-OCR can also be connected in series as follows\r\n  // auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &rec_model);\r\n  auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &rec_model);\r\n\r\n  // Set inference batch size for cls model and rec model, the value could be -1 and 1 to positive infinity.\r\n  // When inference batch size is set to -1, it means that the inference batch size \r\n  // of the cls and rec models will be the same as the number of boxes detected by the det model. \r\n  ppocr_v3.SetRecBatchSize(rec_batch_size);\r\n\r\n  if (!ppocr_v3.Initialized()) {\r\n    std::cerr << \"Failed to initialize PP-OCR.\" << std::endl;\r\n    return;\r\n  }\r\n  std::cout << \"model initialized done\" << std::endl;\r\n\r\n  std::vector<decltype(ppocr_v3.Clone())> models;\r\n  for (int i = 0; i < thread_num; ++i) {\r\n    models.emplace_back(std::move(ppocr_v3.Clone()));\r\n  }\r\n\r\n  std::vector<std::vector<cv::Mat>> thread_images;\r\n  assignThreadImg(&thread_images, imgs, thread_num);\r\n  std::cout << \"assignThreadImg done\" << std::endl;\r\n\r\n  // warmup model\r\n  std::vector<std::vector<cv::Mat>> warmup_imgs;\r\n  std::vector<fastdeploy::vision::OCRResult> res;\r\n  auto warmupTimes = 10;\r\n  assignBatchImg(imgs, warmup_imgs, batchsize);\r\n  for (auto i = 0; i < warmupTimes; i++) {\r\n    for (auto modelIndex = 0; modelIndex < thread_num; modelIndex++)\r\n    {\r\n      for (auto& images : warmup_imgs) {\r\n        if (!models[modelIndex].get()->BatchPredict(images, &res)) {\r\n          std::cerr << \"Failed to predict.\" << std::endl;\r\n        }\r\n      }\r\n    }\r\n    std::cout << \"warmup \" << i + 1 << \" times done\" << std::endl;\r\n  }\r\n\r\n  for (int i = 0; i < repeatTimes; i++)\r\n  {\r\n    auto startTtime = getTime();\r\n    std::vector<std::thread> threads;\r\n\r\n    for (int i = 0; i < thread_num; ++i) {\r\n      threads.emplace_back(Predict, models[i].get(), batchsize, thread_images[i]);\r\n    }\r\n\r\n    for (int i = 0; i < thread_num; ++i) {\r\n      threads[i].join();\r\n    }\r\n    auto endTime = getTime();\r\n    auto inferTime = getTimeDiff(endTime, startTtime);\r\n    std::cout << \"  ***** infer time per image = \" << inferTime / imgs.size() << \"ms *****  \" << std::endl;\r\n    std::cout << \"  ***** repeat \" << i + 1 << \" times done *****  \" << std::endl;\r\n  }\r\n\r\n  \r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  fastdeploy::RuntimeOption option;\r\n  int flag = std::atoi(argv[7]);\r\n\r\n  if (flag == 0) {\r\n    option.UseCpu();\r\n  }\r\n  else if (flag == 1) {\r\n    // use paddleinference\r\n    option.UseGpu();\r\n    option.paddle_infer_option.enable_log_info = true;\r\n    option.UsePaddleBackend();\r\n\r\n  }\r\n  else if (flag == 2) {\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n  }\r\n  else if (flag == 3) {\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n    option.EnablePaddleTrtCollectShape();\r\n    option.EnablePaddleToTrt();\r\n  }\r\n  else if (flag == 4) {\r\n    option.UseKunlunXin();\r\n  }\r\n\r\n  std::string det_model_dir = argv[1];\r\n  std::string rec_model_dir = argv[2];\r\n  std::string rec_label_file = argv[3];\r\n  std::string image_file_path = argv[4];\r\n  int batchsize = std::atoi(argv[5]);\r\n  int repeatTimes = std::atoi(argv[6]);\r\n  int thread_num = std::atoi(argv[8]);\r\n\r\n  std::vector<std::string> flist;\r\n  loadImgFiles(image_file_path, \"jpg\", flist);\r\n  std::cout << \" --- load images file done --- \" << std::endl;\r\n\r\n  std::vector<cv::Mat> imgs;\r\n  for (auto i = 0; i < flist.size(); ++i)\r\n  {\r\n    auto img = cv::imread(flist[i]);\r\n    imgs.push_back(std::move(img));\r\n  }\r\n\r\n  InitAndInfer(det_model_dir, rec_model_dir, rec_label_file, imgs, option, thread_num, batchsize, repeatTimes);\r\n  return 0;\r\n}\r\n```\r\n\r\n- 【多线程推理出现偶发性错误】\r\n- - 在 [multi_thread_ocr.cc](https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.5/tutorials/multi_thread/cpp/pipeline/multi_thread_ocr.cc)多线程预测代码的基础上进行修改，当重复执行多线程预测时有时会出现报错`[ERROR] fastdeploy/vision/common/processors/mat_batch.cc(33)::fastdeploy::vision::FDMatBatch::Tensor    Mats shapes are not consistent.`导致程序停止，该问题出现概率较高。源代码可直接编译运行，输入图像分辨均为2048*2048。请教为何会出现这类问题，以及如何解决？\r\n",
        "state": "open",
        "user": "tsing-luo",
        "closed_by": null,
        "created_at": "2023-04-23T03:13:48+00:00",
        "updated_at": "2023-04-26T02:56:27+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1853,
        "title": "编译paddleseg infer.cs,没推理结果",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.4\r\n- 【编译命令】自行编译C# API\r\n- cmake .. -G \"Visual Studio 16 2019\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_VISION=ON -DWITH_GPU=ON -DENABLE_TEXT=ON -DWITH_CAPI=ON -DWITH_CSHARPAPI=ON -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy_CUDA_csharp\" -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\" -DTRT_DIRECTORY=\"E:\\work\\tools\\TensorRT-8.5.2.2.Windows10.x86_64.cuda-11.8.cudnn8.6\\TensorRT-8.5.2.2\"\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060， CUDA 11.2 CUDNN 8.4\r\n- 【编译语言】： C#\r\n\r\n修改了OpenCvSharp版本4.7.0.20230115为4.4.2.0.20200108，以对应net4.0版本，编译成功C# API\r\n\r\n编译E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp中的infer.cs  使用以下方法：\r\n\r\n1、vs2019 创建控制台应用.net framework新项目D:\\paddleseg_csharp\\paddleseg_demo_gpu，选择net4.0版本\r\n将E:\\work\\tools\\fastdeploy-win-x64-gpu-1.0.4\\examples\\vision\\segmentation\\paddleseg\\cpu-gpu\\csharp中的infer.cs代码拷贝到D:\\paddleseg_csharp\\paddleseg_demo_gpu  infer.cs\r\n2、下载opencvsharp4.2.0.20200108版本，和编译fastdeploy_csharp API一致，添加引用，引用编译好的fastdeploy_csharp\r\n3、设置配置管理器：release x64,生成paddle_demo.exe.\r\n4、将所有dll拷贝到可执行程序同目录，fastdeploy_init.bat install %cd% D:\\paddleseg_csharp\\paddleseg_demo_gpu\\bin\\x64\\Release\r\n5、运行paddleseg_demo_gpu PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer cityscapes_demo.png 0\r\n\r\n执行没报错，但没显示推理结果；改代码为Cv2.ImWrite(\"result.jpg\", res_img);也未保存结果\r\n\r\n![be6d01a40e7968de0e7f051e882090e](https://user-images.githubusercontent.com/112677205/233589835-75aca0df-badf-4d50-8a95-74ffe40ce7f1.png)\r\n\r\n\r\n",
        "state": "open",
        "user": "zhinangubei",
        "closed_by": null,
        "created_at": "2023-04-21T08:44:03+00:00",
        "updated_at": "2023-07-28T02:16:45+00:00",
        "closed_at": null,
        "comments_count": [
            "zyz207",
            "zhinangubei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1856,
        "title": "libprotobuf.lib 无法解析的外部符号",
        "body": "windows10 + vs2017编译仓库最新代码。\r\n在新建的build文件夹下执行\r\ncmake -A x64 .. -DTRT_DIRECTORY=\"E:/TensorRT-8.5.2.2\"  \\ -DCUDA_DIRECTORY=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.6/\" -DENABLE_VISION=ON \\ -DOPENCV_DIRECTORY=\"E:/opencv/x64/vc15/lib\" \\ -DENABLE_TRT_BACKEND=ON \\ -DWITH_GPU=ON\r\n编译不通过:\r\nlibprotobuf.lib(any.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(map_field.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(extension_set_heavy.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(text_format.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(dynamic_message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(generated_message_reflection.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(wire_format.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(implicit_weak_message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(descriptor.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(message_lite.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(arenastring.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(reflection_ops.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_begin_initialize\r\n5>libprotobuf.lib(any.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(map_field.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(extension_set_heavy.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(text_format.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(dynamic_message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(generated_message_reflection.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(wire_format.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(implicit_weak_message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(descriptor.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(message_lite.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(arenastring.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(reflection_ops.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(message.obj) : error LNK2001: 无法解析的外部符号 __imp___std_init_once_complete\r\n5>libprotobuf.lib(dynamic_message.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(substitute.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(any.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(text_format.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(descriptor.pb.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(tokenizer.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(descriptor_database.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(extension_set_heavy.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(status.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(int128.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(io_win32.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(extension_set.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(descriptor.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(map_field.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(stringprintf.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(common.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(strutil.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(zero_copy_stream_impl.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(implicit_weak_message.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(unknown_field_set.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(generated_message_reflection.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(wire_format.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(zero_copy_stream_impl_lite.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(reflection_ops.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(message.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(coded_stream.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(arena.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(message_lite.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(arenastring.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(repeated_field.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(generated_message_util.obj) : error LNK2001: 无法解析的外部符号 __CxxFrameHandler4\r\n5>libprotobuf.lib(dynamic_message.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(substitute.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(any_lite.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(any.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(io_win32.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(text_format.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(tokenizer.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(descriptor_database.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(stringprintf.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(extension_set_heavy.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(status.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(int128.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(zero_copy_stream_impl.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(extension_set.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(descriptor.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(map_field.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(zero_copy_stream_impl_lite.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(common.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(strutil.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(zero_copy_stream.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(parse_context.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(unknown_field_set.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(generated_message_reflection.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(wire_format.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(message.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(coded_stream.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(arena.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(wire_format_lite.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(message_lite.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(repeated_field.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(generated_message_util.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(reflection_ops.obj) : error LNK2001: 无法解析的外部符号 __GSHandlerCheck_EH4\r\n5>libprotobuf.lib(arena.obj) : error LNK2019: 无法解析的外部符号 __dyn_tls_on_demand_init，该符号在函数 \"public: void __cdecl google::protobuf::internal::ThreadSafeArena::AddCleanup(void *,void (__cdecl*)(void *))\" (?AddCleanup@ThreadSafeArena@internal@protobuf@google@@QEAAXPEAXP6AX0@Z@Z) 中被引用\r\n5>libprotobuf.lib(arena.obj) : error LNK2019: 无法解析的外部符号 __tls_guard，该符号在函数 \"public: void __cdecl google::protobuf::internal::ThreadSafeArena::AddCleanup(void *,void (__cdecl*)(void *))\" (?AddCleanup@ThreadSafeArena@internal@protobuf@google@@QEAAXPEAXP6AX0@Z@Z) 中被引用\r\n",
        "state": "open",
        "user": "597871382",
        "closed_by": null,
        "created_at": "2023-04-22T09:49:57+00:00",
        "updated_at": "2023-04-27T12:53:26+00:00",
        "closed_at": null,
        "comments_count": [
            "597871382",
            "superprogrammai",
            "hhxdestiny",
            "DefTruth",
            "597871382"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1854,
        "title": "CPU版本UIE示例无法跑通",
        "body": "- 【模型跑不通】\r\nUIE模型的示例代码跑不通，安装的是cpu版本\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/text/uie/serving/README_CN.md\r\n镜像版本：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.3-cpu-only-21.10\r\n机器：centos7.9\r\n看到之前的Issues说镜像中的FastDeploy的python版本有问题 已重新安装\r\npython3 -m pip install --upgrade --force-reinstall fastdeploy-python\r\n\r\n执行fastdeployserver --model-repository=/uie_serving/models --backend-config=python,shm-default-byte-size=10485760\r\n有如下错误\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/124348939/233597670-5e717988-4269-49d7-8b46-a82cb610aee7.png)\r\n\r\n",
        "state": "closed",
        "user": "TherChenYang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-21T09:16:57+00:00",
        "updated_at": "2024-11-26T06:40:47+00:00",
        "closed_at": "2024-11-26T06:40:46+00:00",
        "comments_count": [
            "TherChenYang",
            "huangjun11",
            "TherChenYang",
            "huangjun11",
            "TherChenYang",
            "huangjun11"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1855,
        "title": "CMake Error: Found relative path while evaluating include directories",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.0\r\nOS Platform: Windows x64\r\nHardware: CPU noavx\r\nProgram Language: C++\r\nCMake: 3.18, 3.25\r\nVisual Studio: 2019\r\n\r\n## Problem description\r\nCan't generate Visual Studio project with CMAKE, while set ENABLE_PADDLE_BACKEND=ON.\r\nCMake Error in CMakeLists.txt:\r\n  Found relative path while evaluating include directories\r\n\r\n[cmake_log.txt](https://github.com/PaddlePaddle/FastDeploy/files/11298613/cmake_log.txt)\r\n",
        "state": "closed",
        "user": "unblock7",
        "closed_by": "DefTruth",
        "created_at": "2023-04-21T19:11:11+00:00",
        "updated_at": "2023-04-25T12:43:43+00:00",
        "closed_at": "2023-04-25T12:43:43+00:00",
        "comments_count": [
            "DreamMaker777",
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1861,
        "title": "FDinfer FDtracker GStreamer",
        "body": "FDinfer FDtracker GStreamer 是否可用？不使用deepstream sdk，能实现完整的多路视频流解析、跟踪和预测等流程吗？请问是否有demo，或如何实现？",
        "state": "open",
        "user": "wpfnlp",
        "closed_by": null,
        "created_at": "2023-04-24T01:37:16+00:00",
        "updated_at": "2023-04-24T01:37:16+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1858,
        "title": "FastDeploy RKNPU2 Memory Leak",
        "body": "## Environment\r\n\r\nFastDeploy version: latest code in develop branch\r\nOS Platform: Linux (Linux 5.10.110-rockchip-rk3588 #23.02.2 SMP Fri Feb 17 23:59:20 UTC 2023)\r\nHardware: e.g. Orange Pi 5 Rockchip RK3588S 8-core 64-bit processor\r\nProgram Language: e.g. Python 3.9\r\n\r\n## Problem description\r\nAfter running about 225 inferences I get the below errors\r\n\r\n```python\r\nE RKNN: [16:11:55.647] failed to allocate handle, ret: -1, errno: 14, errstr: Bad address, sleep one second and try again!\r\nE RKNN: [16:11:56.656] failed to allocate handle, ret: -1, errno: 14, errstr: Bad address\r\nE RKNN: [16:11:56.656] failed to malloc npu memory!, size: 7397955, flags: 0x2\r\nE RKNN: [16:11:56.656] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(180)::LoadModel    The function(rknn_init) failed! ret=-6\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(123)::Init Load model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(328)::CreateRKNPU2Backend Failed to initialize RKNPU2 backend.\r\nAborted\r\n````\r\n\r\n```python\r\ndef do_detect(img: Image, score_threshold: float = 0.3):\r\n\r\n    # Configure runtime, load model\r\n    runtime_option = fd.RuntimeOption()\r\n    runtime_option.use_rknpu2()\r\n\r\n    model = fd.vision.detection.RKYOLOV7(\r\n    model_file,\r\n    runtime_option=runtime_option,\r\n    model_format=fd.ModelFormat.RKNN)\r\n    \r\n    # Predicting Image Results\r\n    im = np.array(img)\r\n    start_inference_time = time.perf_counter()\r\n    result = model.predict(im, conf_threshold=score_threshold, nms_iou_threshold=0.5)\r\n    inferenceMs = int((time.perf_counter() - start_inference_time) * 1000)\r\n\r\n    \"\"\"\r\n    with open(\"log.txt\", \"a\") as text_file:\r\n        text_file.write(str(result) + \"\\n\")    \r\n    \"\"\"\r\n    result = str(result)\r\n    lines = result.strip().split(\"\\n\")\r\n\r\n    outputs = []\r\n\r\n    for line in lines[1:]:\r\n        # Split the line by comma to get a list of values\r\n        values = line.split(\",\")\r\n        values = [x.strip(' ') for x in values]\r\n        \r\n        \"\"\"\r\n        with open(\"values.txt\", \"a\") as text_file:\r\n            text_file.write(str(values) + \"\\n\")\r\n        \"\"\"\r\n\r\n        # Convert the values to appropriate data types\r\n        xmin = float(values[0])\r\n        ymin = float(values[1])\r\n        xmax = float(values[2])\r\n        ymax = float(values[3])\r\n        score = float(values[4])\r\n        label_id = int(values[5])\r\n\r\n        # if score >= score_threshold:\r\n        detection = {\r\n            \"confidence\": score,\r\n            \"label\": str(extract_label_from_file(label_id)),\r\n            \"x_min\": int(xmin),\r\n            \"y_min\": int(ymin),\r\n            \"x_max\": int(xmax),\r\n            \"y_max\": int(ymax),\r\n        }\r\n\r\n        outputs.append(detection)\r\n    \r\n    \"\"\"\r\n    with open(\"outputs.txt\", \"a\") as text_file:\r\n        text_file.write(str(outputs) + \"\\n\")\r\n    \"\"\"\r\n    \r\n    return {\r\n        \"success\"     : True,\r\n        \"count\"       : len(outputs),\r\n        \"predictions\" : outputs,\r\n        \"inferenceMs\" : inferenceMs\r\n    }\r\n```\r\n",
        "state": "open",
        "user": "MikeLud",
        "closed_by": null,
        "created_at": "2023-04-23T03:53:26+00:00",
        "updated_at": "2025-02-10T14:02:39+00:00",
        "closed_at": null,
        "comments_count": [
            "MikeLud",
            "MikeLud",
            "Zheng-Bicheng",
            "MikeLud",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "MikeLud",
            "MikeLud",
            "vvvillainy"
        ],
        "labels": [
            "rknpu2",
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1864,
        "title": "部署ppocrv2服务出错，调用不起来。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-cpu-1.0.6\r\n- 【系统平台】: Ubuntu 18.04\r\n- 【编译语言】： Python\r\n\r\n调用的是docker的CPU版本：docker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.6-cpu-only-21.10，换成官网下载的ppocrv2模型，运行 fastdeployserver –model-repository=/ocr_serving/models 出错。\r\nI0424 08:22:34.715656 2414 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nI0424 08:22:34.816187 2414 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0424 08:22:34.891727 2414 fastdeploy_runtime.cc:1207] TRITONBACKEND_Initialize: fastdeploy\r\nI0424 08:22:34.891743 2414 fastdeploy_runtime.cc:1216] Triton TRITONBACKEND API version: 1.6\r\nI0424 08:22:34.891746 2414 fastdeploy_runtime.cc:1221] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0424 08:22:34.891748 2414 fastdeploy_runtime.cc:1250] backend configuration:\r\n{}\r\nI0424 08:22:34.891778 2414 fastdeploy_runtime.cc:1280] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0424 08:22:34.892541 2414 fastdeploy_runtime.cc:1319] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (CPU device 0)\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\nI0424 08:22:34.916415 2414 model_repository_manager.cc:1022] loading: det_runtime:1\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0424 08:22:34.997248 2414 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\nI0424 08:22:34.998388 2414 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0424 08:22:35.016748 2414 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0424 08:22:35.116930 2414 model_repository_manager.cc:1022] loading: det_postprocess:1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nI0424 08:22:35.217217 2414 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0424 08:22:35.242377 2414 fastdeploy_runtime.cc:1280] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\nI0424 08:22:35.242415 2414 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0424 08:22:35.243649 2414 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0424 08:22:35.317457 2414 model_repository_manager.cc:1022] loading: rec_runtime:1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0424 08:22:35.469041 2414 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nI0424 08:22:35.469148 2414 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0424 08:22:35.686333 2414 fastdeploy_runtime.cc:1319] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (CPU device 0)\r\nI0424 08:22:35.686457 2414 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0424 08:22:35.812596 2414 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\nI0424 08:22:35.812914 2414 fastdeploy_runtime.cc:1280] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(189)::SetPaddleMKLDNN\t`RuntimeOption::SetPaddleMKLDNN` will be removed in v1.2.0, please modify its member variable directly, e.g `option.paddle_infer_option.enable_mkldnn = true`\r\nI0424 08:22:35.813652 2414 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0424 08:22:36.070492 2414 fastdeploy_runtime.cc:1319] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (CPU device 0)\r\nI0424 08:22:36.070592 2414 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0424 08:22:36.447470 2414 fastdeploy_runtime.cc:1351] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0424 08:22:36.447530 2414 fastdeploy_runtime.cc:1300] TRITONBACKEND_ModelFinalize: delete model state\r\n**E0424 08:22:36.447556 2414 model_repository_manager.cc:1186] failed to load 'rec_runtime' version 1: Invalid argument: unexpected inference input 'softmax_5.tmp_0', allowed inputs are: softmax_0.tmp_0\r\nE0424 08:22:36.447754 2414 model_repository_manager.cc:1375] Invalid argument: ensemble 'rec_pp' depends on 'rec_runtime' which has no loaded version**\r\nI0424 08:22:36.448036 2414 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0424 08:22:36.548360 2414 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0424 08:22:36.648521 2414 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0424 08:22:36.648607 2414 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0424 08:22:36.648670 2414 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0424 08:22:36.648692 2414 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0424 08:22:36.648741 2414 server.cc:592] \r\n+-----------------+---------+-------------------------------------------------------------------------------------+\r\n| Model           | Version | Status                                                                              |\r\n+-----------------+---------+-------------------------------------------------------------------------------------+\r\n| cls_postprocess | 1       | READY                                                                               |\r\n| cls_pp          | 1       | READY                                                                               |\r\n| cls_runtime     | 1       | READY                                                                               |\r\n| det_postprocess | 1       | READY                                                                               |\r\n| det_preprocess  | 1       | READY                                                                               |\r\n| det_runtime     | 1       | READY                                                                               |\r\n| pp_ocr          | 1       | READY                                                                               |\r\n| rec_postprocess | 1       | READY                                                                               |\r\n| rec_runtime     | 1       | UNAVAILABLE: Invalid argument: unexpected inference input 'softmax_5.tmp_0', allowe |\r\n|                 |         | d inputs are: softmax_0.tmp_0                                                       |\r\n+-----------------+---------+-------------------------------------------------------------------------------------+\r\n\r\nI0424 08:22:36.648803 2414 tritonserver.cc:1920] \r\n+----------------------------------+------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                        |\r\n+----------------------------------+------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                       |\r\n| server_version                   | 2.15.0                                                                       |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) |\r\n|                                  |  schedule_policy model_configuration system_shared_memory cuda_shared_memory |\r\n|                                  |  binary_tensor_data statistics                                               |\r\n| model_repository_path[0]         | /ocr_serving/models                                                          |\r\n| model_control_mode               | MODE_NONE                                                                    |\r\n| strict_model_config              | 1                                                                            |\r\n| rate_limit                       | OFF                                                                          |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                    |\r\n| response_cache_byte_size         | 0                                                                            |\r\n| min_supported_compute_capability | 0.0                                                                          |\r\n| strict_readiness                 | 1                                                                            |\r\n| exit_timeout                     | 30                                                                           |\r\n+----------------------------------+------------------------------------------------------------------------------+\r\n\r\nI0424 08:22:36.648815 2414 server.cc:252] Waiting for in-flight requests to complete.\r\nI0424 08:22:36.648821 2414 model_repository_manager.cc:1055] unloading: rec_postprocess:1\r\nI0424 08:22:36.648860 2414 model_repository_manager.cc:1055] unloading: pp_ocr:1\r\nI0424 08:22:36.648875 2414 model_repository_manager.cc:1055] unloading: det_runtime:1\r\nI0424 08:22:36.648887 2414 model_repository_manager.cc:1055] unloading: det_preprocess:1\r\nI0424 08:22:36.648894 2414 model_repository_manager.cc:1166] successfully unloaded 'pp_ocr' version 1\r\nI0424 08:22:36.648902 2414 model_repository_manager.cc:1055] unloading: det_postprocess:1\r\nI0424 08:22:36.648948 2414 model_repository_manager.cc:1055] unloading: cls_pp:1\r\nI0424 08:22:36.648960 2414 model_repository_manager.cc:1055] unloading: cls_runtime:1\r\nI0424 08:22:36.648984 2414 model_repository_manager.cc:1055] unloading: cls_postprocess:1\r\nI0424 08:22:36.649028 2414 model_repository_manager.cc:1166] successfully unloaded 'cls_pp' version 1\r\nI0424 08:22:36.649060 2414 fastdeploy_runtime.cc:1351] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0424 08:22:36.649037 2414 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\r\nI0424 08:22:36.649258 2414 fastdeploy_runtime.cc:1351] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0424 08:22:36.653471 2414 fastdeploy_runtime.cc:1300] TRITONBACKEND_ModelFinalize: delete model state\r\nI0424 08:22:36.653493 2414 model_repository_manager.cc:1166] successfully unloaded 'cls_runtime' version 1\r\nI0424 08:22:36.655704 2414 fastdeploy_runtime.cc:1300] TRITONBACKEND_ModelFinalize: delete model state\r\nI0424 08:22:36.655827 2414 model_repository_manager.cc:1166] successfully unloaded 'det_runtime' version 1\r\nCleaning up...\r\nI0424 08:22:37.649199 2414 server.cc:267] Timeout 29: Found 4 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nCleaning up...\r\nCleaning up...\r\nI0424 08:22:37.691590 2414 model_repository_manager.cc:1166] successfully unloaded 'cls_postprocess' version 1\r\nI0424 08:22:37.692992 2414 model_repository_manager.cc:1166] successfully unloaded 'det_postprocess' version 1\r\nI0424 08:22:37.705958 2414 model_repository_manager.cc:1166] successfully unloaded 'rec_postprocess' version 1\r\nI0424 08:22:37.706510 2414 model_repository_manager.cc:1166] successfully unloaded 'det_preprocess' version 1\r\nI0424 08:22:38.649406 2414 server.cc:267] Timeout 28: Found 0 li\r\n\r\n请问如何解决？谢谢！",
        "state": "closed",
        "user": "Jaccica",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-24T08:27:47+00:00",
        "updated_at": "2024-05-21T06:40:43+00:00",
        "closed_at": "2024-05-21T06:40:42+00:00",
        "comments_count": [
            "jiyulongxu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1869,
        "title": "什么时候支持最新的RT-DETR模型？",
        "body": "谢谢！",
        "state": "closed",
        "user": "lifw555",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-25T03:06:37+00:00",
        "updated_at": "2024-10-15T06:42:03+00:00",
        "closed_at": "2024-10-15T06:42:03+00:00",
        "comments_count": [
            "DefTruth",
            "qinxianyuzi",
            "SongYii"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1871,
        "title": "YOLOv8 识别不到",
        "body": "```\r\n yolo export model=D:\\YOLOv8\\weights\\best.pt format=onnx\r\n\r\nUltralytics YOLOv8.0.84  Python-3.8.16 torch-1.10.1 CPU\r\nModel summary (fused): 168 layers, 11128680 parameters, 0 gradients, 28.5 GFLOPs\r\n\r\nPyTorch: starting from D:\\YOLOv8\\weights\\best.pt with input shape (1, 3, 1280, 1280) BCHW and output shape(s) (1, 12, 33600) (21.6 MB)\r\n\r\nONNX: starting export with onnx 1.13.1 opset 13...\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nWARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\r\nONNX: export success  2.0s, saved as D:\\YOLOv8\\weights\\best.onnx (43.1 MB)\r\n\r\nExport complete (3.3s)\r\nResults saved to D:\\YOLOv8\\weights\r\nPredict:         yolo predict task=detect model=D:\\YOLOv8\\weights\\best.onnx imgsz=1280\r\nValidate:        yolo val task=detect model=D:\\YOLOv8\\weights\\best.onnx imgsz=1280 data=D:/YOLOv8/ultralytics/VOCC.yaml\r\nVisualize:       https://netron.app\r\n\r\n\r\n增加\r\nopset=11\r\n结果一样\r\n\r\n```\r\n\r\n\r\n\r\n\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/yolov8/python\r\n\r\n```python\r\n# GPU inference\r\npython infer.py --model xxxxx.onnx --image 000000014439.jpg --device gpu\r\n```\r\n\r\n不管是源码 还是命令行\r\n都是 0 识别\r\n\r\n但是用官方yolov8 却可以正常识别到",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-25T08:23:18+00:00",
        "updated_at": "2024-11-19T06:41:50+00:00",
        "closed_at": "2024-11-19T06:41:50+00:00",
        "comments_count": [
            "DefTruth",
            "monkeycc",
            "monkeycc",
            "ZhangJie321321"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1874,
        "title": "支持Atlas 200的边缘装置上吗？",
        "body": "如题",
        "state": "closed",
        "user": "eyu11",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-26T02:21:24+00:00",
        "updated_at": "2024-05-14T06:42:10+00:00",
        "closed_at": "2024-05-14T06:42:10+00:00",
        "comments_count": [
            "psnow"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1872,
        "title": "win10 cpu环境下部署C++版本ppyoloe_r的预测结果不正确",
        "body": "fastdeploy是2023.4.25的版本，编译环境：win10，vs2019，cmake；连同example编译成功；预测时出现错误。\r\n\r\nvision_detection_paddledetection_infer_ppyoloe_r.exe输出结果错乱：\r\n![2](https://user-images.githubusercontent.com/31379701/234246807-2b67c4de-8350-4312-8ee3-532f6e8050d0.jpg)\r\npython训练和预测的结果都正确，按照教程导出的文件：\r\n![3](https://user-images.githubusercontent.com/31379701/234247192-67c3f7d1-c06c-4781-9ec0-ac9f5e56f9d1.jpg)\r\n看了一下源码，DetectionResult::Str()和VisDetection()是支持rotated box的：\r\n![4](https://user-images.githubusercontent.com/31379701/234247816-062fd7d3-4c13-43cd-a4eb-1fa03d307ef2.jpg)\r\n![5](https://user-images.githubusercontent.com/31379701/234247861-7a29d86d-d45b-4f97-88bd-9bd1b78d81b8.jpg)\r\n\r\n\r\n同一图像，用vision_detection_paddledetection_infer_yolov3.exe输出结果正确：\r\n![6](https://user-images.githubusercontent.com/31379701/234248025-f6758c0b-e460-43a0-a559-d122b75ff22b.jpg)\r\n\r\n请大佬帮助解决一下，谢谢！\r\n",
        "state": "closed",
        "user": "DreamMaker777",
        "closed_by": "DreamMaker777",
        "created_at": "2023-04-25T10:23:20+00:00",
        "updated_at": "2023-11-20T03:13:41+00:00",
        "closed_at": "2023-04-26T02:31:17+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "DefTruth",
            "DreamMaker777",
            "DreamMaker777",
            "DreamMaker777",
            "DreamMaker777",
            "DefTruth",
            "DefTruth",
            "DreamMaker777",
            "DreamMaker777",
            "DreamMaker777",
            "DefTruth",
            "DreamMaker777",
            "thunder95",
            "DreamMaker777",
            "thunder95",
            "thunder95",
            "DreamMaker777",
            "huyunlei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1877,
        "title": "win10 cpu部署c++版本ppyoloe_r时不支持openvino后端",
        "body": "win10 cpu部署c++版本ppyoloe_r时不支持openvino后端，但是部署yolov3时是支持的。请问ppyoloe_r什么时候支持openvino后端部署？",
        "state": "open",
        "user": "DreamMaker777",
        "closed_by": null,
        "created_at": "2023-04-26T09:10:40+00:00",
        "updated_at": "2023-04-26T09:10:40+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1875,
        "title": "fastdeploy编译成功，example中的yolov5编译失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：develop\r\n- 【编译命令】\r\n- `cd examples/vision/detection/yolov5/cpp/\r\n-  mkdir build &&cd build\r\n-  cmake ..\r\n- `\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU A2， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.20.0\r\n--   CMake command             : /usr/local/src/cmake-3.20.0-linux-x86_64/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   WITH_GPU                  : ON\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_VISION             : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   OPENCV_DIRECTORY          : /usr/include/opencv4\r\n--   DEPENDENCY_LIBS           : FDLIB-NOTFOUND;/usr/lib/x86_64-linux-gnu/libcudart.so;TRT_INFER_LIB-NOTFOUND;TRT_ONNX_LIB-NOTFOUND;TRT_PLUGIN_LIB-NOTFOUND;PADDLE2ONNX_LIB-NOTFOUND\r\n-- Configuring done\r\nCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\n\r\n部分变量找不到，我去修改时发现其他几个都找到了，唯独没有找到 lib文件夹，我是按照 docs/cn/build_and_install/gpu.md 文档编译的，编译成功后并没有这个lib文件",
        "state": "closed",
        "user": "hch-baobei",
        "closed_by": "hch-baobei",
        "created_at": "2023-04-26T02:32:03+00:00",
        "updated_at": "2024-07-09T10:55:00+00:00",
        "closed_at": "2023-04-26T02:51:37+00:00",
        "comments_count": [
            "hch-baobei",
            "mahesh11T",
            "danny-zhu",
            "mahesh11T",
            "mahesh11T",
            "AriannST"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1876,
        "title": "fd_serving_ocr服务器部署,替换成英文模型，服务无法正常启动",
        "body": "\r\n1、使用官方文档能成功复现：https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving\r\n\r\n2、但是替换成英文模型：![03c3afe1-80dd-4d40-b290-b3b68d1f235c](https://user-images.githubusercontent.com/60127735/234490954-8a4cc604-e888-4e67-a4fb-64aa9e3316e4.jpeg)\r\n\r\n3、报错\r\n![image](https://user-images.githubusercontent.com/60127735/234491636-e1153d4e-a60f-47d7-882e-dc7c2bb00e94.png)\r\n![image](https://user-images.githubusercontent.com/60127735/234491761-5aa5434e-def6-4718-bad6-b91f59f5b931.png)\r\n![image](https://user-images.githubusercontent.com/60127735/234491855-8c3891df-f265-454e-963c-d5143cd54f30.png)\r\n @DefTruth @jiangjiajun @yunyaoXYY \r\n\r\n\r\n",
        "state": "open",
        "user": "teymur-git",
        "closed_by": null,
        "created_at": "2023-04-26T06:47:09+00:00",
        "updated_at": "2023-04-27T08:50:54+00:00",
        "closed_at": null,
        "comments_count": [
            "teymur-git",
            "teymur-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1879,
        "title": "使用人脸识别的时候报错了,TypeError: 'str' object is not callable",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n![image](https://user-images.githubusercontent.com/104914091/234803373-db6decbb-0a91-4918-ba07-3ca914386a46.png)\r\n\r\n`python examples\\vision\\faceid\\insightface\\python\\infer_arcface.py --model ms1mv3_arcface_r100.onnx --face 3.jpg --face_positive h1.jpg --face_negative hyh.jpg --device gpu`\r\n\r\n报错:\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\r\n[INFO] fastdeploy/runtime/runtime.cc(293)::fastdeploy::Runtime::CreateOrtBackend        Runtime initialized with Backend::ORT in Device::GPU.\r\nFaceRecognitionResult: [Dim(512), Min(-2.607002), Max(2.137851), Mean(0.006605)]\r\nFaceRecognitionResult: [Dim(512), Min(-0.931618), Max(1.009867), Mean(-0.024876)]\r\nFaceRecognitionResult: [Dim(512), Min(-2.600491), Max(3.553268), Mean(0.038227)]\r\nCosine 01:  0.18622680545915404\r\nCosine 02:  0.016184934865728532\r\nTraceback (most recent call last):\r\n  File \"examples\\vision\\faceid\\insightface\\python\\infer_arcface.py\", line 81, in <module>\r\n    print(model.runtime_option)\r\n  File \"D:\\anaconda3\\envs\\paddlex\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 176, in RuntimeOptionStr\r\n    attrs = dir(runtime_option)\r\nTypeError: 'str' object is not callable\r\n",
        "state": "open",
        "user": "HiaHong",
        "closed_by": null,
        "created_at": "2023-04-27T08:22:50+00:00",
        "updated_at": "2023-04-27T10:59:53+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "HiaHong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1880,
        "title": "c_api 模型",
        "body": "在c_api中，好像没有找到指定模型宽高的方法\r\n\r\n在c的代码中，发现应该是有nms过滤的，希望也可以开放一个api，来做过滤\r\n",
        "state": "open",
        "user": "oldma3095",
        "closed_by": null,
        "created_at": "2023-04-27T09:06:57+00:00",
        "updated_at": "2023-04-28T10:11:35+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1882,
        "title": "Detect BatchPredict Result Error",
        "body": "\r\nBatchPredict  postprocessor ,  If the inference encounters an image that does not recognize the object, subsequent image recognition will be empty, and the code that causes the problem is as follows: \r\n\r\n    if ((*results)[bs].boxes.size() == 0) {\r\n      return true;\r\n    }\r\n",
        "state": "open",
        "user": "Du-Sen-Lin",
        "closed_by": null,
        "created_at": "2023-04-27T09:48:01+00:00",
        "updated_at": "2023-04-27T09:48:01+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1884,
        "title": "请问python版本SegmentationResult怎么转成掩模图",
        "body": null,
        "state": "open",
        "user": "Dandelion111",
        "closed_by": null,
        "created_at": "2023-04-27T12:06:43+00:00",
        "updated_at": "2023-04-27T12:06:43+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1881,
        "title": "fastdeploy能增加支持RT-DERT部署么",
        "body": "fastdeploy能增加支持RT-DERT部署么，想看看部署的效果",
        "state": "closed",
        "user": "Zsk747",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-04-27T09:30:37+00:00",
        "updated_at": "2024-04-30T06:42:48+00:00",
        "closed_at": "2024-04-30T06:42:48+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1887,
        "title": "Tracking fails due to preprocess error",
        "body": "Environment : Windows 10\r\n\r\nFastDeploy version: 1.0\r\nOS Platform: Windows x64 \r\nHardware: Nvidia GPU 2080Ti  CUDA 11.6 CUDNN 8.4\r\nProgram Language: C++\r\n\r\n## Problem description\r\n\r\nAfter following the tracking example and using the recommended model I receive \"LetterBoxResize Not Implement Yet\" error.Thanks in advance.\r\n",
        "state": "open",
        "user": "UygarUsta99",
        "closed_by": null,
        "created_at": "2023-04-28T14:18:31+00:00",
        "updated_at": "2023-04-28T14:18:31+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1883,
        "title": "onnx部署推理失败",
        "body": "## 环境\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】：cpu \r\n- 【编译语言】：Python(3.10）\r\n- 【FastDeploy版本】： fastdeploy-python=1.0.0\r\n-  onnx=1.13.1\r\n-  onnxruntime = 1.14.1\r\n\r\nonnx推理时报错：\r\n\r\n```\r\n[INFO] fastdeploy/runtime.cc(500)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\n[ERROR] fastdeploy/backends/ort/ort_backend.cc(229)::Infer      [OrtBackend] Size of the inputs(1) should keep same with the inputs of this model(3).\r\n[ERROR] fastdeploy/pybind/main.cc(42)::FDDataTypeToNumpyDataType        The function doesn't support data type of FDDataType::INT8.\r\nAborted (core dumped)\r\n```\r\n\r\n发生报错的代码：\r\n```\r\nonnx_path = \"./YOLOv3/inference_model/model.onnx\"\r\n\r\nimport fastdeploy as fd\r\nfrom fastdeploy import ModelFormat\r\nimport numpy as np\r\n\r\noption = fd.RuntimeOption()\r\noption.set_model_path(onnx_path, model_format=ModelFormat.ONNX)\r\n\r\noption.use_cpu()\r\noption.use_ort_backend()\r\noption.set_cpu_thread_num(12)\r\n\r\nruntime = fd.Runtime(option)\r\ninput_name = runtime.get_input_info(0).name\r\nresults = runtime.infer({\r\n    input_name: np.random.rand(1, 3, 320, 320).astype(\"float32\")   # <- 报错的位置\r\n})\r\n\r\nprint(results[0].shape)\r\n```\r\n\r\n**其他信息：**\r\n原始的模型推理可以正常出结果。原始的推理如下：\r\n```\r\nimport os\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport paddlex as pdx\r\nfrom paddlex import transforms as T\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\r\nmodel = pdx.load_model('./YOLOv3/best_model')  # 加载训练好的模型\r\n\r\nimg_path = \"./test_images_b/img.jpg\"\r\nimgsrc = cv2.imread(img_path)\r\nres = model.predict(img_path)\r\nprint(res)\r\n```\r\n\r\n正常推理ok的模型，在做了两步转换得到的onnx后，推理出了问题。转换过程是：\r\n```\r\npaddlex \\\r\n--export_inference \\\r\n--model_dir=./YOLOv3/best_model \\\r\n--save_dir=./YOLOv3 \\\r\n--fixed_input_shape=[1,3,320,320]\r\n\r\npaddle2onnx \\\r\n--model_dir./YOLOv3/inference_model \\\r\n--model_filename ./YOLOv3/inference_model/model.pdmodel \\\r\n--params_filename ./YOLOv3/inference_model/model.pdiparams \\\r\n--save_file ./YOLOv3/inference_model/model.onnx \\\r\n--enable_dev_version True\r\n```\r\n\r\n全部按照文档做的转换，不知道是哪里出了问题。",
        "state": "closed",
        "user": "ThistleInTheSun",
        "closed_by": "ThistleInTheSun",
        "created_at": "2023-04-27T10:45:23+00:00",
        "updated_at": "2023-04-28T02:24:25+00:00",
        "closed_at": "2023-04-28T02:24:25+00:00",
        "comments_count": [
            "ThistleInTheSun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1885,
        "title": "freeLibrary自封装的ocr dll 程序崩溃",
        "body": "windows10+vs2017\r\n自定义封装了一个dll，加载这个dll，并且dll中只要创建了det或者cls或者rec指针对象，结束的时候释放这些指针对象，再freeLibrary这个dll，就会导致程序崩溃。如果不释放这些指针对象，就不会崩溃，但是会有内存泄漏。有人遇到过类似问题吗",
        "state": "open",
        "user": "597871382",
        "closed_by": null,
        "created_at": "2023-04-27T12:37:55+00:00",
        "updated_at": "2023-04-27T12:51:20+00:00",
        "closed_at": null,
        "comments_count": [
            "597871382"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1886,
        "title": "simple  server服务耗时 与单独测试耗时为什么差别很大。",
        "body": "simple 服务耗时 与单独测试耗时为什么差别很大。\r\n 测试 ocr服务 。如果单独运行这个一张图片耗时差不多0.08s  \r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/cpu-gpu/python/infer.py\r\n这样打印的耗时。\r\n<img width=\"416\" alt=\"image\" src=\"https://user-images.githubusercontent.com/22782047/235048994-4f8def3f-1fb2-4cf7-9995-86309d7e0362.png\">\r\n\r\nsimple  server 服务的话 耗时是0.15s 了 为什么这么大的区别呢？\r\n这里打印的耗时。\r\n<img width=\"870\" alt=\"image\" src=\"https://user-images.githubusercontent.com/22782047/235048894-32d51dae-e4c9-4242-af83-ab79c69ede8a.png\">\r\n\r\n图片一样，机器一样。\r\n",
        "state": "open",
        "user": "cumthxy",
        "closed_by": null,
        "created_at": "2023-04-28T03:42:17+00:00",
        "updated_at": "2023-04-28T08:59:22+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII",
            "cumthxy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1889,
        "title": "Dynamic input shape of model",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.0\r\nOS Platform: Windows x64\r\nHardware: CPU noavx\r\nProgram Language: C++\r\nCMake: 3.18, 3.25\r\nVisual Studio: 2019\r\n\r\n## Code\r\n```\r\nconst std::string& model_file = \"C:/modnet-resnet50_vd/model.pdmodel\";\r\nconst std::string& params_file = \"C:/modnet-resnet50_vd/model.pdiparams\";\r\nconst std::string& config_file = \"C:/modnet-resnet50_vd/deploy.yaml\";\r\n\r\nauto option = fastdeploy::RuntimeOption();\r\n    \r\noption.UsePaddleInferBackend();\r\noption.UseCpu();\r\noption.SetPaddleMKLDNN(false);\r\n    \r\nauto model = fastdeploy::vision::matting::PPMatting(model_file, params_file, config_file, option);\r\n```\r\n\r\n## Problem description\r\n1[WARNING] fastdeploy/vision/matting/ppmatting/ppmatting.cc(78)::fastdeploy::vision::matting::PPMatting::BuildPreprocessPipelineFromConfig\r\nDetected dynamic input shape of your model, only Paddle Inference / OpenVINO support this model now.\r\n\r\n",
        "state": "closed",
        "user": "unblock7",
        "closed_by": "unblock7",
        "created_at": "2023-04-29T18:55:24+00:00",
        "updated_at": "2023-04-29T19:47:41+00:00",
        "closed_at": "2023-04-29T19:47:41+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1890,
        "title": "Dynamic input shape model",
        "body": "## Environment\r\nFastDeploy version: 1.0.0\r\nOS Platform: Windows x64\r\nHardware: CPU noavx\r\nProgram Language: C++\r\nCMake: 3.18, 3.25\r\nVisual Studio: 2019\r\n\r\n## Code\r\n```\r\nconst std::string& model_file = \"C:/modnet-resnet50_vd/model.pdmodel\";\r\nconst std::string& params_file = \"C:/modnet-resnet50_vd/model.pdiparams\";\r\nconst std::string& config_file = \"C:/modnet-resnet50_vd/deploy.yaml\";\r\n\r\nauto option = fastdeploy::RuntimeOption();\r\noption.UsePaddleInferBackend();\r\noption.UseCpu();\r\noption.SetPaddleMKLDNN(false);\r\n    \r\nauto model = fastdeploy::vision::matting::PPMatting(model_file, params_file, config_file, option);\r\n```\r\n\r\n## Problem description\r\nconsole output:\r\n1[WARNING] fastdeploy/vision/matting/ppmatting/ppmatting.cc(78)::fastdeploy::vision::matting::PPMatting::BuildPreprocessPipelineFromConfig\r\nDetected dynamic input shape of your model, only Paddle Inference / OpenVINO support this model now.\r\n\r\nI specified option.UsePaddleInferBackend();\r\nWhy is this model not supported. Should I use a different library?",
        "state": "open",
        "user": "unblock7",
        "closed_by": null,
        "created_at": "2023-04-29T19:04:35+00:00",
        "updated_at": "2023-04-29T19:58:25+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1888,
        "title": "Custom trained yolov5/8 on rknpu2 for RK3588",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.6\r\nOS Platform: e.g. ubuntu Linux x64 &  debian 11 on rk3588s board\r\nHardware: e.g. Nvidia GPU 3070Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nI am trying to train my custom yolov5/v8 model, convert it to rknn format and then run inference on the same.\r\nCan someone explain the steps for me to find out what I am doing wrong.\r\n\r\nI am trying to convert the default models from onnx to rknn using the fastdeploy tools/rknpu2 step. It fails with the following error \r\n\"\"\"\r\n{'mean': [[0, 0, 0]], 'std': [[255, 255, 255]], 'model_path': './yolov8n.onnx', 'outputs_nodes': ['p2o.Mul.1', 'p2o.Concat.49'], 'do_quantization': False, 'dataset': './coco_dataset_20.txt', 'output_folder': './yolov8_n_500e_coco'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nW load_onnx: If you don't need to crop the model, don't set 'inputs'/'input_size_list'/'outputs'!\r\nE load_onnx: The 'p2o.Mul.1' in outputs=['p2o.Mul.1', 'p2o.Concat.49'] is invalid!\r\nW load_onnx: ===================== WARN(2) =====================\r\nE rknn-toolkit2 version: 1.4.0-22dcfef4\r\nE load_onnx: Catch exception when loading onnx model: /home/dt/Projects/Work/greenox/rknn/yolov8onnxtorknn/FastDeploy/tools/rknpu2/yolov8n.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1136, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_log.py\", line 113, in rknn.api.rknn_log.RKNNLog.e\r\nE load_onnx: ValueError: The 'p2o.Mul.1' in outputs=['p2o.Mul.1', 'p2o.Concat.49'] is invalid!\r\nTraceback (most recent call last):\r\n  File \"export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\"\"\"\r\n\r\nI tried using the [rknn_model_zoo](https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo/RKNN_model_convert) convert functionality, but it also only converts the standard models given by them.\r\n\r\n\r\nDoes your rkyolo demo at (https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/rkyolo) work on RK3588 and on custom models? If so how can i train custom and run inference on RK3588.\r\n\r\nThank you for your help in advance!\r\n",
        "state": "open",
        "user": "programmeddeath1",
        "closed_by": null,
        "created_at": "2023-04-29T08:19:28+00:00",
        "updated_at": "2024-04-26T09:34:49+00:00",
        "closed_at": null,
        "comments_count": [
            "ilbash",
            "programmeddeath1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1891,
        "title": "使用FastDeploy搭建PP-OCRv3模型服务后怎么配置参数",
        "body": "系统环境/System Environment：docker\r\n版本号/Version： fastdeploy:1.0.4-cpu-only-21.10\r\n\r\n小白一个，目前服务部署起来了，但是我想配置一下文本框识别数量上限这种参数。小白凭个人猜测对着det_postprocess文件夹下的config.pbtxt文件改了两下会报错。求大神指点该怎么写，怎么配置……\r\n![image](https://user-images.githubusercontent.com/41407267/235379811-c009d1a5-8ffa-4096-8596-ed96fb578ed3.png)\r\n\r\n尝试在config.pbtxt中直接加入max_candidates: 2\r\n```\r\n[libprotobuf ERROR /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/protobuf/src/google/protobuf/text_format.cc:317] Error parsing text-format inference.ModelConfig: 46:27: Message type \"inference.ModelConfig.ParametersEntry\" has no field named \"max_candidates\".\r\nE0430 23:07:12.852829 210 model_repository_manager.cc:1890] Poll failed for model directory 'det_postprocess': failed to read text proto from /opt/tritonserver/FastDeploy/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models/det_postprocess/config.pbtxt\r\n```\r\n尝试加入parameters {max_candidates: 2}\r\n```\r\n[libprotobuf ERROR /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/protobuf/src/google/protobuf/text_format.cc:317] Error parsing text-format inference.ModelConfig: 46:27: Message type \"inference.ModelConfig.ParametersEntry\" has no field named \"max_candidates\".\r\nE0430 23:07:12.852829 210 model_repository_manager.cc:1890] Poll failed for model directory 'det_postprocess': failed to read text proto from /opt/tritonserver/FastDeploy/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models/det_postprocess/config.pbtxt\r\n```",
        "state": "open",
        "user": "zhouyiminga",
        "closed_by": null,
        "created_at": "2023-04-30T23:14:46+00:00",
        "updated_at": "2023-04-30T23:16:24+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1896,
        "title": "文件下载缺失",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\nwget 直接下载 zip文件，下载之后配置环境发现没有 fastdeploy/core/config.h  但是在同位置的fd_type.h 目录下有引用，是我哪里操作错了吗？\r\n",
        "state": "closed",
        "user": "hch-baobei",
        "closed_by": "hch-baobei",
        "created_at": "2023-05-04T08:57:11+00:00",
        "updated_at": "2023-05-04T09:00:33+00:00",
        "closed_at": "2023-05-04T09:00:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1892,
        "title": "【 跑官方ARM CPU demo报错 Illegal instruction 】",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-1.0.6 自行编译的。（因为直接下载预编译包，报同样错误）\r\n- 【编译命令】cmake -B build -DCMAKE_TOOLCHAIN_FILE=./cmake/toolchain.cmake -DWITH_TIMVX=OFF  -DTARGET_ABI=arm64 -DENABLE_FLYCV=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-timvx -DENABLE_VISION=ON -DENABLE_LITE_BACKEND=ON -Wno-dev\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 荣品A311D，ARM CPU， 4G内存\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 执行`examples`下的部署示例时，报错Illegal instruction。步骤都是按照这个链接里的教程一步一步操作的：https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/cpu-gpu/cpp\r\n- 模型等文件，也是按照教程下载的，FD库和Demo编译过程都没有报错。\r\n- 执行后，报错如下：\r\n\r\n./build/infer_demo ./ch_PP-OCRv3_det_infer ./ch_ppocr_mobile_v2.0_cls_infer ./ch_PP-OCRv3_rec_infer ./ppocr_keys_v1.txt ./12.jpg 3\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(376)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: HARDWARE        : AMLOGIC\r\n\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 6\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 2208, min freq: 2208, cluster ID: 1, CPU ARCH: A53\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 2208, min freq: 2208, cluster ID: 1, CPU ARCH: A53\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 2400, min freq: 2400, cluster ID: 0, CPU ARCH: A73\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 2400, min freq: 2400, cluster ID: 0, CPU ARCH: A73\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 4, max freq: 2400, min freq: 2400, cluster ID: 0, CPU ARCH: A73\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 5, max freq: 2400, min freq: 2400, cluster ID: 0, CPU ARCH: A73\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is:\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is:\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is:\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 3839260KB\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  5/ 3  9:12:45.445 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  5/ 3  9:12:45.445 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  5/ 3  9:12:45.445 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from ./ch_PP-OCRv3_det_infer/inference.pdmodel\r\n[I  5/ 3  9:12:45.479 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from ./ch_PP-OCRv3_det_infer/inference.pdiparams\r\n[I  5/ 3  9:12:45.482 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!\r\nIllegal instruction\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "Taichipeace",
        "closed_by": "Taichipeace",
        "created_at": "2023-05-03T01:32:37+00:00",
        "updated_at": "2023-05-04T12:38:20+00:00",
        "closed_at": "2023-05-04T12:37:42+00:00",
        "comments_count": [
            "DefTruth",
            "Taichipeace"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1893,
        "title": "paddleseg 模型经过转换用demo infer.cc 推理结果 和直接预测结果差很多 ",
        "body": "\r\n![87e75749099afc9ce8d4f10ec864b36](https://user-images.githubusercontent.com/112677205/235872910-3a650378-58b4-4b4a-bb3b-53edf63cd4ee.png)\r\n",
        "state": "open",
        "user": "zhinangubei",
        "closed_by": null,
        "created_at": "2023-05-03T08:57:39+00:00",
        "updated_at": "2023-05-03T09:00:13+00:00",
        "closed_at": null,
        "comments_count": [
            "zhinangubei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1894,
        "title": "如果提高FastDeploy Serving 的QPS。",
        "body": "使用默认设置 FastDeploy Serving C++部署 ppocr_v3  qps最大只有4 左右。如果提升？",
        "state": "closed",
        "user": "cumthxy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-04T05:37:34+00:00",
        "updated_at": "2024-05-28T06:38:36+00:00",
        "closed_at": "2024-05-28T06:38:36+00:00",
        "comments_count": [
            "xiaomi0922",
            "cumthxy",
            "xiaomi0922"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1895,
        "title": "FastDeploy多推理引擎支持lora吗？",
        "body": "想了解一下FastDeploy已有的高性能推理引擎（trt/paddle_tensorrt/ait等）支持lora和controlnet吗？如果没有，有没有计划或者思路可以分享一下",
        "state": "closed",
        "user": "zwj536",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-04T08:52:35+00:00",
        "updated_at": "2024-05-07T06:40:34+00:00",
        "closed_at": "2024-05-07T06:40:34+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1901,
        "title": "PyTorch版YOLOv5使用PaddleSlim自动压缩后的模型，可以在移动端部署么？",
        "body": "使用 PaddleSlim/example/auto_compression/pytorch_yolo_series/configs/yolov5s_qat_dis.yaml 压缩的模型，想部署在移动端，有可以参考的Demo么？\r\n\r\nPicoDet在精度方面还是不理想，故换用了YOLOv5，但是不知道要怎么部署到移动端，官方给的Demo基本上都是关于PicoDet的。",
        "state": "open",
        "user": "dium6i",
        "closed_by": null,
        "created_at": "2023-05-05T07:49:17+00:00",
        "updated_at": "2023-05-05T07:49:17+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1897,
        "title": "ppcls推理模型在Python端运行正常，在C++端无法运行",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.4\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】： C++ \r\n\r\n- 【FastDeploy版本】：fastdeploy-python    1.0.6\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】：Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用Vetr车辆数据集跑了个模型，在Python端使用fastdeploy运行正常，同样的模型和配置文件在C++端异常\r\n\r\n![vs](https://user-images.githubusercontent.com/15247604/236162578-be080381-cff2-4667-9312-e6ce10b7ccba.png)\r\n![vs2](https://user-images.githubusercontent.com/15247604/236164119-dce61e6c-74e0-4b5d-98ae-67d17ed75a1c.png)\r\n\r\n![python](https://user-images.githubusercontent.com/15247604/236162580-51efb339-d228-4c97-a075-f1fadd2bc90f.png)\r\n\r\n模型和配置文件如下：\r\n[ppclas.zip](https://github.com/PaddlePaddle/FastDeploy/files/11395843/ppclas.zip)\r\n",
        "state": "open",
        "user": "qumoy",
        "closed_by": null,
        "created_at": "2023-05-04T09:25:24+00:00",
        "updated_at": "2023-05-06T01:51:04+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "qumoy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1898,
        "title": "在vs2022使用预编译库的程序无法正常启动(0xc0000142)",
        "body": "\r\n## 环境\r\n![fastdeploy预编译库1 0 6](https://user-images.githubusercontent.com/107804044/236193616-9d021fb4-3b9b-41fc-83ca-953f7b66dce4.png)\r\n- 【FastDeploy版本】：[Visual Studio 16 2019编译产出的fastdeploy-win-x64-1.0.6,官方预编译库](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/download_prebuilt_libraries.md)\r\n\r\n- 【系统平台】: win10_20H2;vs2022\r\n- 【硬件】： cpu: i5-12400f\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n尝试用fastdeploy部署模型\r\n已经正确链接lib,并拷贝对应的dll到项目,\r\n控制台无任何输出,运行程序直接弹窗报错:`应用程序无法正常启动(0xc0000142)。请单击“确定”关闭应用程序。 `.\r\n调用堆栈,使用dll和lib如下\r\n\r\n![应用程序无法正常启动](https://user-images.githubusercontent.com/107804044/236193594-178a47a8-8d6b-412f-8d66-22aaa05d5d9e.png)\r\n![调用堆栈](https://user-images.githubusercontent.com/107804044/236193611-69278ccb-d95d-4395-8772-5b31fbe5f472.png)\r\n![断点没有断下来,异常发生在调用main前](https://user-images.githubusercontent.com/107804044/236193602-18c843d1-4368-46e6-999d-1e01146e209b.png)\r\n![把预编译库下的所有dll拷贝到项目](https://user-images.githubusercontent.com/107804044/236193581-2f75bf62-5392-468f-8d8e-c54ed3b1cd1d.png)\r\n![链接的lib](https://user-images.githubusercontent.com/107804044/236193618-ab8d8709-be59-40f0-af39-075d0ff6d578.png)\r\n\r\n## 排查问题\r\n尝试安装微软常用运行时库重启无解,拷贝项目的x64\\Debug目录到vm虚拟机win11报相同错误,初步判断是编译出来的程序有问题,也就是说预编译库和vs2022不兼容?\r\n难道我需要自己编译fastdeploy?\r\n\r\n",
        "state": "closed",
        "user": "1821746019",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-04T11:46:26+00:00",
        "updated_at": "2024-05-21T06:40:43+00:00",
        "closed_at": "2024-05-21T06:40:43+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth",
            "1821746019",
            "1821746019",
            "DefTruth",
            "1821746019",
            "alasox",
            "1821746019",
            "WilliamQf-AI",
            "universea"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1899,
        "title": "怎样编译ios版本",
        "body": "怎样编译ios版本",
        "state": "open",
        "user": "WilliamQf-AI",
        "closed_by": null,
        "created_at": "2023-05-05T02:35:05+00:00",
        "updated_at": "2023-11-15T07:50:38+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "ndghw",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1902,
        "title": "ppocr部署异常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-win-x64-1.0.6\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】： C++\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n问题是这样，使用PPOCRV3的实力代码测试，封装的方法直接使用是没问题的（如下图1）。工程里面需要把方法修改一下，实例化PPOCRV3一个方法，推理图片一个方法（将图1红色方框抽取出来，如图2），使用推理图片方法的时候一直报内存错误的提示；\r\n\r\n![ocr0](https://user-images.githubusercontent.com/15247604/236418020-5927d3c4-6ae7-44f1-a480-103561e66317.png)\r\n![ocr2](https://user-images.githubusercontent.com/15247604/236418027-ccb3785f-919c-4b96-ad0c-e527b82936b1.png)\r\n![ocr1](https://user-images.githubusercontent.com/15247604/236418042-3d54b46f-a959-441f-a7fa-a23d92474543.png)\r\n",
        "state": "open",
        "user": "qumoy",
        "closed_by": null,
        "created_at": "2023-05-05T09:04:33+00:00",
        "updated_at": "2023-05-10T09:23:53+00:00",
        "closed_at": null,
        "comments_count": [
            "zhenhuamo",
            "qumoy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1903,
        "title": "fastdeploy 配置运行报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 1.0.6\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】： Nvidia GPU 3050， CUDA  11.2.2  CUDNN  8.2.1.32\r\n- 【编译语言】：  Python(3.8）\r\n\r\n\r\n- 【模型跑不通】\r\n  File \"D:\\CV\\dataset\\ppyolo_infer.py\", line 13, in __init__\r\n    self.model = fd.vision.detection.PPYOLO(model_file, params_file, config_file,\r\n  File \"D:\\Users\\sun\\anaconda3\\envs\\fd\\lib\\site-packages\\fastdeploy\\vision\\detection\\ppdet\\__init__.py\", line 168, in __init__\r\n    self._model = C.vision.detection.PPYOLO(\r\nRuntimeError: \r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\nNot support stack backtrace yet.\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nPreconditionNotMetError: The third-party dynamic library (mklml.dll) that Paddle depends on is not configured correctly. (error code is 182)\r\n  Suggestions:\r\n  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.\r\n  2. Configure third-party dynamic library environment variables as follows:\r\n  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`\r\n  - Windows: set PATH by `set PATH=XXX; (at ..\\paddle\\phi\\backends\\dynload\\dynamic_loader.cc:305)\r\n",
        "state": "closed",
        "user": "ridges21",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-05T09:10:14+00:00",
        "updated_at": "2024-09-10T06:41:30+00:00",
        "closed_at": "2024-09-10T06:41:30+00:00",
        "comments_count": [
            "DefTruth",
            "ridges21",
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1904,
        "title": "[Question] text_to_img的stable_diffusion C++实现",
        "body": "https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/multimodal/stable_diffusion/cpp\r\n\r\n我已经看到stable_diffusion   有 inpaint的内容\r\n有没有对与text_to_img的cpp代码参考？",
        "state": "closed",
        "user": "engineer1109",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-05T09:11:15+00:00",
        "updated_at": "2024-05-07T06:40:35+00:00",
        "closed_at": "2024-05-07T06:40:35+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1905,
        "title": "onnx转rknn模型出错",
        "body": "测试Paddle/FastDeploy/examples/vision/keypointdetection/tiny_pose/rknpu2/cpp/这个例子转换模型时出错，信息如下：\r\n\r\npython tools/rknpu2/export.py --config_path tools/rknpu2/config/PP_TinyPose_256x192_unquantized.yaml --target_platform rk3588\r\n{'mean': [[123.675, 116.28, 103.53]], 'std': [[58.395, 57.12, 57.375]], 'model_path': './PP_TinyPose_256x192_infer/PP_TinyPose_256x192_infer.onnx', 'outputs_nodes': ['conv2d_441.tmp_1'], 'do_quantization': False, 'dataset': None, 'output_folder': './PP_TinyPose_256x192_infer'}\r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\nW load_onnx: If you don't need to crop the model, don't set 'inputs'/'input_size_list'/'outputs'!\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\nE load_onnx: Catch exception when loading onnx model: /home/gaylord/Paddle/FastDeploy/PP_TinyPose_256x192_infer/PP_TinyPose_256x192_infer.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1152, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 617, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx: TypeError: '<=' not supported between instances of 'NoneType' and 'int'\r\nTraceback (most recent call last):\r\n  File \"tools/rknpu2/export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!",
        "state": "closed",
        "user": "itouchsky",
        "closed_by": "Zheng-Bicheng",
        "created_at": "2023-05-06T03:07:25+00:00",
        "updated_at": "2023-05-19T11:40:19+00:00",
        "closed_at": "2023-05-19T11:40:19+00:00",
        "comments_count": [
            "itouchsky"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1908,
        "title": "FastDeploy模型预测耗时过长",
        "body": "\r\n\r\n## 环境\r\n- 【FastDeploy版本】： fastdeploy- Windows-gpu-1.0.6\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 笔记本电脑 Nvidia GPU 2060 6G显存， CUDA 11.2  CUDNN 8.2\r\n- 【编译语言】： Python(3.9）\r\n\r\n## 问题\r\n- 【性能问题】\r\n- - 在使用语义分割模型（PP_LiteSeg_B_STDC2_cityscapes_without_argmax_infer）每张图像耗时约600ms，\r\n- - 分类模型（ResNet50_vd_infer）耗时6ms，\r\n- - 检测模型（ppyoloe_plus_crn_m_80e_coco）耗时1400ms（模型均是模型库中下载）\r\n- - 分割和检测耗时很长，请问哪方面可能影响了速度，如何排查和解决长耗时\r\n",
        "state": "closed",
        "user": "Mysister",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-07T06:09:50+00:00",
        "updated_at": "2024-11-26T06:40:47+00:00",
        "closed_at": "2024-11-26T06:40:47+00:00",
        "comments_count": [
            "DefTruth",
            "tigflanker"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1910,
        "title": "from .libs.fastdeploy_main import * ImportError: /.local/lib/python3.10/site-packages/fastdeploy/libs/third_libs/paddle_inference/paddle/lib/libpaddle_inference.so: undefined symbol: dnnl_layer_normalization_v2_forward_desc_init",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python 3.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\npaddlepaddle-gpu 2.4.2 post116\r\nFastDeploy 1.0.6\r\n\r\n代码脚本导入就出错\r\nfrom pprint import pprint\r\nfrom paddlenlp import Taskflow\r\nimport argparse\r\nimport distutils.util\r\nimport math\r\nimport os\r\nimport re\r\nfrom pprint import pprint\r\nimport fastdeploy as fd\r\nimport six\r\nfrom paddlenlp.transformers import AutoTokenizer\r\nfrom paddlenlp.utils.tools import get_bool_ids_greater_than, get_span\r\n\r\n \r\n\r\n错误：\r\nfrom .libs.fastdeploy_main import *\r\nImportError:.local/lib/python3.10/site-packages/fastdeploy/libs/third_libs/paddle_inference/paddle/lib/libpaddle_inference.so: undefined symbol: dnnl_layer_normalization_v2_forward_desc_init\r\n\r\n\r\n",
        "state": "closed",
        "user": "cqray1990",
        "closed_by": "cqray1990",
        "created_at": "2023-05-07T14:03:24+00:00",
        "updated_at": "2023-05-11T14:13:33+00:00",
        "closed_at": "2023-05-11T14:13:33+00:00",
        "comments_count": [
            "DefTruth",
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1909,
        "title": "PaddleDetection目标检测模型PPYOLOE为例展示CPU上的推理示例，我在windows上推理，执行后没有检测出来",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.6\r\n- 【编译命令】 使用的Release版本\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】： i7-6700HQ CPU\r\n- 【编译语言】： C++ \r\n\r\n参考这个文档来的，https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/quick_start/models/cpp.md\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n能跑通\r\n\r\nC:\\Project\\Paddle\\PaddleDetection\\build\\Release>infer_demo.exe\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::fastdeploy::vision::FuseNormalizeCast      Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::fastdeploy::OpenVINOBackend::InitFromPaddle     number of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::fastdeploy::OpenVINOBackend::InitFromPaddle     affinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::fastdeploy::OpenVINOBackend::InitFromPaddle     Compile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::fastdeploy::Runtime::CreateOpenVINOBackend   Runtime initialized with Backend::OPENVINO in Device::CPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\nVisualized result save in vis_result.jpg\r\n\r\n\r\n\r\n- 【模型精度问题】\r\n\r\n vis_result.jpg 没有检测出来\r\n![image](https://user-images.githubusercontent.com/17959627/236675633-16e676e4-bb03-43ac-b726-0f1a4ec61ff9.png)\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "YouSmart2016",
        "closed_by": null,
        "created_at": "2023-05-07T11:49:59+00:00",
        "updated_at": "2023-05-19T04:08:39+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "YouSmart2016",
            "YouSmart2016"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1911,
        "title": "c++部署如何只保存分割结果",
        "body": "看到有函数VisSegmentation可以得到分割效果，使用哪个函数可以只保存分割的灰度图结果（即每个像素的标签，res.Str()中二维矩阵的部分）",
        "state": "closed",
        "user": "guoyunqingyue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-07T14:14:35+00:00",
        "updated_at": "2024-05-14T06:42:11+00:00",
        "closed_at": "2024-05-14T06:42:11+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1912,
        "title": "TensorRT后端压测报错",
        "body": "## 环境\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.3\r\n\r\n## 问题日志及出现问题的操作流程\r\n部署 TensorRT bert 模型后，进行压测，一压测就会有如下报错\r\n<img width=\"944\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33142144/236716433-e1a8bd75-02ca-4ce3-bfa0-8376890dca89.png\">\r\n尝试用onnx推理后端，是可以进行压测\r\n",
        "state": "closed",
        "user": "Lennon-cheng",
        "closed_by": "Lennon-cheng",
        "created_at": "2023-05-08T02:27:11+00:00",
        "updated_at": "2023-06-08T00:10:36+00:00",
        "closed_at": "2023-06-08T00:10:36+00:00",
        "comments_count": [
            "DefTruth",
            "Lennon-cheng",
            "pangdahua"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1914,
        "title": "Trt后端加速报错",
        "body": "cuda 11.8 tenosrt 8.5.2.2\r\n![image](https://user-images.githubusercontent.com/58615953/236776977-69784f7e-9538-4b1b-991b-d735d7efe903.png)\r\n\r\n",
        "state": "open",
        "user": "GeT-RiGhTTT",
        "closed_by": null,
        "created_at": "2023-05-08T08:37:39+00:00",
        "updated_at": "2023-05-08T08:37:39+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1915,
        "title": "原生yolo8与fd精度有差异",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 1.0.6\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: win11\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型精度问题】\r\nyolo8模型训练时imgsz=640，用原生yolo8预测时imgsz=4000，预测图片如下\r\n![image](https://user-images.githubusercontent.com/13513093/236782230-01071026-1198-4665-add8-92486ef8fbf3.png)\r\n\r\n导出onnx时imgsz也是4000，用fd进行预测，代码如下\r\n\r\n    cap = cv2.VideoCapture(test_img2)\r\n\r\n    success, frame = cap.read()\r\n\r\n    preprocessor = fd.vision.detection.YOLOv8Preprocessor()\r\n    preprocessor.size = [4000, 4000]\r\n\r\n    postprocessor = fd.vision.detection.YOLOv8Postprocessor()\r\n    postprocessor.nms_threshold = nms\r\n    postprocessor.conf_threshold = conf\r\n\r\n    # 暖机\r\n    res = model.predict(np.zeros_like(frame))\r\n    res = model.predict(frame)\r\n\r\n预测图片如下：\r\n![image](https://user-images.githubusercontent.com/13513093/236783535-684d06c4-b997-42da-bc1c-631d31b87d7e.png)\r\n\r\n可以看到基本没有预测效果\r\n\r\n但是，将图片切成640X640的小图，用fd预测，并拼接后结果与原生yolo8预测基本一致，但也有可见精度差异。\r\n\r\n求助以上结果产生原因及解决办法\r\n\r\n\r\n",
        "state": "open",
        "user": "beckhz",
        "closed_by": null,
        "created_at": "2023-05-08T09:10:21+00:00",
        "updated_at": "2023-05-08T09:13:41+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1920,
        "title": "范例vision/detection/paddledetection/serving无法跑通",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：  Nvidia GPU 1060\r\n\r\n\r\n已完全按照 [https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README_CN.md](使用VisualDL进行可视化部署) 文档部署\r\n\r\n使用Gradio执行报 \r\n```\r\nError: [400] in ensemble 'ppdet', Failed to process the request(s) for model instance 'preprocess_0', message: Stub process is not healthy.\u0000\r\n```\r\n\r\n在终端使用paddledet_grpc_client.py执行报\r\n```\r\nroot@7B89:/FastDeploy/vision/detection/paddledetection/serving# python3 paddledet_grpc_client.py\r\ntm: name: \"INPUT\"\r\ndatatype: \"UINT8\"\r\nshape: -1\r\nshape: -1\r\nshape: -1\r\nshape: 3\r\n\r\nTraceback (most recent call last):\r\n  File \"paddledet_grpc_client.py\", line 103, in <module>\r\n    result = runner.Run([im, ])\r\n  File \"paddledet_grpc_client.py\", line 73, in Run\r\n    results = self._client.infer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 1469, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 75, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble 'ppdet', Failed to process the request(s) for model instance 'preprocess_0', message: Stub process is not healthy.\r\n```\r\n\r\nvisualDL 日志报\r\n```\r\n[ERROR] fastdeploy/pybind/main.cc(142)::PyArrayToCvMat\tRequire rank of array to be 3 with HWC format while converting it to cv::Mat.\r\nE0509 09:31:38.142933 1099 python.cc:1942] Stub process is unhealthy and it will be restarted.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\u0000\r\n```\r\n\r\n",
        "state": "open",
        "user": "witcom",
        "closed_by": null,
        "created_at": "2023-05-09T09:47:00+00:00",
        "updated_at": "2023-05-09T09:47:00+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1916,
        "title": "GPU  部署 OCR fastdeploy serving  docker容器后启动服务端失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【编译命令】参考 https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： Nvidia GPU Tesla P4，  CUDA Version: 11.6，  CUDNN 8.5\r\n- 【编译语言】：Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 按照网页上的步骤，先拉取docker镜像，然后安装docker镜像，进入容器，一切顺利，已按照要求调整了下载模型文件的路径，并且安装了libgl1\r\n- 启动服务端时无法正常启动，进入容器和对应的路径后运行fastdeployserver --model-repository=/models，显示：\r\n\r\nI0508 08:12:59.173877 147 metrics.cc:298] Collecting metrics for GPU 0: Tesla P4\r\nI0508 08:12:59.324578 147 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7efd90000000' with size 268435456\r\nI0508 08:12:59.325028 147 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0508 08:12:59.325963 147 tritonserver.cc:1920]\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                            |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                           |\r\n| server_version                   | 2.15.0                                                                           |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) sch |\r\n|                                  | edule_policy model_configuration system_shared_memory cuda_shared_memory binary_ |\r\n|                                  | tensor_data statistics                                                           |\r\n| model_repository_path[0]         | /models                                                                          |\r\n| model_control_mode               | MODE_NONE                                                                        |\r\n| strict_model_config              | 1                                                                                |\r\n| rate_limit                       | OFF                                                                              |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                        |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                         |\r\n| response_cache_byte_size         | 0                                                                                |\r\n| min_supported_compute_capability | 6.0                                                                              |\r\n| strict_readiness                 | 1                                                                                |\r\n| exit_timeout                     | 30                                                                               |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n\r\nI0508 08:12:59.326010 147 server.cc:249] No server context available. Exiting immediately.\r\nerror: creating server: Internal - failed to stat file /models\r\n\r\n想问一下这是哪里出了问题\r\n",
        "state": "closed",
        "user": "0Lisixian0",
        "closed_by": "0Lisixian0",
        "created_at": "2023-05-08T09:14:13+00:00",
        "updated_at": "2023-05-09T01:02:43+00:00",
        "closed_at": "2023-05-09T01:02:43+00:00",
        "comments_count": [
            "0Lisixian0"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1917,
        "title": "rk3588编译后报错FastDeploy initalized failed！",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】：\r\n              export ENABLE_RKNPU2_BACKEND=ON\r\n              export ENABLE_VISION=ON\r\n              export RKNN2_TARGET_SOC=RK3588\r\n              python3 setup.py build\r\n              python3 setup.py bdist_wheel\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】：rk3588\r\n- 【编译语言】： python3.8 / python3.9\r\n\r\n## 问题日志及出现问题的操作流程  （pytho3.8\\3.9都编译过，都报这个错误，但是用https://wiki.t-firefly.com/zh_CN/Core-3588J/usage_fastdeploy.html   里面给的预编译包是不会报错的，不过这是老版本的没有rkyolo）\r\n   cd dist\r\n   pip3 install fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl\r\n  安装了这个编译好的包之后，我进入python导入fastdeploy报错:FastDeploy initalized failed!\r\n\r\n![5ee48918d44ef90e6ccf5cb454f32b7](https://user-images.githubusercontent.com/117901756/236792478-8e9618c1-522b-4f2a-a583-0e3ed64f032d.jpg)\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "376428152",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-08T09:49:29+00:00",
        "updated_at": "2024-07-09T06:40:37+00:00",
        "closed_at": "2024-07-09T06:40:37+00:00",
        "comments_count": [
            "zg651413411"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1919,
        "title": "jetson orin中编译报错",
        "body": "编译python时按照文档的编译步骤，报错，是什么原因呢\r\n![image](https://user-images.githubusercontent.com/26866665/237053295-66e452bb-d3a0-4f01-b15a-6ad3b7de06e2.png)\r\n",
        "state": "open",
        "user": "kankanjiuzou123",
        "closed_by": null,
        "created_at": "2023-05-09T09:21:28+00:00",
        "updated_at": "2023-05-11T05:10:35+00:00",
        "closed_at": null,
        "comments_count": [
            "kankanjiuzou123",
            "elecfier",
            "kankanjiuzou123",
            "elecfier",
            "kankanjiuzou123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1921,
        "title": "能不能给个java调用fastdeploy服务化serving的示例呀，grpc或者http的都可以晒，这个也莫有接口文档，脑袋有点疼呀",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n",
        "state": "closed",
        "user": "xiac72145",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-09T09:56:25+00:00",
        "updated_at": "2024-06-18T06:41:09+00:00",
        "closed_at": "2024-06-18T06:41:09+00:00",
        "comments_count": [
            "polarisunny",
            "sniperking1234"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1923,
        "title": "GPU推理内存持续增加",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-gpu-1.0.6\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 1660Ti， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ \r\n\r\n系统开启GPU推理，模型仅创建一次，定时循环3分钟GPU推理一次，但是<font color =red>系统内存一直在持续增加。</font>\r\n\r\n代码如下：\r\n\r\n```cpp\r\n\r\nfastdeploy::vision::detection::YOLOv3 *model = nullptr;\r\n\r\nvoid CreateModel(){\r\n\tstring model_file =  \"/model/yolov3_darknet53_270e_voc/model.pdmodel\";\r\n\tstring params_file =\"/model/yolov3_darknet53_270e_voc7/model.pdiparams\";\r\n\tstring config_file = \"/model/yolov3_darknet53_270e_voc/infer_cfg.yml\";\r\n\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\tif (model == nullptr)\r\n\t{\r\n\t\tmodel = new fastdeploy::vision::detection::YOLOv3(model_file, params_file, config_file, option);\r\n\t\tif (!model ->Initialized())\r\n\t\t{\r\n\t\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\t\treturn;\r\n\t\t}\r\n        }\r\n}\r\n\r\nvoid ProcessData(const std::string& image_file){\r\n  auto im = cv::imread(image_file);\r\n  fastdeploy::vision::DetectionResult res;\r\n  if (!model.Predict(&im, &res)) {\r\n    std::cerr << \"Failed to predict.\" << std::endl;\r\n    return;\r\n  }\r\n  std::cout << res.Str() << std::endl;\r\n\r\n  auto vis_im = fastdeploy::vision::VisDetection(im, res);\r\n  cv::imwrite(\"vis_result.jpg\", vis_im);\r\n  std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n```",
        "state": "closed",
        "user": "chccc1994",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-10T01:42:15+00:00",
        "updated_at": "2024-06-25T06:41:01+00:00",
        "closed_at": "2024-06-25T06:41:01+00:00",
        "comments_count": [
            "DefTruth",
            "kankanjiuzou123",
            "chccc1994",
            "chccc1994",
            "ytzhang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1922,
        "title": "Jetson执行目标检测模型PPYOLOE测试代码报错libfastdeploy.so: undefined reference to `paddle2onnx::Export",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.6\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Jetson 4.6.1， CUDA 10.2 CUDNN 8.2.1.32\r\n- 【编译语言】： C++ \r\n\r\nC++部署\r\n确认开发环境已准备FastDeploy C++部署库，参考[FastDeploy安装](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install)安装预编译的FastDeploy，或根据自己需求进行编译安装。\r\n\r\n本文档以PaddleDetection目标检测模型PPYOLOE为例展示CPU上的推理示例：\r\n假设当前目录已经准备好infer_demo.cc和CMakeLists.txt两个文件，即可进行编译\r\n\r\nLinux & Mac\r\n打开命令行终端，进入infer_demo.cc和CmakeLists.txt所在的目录，执行如下命令\r\n\r\nmkdir build & cd build\r\ncmake ..\r\nmake -j\r\n执行make -j时候，报错\r\n(fastdeploy) nvidia@nx:~/FastDeploy/test_demo/build$ make -j\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer_demo.cc.o\r\n[100%] Linking CXX executable infer_demo\r\n/home/nvidia/FastDeploy/build/installed_fastdeploy/lib/libfastdeploy.so: undefined reference to `paddle2onnx::Export(void const*, long, void const*, long, char**, int*, int, bool, bool, bool, bool, bool, paddle2onnx::CustomOp*, int, char const*, char**, int*, char const*, bool*, bool)'\r\n/home/nvidia/FastDeploy/build/installed_fastdeploy/lib/libfastdeploy.so: undefined reference to `paddle2onnx::ConvertFP32ToFP16(char const*, int, char**, int*)'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/infer_demo.dir/build.make:119: recipe for target 'infer_demo' failed\r\nmake[2]: *** [infer_demo] Error 1\r\nCMakeFiles/Makefile2:109: recipe for target 'CMakeFiles/infer_demo.dir/all' failed\r\nmake[1]: *** [CMakeFiles/infer_demo.dir/all] Error 2\r\nMakefile:90: recipe for target 'all' failed\r\nmake: *** [all] Error 2",
        "state": "open",
        "user": "CandayLT",
        "closed_by": null,
        "created_at": "2023-05-09T10:05:05+00:00",
        "updated_at": "2023-05-10T03:40:15+00:00",
        "closed_at": null,
        "comments_count": [
            "CandayLT"
        ],
        "labels": [
            "Linux aarch64",
            "Jetson"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1924,
        "title": "使用fastdeploy安卓库时编译报错",
        "body": "使用fastdeploy安卓库时编译报错，如图：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/2edc530e-f9ab-456e-ba02-b0c406b733fd)\r\n\r\n",
        "state": "closed",
        "user": "WilliamQf-AI",
        "closed_by": "WilliamQf-AI",
        "created_at": "2023-05-10T02:40:54+00:00",
        "updated_at": "2023-05-10T02:54:26+00:00",
        "closed_at": "2023-05-10T02:54:26+00:00",
        "comments_count": [
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1930,
        "title": "编译出现 subprocess.check_call  failed",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\nexport WITH_KUNLUNXIN=ON\r\nexport WITH_GPU=OFF\r\nexport ENABLE_ORT_BACKEND=OFF\r\nexport ENABLE_PADDLE_BACKEND=OFF # 这里需要关闭PADDLE_BACKEND, 不然FD会依赖PaddlePaddle\r\nexport ENABLE_VISION=OFF\r\nexport PADDLELITE_URL=http://bjdd-isa-ai-chip1.bjdd:8020/pdl-0510-3.tgz\r\n#export PADDLELITE_URL=http://bjdd-isa-ai-chip1.bjdd:8018/lite0426-sw.tgz\r\nexport PATH=/opt/compiler/gcc-8.2/bin:$PATH\r\n\r\npython setup.py build\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "open",
        "user": "linkk08",
        "closed_by": null,
        "created_at": "2023-05-11T09:41:36+00:00",
        "updated_at": "2023-05-11T09:41:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1926,
        "title": "RuntimeError: Can't set input blob with name: x, because model input (shape={?,3,32,?}) and blob (shape=(6.3.48.320)) are incompatible",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-1.0.0\r\n- 【编译命令】\r\n- 【系统平台】: win10\r\n- 【硬件】：  Nvidia GPU 3080TI， CUDA 11.4 CUDNN 8.1\r\n- 【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n\r\n执行：examples\\vision\\ocr\\PP-OCR\\cpu-gpu\\python\\infer.py\r\n`python infer.py --det_model ch_ppocr_server_v2.0_det_infer --rec_model ch_ppocr_server_v2.0_rec_infer --rec_label_file ppocr_keys_v1.txt --image 23.png --device cpu --backend openvino`\r\n\r\n`\r\n(pytorch-gpu) E:\\program\\FastDeploy-develop\\FastDeploy-develop\\examples\\vision\\ocr\\PP-OCR\\cpu-gpu\\python>python infer.py --det_model ch_ppocr_server_v2.0_det_infer --rec_model ch_ppocr_server_v2.0_rec_infer --rec_label_file ppocr_keys_v1.txt --image 23.png --device cpu --backe\r\nnd openvino\r\nC:\\Users\\Administrator\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\r\nC:\\Users\\Administrator\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\r\nC:\\Users\\Administrator\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\r\n  warnings.warn(\"loaded more than 1 DLL from .libs:\"\r\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::fastdeploy::OpenVINOBackend::InitFromPaddle     number of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::fastdeploy::OpenVINOBackend::InitFromPaddle     affinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::fastdeploy::OpenVINOBackend::InitFromPaddle     Compile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::fastdeploy::Runtime::CreateOpenVINOBackend   Runtime initialized with Backend::OPENVINO in Device::CPU.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::fastdeploy::OpenVINOBackend::InitFromPaddle     number of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::fastdeploy::OpenVINOBackend::InitFromPaddle     affinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::fastdeploy::OpenVINOBackend::InitFromPaddle     Compile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::fastdeploy::Runtime::CreateOpenVINOBackend   Runtime initialized with Backend::OPENVINO in Device::CPU.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 132, in <module>\r\n    result = ppocr_v3.predict(im)\r\n  File \"C:\\Users\\Administrator\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\fastdeploy\\vision\\ocr\\ppocr\\__init__.py\", line 702, in predict\r\n    return self.system_.predict(input_image)\r\nRuntimeError: Can't set input blob with name: x, because model input (shape={?,3,32,?}) and blob (shape=(6.3.48.320)) are incompatible\r\n\r\n`\r\n\r\n",
        "state": "open",
        "user": "JackonLiu",
        "closed_by": null,
        "created_at": "2023-05-10T08:18:19+00:00",
        "updated_at": "2023-05-12T01:42:37+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "JackonLiu",
            "ChaoII",
            "JackonLiu",
            "ChaoII",
            "JackonLiu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1928,
        "title": "ocr识别出来的结果为乱码",
        "body": "windows环境下ocr识别出来的结果为乱码，模型为 ch_PP-OCRv3_，已经检查并替换过ppocr_keys_v1.txt\r\n如下图：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/e520aa4b-cd2a-4ddc-86df-7750278e6125)\r\n",
        "state": "closed",
        "user": "WilliamQf-AI",
        "closed_by": "WilliamQf-AI",
        "created_at": "2023-05-11T03:38:23+00:00",
        "updated_at": "2023-05-11T07:07:24+00:00",
        "closed_at": "2023-05-11T07:07:23+00:00",
        "comments_count": [
            "WilliamQf-AI",
            "DefTruth",
            "DefTruth",
            "WilliamQf-AI",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1931,
        "title": "PaddleOCR服务化部署仅能识别样例图片，其他任何图片都报错无法正常识别",
        "body": "\r\n- 【FastDeploy版本】fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【编译命令】参考(https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving)\r\n- 【系统平台】: Linux x64\r\n- 【硬件】：  Nvidia GPU Tesla P4， CUDA 11.6 CUDNN 8.5\r\n- 【编译语言】：Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【无法正常识别样例以外的图片】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认可以正确执行\r\n- - 按照教程，服务端运行后，在客户端运行python3 client.py，对于样例图片12.jpg可以正常识别，识别结果：\r\n-(ocr) [topsci@localhost fastdeploy_serving]$ python3 client.py\r\ntext= 上海斯格威铂尔大酒店   score= 0.9804123044013977   bbox= [ 42 413 483 391 484 428  43 450]\r\ntext= 打浦路15号   score= 0.9606184363365173   bbox= [187 456 399 448 400 480 188 488]\r\ntext= 绿洲仕格维花园公寓   score= 0.9806857705116272   bbox= [ 23 507 513 488 515 529  24 548]\r\ntext= 打浦路252935号   score= 0.9498506784439087   bbox= [ 74 553 427 542 428 571  75 582]\r\n\r\n-- 如果更改client.py文件中的图片，例如将im = cv2.imread(\"12.jpg\")改为im = cv2.imread(\"002.jpg\")，“002.jpg”为另一张图片，则会报错，报错内容：\r\n-(ocr) [topsci@localhost fastdeploy_serving]$ python3 client.py\r\nTraceback (most recent call last):\r\n  File \"/home/topsci/code/OCR/fastdeploy_serving/client.py\", line 100, in <module>\r\n    result = runner.Run([im, ])\r\n  File \"/home/topsci/code/OCR/fastdeploy_serving/client.py\", line 71, in Run\r\n    results = self._client.infer(\r\n  File \"/home/topsci/anaconda3/envs/ocr/lib/python3.9/site-packages/tritonclient/grpc/__init__.py\", line 1446, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/home/topsci/anaconda3/envs/ocr/lib/python3.9/site-packages/tritonclient/grpc/__init__.py\", line 76, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble 'pp_ocr', Failed to process the request(s) for model instance 'det_postprocess_0', message: TritonModelException: in ensemble 'cls_pp', softmax_0.tmp_0: failed to perform CUDA copy: invalid argument\r\n\r\nAt:\r\n  /ocr_serving/code/OCR/fastdeploy_serving/models/det_postprocess/1/model.py(176): execute\r\n\r\n想问一下这是怎么回事。\r\n",
        "state": "open",
        "user": "0Lisixian0",
        "closed_by": null,
        "created_at": "2023-05-11T10:24:24+00:00",
        "updated_at": "2023-05-12T02:35:08+00:00",
        "closed_at": null,
        "comments_count": [
            "0Lisixian0"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1927,
        "title": "ort、paddle、openvino、pplite在paddleocr训练好的模型中哪个推理性能最好？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】： fastdeploy-gpu-1.0.0\r\n【编译命令】\r\n【系统平台】: win10\r\n【硬件】： Nvidia GPU 3080TI， CUDA 11.4 CUDNN 8.1\r\n【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\nort、paddle、openvino、pplite在paddleocr训练好的模型中哪个推理性能最好？\r\n\r\n模型用的是这两个\r\ndet_model = 'ch_ppocr_server_v2.0_det_infer'\r\nrec_model = 'ch_ppocr_server_v2.0_rec_infer'",
        "state": "closed",
        "user": "JackonLiu",
        "closed_by": "JackonLiu",
        "created_at": "2023-05-10T08:44:41+00:00",
        "updated_at": "2023-05-12T01:40:40+00:00",
        "closed_at": "2023-05-12T01:40:40+00:00",
        "comments_count": [
            "JackonLiu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1932,
        "title": "ocr 里面pptrt 与 trt 有什么区别吗？",
        "body": "ocr 里面pptrt 与 trt 有什么区别吗？底层实现有什么区别吗？那个性能更好一点",
        "state": "open",
        "user": "intjun",
        "closed_by": null,
        "created_at": "2023-05-11T10:47:52+00:00",
        "updated_at": "2023-05-11T10:47:52+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1935,
        "title": "PPLiteSeg在GPU上要怎么使用INT8加速",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Windows x64 (arm or intel)\r\nHardware: e.g. Nvidia GPU 3060  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPPLiteSeg在GPU上要怎么使用INT8加速，只看到了PaddleLite支持int8，在CPU或者GPU上要怎么使用int8加速呢\r\n",
        "state": "open",
        "user": "zyz207",
        "closed_by": null,
        "created_at": "2023-05-12T01:35:18+00:00",
        "updated_at": "2023-05-12T01:35:18+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1938,
        "title": "RV1126上按教程部署 PP-LiteSeg 会提示 Malloc error",
        "body": "按照教程编译的 [fastDeploy](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rv1126.md) 和 [PP-LiteSeg](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/segmentation/paddleseg/semantic_segmentation/rockchip/rv1126/cpp)\r\n\r\n然后到板子上运行会报错：\r\n\r\n[3  4/20 17:35:11. 74 ...r/src/driver/verisilicon_timvx/engine.cc:193 Build] Build the tim-vx graph success.\r\nD [_check_swapped_tensors:114]Check swapped tensors\r\n[3  4/20 17:35:11.104 ...r/src/driver/verisilicon_timvx/engine.cc:260 Execute] Process cost 13721 us\r\n[3  4/20 17:35:11.108 ...le-Lite/lite/kernels/nnadapter/engine.cc:248 Execute] Process cost 33372 us\r\n[F  4/20 17:35:11.113 ...ite/lite/backends/host/target_wrapper.cc:33 Malloc] Check failed: p: Error occurred in TargetWrapper::Malloc period: no enough for mallocing 159383552 bytes.\r\nterminate called after throwing an instance of 'paddle::lite::PaddleLiteException'\r\n  what():  Paddle-Lite C++ Exception: \r\n[F  4/20 17:35:11.113 ...ite/lite/backends/host/target_wrapper.cc:33 Malloc] Check failed: p: Error occurred in TargetWrapper::Malloc period: no enough for mallocing 159383552 bytes.\r\n\r\n前面的 log 过长，都是模型信息就补贴上来了。\r\n\r\n同时我试了按照教程部署 PaddleClas 和 yolov5 都没有问题。\r\n\r\n报错原因是资源不够，但是这不是官方例程么？板子也没有运行其他程序怎么会资源不够呢，可以的话请大佬们指教一下\r\n\r\n\r\n",
        "state": "open",
        "user": "yiyang19",
        "closed_by": null,
        "created_at": "2023-05-12T10:03:05+00:00",
        "updated_at": "2023-05-12T10:04:24+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1941,
        "title": "系统部署FastDeploy，长时间运行 提示opencv错误 0xc0000005",
        "body": "## 环境\r\n【FastDeploy版本】： fastdeploy-win-gpu-1.0.6\r\n【系统平台】: Windows x64(Windows10)\r\n【硬件】： Nvidia GPU 1660Ti， CUDA 11.2 CUDNN 8.3\r\n【编译语言】： C++\r\n\r\n系统开启GPU推理，模型仅创建一次，定时循环3分钟GPU推理一次,。长时间运行，系统崩溃，查看`window10`系统错误日志，显示 错误模块名称: opencv_world3416.dll，异常代码: 0xc0000005.\r\n\r\n```\r\n错误模块名称: opencv_world3416.dll，版本: 3.4.16.0，时间戳: 0x6160c391\r\n异常代码: 0xc0000005\r\n错误偏移量: 0x000000000192295f\r\n错误进程 ID: 0x1efc \r\n```\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/68001817/662a2342-136e-44b8-83b4-7880c5df66b9)\r\n\r\n\r\n+ 代码如下：\r\n```c++\r\nfastdeploy::vision::detection::YOLOv3 *model = nullptr;\r\n\r\nvoid CreateModel(){\r\n\tstring model_file =  \"/model/yolov3_darknet53_270e_voc/model.pdmodel\";\r\n\tstring params_file =\"/model/yolov3_darknet53_270e_voc7/model.pdiparams\";\r\n\tstring config_file = \"/model/yolov3_darknet53_270e_voc/infer_cfg.yml\";\r\n\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseGpu();\r\n\tif (model == nullptr)\r\n\t{\r\n\t\tmodel = new fastdeploy::vision::detection::YOLOv3(model_file, params_file, config_file, option);\r\n\t\tif (!model ->Initialized())\r\n\t\t{\r\n\t\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\t\treturn;\r\n\t\t}\r\n        }\r\n}\r\n\r\nvoid ProcessData(const std::string& image_file){\r\n  auto im = cv::imread(image_file);\r\n  fastdeploy::vision::DetectionResult res;\r\n  if (!model.Predict(&im, &res)) {\r\n    std::cerr << \"Failed to predict.\" << std::endl;\r\n    return;\r\n  }\r\n  std::cout << res.Str() << std::endl;\r\n\r\n  auto vis_im = fastdeploy::vision::VisDetection(im, res);\r\n  cv::imwrite(\"vis_result.jpg\", vis_im);\r\n  std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\n// 析构函数\r\ndelete model;\r\n```",
        "state": "open",
        "user": "chccc1994",
        "closed_by": null,
        "created_at": "2023-05-13T08:21:25+00:00",
        "updated_at": "2023-05-13T08:23:26+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1940,
        "title": "vs2019调用fastdeploy-cpu库生成的程序，在win7下运行报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\nvs2019调用fastdeploy-cpu库生成的程序，在win7下运行报错，请问如何解决呢\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "open",
        "user": "longge-vision",
        "closed_by": null,
        "created_at": "2023-05-12T12:12:36+00:00",
        "updated_at": "2023-05-12T12:12:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1942,
        "title": "项目使用c++14， fastdeploy默认使用c++11标准，如何解决冲突？",
        "body": "fastdeploy默认使用c++11标准\r\n```\r\n# Set C++11 as standard for the whole project\r\nif(NOT MSVC)\r\n  set(CMAKE_CXX_STANDARD 11)\r\n  set(CMAKE_CXX_FLAGS \"-Wno-format -g0 -O3\")\r\n  if(NEED_ABI0)\r\n    add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\r\n  else()\r\n    add_definitions(-D_GLIBCXX_USE_CXX11_ABI=1)\r\n  endif()\r\nendif(NOT MSVC)\r\n```\r\n\r\n如题，实际使用会报一些语法错误，fastdeploy是否可以使用11以上的编译器版本14或17？",
        "state": "open",
        "user": "wanggao1990",
        "closed_by": null,
        "created_at": "2023-05-15T02:35:17+00:00",
        "updated_at": "2025-04-17T13:29:05+00:00",
        "closed_at": null,
        "comments_count": [
            "minyi0"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1945,
        "title": "【提供的ONNX转换RKNN例子无法执行】ONNX转换RKNN错误",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 使用官网提供的C++在rk3588上面编译 fastdeploy 测试没有问题\r\n- 【编译命令】 使用官网提供的C++在rk3588上面编译 fastdeploy 测试没有问题\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： RK3588\r\n- 【编译语言】： Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n参照下面的例子尝试ONNX模型转换RKNN模型（模型对象 picodet_s_416_coco_lcnet_unquantized），但是出现错误\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md\r\n已经确认  固定shape到 [1,3,416,416] 成功\r\npaddle2onnx命令\r\npaddle2onnx --model_dir /home/seikoist-qiu/total_test_for_emotion/PPHGNet_small_v6_infer/picodet_s_416_coco_lcnet \\\r\n--model_filename model.pdmodel \\\r\n--params_filename model.pdiparams \\\r\n--opset_version 12 \\\r\n--save_file model.onnx --enable_dev_version True \\\r\n--enable_onnx_checker True\r\n\r\n## 错误日志\r\n···\r\n(rk3588) topeet@ubuntu-vmware16:~/ml/rknn-toolkit2/examples/onnx/yolov5$ python onnx_rknn.py \r\nW __init__: rknn-toolkit2 version: 1.4.0-22dcfef4\r\n--> Config model\r\nW config: 'target_platform' is None, use rk3566 as default, Please set according to the actual platform!\r\ndone\r\n--> Loading model\r\nW load_onnx: The config.mean_values is None, zeros will be set for input 1!\r\nW load_onnx: The config.std_values is None, ones will be set for input 1!\r\ndone\r\n--> Building model\r\nI base_optimize ...\r\nI base_optimize done.\r\nI \r\nI fold_constant ...\r\nE build: Catch exception when building RKNN model!\r\nE build: Traceback (most recent call last):\r\nE build:   File \"rknn/api/rknn_base.py\", line 1541, in rknn.api.rknn_base.RKNNBase.build\r\nE build:   File \"rknn/api/graph_optimizer.py\", line 627, in rknn.api.graph_optimizer.GraphOptimizer.fold_constant\r\nE build:   File \"rknn/api/session.py\", line 28, in rknn.api.session.Session.__init__\r\nE build:   File \"rknn/api/session.py\", line 71, in rknn.api.session.Session.sess_build\r\nE build:   File \"/home/topeet/anaconda3/envs/rk3588/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 335, in __init__\r\nE build:     self._create_inference_session(providers, provider_options, disabled_optimizers)\r\nE build:   File \"/home/topeet/anaconda3/envs/rk3588/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 370, in _create_inference_session\r\nE build:     sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)\r\nE build: onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /onnxruntime_src/onnxruntime/core/graph/model.cc:129 onnxruntime::Model::Model(onnx::ModelProto&&, const PathString&, const IOnnxRuntimeOpSchemaRegistryList*, const onnxruntime::logging::Logger&, bool) Unsupported model IR version: 9, max supported IR version: 8\r\nBuild model failed!\r\n···",
        "state": "closed",
        "user": "qiulongquan",
        "closed_by": "qiulongquan",
        "created_at": "2023-05-15T15:11:58+00:00",
        "updated_at": "2023-05-17T02:07:54+00:00",
        "closed_at": "2023-05-17T02:07:54+00:00",
        "comments_count": [
            "qiulongquan",
            "qiulongquan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1947,
        "title": "执行完成没有报告返回Runtime(ms): 0.0",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) / Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程  \r\n- 附上详细的问题日志有助于快速定位分析\r\n只有日志\r\n执行 性能测试\r\n[Benchmark Testing](https://github.com/PaddlePaddle/FastDeploy/blob/develop/benchmark)\r\n~~~shell\r\npython benchmark_ppcls.py --model MobileNetV1_x0_25_infer --image ILSVRC2012_val_00000010.jpeg --cpu_num_thread 8 --iter_num 2000 --backend paddle\r\n~~~\r\n\r\n\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0516 16:18:09.791821 43208 analysis_config.cc:972] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::CPU.\r\nRuntime(ms): 0.0 \r\n\r\n\r\n",
        "state": "closed",
        "user": "verseboys",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-16T08:25:03+00:00",
        "updated_at": "2024-05-21T06:40:44+00:00",
        "closed_at": "2024-05-21T06:40:44+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1951,
        "title": "用FastDeploy完全按照官方流程转换ppyoloe_plus_crn_s_80e_coco.onnx成rknn,转成功后推理框不正确",
        "body": "python3 infer.py --model_file ./ppyoloe_plus_crns_crn_s_80e_coco/ppyoloe_plus_crn_s_80e_coco_rk3588_quantized.rknn --config_file ./ppyoloe_plus_crn_s_80e_coco/infer_cfg.yml --image 000000014439.jpg\r\n\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=image, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=p2o.Mul.157, n_dims=4, dims=[1, 8400, 4, 1], n_elems=33600, size=33600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-72, scale=4.541211, pass_through=0\r\nindex=1, name=p2o.Concat.29, n_dims=4, dims=[1, 80, 8400, 1], n_elems=672000, size=672000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003515, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(341)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\ninfer:  0.1364881992340088\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n-22.706053,-8.599918, 404.167755, 260.864166, 0.868105, 0\r\n-90.824211,-63.066059, 331.508392, 203.531387, 0.829444, 0\r\n168.024796,-45.866226, 903.700928, 435.729187, 0.790784, 0\r\n113.530266,65.932701, 113.530266, 68.799347, 0.776726, 0\r\n172.566010,-28.666391, 594.898621, 232.197784, 0.748609, 0\r\n386.002899,60.199425, 386.002899, 60.199425, 0.745094, 0\r\n386.002899,65.932701, 386.002899, 68.799347, 0.734550, 0\r\n390.544128,8.599918, 808.335510, 269.464081, 0.724007, 0\r\n113.530266,71.665977, 113.530266, 71.665977, 0.695890, 0\r\n336.049591,51.599506, 336.049591, 51.599506, 0.685346, 0\r\n354.214417,65.932701, 354.214417, 68.799347, 0.685346, 0\r\n372.379272,83.132538, 372.379272, 85.999176, 0.685346, 0\r\n336.049591,57.332783, 336.049591, 57.332783, 0.678317, 0\r\n386.002899,57.332783, 386.002899, 57.332783, 0.671288, 0\r\n354.214417,71.665977, 354.214417, 71.665977, 0.657229, 0\r\n336.049591,60.199425, 336.049591, 60.199425, 0.618569, 0\r\n168.024796,57.332783, 172.566010, 57.332783, 0.611540, 0\r\n326.967163,57.332783, 331.508392, 57.332783, 0.611540, 0\r\n113.530266,60.199425, 113.530266, 60.199425, 0.611540, 0\r\n372.379272,77.399261, 372.379272, 74.532623, 0.611540, 0\r\n168.024796,51.599506, 172.566010, 51.599506, 0.600996, 0\r\n372.379272,91.732452, 372.379272, 91.732452, 0.593967, 0\r\n354.214417,60.199425, 354.214417, 60.199425, 0.572879, 0\r\n326.967163,51.599506, 331.508392, 51.599506, 0.551791, 0\r\n386.002899,51.599506, 386.002899, 51.599506, 0.544762, 0\r\n326.967163,37.266312, 336.049591, 83.132538, 0.537733, 0\r\n186.189636,45.866226, 199.813263, 60.199425, 0.516645, 0\r\n195.272064,51.599506, 195.272064, 51.599506, 0.506102, 0\r\n-122.612686,8.599918, 249.766586, 269.464081, 0.499072, 0\r\n594.898621,149.065247, 594.898621, 149.065247, 0.499072, 0\r\n195.272064,57.332783, 195.272064, 57.332783, 0.495558, 0\r\n376.920471,83.132538, 372.379272, 85.999176, 0.495558, 0\r\n358.755646,65.932701, 363.296844, 68.799347, 0.470956, 0\r\n40.870895,131.865402, 40.870895, 131.865402, 0.470956, 0\r\n113.530266,77.399261, 113.530266, 74.532623, 0.463927, 0\r\n40.870895,126.132118, 40.870895, 126.132118, 0.463927, 0\r\n186.189636,51.599506, 181.648422, 51.599506, 0.453383, 0\r\n354.214417,77.399261, 354.214417, 74.532623, 0.453383, 0\r\n458.662262,14.333196, 467.744690, 34.399673, 0.446354, 0\r\n336.049591,65.932701, 336.049591, 68.799347, 0.446354, 0\r\n594.898621,143.331955, 594.898621, 143.331955, 0.446354, 0\r\n326.967163,60.199425, 331.508392, 60.199425, 0.442839, 0\r\n358.755646,71.665977, 363.296844, 71.665977, 0.421751, 0\r\n358.755646,60.199425, 363.296844, 60.199425, 0.404178, 0\r\n277.013855,123.265488, 277.013855, 123.265488, 0.397149, 0\r\n186.189636,57.332783, 181.648422, 57.332783, 0.383091, 0\r\n49.953316,126.132118, 45.412106, 126.132118, 0.383091, 0\r\n386.002899,45.866226, 386.002899, 48.732868, 0.376062, 0\r\n122.612686,65.932701, 118.071480, 68.799347, 0.376062, 0\r\n376.920471,80.265900, 372.379272, 83.132538, 0.376062, 0\r\n376.920471,91.732452, 372.379272, 91.732452, 0.376062, 0\r\n36.329685,131.865402, 36.329685, 131.865402, 0.376062, 0\r\n36.329685,126.132118, 36.329685, 126.132118, 0.369032, 0\r\n603.981018,143.331955, 599.439819, 143.331955, 0.369032, 0\r\n326.967163,37.266312, 345.132019, 77.399261, 0.362003, 0\r\n122.612686,71.665977, 118.071480, 71.665977, 0.362003, 0\r\n177.107208,117.532211, 177.107208, 114.665565, 0.362003, 0\r\n49.953316,131.865402, 45.412106, 131.865402, 0.362003, 0\r\n40.870895,134.732040, 40.870895, 134.732040, 0.362003, 0\r\n372.379272,100.332367, 372.379272, 97.465736, 0.354974, 0\r\n386.002899,71.665977, 386.002899, 71.665977, 0.351460, 0\r\n-213.436905,-111.798927, 544.945251, 366.929810, 0.340916, 0\r\n376.920471,57.332783, 372.379272, 57.332783, 0.340916, 0\r\n372.379272,71.665977, 372.379272, 71.665977, 0.333887, 0\r\n277.013855,117.532211, 277.013855, 114.665565, 0.323343, 0\r\n49.953316,134.732040, 45.412106, 134.732040, 0.323343, 0\r\n603.981018,134.732040, 599.439819, 134.732040, 0.323343, 0\r\n376.920471,60.199425, 372.379272, 60.199425, 0.316314, 0\r\n594.898621,134.732040, 594.898621, 134.732040, 0.316314, 0\r\n254.307800,88.865814, 672.099182, 352.596619, 0.302255, 0\r\n72.659370,169.131714, 77.200584, 169.131714, 0.390120, 24\r\n81.741791,169.131714, 81.741791, 169.131714, 0.390120, 24\r\n63.576950,169.131714, 63.576950, 169.131714, 0.376062, 24\r\n72.659370,160.531799, 77.200584, 160.531799, 0.362003, 24\r\n63.576950,160.531799, 63.576950, 160.531799, 0.351460, 24\r\n81.741791,160.531799, 81.741791, 160.531799, 0.351460, 24\r\n63.576950,143.331955, 63.576950, 143.331955, 0.333887, 24\r\n63.576950,149.065247, 63.576950, 149.065247, 0.333887, 24\r\n13.623632,169.131714, 13.623632, 169.131714, 0.316314, 24\r\n72.659370,149.065247, 77.200584, 149.065247, 0.309284, 24\r\n72.659370,157.665161, 77.200584, 157.665161, 0.309284, 24\r\n72.659370,143.331955, 77.200584, 143.331955, 0.302255, 24\r\n-59.035740,-37.266312, 699.346436, 441.462433, 0.685346, 33\r\n90.824211,131.865402, 90.824211, 131.865402, 0.572879, 56\r\n-99.906631,8.599918, 322.425964, 269.464081, 0.541248, 56\r\n90.824211,134.732040, 90.824211, 134.732040, 0.516645, 56\r\n95.365425,134.732040, 95.365425, 134.732040, 0.506102, 56\r\n95.365425,131.865402, 95.365425, 131.865402, 0.495558, 56\r\n81.741791,131.865402, 81.741791, 131.865402, 0.411208, 56\r\n104.447845,143.331955, 108.989059, 143.331955, 0.397149, 56\r\n104.447845,134.732040, 108.989059, 134.732040, 0.362003, 56\r\n104.447845,149.065247, 108.989059, 149.065247, 0.362003, 56\r\n81.741791,134.732040, 81.741791, 134.732040, 0.351460, 56\r\n113.530266,134.732040, 113.530266, 134.732040, 0.340916, 56\r\n90.824211,143.331955, 90.824211, 143.331955, 0.333887, 56\r\n113.530266,143.331955, 113.530266, 143.331955, 0.323343, 56\r\n95.365425,143.331955, 95.365425, 143.331955, 0.316314, 56\r\n113.530266,149.065247, 113.530266, 149.065247, 0.316314, 56\r\n\r\nVisualized result save in ./visualized_result.jpg\r\n",
        "state": "open",
        "user": "CachCheng",
        "closed_by": null,
        "created_at": "2023-05-17T02:15:04+00:00",
        "updated_at": "2023-05-17T02:15:04+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1946,
        "title": " multimodal stable_diffusion cpp无法编译",
        "body": "- 【FastDeploy版本】： 所有版本\r\n- 【编译命令】cmake .. -DFASTDEPLOY_INSTALL_DIR=D:\\Paddle\\compiled_fastdeploy -DTRT_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT-8.4.2.4\" -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\"\r\nmsbuild main.sln /m /p:Configuration=Release /p:Platform=x64\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】：Nvidia GPU 3080， CUDA 11.7 TensorRT-8.4.2.4\r\n- 【编译语言】： C++ vs2022\r\n\r\n## 问题说明\r\n编译fastdeploy以及其它cpp examples都没问题，只有multimodal下stable_diffusion cpp无法link。\r\n\r\n## 错误信息\r\n “D:\\project\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\main.sln”(默认目标) (1) ->\r\n       “D:\\project\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\main.vcxproj.metaproj”(默认目标) (2) ->\r\n       “D:\\project\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\main.vcxproj”(默认目标) (4) ->\r\n       (Link 目标) ->\r\n         pipeline_stable_diffusion_inpaint.obj : error LNK2019: 无法解析的外部符号 \"void __cdecl fastdeploy::function::GaussianR\r\n       andom(class std::vector<__int64,class std::allocator<__int64> > const &,struct fastdeploy::FDTensor *,enum fastd\r\n       eploy::FDDataType,float,float,int)\" (?GaussianRandom@function@fastdeploy@@YAXAEBV?$vector@_JV?$allocator@_J@std@\r\n       @@std@@PEAUFDTensor@2@W4FDDataType@2@MMH@Z)，函数 \"public: void __cdecl fastdeploy::StableDiffusionInpaintPipeline:\r\n       :Predict(class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char>\r\n        >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >\r\n       > > const &,class cv::Mat const &,class cv::Mat const &,class std::vector<struct fastdeploy::FDTensor,class std:\r\n       :allocator<struct fastdeploy::FDTensor> > *,int,int,int,float,class std::vector<class std::basic_string<char,str\r\n       uct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct\r\n        std::char_traits<char>,class std::allocator<char> > > > const &,int,float,unsigned int,struct fastdeploy::FDTen\r\n       sor const *,bool,void (__cdecl*)(int,int,struct fastdeploy::FDTensor *),int)\" (?Predict@StableDiffusionInpaintPi\r\n       peline@fastdeploy@@QEAAXAEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@\r\n       V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBVMat@cv@@1PEAV?$vector@UFDTensor@fastd\r\n       eploy@@V?$allocator@UFDTensor@fastdeploy@@@std@@@4@HHHM0HMIPEBUFDTensor@2@_NP6AXHHPEAU82@@ZH@Z) 中引用了该符号 [D:\\proj\r\n       ect\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\main.vcxproj]\r\n         D:\\project\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\Release\\main.exe : fatal error LNK1120\r\n       : 1 个无法解析的外部命令 [D:\\project\\ai\\FastDeploy\\examples\\multimodal\\stable_diffusion\\cpp\\build\\main.vcxproj]\r\n",
        "state": "closed",
        "user": "tobzk",
        "closed_by": "tobzk",
        "created_at": "2023-05-16T03:30:28+00:00",
        "updated_at": "2023-05-17T03:12:56+00:00",
        "closed_at": "2023-05-17T03:12:56+00:00",
        "comments_count": [
            "DefTruth",
            "tobzk",
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1949,
        "title": "android平台使用nb模型",
        "body": "如下图所示:\r\n官方示例代码，是否这样修改后就可以在直接使用nb模型？\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/fb10e5e6-3374-4c3f-90f8-6df340eaf48f)\r\n\r\n",
        "state": "closed",
        "user": "WilliamQf-AI",
        "closed_by": "WilliamQf-AI",
        "created_at": "2023-05-16T10:23:12+00:00",
        "updated_at": "2023-05-16T11:31:01+00:00",
        "closed_at": "2023-05-16T11:31:01+00:00",
        "comments_count": [
            "DefTruth",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1948,
        "title": "请问版面分析的返回字段DetectionResult的字段到底是什么意思？",
        "body": "如下图：\r\n问题1：label_ids被解释为“目标类别”，但是其数据类型为int32_t,请问其值具体代表的意思是什么？目前猜测是 {\"text\",  \"title\", \"figure\", \r\n    \"figure_caption\",\"table\",  \"table_caption\", \"header\", \"footer\", \"reference\", \"equation\"} 或者{\"text\", \"title\", \"list\", \"table\", \"figure\"}的 \r\n     index，但是实际测试好像对应不上。\r\n问题2：label_ids的描述对象，是否是同一个index的boxes的成员（例如：label_ids[i]描述了boxes[i]的类型）？\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/a195480a-c062-40a3-96c9-f46f93e53b64)\r\n唯一找到的文档没有看到具体说明。\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/api/vision_results/detection_result_CN.md\r\n",
        "state": "closed",
        "user": "WilliamQf-AI",
        "closed_by": "WilliamQf-AI",
        "created_at": "2023-05-16T10:05:14+00:00",
        "updated_at": "2023-05-25T01:49:48+00:00",
        "closed_at": "2023-05-25T01:49:11+00:00",
        "comments_count": [
            "WilliamQf-AI",
            "WilliamQf-AI",
            "DefTruth",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1952,
        "title": "RK3588板子上C++编译fastdeploy成功，但是import fastdeploy报错无法找到fastdeploy模块",
        "body": "## 环境\r\n【FastDeploy版本】： 使用官网提供的C++在rk3588上面编译 fastdeploy \r\n【编译命令】 使用官网提供的C++在rk3588上面编译 fastdeploy \r\n【系统平台】: Linux x64(Ubuntu 20.04)\r\n【硬件】： RK3588\r\n【编译语言】： Python3.9\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n参照下面的流程在RK3588板子上面编译fastdeploy，成功编译并执行永久配置\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/build.md\r\n永久配置\r\nsource PathToFastDeploySDK/fastdeploy_init.sh\r\nsudo cp PathToFastDeploySDK/fastdeploy_libs.conf /etc/ld.so.conf.d/\r\nsudo ldconfig\r\n\r\n但是无法在Fastdeploy目录以外正常导入fastdeploy，import fastdeploy会出现模块无法找到\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/36796264/a210318c-b068-404c-8286-f2ab072571de)\r\n\r\nFastdeploy目录下的其他目录也无法正常导入fastdeploy\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/36796264/ab43448a-64e0-4145-8f33-5b1471ac7f82)\r\n\r\n这个是什么原因 希望进行回复  谢谢",
        "state": "open",
        "user": "qiulongquan",
        "closed_by": null,
        "created_at": "2023-05-17T02:17:19+00:00",
        "updated_at": "2023-05-21T06:01:05+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "qiulongquan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1953,
        "title": "FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.2\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： AMD\r\n- 【编译语言】： Python(3.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n安装说明替换模型 使用ov 报错如下\r\npython benchmark_ppcls.py --model ResNet50_vd_old  --image 2.jpeg --cpu_num_thread 8 --iter_num 2000 --backend ov\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n\r\n",
        "state": "open",
        "user": "verseboys",
        "closed_by": null,
        "created_at": "2023-05-17T08:46:43+00:00",
        "updated_at": "2023-05-17T15:17:39+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "verseboys"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1957,
        "title": "预训练模型导出的模型识别不到任何文字",
        "body": "- 【FastDeploy版本】： fastdeploy:1.0.4-cpu-only-21.10\r\n- 【系统平台】: Ubuntu 20.04.5 LTS\r\n- 【硬件】： cpu服务器\r\n\r\n我使用fastdeploy部署了服务。但是我需要微调文本框识别的参数，就根据官方配置文件（det_mv3_db.yml）及官方的预训练模型（ch_PP-OCRv3_det_slim_distill_train）重新导出了一份模型，我把这个模型改名替换了det_runtime/1目录下的model.pdiparams、model.pdmodel文件，就识别不到任何文字了。我是搞错什么地方了吗。\r\n\r\n导出新模型的指令，除了在指令中额外添加了PostProcess.max_candidates=2参数，没有修改原配置文件\r\n```\r\npython3 tools/export_model.py -c /opt/tritonserver/PaddleOCR-dygraph/configs/det/det_mv3_db.yml -o Global.pretrained_model=/opt/tritonserver/ch_PP-OCRv3_det_slim_distill_train/best_accuracy.pdparams Global.save_inference_dir=\"/opt/tritonserver/exportmodel/202305180526\" PostProcess.max_candidates=2\r\n```\r\n导出的模型情况，看这大小应该是有读取成功预训练模型了\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/d615c21f-c206-4f61-9123-88a041c0159b)\r\n把模型替换到det_runtime/1目录下\r\n```\r\ncp /opt/tritonserver/exportmodel/202305180526/inference.pdiparams model.pdiparams\r\ncp /opt/tritonserver/exportmodel/202305180526/inference.pdmodel model.pdmodel\r\n```\r\n启动服务无报错，但是再也识别不出内容。\r\n\r\n更换回原来的model.pdiparams、model.pdmodel文件就能正常识别\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/208167ab-4cab-4a74-9b2c-b646874e7a1e)\r\n替换模型之后测试连demo client.py都识别不出内容了。什么图片都识别不出文字\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/aa3f99c0-46ab-4ac5-9f4f-50bc2253cbda)\r\n\r\n这个是替换模型之后的启动日志，不知道有没有帮助\r\n```\r\nI0517 22:04:34.448830 248 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nI0517 22:04:34.549284 248 model_repository_manager.cc:1022] loading: det_runtime:1\r\nI0517 22:04:34.658117 248 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0517 22:04:34.746633 248 fastdeploy_runtime.cc:1182] TRITONBACKEND_Initialize: fastdeploy\r\nI0517 22:04:34.746668 248 fastdeploy_runtime.cc:1191] Triton TRITONBACKEND API version: 1.6\r\nI0517 22:04:34.746677 248 fastdeploy_runtime.cc:1196] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0517 22:04:34.746686 248 fastdeploy_runtime.cc:1225] backend configuration:\r\n{}\r\nI0517 22:04:34.746813 248 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\nI0517 22:04:34.748343 248 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\nI0517 22:04:34.748880 248 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (CPU device 0)\r\nI0517 22:04:34.758445 248 model_repository_manager.cc:1022] loading: det_postprocess:1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(226)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\nI0517 22:04:34.858798 248 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0517 22:04:34.959102 248 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0517 22:04:35.050034 248 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (CPU device 0)\r\nI0517 22:04:35.059367 248 model_repository_manager.cc:1022] loading: rec_runtime:1\r\nI0517 22:04:35.070668 248 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(226)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0517 22:04:35.384974 248 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\nI0517 22:04:35.389189 248 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0517 22:04:35.916108 248 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0517 22:04:35.919931 248 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0517 22:04:36.355174 248 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\nI0517 22:04:36.356538 248 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0517 22:04:36.357830 248 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0517 22:04:36.802701 248 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0517 22:04:36.804998 248 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0517 22:04:37.230802 248 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (CPU device 0)\r\nI0517 22:04:37.231917 248 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(226)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(279)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\r\nI0517 22:04:37.618179 248 model_repository_manager.cc:1183] successfully loaded 'rec_runtime' version 1\r\nI0517 22:04:37.618546 248 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0517 22:04:37.718835 248 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0517 22:04:37.819114 248 model_repository_manager.cc:1022] loading: rec_pp:1\r\nI0517 22:04:37.919632 248 model_repository_manager.cc:1183] successfully loaded 'rec_pp' version 1\r\nI0517 22:04:37.919750 248 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0517 22:04:37.919840 248 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0517 22:04:37.920195 248 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0517 22:04:37.920245 248 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0517 22:04:37.920319 248 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| cls_pp          | 1       | READY  |\r\n| cls_runtime     | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| det_runtime     | 1       | READY  |\r\n| pp_ocr          | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n| rec_pp          | 1       | READY  |\r\n| rec_runtime     | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0517 22:04:37.920458 248 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                 |\r\n| server_version                   | 2.15.0                                                                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\r\n| model_repository_path[0]         | /opt/tritonserver/FastDeploy/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models                                                                                              |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                              |\r\n| strict_model_config              | 1                                                                                                                                                                                      |\r\n| rate_limit                       | OFF                                                                                                                                                                                    |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\r\n| response_cache_byte_size         | 0                                                                                                                                                                                      |\r\n| min_supported_compute_capability | 0.0                                                                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0517 22:04:37.922368 248 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0517 22:04:37.922825 248 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0517 22:04:37.964261 248 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\n```\r\n",
        "state": "closed",
        "user": "zhouyiminga",
        "closed_by": "zhouyiminga",
        "created_at": "2023-05-17T22:07:07+00:00",
        "updated_at": "2023-05-18T16:38:38+00:00",
        "closed_at": "2023-05-18T16:38:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1955,
        "title": "RTDERT模型转换问题。",
        "body": "RT-DERT的paddle转onnx，只能设置Use opset_version = 16 for ONNX export。但是在rknpu上，只能支持load_onnx: Unsupport onnx opset 16, need <= 12!\r\npaddle到onnx，最低16，onnx到rknn，fastdeploy最高12.怎么解决\r\n",
        "state": "open",
        "user": "Zsk747",
        "closed_by": null,
        "created_at": "2023-05-17T12:46:47+00:00",
        "updated_at": "2023-05-17T12:46:47+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1954,
        "title": "fastdeploy部署paddleClas,无法加载模型",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [说明具体的版本，如fastdeploy-linux-gpu-0.8.0](fastdeploy:1.0.4-cpu-only-21.10)\r\n- 【系统平台】: Centos7\r\n- 【硬件】： CPU\r\n- 【编译语言】： Python(3.7或3.8等）\r\n\r\nE0517 08:58:57.562967 604 model_repository_manager.cc:1890] Poll failed for model directory 'runtime': instance group runtime_0 of model runtime has kind KIND_CPU but specifies one or more GPUs\r\nE0517 08:58:57.563080 604 model_repository_manager.cc:1375] Invalid argument: ensemble paddlecls contains models that are not available: runtime\r\nI0517 08:58:57.563135 604 model_repository_manager.cc:1022] loading: postprocess:1\r\nI0517 08:58:57.663327 604 model_repository_manager.cc:1022] loading: preprocess:1\r\nI0517 08:58:57.665970 604 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: postprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 16, 'input': [{'name': 'post_input', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1000], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'post_output', 'data_type': 'TYPE_STRING', 'dims': [-1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['post_input']\r\npostprocess output names: ['post_output']\r\nI0517 08:58:57.897360 604 model_repository_manager.cc:1183] successfully loaded 'postprocess' version 1\r\nI0517 08:58:57.897528 604 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: preprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 16, 'input': [{'name': 'preprocess_input', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'preprocess_output', 'data_type': 'TYPE_FP32', 'dims': [3, 224, 224], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['preprocess_input']\r\npreprocess output names: ['preprocess_output']\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nI0517 08:58:58.124272 604 model_repository_manager.cc:1183] successfully loaded 'preprocess' version 1\r\nI0517 08:58:58.124385 604 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0517 08:58:58.124451 604 server.cc:549] \r\n+---------+-------------------------------------------------------+--------------------------------------------------+\r\n| Backend | Path                                                  | Config                                           |\r\n+---------+-------------------------------------------------------+--------------------------------------------------+\r\n| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"shm-default-byte-size\":\"10485760\"}} |\r\n+---------+-------------------------------------------------------+--------------------------------------------------+\r\n\r\nI0517 08:58:58.124522 604 server.cc:592] \r\n+-------------+---------+--------+\r\n| Model       | Version | Status |\r\n+-------------+---------+--------+\r\n| postprocess | 1       | READY  |\r\n| preprocess  | 1       | READY  |\r\n+-------------+---------+--------+\r\n\r\nI0517 08:58:58.124636 604 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                        |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                       |\r\n| server_version                   | 2.15.0                                                                                                                                       |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_s |\r\n|                                  | hared_memory binary_tensor_data statistics                                                                                                   |\r\n| model_repository_path[0]         | /serving/models                                                                                                                              |\r\n| model_control_mode               | MODE_NONE                                                                                                                                    |\r\n| strict_model_config              | 1                                                                                                                                            |\r\n| rate_limit                       | OFF                                                                                                                                          |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                    |\r\n| response_cache_byte_size         | 0                                                                                                                                            |\r\n| min_supported_compute_capability | 0.0                                                                                                                                          |\r\n| strict_readiness                 | 1                                                                                                                                            |\r\n| exit_timeout                     | 30                                                                                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0517 08:58:58.124741 604 server.cc:252] Waiting for in-flight requests to complete.\r\nI0517 08:58:58.124752 604 model_repository_manager.cc:1055] unloading: preprocess:1\r\nI0517 08:58:58.124783 604 model_repository_manager.cc:1055] unloading: postprocess:1\r\nI0517 08:58:58.124834 604 server.cc:267] Timeout 30: Found 2 live models and 0 in-flight non-inference requests\r\nI0517 08:58:59.124954 604 server.cc:267] Timeout 29: Found 2 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nCleaning up...\r\nI0517 08:58:59.172150 604 model_repository_manager.cc:1166] successfully unloaded 'preprocess' version 1\r\nI0517 08:58:59.172710 604 model_repository_manager.cc:1166] successfully unloaded 'postprocess' version 1\r\nI0517 08:59:00.125229 604 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models",
        "state": "closed",
        "user": "jiyulongxu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-17T09:32:22+00:00",
        "updated_at": "2024-05-28T06:38:37+00:00",
        "closed_at": "2024-05-28T06:38:37+00:00",
        "comments_count": [
            "whisky-12"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1964,
        "title": "PaddleSeg/PP-Matting  示例文档打开404了",
        "body": "![image](https://github.com/PaddlePaddle/FastDeploy/assets/28799892/560b86ee-d61e-4015-8f1f-f627b197804a)\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/28799892/2c01aa71-3a5b-46cc-bc9d-54e17e43869a)\r\n\r\n",
        "state": "closed",
        "user": "xiaohuimc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-19T03:21:28+00:00",
        "updated_at": "2024-05-21T06:40:46+00:00",
        "closed_at": "2024-05-21T06:40:46+00:00",
        "comments_count": [
            "DefTruth"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1958,
        "title": "请问添加自定义推理后端？",
        "body": "你好，请问如何添加自定义后端？有相关文档吗？\r\n",
        "state": "closed",
        "user": "hellojiabin",
        "closed_by": "hellojiabin",
        "created_at": "2023-05-18T03:42:19+00:00",
        "updated_at": "2023-05-18T05:57:03+00:00",
        "closed_at": "2023-05-18T05:57:03+00:00",
        "comments_count": [
            "DefTruth",
            "hellojiabin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1970,
        "title": "【PPHGNet模型转换RKNN模型】RK3588上面infer.py执行预测报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】： 使用官网提供的C++在rk3588上面编译 fastdeploy\r\n【编译命令】 使用官网提供的C++在rk3588上面编译 fastdeploy\r\n【系统平台】: Linux x64(Ubuntu 20.04)\r\n【硬件】： RK3588\r\n【编译语言】： Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n我们需要PPHGNet samll版 分类模型 的rknn模型转换支持，尝试使用现在公开的 ResNet50_vd\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/classification/paddleclas/rockchip/rknpu2\r\n但是转换成rknn模型后运行 infer.py 发生报错。（报错日志下面粘贴）\r\n1.请问现在是不是还没有支持PPHGNet模型\r\n2.什么时候可以支持\r\n3.现阶段通过其他方式 是否可以实现PPHGNet模型的rknn模型转换并正常输出预测结果\r\n\r\n```\r\n(rk3588_py3.9) topeet@iTOP-RK3588:~/ml/FastDeploy/examples/vision/classification/paddleclas/rockchip/rknpu2/python $ python infer.py\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.7.2\r\nindex=0, name=x, n_dims=4, dims=[1, 224, 224, 3], n_elems=150528, size=301056, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=softmax_1.tmp_0, n_dims=2, dims=[1, 7, 0, 0], n_elems=7, size=14, fmt=UNDEFINED, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(341)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory       The input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\nE RKNN: [14:02:35.123] rknn_set_io_mem, input memory size(57510) < model input size(150528)\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(441)::InitRKNNTensorMemory The function(rknn_set_io_mem) failed! ret=-1\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(474)::Infer        Init tensor memory failed.\r\nSegmentation fault (core dumped)\r\n```",
        "state": "open",
        "user": "qiulongquan",
        "closed_by": null,
        "created_at": "2023-05-21T06:03:52+00:00",
        "updated_at": "2023-05-23T00:12:15+00:00",
        "closed_at": null,
        "comments_count": [
            "qiulongquan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1965,
        "title": "部署实例分割模型如何获取mask和轮廓contours",
        "body": "我用paddledection训练好的实例分割模型（mask rcnn算法），采用fastdeploy部署，想要得到实际的mask和轮廓contours，写了如下代码。但是最终得到结果不对，保存的轮廓图mask_big如下：\r\n![1 jpgmask_43](https://github.com/PaddlePaddle/FastDeploy/assets/66996161/fad18e5c-3355-45d9-a363-bef5802eb798)，是不对的，请问怎么修改代码？\r\n#include \"fastdeployision.h\"\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\n__declspec(dllexport) void main(const std::string& model_dir, const std::string& image_dir, int run_option);\r\n\r\nvoid main(const std::string& model_dir, const std::string& image_file, int run_option) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n    auto option = fastdeploy::RuntimeOption();\r\n\r\n    if (run_option == 0) {\r\n        option.UseCpu();\r\n    }\r\n    else if (run_option == 1) {\r\n        option.UseGpu();\r\n    }\r\n    else {\r\n        std::cerr << \"Invalid run option.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto model = fastdeploy::vision::detection::MaskRCNN(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto im = cv::imread(image_file);\r\n    int img_h = im.rows;\r\n    int img_w = im.cols;\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    std::cout << res.Str() << std::endl;\r\n    for (int i = 0; i < res.masks.size(); ++i) {\r\n        const auto& mask = res.masks[i];\r\n        cv::Mat mask_big = cv::Mat(img_h, img_w, CV_32S);\r\n        if (i==0){\r\n        std::cout << \"mask.data.data()====>\"<<  mask.data.data() << std::endl;\r\n        }\r\n\r\n        std::memcpy(mask_big.data, mask.data.data(), mask.data.size() * sizeof(int));\r\n\r\n        std::string filename1 = image_file + \"mask_\" + std::to_string(i + 1) + \".jpg\";\r\n        // 保存当前轮廓图像\r\n        cv::imwrite(filename1, mask_big);\r\n\r\n        std::vector<std::vector<cv::Point>> contours;\r\n        cv::Mat hierarchy;\r\n        // 将掩码矩阵的数据类型从32位整数转换为8位无符号整数。这通常是为了后续的处理和显示需要。\r\n        mask_big.convertTo(mask_big, CV_8U);\r\n        cv::findContours(mask_big, contours, hierarchy, cv::RETR_CCOMP, cv::CHAIN_APPROX_SIMPLE);\r\n        // Print contour coordinates\r\n        if (i == 0) {\r\n            std::cout << \"Contour coordinates id is \" << i << std::endl;\r\n            std::cout << \"Contours.size(): \" << contours.size() << std::endl;\r\n            std::cout << \"Contours.data(): \" << contours.data() << std::endl;;\r\n        }\r\n\r\n        for (const auto& contour : contours) {\r\n\r\n            cv::Mat result(img_h, img_w, CV_8UC3, cv::Scalar(0, 0, 0));\r\n            // 绘制轮廓\r\n            cv::drawContours(result, contours, -1, cv::Scalar(255, 255, 255));\r\n            // 生成文件名\r\n            std::string filename = image_file + \"contour_\" + std::to_string(i + 1) + \".jpg\";\r\n\r\n            // 保存当前轮廓图像\r\n            cv::imwrite(filename, result);\r\n\r\n            for (const auto& point : contour) {\r\n                if (i == 0) {\r\n                    std::cout << \"point===>\" << point << std::endl;\r\n                    std::cout << \"(\" << point.x << \", \" << point.y << \") \" << std::endl;\r\n                }\r\n\r\n            }\r\n        }\r\n    }\r\n    auto vis_im = fastdeploy::vision::VisDetection(im, res, 0.5);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n\r\n}\r\n\r\n\r\n我查看了官当文档，没看懂mask.data()和mask.data.data()分别是指什么？\r\nstruct FASTDEPLOY_DECL Mask : public BaseResult {\r\n  /// Mask data buffer\r\n  std::vector<uint8_t> data;\r\n  /// Shape of mask\r\n  std::vector<int64_t> shape;  // (H,W) ...\r\n  ResultType type = ResultType::MASK;\r\n\r\n  /// clear Mask result\r\n  void Clear();\r\n\r\n  /// Clear Mask result and free the memory\r\n  void Free();\r\n\r\n  /// Return a mutable pointer of the mask data buffer\r\n  void* Data() { return data.data(); }\r\n\r\n  /// Return a pointer of the mask data buffer for read only\r\n  const void* Data() const { return data.data(); }\r\n\r\n  /// Reserve size for mask data buffer\r\n  void Reserve(int size);\r\n\r\n  /// Resize the mask data buffer\r\n  void Resize(int size);\r\n\r\n  /// Debug function, convert the result to string to print\r\n  std::string Str();\r\n};\r\n",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "happybear1015",
        "created_at": "2023-05-19T05:32:58+00:00",
        "updated_at": "2024-05-28T13:01:19+00:00",
        "closed_at": "2023-07-06T07:54:24+00:00",
        "comments_count": [
            "qianbin1989228",
            "zhouweic36"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1972,
        "title": "多线程通过fd.Runtime()创建多runtime是否会相互影响？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU P40， CUDA 11.2 CUDNN 8.2\r\n- 【编译语言】：Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n 我使用多线程创建多个runtime，使用flask作为服务端，在极高的并发下，infer会出现相同的输入，有着不同的结果输出？请问多线程多个runtime同时infer是否会有影响\r\n",
        "state": "open",
        "user": "ashe2333",
        "closed_by": null,
        "created_at": "2023-05-22T08:00:10+00:00",
        "updated_at": "2023-05-22T08:00:10+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1977,
        "title": "我在使用uie进行推理，选用trt后端，fp16模式进行推理，但是使用uie-base fp16的时候，什么信息都检测不出来，使用uie-medium可以正常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.6\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060， CUDA 11.8 CUDNN 8.3\r\n- 【编译语言】：Python3.9\r\n![d23438fb8348fa2b9a188331fe9b223](https://github.com/PaddlePaddle/FastDeploy/assets/61367424/cf9ca09f-f540-4a90-839f-73fc842ef4ff)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/61367424/7d1e552d-e046-4198-9b85-8673b5cb2eeb)\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n",
        "state": "open",
        "user": "lzh1998-jansen",
        "closed_by": null,
        "created_at": "2023-05-23T07:53:44+00:00",
        "updated_at": "2023-05-23T07:53:44+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1971,
        "title": "每次调用tensorrt后端耗时都很长",
        "body": "同一个模型，调用tensorrt后端时，每次调用都需要很长的时间加载，请问这是为什么？第一次调用完后，tensorrt平台的模型没有保存吗？或者保存的地方怎么查看呢？感谢回复。",
        "state": "open",
        "user": "HGD-ai",
        "closed_by": null,
        "created_at": "2023-05-21T08:34:49+00:00",
        "updated_at": "2023-05-22T11:25:55+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "HGD-ai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1973,
        "title": "nvidia jetson TX2编译fastdeploy错误，求大佬解决方法",
        "body": "## 环境\r\n- 【编译命令】\r\n- git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DBUILD_ON_JETSON=ON \\\r\n         -DENABLE_VISION=ON \\\r\n         -DPADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\nmake -j8\r\n- 【系统平台】: Ubuntu 18.04\r\n- 【硬件】： nvidia jetson TX2\r\n- 【编译语言】： C++\r\n\r\n\r\n## 问题日志及出现问题的操作流程，在make时编译错误\r\n\r\ndavid@david-desktop:~/下载/FastDeploy/build$ make -j8\r\n\r\nScanning dependencies of target extern_onnxruntime\r\nScanning dependencies of target extern_paddle2onnx\r\nScanning dependencies of target yaml-cpp\r\n[  1%] Creating directories for 'extern_paddle2onnx'\r\n[  2%] Creating directories for 'extern_onnxruntime'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  4%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n[  4%] Performing download step (download, verify and extract) for 'extern_paddle2onnx'\r\n-- Downloading...\r\n   dst='/home/david/下载/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.7.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/paddle2onnx-linux-aarch64-1.0.7.tgz'\r\n-- Downloading...\r\n   dst='/home/david/下载/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/david/下载/FastDeploy/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n     dst='/home/david/下载/FastDeploy/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  7%] No patch step for 'extern_onnxruntime'\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  7%] No update step for 'extern_onnxruntime'\r\n[  7%] No configure step for 'extern_onnxruntime'\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  8%] No build step for 'extern_onnxruntime'\r\n-- Downloading... done\r\n[  8%] Performing install step for 'extern_onnxruntime'\r\n-- extracting...\r\n     src='/home/david/下载/FastDeploy/build/third_libs/paddle2onnx/src/paddle2onnx-linux-aarch64-1.0.7.tgz'\r\n     dst='/home/david/下载/FastDeploy/build/third_libs/paddle2onnx/src/extern_paddle2onnx'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  9%] No patch step for 'extern_paddle2onnx'\r\n[  9%] Completed 'extern_onnxruntime'\r\n[  9%] No update step for 'extern_paddle2onnx'\r\n[  9%] Built target extern_onnxruntime\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[ 10%] No configure step for 'extern_paddle2onnx'\r\n[ 11%] No build step for 'extern_paddle2onnx'\r\n[ 11%] Performing install step for 'extern_paddle2onnx'\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[ 11%] Completed 'extern_paddle2onnx'\r\n[ 11%] Built target extern_paddle2onnx\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[ 17%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[ 17%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 17%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 18%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 18%] Linking CXX static library libyaml-cpp.a\r\n[ 18%] Built target yaml-cpp\r\nScanning dependencies of target yaml-cpp-sandbox\r\nScanning dependencies of target yaml-cpp-read\r\nScanning dependencies of target fastdeploy\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-sandbox.dir/sandbox.cpp.o\r\n[ 18%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-read.dir/read.cpp.o\r\nScanning dependencies of target yaml-cpp-parse\r\n[ 19%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-parse.dir/parse.cpp.o\r\n[ 19%] Linking CXX executable parse\r\n[ 20%] Linking CXX executable read\r\n[ 20%] Built target yaml-cpp-parse\r\n[ 20%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/allocate.cc.o\r\n[ 20%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_type.cc.o\r\n[ 21%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cast.cc.o\r\n[ 22%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_tensor.cc.o\r\n[ 22%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/benchmark/utils.cc.o\r\n[ 22%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/fastdeploy_model.cc.o\r\n[ 22%] Built target yaml-cpp-read\r\n[ 22%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/clip.cc.o\r\n[ 23%] Linking CXX executable sandbox\r\n[ 23%] Built target yaml-cpp-sandbox\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/concat.cc.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cumprod.cc.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/eigen.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/elementwise.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/full.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gather_scatter_along_axis.cc.o\r\n[ 26%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gaussian_random.cc.o\r\n[ 26%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/isfinite.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/linspace.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/math.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/pad.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/quantile.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/reduce.cc.o\r\n[ 29%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/slice.cc.o\r\n[ 29%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/softmax.cc.o\r\n[ 29%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/sort.cc.o\r\n[ 30%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/split.cc.o\r\n[ 30%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/tile.cc.o\r\n[ 30%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/transpose.cc.o\r\n[ 31%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/enum_variables.cc.o\r\n[ 31%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/runtime.cc.o\r\n[ 32%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/runtime_option.cc.o\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:27:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h: In instantiation of ‘void fastdeploy::FDInferDeleter::operator()(T*) const [with T = nvonnxparser::IParser]’:\r\n/usr/include/c++/7/bits/unique_ptr.h:263:17:   required from ‘std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = nvonnxparser::IParser; _Dp = fastdeploy::FDInferDeleter]’\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:72:52:   required from here\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h:37:7: error: ‘virtual nvonnxparser::IParser::~IParser()’ is protected within this context\r\n       delete obj;\r\n       ^~~~~~\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:25:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/usr/include/aarch64-linux-gnu/NvOnnxParser.h:214:13: note: declared protected here\r\n     virtual ~IParser() {}\r\n             ^\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:27:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h: In instantiation of ‘void fastdeploy::FDInferDeleter::operator()(T*) const [with T = nvinfer1::IBuilder]’:\r\n/usr/include/c++/7/bits/unique_ptr.h:263:17:   required from ‘std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = nvinfer1::IBuilder; _Dp = fastdeploy::FDInferDeleter]’\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:72:52:   required from here\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h:37:7: error: ‘virtual nvinfer1::IBuilder::~IBuilder()’ is protected within this context\r\n       delete obj;\r\n       ^~~~~~\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:24:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/usr/include/aarch64-linux-gnu/NvInfer.h:7098:13: note: declared protected here\r\n     virtual ~IBuilder()\r\n             ^\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:27:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h: In instantiation of ‘void fastdeploy::FDInferDeleter::operator()(T*) const [with T = nvinfer1::INetworkDefinition]’:\r\n/usr/include/c++/7/bits/unique_ptr.h:263:17:   required from ‘std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = nvinfer1::INetworkDefinition; _Dp = fastdeploy::FDInferDeleter]’\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:72:52:   required from here\r\n/home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/utils.h:37:7: error: ‘virtual nvinfer1::INetworkDefinition::~INetworkDefinition()’ is protected within this context\r\n       delete obj;\r\n       ^~~~~~\r\nIn file included from /home/david/下载/FastDeploy/./fastdeploy/runtime/backends/tensorrt/trt_backend.h:24:0,\r\n                 from /home/david/下载/FastDeploy/fastdeploy/runtime/runtime.cc:25:\r\n/usr/include/aarch64-linux-gnu/NvInfer.h:5340:13: note: declared protected here\r\n     virtual ~INetworkDefinition() {}\r\n             ^\r\nCMakeFiles/fastdeploy.dir/build.make:710: recipe for target 'CMakeFiles/fastdeploy.dir/fastdeploy/runtime/runtime.cc.o' failed\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/fastdeploy/runtime/runtime.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:175: recipe for target 'CMakeFiles/fastdeploy.dir/all' failed\r\nmake[1]: *** [CMakeFiles/fastdeploy.dir/all] Error 2\r\nMakefile:151: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n",
        "state": "closed",
        "user": "1mageLiu",
        "closed_by": "1mageLiu",
        "created_at": "2023-05-22T09:36:18+00:00",
        "updated_at": "2023-07-09T06:01:17+00:00",
        "closed_at": "2023-05-23T01:02:09+00:00",
        "comments_count": [
            "1mageLiu",
            "Gentek36",
            "1mageLiu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1975,
        "title": "Docker容器内不能联网",
        "body": "\r\n#镜像名称 \r\nregistry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n# 运行容器命令\r\ndocker run -it --gpus all --ipc=host --name fd_serving -v `pwd`/:/FastDeploy registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10  bash\r\n\r\n--------------------------------------------------------------------------------------------------\r\nroot@dcc9bc64f269:/# visualdl --version\r\nVisualDL 2.4.2\r\n--------------------------------------------------------------------------------------------------\r\n【github文档】本文假定用户已经启动了fastdeploy的镜像，环境中拥有了fastdeployserver命令，并且安装了**visualdl>=2.5.0**，以便能够正常使用VisualDL进行serving可视化部署。\r\n--------------------------------------------------------------------------------------------------\r\n\r\n安装visualdl>=2.5.0时候发现apt-get不能使用网络。\r\n\r\n报错：\r\nroot@dcc9bc64f269:/# apt-get update\r\nErr:1 http://archive.ubuntu.com/ubuntu focal InRelease\r\n  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nErr:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\r\n  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nErr:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\r\n  Unable to connect to 172.19.56.199:3128:\r\nErr:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\r\n  Unable to connect to 172.19.56.199:3128:\r\nErr:5 http://security.ubuntu.com/ubuntu focal-security InRelease\r\n  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nReading package lists... Done\r\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/focal/InRelease  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease  Unable to connect to 172.19.56.199:3128:\r\nW: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease  Unable to connect to 172.19.56.199:3128:\r\nW: Failed to fetch http://security.ubuntu.com/ubuntu/dists/focal-security/InRelease  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nW: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/InRelease  Could not connect to 172.19.56.199:3128 (172.19.56.199), connection timed out\r\nW: Some index files failed to download. They have been ignored, or old ones used instead.\r\n----------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n报错：\r\nroot@dcc9bc64f269:/# python3 -m pip install visualdl==2.5.2\r\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5b6529ec40>, 'Connection to 172.19.56.199 timed out. (connect timeout=15)')': /simple/visualdl/\r\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5b6529edc0>, 'Connection to 172.19.56.199 timed out. (connect timeout=15)')': /simple/visualdl/\r\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5b6529ee80>, 'Connection to 172.19.56.199 timed out. (connect timeout=15)')': /simple/visualdl/\r\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5b6529ee50>, 'Connection to 172.19.56.199 timed out. (connect timeout=15)')': /simple/visualdl/\r\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5b652c6040>, 'Connection to 172.19.56.199 timed out. (connect timeout=15)')': /simple/visualdl/\r\nERROR: Could not find a version that satisfies the requirement visualdl==2.5.2 (from versions: none)\r\nERROR: No matching distribution found for visualdl==2.5.2\r\n------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n在使用nvidia triton镜像时可以直接进行联网操作\r\n\r\n\r\n",
        "state": "closed",
        "user": "loneclown",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-23T06:56:02+00:00",
        "updated_at": "2024-06-18T06:41:10+00:00",
        "closed_at": "2024-06-18T06:41:10+00:00",
        "comments_count": [
            "fuloong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1979,
        "title": "C#部署fastdeploy所需的动态库问题",
        "body": "cudn11.2,cudnn8.4,windows10, c++,cmake gui 编译，编译后vs2019生成fastdeploy csharp.dll和fastdeploy.dll，调用时提示无法加载  #DLL“fastdeploy.dll”，此动态库依赖项很多，应该怎么把其所需的子项一并生成输出到指定目录，其一共有多少动态库。目前使用Paddle Inference和TensorRT两种\r\n![无标题](https://github.com/PaddlePaddle/FastDeploy/assets/126551342/8fe9840c-a772-4530-b959-5845e9c675b7)\r\n",
        "state": "closed",
        "user": "sanyangAZ",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-23T09:26:15+00:00",
        "updated_at": "2024-08-06T06:40:03+00:00",
        "closed_at": "2024-08-06T06:40:02+00:00",
        "comments_count": [
            "DefTruth",
            "guoyunqingyue"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1980,
        "title": "rk3888 rtdetr request",
        "body": "rtdetr 模型部署，希望能尽快支持呢？或者有教程吗？我想部署一些暂不支持的模型；rk芯片\r\n",
        "state": "closed",
        "user": "GDbbq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-24T01:15:07+00:00",
        "updated_at": "2025-01-16T03:26:13+00:00",
        "closed_at": "2024-06-11T06:41:14+00:00",
        "comments_count": [
            "Zsk747",
            "YOU-007",
            "qazqaz44944",
            "happybear1015"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1981,
        "title": "使用OCR检测，如何获取单个字符的位置坐标？",
        "body": "请问使用OCR检测之后，如何获取单个字符的位置坐标？",
        "state": "open",
        "user": "zhenhuamo",
        "closed_by": null,
        "created_at": "2023-05-24T02:06:30+00:00",
        "updated_at": "2023-05-24T02:25:40+00:00",
        "closed_at": null,
        "comments_count": [
            "WilliamQf-AI",
            "WilliamQf-AI",
            "zhenhuamo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1983,
        "title": "无法在 DLL“fastdeploy.dll”中找到名为“FD_C_CreateRuntimeOptionWrapper”的入口点",
        "body": null,
        "state": "closed",
        "user": "sanyangAZ",
        "closed_by": "sanyangAZ",
        "created_at": "2023-05-24T04:06:22+00:00",
        "updated_at": "2023-05-24T07:47:11+00:00",
        "closed_at": "2023-05-24T07:44:28+00:00",
        "comments_count": [
            "sanyangAZ"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1985,
        "title": "API调用运行崩溃",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-win-x64-1.0.6\r\n- 【编译命令】通过两种方式：1）下载编译好的fastdeploy-win-x64-1.0.6的包；2）下载源码，用VS2019编译不包含GPU、TensorRT的fastdeploy版本\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【编译语言】： C++(VS2019)\r\n\r\n测试fastdeploy C++ API的调用正确性\r\n只包含一行代码：\r\n#include \"fastdeploy/runtime.h\"\r\nnamespace fd = fastdeploy;\r\n\r\nint main()\r\n{\r\n     //setup option\r\n    fd::RuntimeOption runtime_option;\r\n   \r\n    return 0；\r\n}\r\n- 【运行崩溃】\r\n- - 编译没有问题，运行时出错，错误信息：\r\n- 0x00007FFD266F40AC 处引发的异常: Microsoft C++ 异常: InferenceEngine::GeneralError，位于内存位置 0x0000004B21AFE710 处。\r\n0x00007FFD266F40AC 处引发的异常: Microsoft C++ 异常: ov::Exception，位于内存位置 0x0000004B21AFEC50 处。\r\n0x00007FFD266F40AC 处有未经处理的异常: Microsoft C++ 异常: ov::Exception，位于内存位置 0x0000004B21AFEC50 处。\r\n\r\n",
        "state": "closed",
        "user": "fenxiangwpp",
        "closed_by": "fenxiangwpp",
        "created_at": "2023-05-24T07:26:54+00:00",
        "updated_at": "2023-11-27T01:51:13+00:00",
        "closed_at": "2023-05-26T01:12:47+00:00",
        "comments_count": [
            "WilliamQf-AI",
            "DefTruth",
            "allanpk716",
            "allanpk716"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1989,
        "title": "fastdeploy服务化部署多次调用pp_ocrv3时第一张图片结果正确，但从第二张开始都返回结果异常（为第一张的部分结果）或者报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【FastDeploy版本】：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10 docker版本\r\n- 【编译命令】 详细步骤参考https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving   测试模型:pp_ocrv3\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】：  Nvidia GPU 1080Ti， CUDA 11.4 \r\n- 【编译语言】python3.7.9 \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n问题说明：\r\n拉取fastdeploy gpu docker镜像后,按照PaddleOCR/deploy/fastdeploy/serving/fastdeploy_serving/readme.md来操作，一切都顺利。\r\n运行python3 client.py （图片为官方例子wget https://gitee.com/paddlepaddle/PaddleOCR/raw/release/2.6/doc/imgs/12.jpg）也结果正常。但更改测试图片路径，换成其他测试图片后，返回的结果则报错。把fastdeploy服务端停掉重启，仍然是第一张结果正确，从第二张开始报错或者重复第一张部分结果。 换visualdl测试，以及https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN/client.md 中[如何编写客户端 HTTP/GRPC 请求】也是一样的结果  （中间已经修正已经有人反馈的 np.astype()报错）\r\n\r\n详细日志：\r\n服务端：\r\nI0525 01:47:29.440687 141 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA GeForce GTX 1080 Ti\r\nI0525 01:47:29.776246 141 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fa44e000000' with size 268435456\r\nI0525 01:47:29.783311 141 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0525 01:47:29.888578 141 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\nI0525 01:47:29.989086 141 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0525 01:47:30.089545 141 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0525 01:47:30.129062 141 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0525 01:47:30.190203 141 model_repository_manager.cc:1022] loading: det_runtime:1\r\nI0525 01:47:30.290735 141 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0525 01:47:30.391017 141 model_repository_manager.cc:1022] loading: rec_runtime:1\r\nI0525 01:47:30.491485 141 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0525 01:47:35.670106 141 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0525 01:47:35.687562 141 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0525 01:47:35.905641 141 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\nI0525 01:47:44.033783 141 fastdeploy_runtime.cc:1182] TRITONBACKEND_Initialize: fastdeploy\r\nI0525 01:47:44.033956 141 fastdeploy_runtime.cc:1191] Triton TRITONBACKEND API version: 1.6\r\nI0525 01:47:44.033982 141 fastdeploy_runtime.cc:1196] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0525 01:47:44.033996 141 fastdeploy_runtime.cc:1225] backend configuration:\r\n{}\r\nI0525 01:47:44.034112 141 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0525 01:47:44.284183 141 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\nI0525 01:47:44.284243 141 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\nI0525 01:47:44.284799 141 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\nI0525 01:47:44.285053 141 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\nI0525 01:47:44.285301 141 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0525 01:47:44.532352 141 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (GPU device 0)\r\nI0525 01:47:44.532478 141 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:48:02.304435 141 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (GPU device 0)\r\nI0525 01:48:02.304834 141 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:48:02.888779 141 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (GPU device 0)\r\nI0525 01:48:02.888877 141 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:48:03.708838 141 model_repository_manager.cc:1183] successfully loaded 'rec_runtime' version 1\r\nI0525 01:48:03.709044 141 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0525 01:48:03.809249 141 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0525 01:48:03.909523 141 model_repository_manager.cc:1022] loading: rec_pp:1\r\nI0525 01:48:04.009928 141 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0525 01:48:04.009978 141 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0525 01:48:04.010319 141 model_repository_manager.cc:1183] successfully loaded 'rec_pp' version 1\r\nI0525 01:48:04.010535 141 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0525 01:48:04.010661 141 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0525 01:48:04.010856 141 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| cls_pp          | 1       | READY  |\r\n| cls_runtime     | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| det_runtime     | 1       | READY  |\r\n| pp_ocr          | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n| rec_pp          | 1       | READY  |\r\n| rec_runtime     | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0525 01:48:04.011233 141 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                               |\r\n| server_version                   | 2.15.0                                                                                                                                                               |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tens |\r\n|                                  | or_data statistics                                                                                                                                                   |\r\n| model_repository_path[0]         | /ocr_serving/models                                                                                                                                                  |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                            |\r\n| strict_model_config              | 1                                                                                                                                                                    |\r\n| rate_limit                       | OFF                                                                                                                                                                  |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                            |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                             |\r\n| response_cache_byte_size         | 0                                                                                                                                                                    |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                  |\r\n| strict_readiness                 | 1                                                                                                                                                                    |\r\n| exit_timeout                     | 30                                                                                                                                                                   |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0525 01:48:04.060757 141 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0525 01:48:04.093230 141 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0525 01:48:04.158925 141 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\n^CSignal (2) received.\r\nI0525 01:52:23.265235 141 server.cc:252] Waiting for in-flight requests to complete.\r\nI0525 01:52:23.265277 141 model_repository_manager.cc:1055] unloading: rec_runtime:1\r\nI0525 01:52:23.265420 141 model_repository_manager.cc:1055] unloading: rec_pp:1\r\nI0525 01:52:23.265506 141 model_repository_manager.cc:1055] unloading: rec_postprocess:1\r\nI0525 01:52:23.265664 141 model_repository_manager.cc:1055] unloading: pp_ocr:1\r\nI0525 01:52:23.265807 141 model_repository_manager.cc:1055] unloading: det_runtime:1\r\nI0525 01:52:23.265946 141 model_repository_manager.cc:1055] unloading: det_preprocess:1\r\nI0525 01:52:23.266133 141 model_repository_manager.cc:1166] successfully unloaded 'pp_ocr' version 1\r\nI0525 01:52:23.266244 141 fastdeploy_runtime.cc:1326] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0525 01:52:23.266660 141 fastdeploy_runtime.cc:1326] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0525 01:52:23.266838 141 model_repository_manager.cc:1055] unloading: det_postprocess:1I0525 01:52:23.266853 141 model_repository_manager.cc:1166] successfully unloaded 'rec_pp' version 1\r\n\r\nI0525 01:52:23.267089 141 model_repository_manager.cc:1055] unloading: cls_pp:1\r\nI0525 01:52:23.267527 141 model_repository_manager.cc:1055] unloading: cls_runtime:1\r\nI0525 01:52:23.267647 141 model_repository_manager.cc:1055] unloading: cls_postprocess:1I0525 01:52:23.267647 141 model_repository_manager.cc:1166] successfully unloaded 'cls_pp' version 1\r\n\r\nI0525 01:52:23.267983 141 fastdeploy_runtime.cc:1326] TRITONBACKEND_ModelInstanceFinalize: delete instance stateI0525 01:52:23.267984 141 server.cc:267] Timeout 30: Found 7 live models and 0 in-flight non-inference requests\r\n\r\nI0525 01:52:23.312637 141 fastdeploy_runtime.cc:1275] TRITONBACKEND_ModelFinalize: delete model state\r\nI0525 01:52:23.312684 141 model_repository_manager.cc:1166] successfully unloaded 'rec_runtime' version 1\r\nI0525 01:52:23.312908 141 fastdeploy_runtime.cc:1275] TRITONBACKEND_ModelFinalize: delete model state\r\nI0525 01:52:23.312940 141 model_repository_manager.cc:1166] successfully unloaded 'cls_runtime' version 1\r\nI0525 01:52:23.315488 141 fastdeploy_runtime.cc:1275] TRITONBACKEND_ModelFinalize: delete model state\r\nI0525 01:52:23.316117 141 model_repository_manager.cc:1166] successfully unloaded 'det_runtime' version 1\r\nCleaning up...\r\nCleaning up...\r\nCleaning up...\r\nI0525 01:52:24.268269 141 server.cc:267] Timeout 29: Found 4 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nI0525 01:52:24.709509 141 model_repository_manager.cc:1166] successfully unloaded 'rec_postprocess' version 1\r\nI0525 01:52:24.709564 141 model_repository_manager.cc:1166] successfully unloaded 'cls_postprocess' version 1\r\nI0525 01:52:24.710411 141 model_repository_manager.cc:1166] successfully unloaded 'det_postprocess' version 1\r\nI0525 01:52:24.712290 141 model_repository_manager.cc:1166] successfully unloaded 'det_preprocess' version 1\r\nI0525 01:52:25.268426 141 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nroot@tan-C246-WU4:/# fastdeployserver --model-repository=/ocr_serving/models\r\nI0525 01:52:30.950754 321 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA GeForce GTX 1080 Ti\r\nI0525 01:52:31.058342 321 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fa9dc000000' with size 268435456\r\nI0525 01:52:31.058537 321 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0525 01:52:31.061195 321 model_repository_manager.cc:1022] loading: cls_postprocess:1\r\nI0525 01:52:31.161715 321 model_repository_manager.cc:1022] loading: det_postprocess:1\r\nI0525 01:52:31.168204 321 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: cls_postprocess_0 (CPU device 0)\r\nI0525 01:52:31.261920 321 model_repository_manager.cc:1022] loading: det_preprocess:1\r\nI0525 01:52:31.362135 321 model_repository_manager.cc:1022] loading: det_runtime:1\r\nmodel_config: {'name': 'cls_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [2], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_INT32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'cls_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0525 01:52:31.382730 321 model_repository_manager.cc:1183] successfully loaded 'cls_postprocess' version 1\r\nI0525 01:52:31.383232 321 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_preprocess_0 (CPU device 0)\r\nI0525 01:52:31.462393 321 model_repository_manager.cc:1022] loading: rec_postprocess:1\r\nI0525 01:52:31.562629 321 model_repository_manager.cc:1022] loading: rec_runtime:1\r\nmodel_config: {'name': 'det_preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 1, 'input': [{'name': 'INPUT_0', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'OUTPUT_0', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'OUTPUT_1', 'data_type': 'TYPE_INT32', 'dims': [4], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['INPUT_0']\r\npreprocess output names: ['OUTPUT_0', 'OUTPUT_1']\r\nI0525 01:52:31.602334 321 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: det_postprocess_0 (CPU device 0)\r\nI0525 01:52:31.602421 321 model_repository_manager.cc:1183] successfully loaded 'det_preprocess' version 1\r\nI0525 01:52:31.662845 321 model_repository_manager.cc:1022] loading: cls_runtime:1\r\nmodel_config: {'name': 'det_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [1, -1, -1], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'POST_INPUT_1', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [4], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'ORI_IMG', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [-1, 1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_2', 'data_type': 'TYPE_FP32', 'dims': [-1, -1, 1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'det_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0', 'POST_INPUT_1', 'ORI_IMG']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1', 'POST_OUTPUT_2']\r\nI0525 01:52:31.817357 321 model_repository_manager.cc:1183] successfully loaded 'det_postprocess' version 1\r\nI0525 01:52:32.036967 321 fastdeploy_runtime.cc:1182] TRITONBACKEND_Initialize: fastdeploy\r\nI0525 01:52:32.036987 321 fastdeploy_runtime.cc:1191] Triton TRITONBACKEND API version: 1.6\r\nI0525 01:52:32.036991 321 fastdeploy_runtime.cc:1196] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0525 01:52:32.036994 321 fastdeploy_runtime.cc:1225] backend configuration:\r\n{}\r\nI0525 01:52:32.039718 321 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: det_runtime (version 1)\r\nI0525 01:52:32.040228 321 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: cls_runtime (version 1)\r\nI0525 01:52:32.040498 321 fastdeploy_runtime.cc:1255] TRITONBACKEND_ModelInitialize: rec_runtime (version 1)\r\nI0525 01:52:32.040728 321 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: rec_postprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'rec_postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 128, 'input': [{'name': 'POST_INPUT_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6625], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'POST_OUTPUT_0', 'data_type': 'TYPE_STRING', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'POST_OUTPUT_1', 'data_type': 'TYPE_FP32', 'dims': [1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'rec_postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['POST_INPUT_0']\r\npostprocess output names: ['POST_OUTPUT_0', 'POST_OUTPUT_1']\r\nI0525 01:52:32.255476 321 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: det_runtime_0 (GPU device 0)\r\nI0525 01:52:32.255607 321 model_repository_manager.cc:1183] successfully loaded 'rec_postprocess' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:52:33.749731 321 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: cls_runtime_0 (GPU device 0)\r\nI0525 01:52:33.749812 321 model_repository_manager.cc:1183] successfully loaded 'det_runtime' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:52:34.263908 321 fastdeploy_runtime.cc:1294] TRITONBACKEND_ModelInstanceInitialize: rec_runtime_0 (GPU device 0)\r\nI0525 01:52:34.264024 321 model_repository_manager.cc:1183] successfully loaded 'cls_runtime' version 1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(28)::BuildOption\tWill use external stream for Paddle Backend.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0525 01:52:34.885203 321 model_repository_manager.cc:1183] successfully loaded 'rec_runtime' version 1\r\nI0525 01:52:34.885399 321 model_repository_manager.cc:1022] loading: cls_pp:1\r\nI0525 01:52:34.985743 321 model_repository_manager.cc:1022] loading: pp_ocr:1\r\nI0525 01:52:35.086236 321 model_repository_manager.cc:1022] loading: rec_pp:1\r\nI0525 01:52:35.186637 321 model_repository_manager.cc:1183] successfully loaded 'pp_ocr' version 1\r\nI0525 01:52:35.186646 321 model_repository_manager.cc:1183] successfully loaded 'cls_pp' version 1\r\nI0525 01:52:35.186980 321 model_repository_manager.cc:1183] successfully loaded 'rec_pp' version 1\r\nI0525 01:52:35.187185 321 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0525 01:52:35.187315 321 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0525 01:52:35.187517 321 server.cc:592] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| cls_postprocess | 1       | READY  |\r\n| cls_pp          | 1       | READY  |\r\n| cls_runtime     | 1       | READY  |\r\n| det_postprocess | 1       | READY  |\r\n| det_preprocess  | 1       | READY  |\r\n| det_runtime     | 1       | READY  |\r\n| pp_ocr          | 1       | READY  |\r\n| rec_postprocess | 1       | READY  |\r\n| rec_pp          | 1       | READY  |\r\n| rec_runtime     | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0525 01:52:35.187893 321 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                               |\r\n| server_version                   | 2.15.0                                                                                                                                                               |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tens |\r\n|                                  | or_data statistics                                                                                                                                                   |\r\n| model_repository_path[0]         | /ocr_serving/models                                                                                                                                                  |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                            |\r\n| strict_model_config              | 1                                                                                                                                                                    |\r\n| rate_limit                       | OFF                                                                                                                                                                  |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                            |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                             |\r\n| response_cache_byte_size         | 0                                                                                                                                                                    |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                  |\r\n| strict_readiness                 | 1                                                                                                                                                                    |\r\n| exit_timeout                     | 30                                                                                                                                                                   |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0525 01:52:35.190192 321 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0525 01:52:35.190929 321 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0525 01:52:35.233310 321 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\n\r\n客户端：\r\nPaddleOCR/deploy/fastdeploy/serving/fastdeploy_serving/client.py\r\n\r\n\r\n第一张图片(paddleocr/doc/imgs/00056221.jpg) 结果：python client.py\r\ntext= 7788.com   score= 0.9582448601722717   bbox= [  3   2 156   2 156  47   3  47]\r\ntext= Z57A001950   score= 0.9086306691169739   bbox= [ 76 102 229 100 229 124  76 126]\r\ntext= 杭州东售   score= 0.9938820600509644   bbox= [408 103 506 103 506 131 408 131]\r\ntext= 2013年07月07日13：39开   score= 0.9525690078735352   bbox= [ 68 140 323 139 323 160  68 161]\r\ntext= 06车12B号   score= 0.8856244087219238   bbox= [392 140 505 140 505 160 392 160]\r\ntext= 二等座   score= 0.9831244945526123   bbox= [443 160 506 159 506 182 443 183]\r\ntext= 杭州东   score= 0.9783030152320862   bbox= [ 92 182 195 182 195 214  92 214]\r\ntext= G7512次   score= 0.9881171584129333   bbox= [238 173 352 175 352 203 238 201]\r\ntext= 上海虹桥   score= 0.8762866258621216   bbox= [384 182 519 184 519 215 384 213]\r\ntext= HangZhouDong   score= 0.9886749386787415   bbox= [ 80 216 221 218 221 239  80 237]\r\ntext= ShangHaiHongQiao   score= 0.890979528427124   bbox= [362 218 527 218 527 238 362 238]\r\ntext= ￥73.00元   score= 0.7457143664360046   bbox= [ 76 246 180 246 180 265  76 265]\r\ntext= 限乘当日当次车   score= 0.9557703733444214   bbox= [ 77 275 218 275 218 296  77 296]\r\ntext= 余友红   score= 0.9852586984634399   bbox= [ 75 302 145 302 145 324  75 324]\r\ntext= 检票口16   score= 0.9810167551040649   bbox= [298 316 404 306 407 334 301 344]\r\ntext= 3623301993****0941   score= 0.9097948670387268   bbox= [ 71 329 284 323 285 350  72 356]\r\ntext=    score= 0.0   bbox= [427 345 449 343 450 353 428 355]\r\ntext= 9004-1300-5707-08A0-0195-0   score= 0.8943517208099365   bbox= [ 63 365 325 365 325 385  63 385]\r\ntext= 和谐号   score= 0.9665305614471436   bbox= [421 359 510 359 510 380 421 380]\r\ntext= Canon PowerShot A3400 IS F2.8 1/20s IS0400   score= 0.8920562863349915   bbox= [ 15 494 241 493 241 504  15 505]\r\n\r\n结果正确\r\n第二张结果：（更改  im = cv2.imread(\"12.jpg\") #\"00056221.jpg\") 后再次python client.py\r\ntext=    score= 0.0   bbox= [  4   3 185   3 185  57   4  57]\r\ntext=    score= 0.0   bbox= [ 91 123 272 120 272 149  91 152]\r\ntext=    score= 0.0   bbox= [485 124 601 124 601 158 485 158]\r\ntext= 13：39开   score= 0.87248694896698   bbox= [ 81 168 383 167 383 192  81 194]\r\ntext= 06车12B号   score= 0.8856244087219238   bbox= [466 168 600 168 600 192 466 192]\r\ntext= 二等座   score= 0.9831244945526123   bbox= [526 192 601 191 601 219 526 220]\r\n 返回结果异常,仍然是第一张部分结果.\r\n\r\n如果更改顺序,第一张为12.jpg   第二张为00056221.jpg，重启服务端后，则第一张识别成功，第二张报错：\r\n第一张结果：\r\ntext= 上海斯格威铂尔大酒店   score= 0.9804123044013977   bbox= [ 42 413 483 391 484 428  43 450]\r\ntext= 打浦路15号   score= 0.9606184363365173   bbox= [187 456 399 448 400 480 188 488]\r\ntext= 绿洲仕格维花园公寓   score= 0.9806857705116272   bbox= [ 23 507 513 488 515 529  24 548]\r\ntext= 打浦路252935号   score= 0.9498506784439087   bbox= [ 74 553 427 542 428 571  75 582]\r\n\r\n第二张结果：\r\n客户端：\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 99, in <module>\r\n    result = runner.Run([im, ])\r\n  File \"client.py\", line 76, in Run\r\n    client_timeout=self._response_wait_t, )\r\n  File \"/home/tan/miniconda3/envs/zoo/lib/python3.7/site-packages/tritonclient/grpc/__init__.py\", line 1469, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/home/tan/miniconda3/envs/zoo/lib/python3.7/site-packages/tritonclient/grpc/__init__.py\", line 75, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble 'pp_ocr', Failed to process the request(s) for model instance 'det_postprocess_0', message: TritonModelException: in ensemble 'rec_pp', softmax_5.tmp_0: failed to perform CUDA copy: invalid argument\r\n\r\nAt:\r\n  /ocr_serving/models/det_postprocess/1/model.py(205): execute\r\n\r\n服务端:\r\nI0525 02:56:07.298978 875 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0525 02:56:07.299714 875 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0525 02:56:07.342227 875 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\n0525 02:56:57.185024 915 pb_stub.cc:402] Failed to process the request(s) for model 'det_postprocess_0', message: TritonModelException: in ensemble 'rec_pp', softmax_5.tmp_0: failed to perform CUDA copy: invalid argument\r\n\r\nAt:\r\n  /ocr_serving/models/det_postprocess/1/model.py(205): execute\r\n\r\n总体结果： 每次重启服务端后，第一张总是正确，第二张开始报错或者重复第一张部分结果。\r\n\r\n辅助测试： \r\n使用https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/serving/README.md 下周yolov5进行同样部署和测试，则没有任何问题。\r\n\r\n\r\n",
        "state": "closed",
        "user": "reaper2012",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-25T03:03:15+00:00",
        "updated_at": "2024-07-09T06:40:39+00:00",
        "closed_at": "2024-07-09T06:40:39+00:00",
        "comments_count": [
            "bdeng3",
            "0Lisixian0",
            "reaper2012",
            "MikeLud",
            "reaper2012",
            "MikeLud",
            "reaper2012",
            "bdeng3"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1988,
        "title": "windows下使用openvino的异常",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7.zip\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【以下代码可以正常初始化模型】\r\nauto option = fastdeploy::RuntimeOption();\r\noption.UseCpu();\r\nauto model = fastdeploy::vision::detection::PPYOLOE(model_filename, params_filename, cfg_file, option);\r\n\r\n- 【想用openvino指定核显推理加速】\r\nauto option = fastdeploy::RuntimeOption();\r\noption.UseCpu();\r\noption.UseOpenVINOBackend();\r\noption.openvino_option.SetDevice(\"GPU\");\r\noption.openvino_option.SetShapeInfo({ {\"image\",{ 1, 3 ,512, 512 }} });\r\nauto model = fastdeploy::vision::detection::PPYOLOE(model_filename, params_filename, cfg_file, option);\r\n在最后一行代码报错：\r\n0x00007FFE2114CD29 处(位于 TensorVision_PF.exe 中)有未经处理的异常: Microsoft C++ 异常: ov::Exception，位于内存位置 0x0000004305AF6D88 处。\r\n\r\n这个报错是参数设置不完整吗？能不能给一个范例的连接？\r\n\r\n还有就是能不能加一个类似TRT的FP16参数？\r\noption.trt_option.enable_fp16 = true;\r\n",
        "state": "closed",
        "user": "534114658",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-25T02:56:18+00:00",
        "updated_at": "2024-11-26T05:33:32+00:00",
        "closed_at": "2024-05-28T06:38:38+00:00",
        "comments_count": [
            "WilliamQf-AI",
            "534114658",
            "WilliamQf-AI",
            "moxiuyuan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1990,
        "title": "PPYOLOER返回的值不是4点坐标，是PPYOLOE的结果",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：  fastdeploy-win-x64-gpu-1.0.7.zip\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【数据返回问题】\r\n- fastdeploy::vision::DetectionResult res;\r\n- 返回的值\r\n- std::vector<std::array<float, 8>> rotated_boxes;\r\n- 没有结果\r\n\r\n返回的结果类似PPYOLOE的结果\r\n- std::vector<std::array<float, 4>> boxes;\r\n",
        "state": "closed",
        "user": "534114658",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-25T04:23:01+00:00",
        "updated_at": "2025-06-17T06:49:51+00:00",
        "closed_at": "2025-06-17T06:49:51+00:00",
        "comments_count": [
            "yaoyingzhang",
            "534114658",
            "hxuaj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1991,
        "title": "使用fastdeploy推理一张图片，部分文字缺失",
        "body": "使用fastdeploy的BatchPredict推理一张图片结果如下：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/498c0219-3358-466b-94b6-959d936c19f6)\r\n\r\n 同一张图片，使用paddleocr推理结果如下：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/995185e2-3c28-4546-87a6-befef04a4162)\r\n\r\n对比之下缺失了整整一行文字，请问这是什么问题，怎样解决？",
        "state": "open",
        "user": "WilliamQf-AI",
        "closed_by": null,
        "created_at": "2023-05-25T04:37:56+00:00",
        "updated_at": "2023-06-12T02:01:38+00:00",
        "closed_at": null,
        "comments_count": [
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1992,
        "title": "使用fastdeploy在win64环境下做table推理崩溃",
        "body": "报错如下：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/82f404da-707d-4a1d-afb9-709b28c4adfe)\r\n\r\n执行这一行时：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/db3c05e6-96b3-409c-82d4-a44856900db2)\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "WilliamQf-AI",
        "closed_by": null,
        "created_at": "2023-05-25T07:52:55+00:00",
        "updated_at": "2023-06-12T01:07:41+00:00",
        "closed_at": null,
        "comments_count": [
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI",
            "thunder95",
            "WilliamQf-AI",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1995,
        "title": "docker镜像版本咨询",
        "body": "按照文档 FastDeploy/examples/vision/detection/paddledetection/serving/README_CN.md\r\n中镜像地址：\r\ndocker pull registry.baidubce.com/paddlepaddle/fastdeploy:x.y.z-gpu-cuda11.4-trt8.4-21.10\r\nx.y.z为镜像版本号，需替换成fastdeploy版本数字\r\n\r\n\r\n问下现在版本支持进度怎么样？是每几个版本制作一次？还是每个版本都有\r\n\r\n我从1.0.7试到了1.0.3才找到镜像。",
        "state": "closed",
        "user": "DeMeng33",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-26T06:55:10+00:00",
        "updated_at": "2025-06-24T06:46:16+00:00",
        "closed_at": "2025-06-24T06:46:16+00:00",
        "comments_count": [
            "dolphinlife"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1999,
        "title": "jetson xavier nx运行example的yolov8报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 0.2.1 和 fastdeploy-python 0.0.0（由源码编译安装，文件名版本就是0.0.0）\r\n- 【编译命令】前者是用pip3 install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html命令直接安装，后者使用下方代码完成安装\r\n-  '\r\n- git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\n\r\n# ENABLE_PADDLE_BACKEND & PADDLEINFERENCE_DIRECTORY为可选项\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport PADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n- \r\n-  '\r\n- \r\n- 【系统平台】: jetson xavier nx 上的ubuntu 18.04 LTS ，64bit\r\n- 【硬件】： Processor：ARMv8 Processor rev 0 (v8l) × 6\r\n- Graphics： NVIDIA Tegra Xavier (nvgpu)/integrated\r\n-  Developer Kit：JetPack 4.6\r\n- CUDA 10.2.300\r\n-  CUDNN: 8.2.1.32\r\n-  TensorRT: 8.0.1.6\r\n- OpenCV version: 3.4.17-dev\r\n   OpenCV Cuda: NO\r\n- 【编译语言】： Python 3.6\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n执行示例时报错，\r\n‘WARNING:root:The installed fastdeploy-python package is not built with GPU, will force to use CPU. To use GPU, following the commands to install fastdeploy-gpu-python.\r\nWARNING:root:    ================= Install GPU FastDeploy===============\r\nWARNING:root:    python -m pip uninstall fastdeploy-python\r\nWARNING:root:    python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n[ERROR] fastdeploy/runtime/runtime_option.cc(216)::UseTrtBackend        The FastDeploy didn't compile with TrtBackend.’\r\n让我删除由源码编译安装的fastdeploy-python；删除后运行无论cpu、gpu和gpu use_trt都报错，显示\r\nAttributeError: module 'fastdeploy' has no attribute 'RuntimeOption'\r\n如果不删除，运行cpu时报错\r\nAttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'vision'\r\n",
        "state": "open",
        "user": "yamato720",
        "closed_by": null,
        "created_at": "2023-05-29T05:07:22+00:00",
        "updated_at": "2023-05-29T05:07:22+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1993,
        "title": "C#部署FastDeploy的问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-win-x64-gpu-1.0.6\r\n- 【编译命令】使用CMake3.24.1\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：Nvidia GeForce RTX3060 Laptop GPU，\r\n- 【环境配置】 CUDA 11.2, CUDNN 11.3，TensorRT-8.4.1.5\r\n- 【编译语言】： C#\r\ncmake编译时设置如下\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/126551342/ed7c78e5-4848-4c54-b175-e6140b4a7320)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/126551342/98fa9dae-80ef-426a-9085-843b8f421e95)\r\n第一次编译成功时未勾选WITH_CAPI\r\n第二次编译成功时勾选了WITH_CAPI\r\n编译成功后，以下为测试代码\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/126551342/09bfc07f-d378-49ab-8f1d-81a4bc6c4614)\r\n第一次没勾选WITH_CAPI，测试报错找不到函数FD_C_CreateRuntimeOptionWrapper入口，\r\n第二次勾选WITH_CAPI，测试报错试图加载格式不正确的程序，或者报错找不到fastdeploy.dll。我检查过了，各个项目都设置的X64,\r\nC#项目的平台都设置的Release,选的.net都是.Net Framework4.7或者4.8,Nuget包opencvSharp4用的是最新的4.7.0.20230115，目前尚未找到解决办法\r\n",
        "state": "open",
        "user": "sanyangAZ",
        "closed_by": null,
        "created_at": "2023-05-25T09:11:02+00:00",
        "updated_at": "2024-10-11T01:45:35+00:00",
        "closed_at": null,
        "comments_count": [
            "MonahovS",
            "zhinangubei",
            "zhcco",
            "guoyunqingyue",
            "flytocc",
            "Aspen0309",
            "BigerPatax",
            "GentlerMan",
            "ITCZhuxy",
            "zhouyuxixixi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 1996,
        "title": "qt c++ ppocrv3 rec 部署配置  链接库  无法成功运行",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.6\r\n- 【编译命令】非自行编译，直接下载\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- 在QT中开发，仅使用ppocrv3中的rec模型。\r\n- 在pro文件中配置了相关动态链接，应用了相关头文件等，\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/53334346/bbd62932-d70b-4019-b374-108f2301ba67)\r\n以release编译程序结果是奔溃，无其他提示。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/53334346/ab201f4d-a5b3-4e69-b242-5fd9a872302a)\r\n\r\n这是没有自行编译的原因吗？\r\n我应该要自己去编写一个ppocrv3中的rec 程序？然后在vsstudio2019中进行编译，然后把相关库链接放到我的exe文件中。\r\n\r\n",
        "state": "closed",
        "user": "kingkingpang",
        "closed_by": "kingkingpang",
        "created_at": "2023-05-27T03:27:58+00:00",
        "updated_at": "2023-06-11T13:48:36+00:00",
        "closed_at": "2023-06-11T13:48:36+00:00",
        "comments_count": [
            "kingkingpang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2003,
        "title": "FastDeploy服务化部署，服务启动失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： paddlepaddle/fastdeploy:0.6.0-cpu-only-21.10\r\n- 【编译命令】直接下载的docker镜像\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： CPU\r\n\r\n\r\n- 【启动服务时出错】\r\n- - 执行命令“fastdeployserver --model-repository=/serving/models”出现以下错误：\r\nW0530 07:35:33.065839 152 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\nI0530 07:35:33.065931 152 model_lifecycle.cc:459] loading: postprocess:1\r\nI0530 07:35:33.066111 152 model_lifecycle.cc:459] loading: preprocess:1\r\nW0530 07:35:33.066222 152 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\nI0530 07:35:33.066271 152 model_lifecycle.cc:459] loading: runtime:1\r\nI0530 07:35:33.346340 178 pb_stub.cc:245]  Failed to initialize Python stub for auto-complete: RuntimeError: FastDeploy initalized failed!\r\n- E0530 07:35:33.355016 152 model_lifecycle.cc:596] failed to load 'postprocess' version 1: Internal: RuntimeError: FastDeploy initalized failed!\r\n- I0530 07:35:33.601141 195 pb_stub.cc:245]  Failed to initialize Python stub for auto-complete: RuntimeError: FastDeploy initalized failed!\r\n- E0530 07:35:33.609669 152 model_lifecycle.cc:596] failed to load 'preprocess' version 1: Internal: RuntimeError: FastDeploy initalized failed!\r\n",
        "state": "open",
        "user": "fenxiangwpp",
        "closed_by": null,
        "created_at": "2023-05-30T07:46:26+00:00",
        "updated_at": "2023-05-30T07:46:26+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2000,
        "title": "BatchPredict Failed to inference while using model:PaddleSeg",
        "body": "使用文档中的Unet分割模型转成rknn部署到瑞芯微上提示该错误：\r\n\r\n配置文件\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/63840313/8af1e89a-0dc8-4470-aa48-b318f69b0b03)\r\n模型转换命令：\r\n\r\npython paddle_infer_shape.py --model_dir ./ppyolo_r50vd_dcn_1x_coco \\\r\n                             --model_filename model.pdmodel \\\r\n                             --params_filename model.pdiparams \\\r\n                             --save_dir ./inference \\\r\n                             --input_shape_dict=\"{'image':[1,3,608,608], 'scale_factor':[1,2],'im_shape':[1,2]}\"\r\n\r\npaddle2onnx --model_dir ./inference_seg --model_filename model.pdmodel --params_filename model.pdiparams --save_file Unet_cityscapes_without_argmax_infer/unet_lane_seg.onnx --enable_dev_version False --opset_version 12 --enable_onnx_checker True \r\n\r\n其中--enable_dev_version改为True的话会提示错误：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/63840313/8359ce1f-2fb6-4190-a57c-23212640c693)\r\n\r\n\r\npython3 ../FastDeploy/tools/rknpu2/export.py --config_path=/data/project/rknn-toolkit2/models/Unet_cityscapes_without_argmax_infer/config.yml --target_platform=RK3588\r\n\r\n\r\n模型转换后预测代码：\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/63840313/eefdc342-c2f5-4a02-8699-416e2b114fe1)\r\n\r\n\r\n预测错误：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/63840313/1d506b65-0981-4459-a37b-5bee9f6f89be)\r\n\r\n\r\n【FastDeploy版本】：develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【模型转换使用系统平台】: Linux x64\r\n- 【预测硬件】： 瑞芯微\r\n- 【编译语言】： C++\r\n\r\n",
        "state": "open",
        "user": "DevilMay-Cry",
        "closed_by": null,
        "created_at": "2023-05-29T12:49:01+00:00",
        "updated_at": "2023-05-31T03:13:41+00:00",
        "closed_at": null,
        "comments_count": [
            "DefTruth",
            "DevilMay-Cry"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2002,
        "title": "运行resnet的C++ example报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： \r\nFastDeploy develop分支\r\n- 【编译命令】\r\n按照https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/classification/resnet/cpp的代码运行\r\n- 【系统平台】: \r\nLinux x64(Ubuntu 18.04) \r\n- 【硬件】：\r\nT4\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n按照RM执行onnx TensorRT推理会报错\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log   3: [runtime.cpp::~Runtime::346] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::346, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.\r\n)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/19405021/26c99d81-ef06-439f-aa78-945b8c9b8a51)\r\n\r\n\r\n",
        "state": "closed",
        "user": "hellojiabin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-05-30T07:33:37+00:00",
        "updated_at": "2024-07-30T06:42:20+00:00",
        "closed_at": "2024-07-30T06:41:50+00:00",
        "comments_count": [
            "121786404",
            "jnulzl",
            "hellojiabin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2004,
        "title": "rk开发板的交叉编译fd，make install 时候没有第三方库",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n按照FastDeploy/examples/vision/detection/rkyolo/cpp/README_CN.md 中的文档，版本都是最新的版本。\r\n\r\n编译命令：\r\ncmake ..  \r\n\t\t  -DCMAKE_C_COMPILER=/usr/bin/aarch64-linux-gnu-gcc \\\r\n\t\t  -DCMAKE_CXX_COMPILER=/usr/bin/aarch64-linux-gnu-g++ \\\r\n          -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake \\\r\n          -DTARGET_ABI=arm64 \\\t\t  \r\n\t\t  -DENABLE_ORT_BACKEND=ON \\\r\n\t      -DENABLE_RKNPU2_BACKEND=ON \\\r\n\t      -DENABLE_VISION=ON \\\r\n\t      -DRKNN2_TARGET_SOC=RK3588 \\\r\n          -DENABLE_ORT_BACKEND=ON \\\r\n          -DCMAKE_INSTALL_PREFIX=${PWD}/build_aarch/install/fastdeploy-0.0.3\r\nmake -j8\r\nmake install DESTDIR=./install/\r\n",
        "state": "open",
        "user": "jmlw8023",
        "closed_by": null,
        "created_at": "2023-05-31T03:45:59+00:00",
        "updated_at": "2023-05-31T03:45:59+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2005,
        "title": "PPYOLOER使用C++推理（fastdeploy-win-x64-gpu-1.0.7）获取目标结果不正确",
        "body": "\r\n auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    auto model = fastdeploy::vision::detection::PPYOLOER(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    const cv::Mat im = cv::imread(image_file);\r\n\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n\r\n使用事例中的网络和图片，获取res.rotated_boxes.size() =0 \r\n请帮忙分析一下是哪里问题？",
        "state": "open",
        "user": "yaoyingzhang",
        "closed_by": null,
        "created_at": "2023-05-31T08:07:54+00:00",
        "updated_at": "2023-05-31T08:07:54+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2006,
        "title": "PPYOLOER使用C++推理（fastdeploy-win-x64-gpu-1.0.7）获取旋转目标数量为0",
        "body": "\r\n auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    auto model = fastdeploy::vision::detection::PPYOLOER(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    const cv::Mat im = cv::imread(image_file);\r\n\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n\r\n使用事例中的网络和图片，获取res.rotated_boxes.size() =0 \r\n请帮忙分析一下是哪里问题？",
        "state": "open",
        "user": "yaoyingzhang",
        "closed_by": null,
        "created_at": "2023-05-31T08:08:29+00:00",
        "updated_at": "2023-05-31T08:08:29+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2012,
        "title": "FastDeploy和python ocr PPocr3推理结果不一致",
        "body": "自己构建的测试集，分别用python和fastdploy推理结果不一致，要修改fastdploy的推理求请教",
        "state": "closed",
        "user": "Edwina414",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-02T08:16:09+00:00",
        "updated_at": "2024-09-17T06:42:43+00:00",
        "closed_at": "2024-09-17T06:42:43+00:00",
        "comments_count": [
            "dizhenx",
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2008,
        "title": "【瑞芯微】unet分割模型推理报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 瑞芯微 fastdeploy\r\n- 【编译命令】瑞芯微编译参数\r\n- 【系统平台】: Linux topeet 5.10.66 #10 SMP Mon Nov 14 21:31:11 PST 2022 aarch64 aarch64 aarch64 GNU/Linux\r\n- 【硬件】： 瑞芯微rk3588 \r\n- 【编译语言】：Python 3.8.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n用的是最新的 rknn-toolkit2 =1.5.1b1+daed8db3 导出模型\r\nfastdeploy 也是 release 1.0.7\r\n```python\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[INFO] fastdeploy/backends/rknpu/rknpu2/rknpu2_backend.cc(56)::GetSDKAndDeviceVersion   rknn_api/rknnrt version: 1.4.0 (a10f100eb@2022-09-09T09:07:14), driver version: 0.7.2\r\nindex=0, name=x, n_dims=4, dims=[1, 1024, 512, 3], n_elems=1572864, size=3145728, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=bilinear_interp_v2_3.tmp_0, n_dims=4, dims=[1, 64, 1024, 512], n_elems=33554432, size=67108864, fmt=NCHW, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime.cc(664)::Init Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/backends/rknpu/rknpu2/rknpu2_backend.cc(314)::Infer        The input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\nE RKNN: [15:11:06.475] failed to submit!, op id: 15, op name: Resize:p2o.Resize.0, flags: 0x5, task start: 0, task number: 4095, run task counter: 1504, int status: 0\r\n[ERROR] fastdeploy/backends/rknpu/rknpu2/rknpu2_backend.cc(393)::Infer  rknn run error! ret=-1\r\n[ERROR] fastdeploy/vision/segmentation/ppseg/model.cc(86)::BatchPredict Failed to inference while using model:PaddleSeg.\r\nSegmentation fault (core dumped)\r\n```\r\ninfer.py\r\n```python\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\n\r\n\r\ndef parse_arguments():\r\n    import argparse\r\n    import ast\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \"--model_file\", required=True, help=\"Path of PaddleSeg model.\")\r\n    parser.add_argument(\r\n        \"--config_file\", required=True, help=\"Path of PaddleSeg config.\")\r\n    parser.add_argument(\r\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\r\n    return parser.parse_args()\r\n\r\n\r\ndef build_option(args):\r\n    option = fd.RuntimeOption()\r\n    option.use_rknpu2()\r\n    return option\r\n\r\n\r\nargs = parse_arguments()\r\n\r\n# 配置runtime，加载模型\r\nruntime_option = build_option(args)\r\nmodel_file = args.model_file\r\nparams_file = \"\"\r\nconfig_file = args.config_file\r\nmodel = fd.vision.segmentation.PaddleSegModel(\r\n    model_file,\r\n    params_file,\r\n    config_file,\r\n    runtime_option=runtime_option,\r\n    model_format=fd.ModelFormat.RKNN)\r\n\r\nmodel.preprocessor.disable_normalize()\r\nmodel.preprocessor.disable_permute()\r\n# 预测图片分割结果\r\nim = cv2.imread(args.image)\r\nresult = model.predict(im.copy())\r\nprint(result)\r\n\r\n# 可视化结果\r\nvis_im = fd.vision.vis_segmentation(im, result, weight=0.5)\r\ncv2.imwrite(\"vis_img.png\", vis_im)\r\n````\r\n模型文件\r\n链接: https://pan.baidu.com/s/1ufvNPemd4Yrkoqgu6KixOQ?pwd=a293 提取码: a293 \r\n\r\n辛苦了！ @Zheng-Bicheng ",
        "state": "closed",
        "user": "pengwei1024",
        "closed_by": "pengwei1024",
        "created_at": "2023-06-01T07:17:24+00:00",
        "updated_at": "2023-07-22T05:41:31+00:00",
        "closed_at": "2023-07-22T05:41:31+00:00",
        "comments_count": [
            "pengwei1024",
            "srd2018",
            "pengwei1024",
            "srd2018"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2016,
        "title": "CreateRKNPUBackend Cannot find an available npu backend to load this model.",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-aarch64-0.0.0 Develop version\r\n- 【编译命令】无，下载了预编译库[fastdeploy-linux-aarch64-0.0.0.tgz](https://fastdeploy.bj.bcebos.com/dev/cpp/fastdeploy-linux-aarch64-0.0.0.tgz)\r\n- 【系统平台】: Linux aarch64(Ubuntu 22.04.2 LTS) \r\n- 【硬件】： 野火鲁班猫LubanCat-Zero-W\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n严格按照如下中文文档进行操作：\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/cpp/README_CN.md\r\n经实验，aarch64-1.0.7版本无法下载，下载链接失效，其他release版本也无法make，只有aarch64-0.0.0 可以make成功，随后添加环境变量\r\n```\r\nsource /home/cat/FastDeploy/examples/vision/detection/paddledetection/rknpu2/cpp/build/fastdeploy-linux-aarch64-0.0.0/fastdeploy_init.sh\r\n```\r\n然后下载文档中的模型和图片运行命令：\r\n```\r\n./infer_picodet_demo ./picodet_s_416_coco_lcnet 000000014439.jpg 1\r\n```\r\n输出结果为：\r\n```\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[ERROR] fastdeploy/fastdeploy_model.cc(276)::CreateRKNPUBackend Cannot find an available npu backend to load this model.\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(32)::Initialize       Failed to initialize fastdeploy backend.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\nSegmentation fault (core dumped)\r\n```\r\n请问如何解决该问题？？？\r\n",
        "state": "closed",
        "user": "leozjr",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-06T14:34:22+00:00",
        "updated_at": "2025-02-16T07:56:06+00:00",
        "closed_at": "2025-01-21T06:40:45+00:00",
        "comments_count": [
            "srd2018",
            "leozjr",
            "srd2018",
            "Luo73",
            "ilbash",
            "Luo73"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2023,
        "title": "可否提供docker镜像环境？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- linux\r\n- cpu or gpu\r\n- paddlepaddle各类产品对环境版本依赖并不完全相同；为避免不必要的麻烦，可否提供docker\r\n\r\n",
        "state": "open",
        "user": "GDbbq",
        "closed_by": "GDbbq",
        "created_at": "2023-06-09T07:36:38+00:00",
        "updated_at": "2023-10-08T04:08:25+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2020,
        "title": "版面分析批量推理只能输出一个结果",
        "body": "![3fe3674f065d7a83c217307ed8b6db0](https://github.com/PaddlePaddle/FastDeploy/assets/40328063/6ff08f72-fb61-41f4-9075-d1cd45463c44)\r\n这是咋回事？",
        "state": "open",
        "user": "WilliamQf-AI",
        "closed_by": null,
        "created_at": "2023-06-08T11:57:13+00:00",
        "updated_at": "2023-06-09T06:15:28+00:00",
        "closed_at": null,
        "comments_count": [
            "WilliamQf-AI",
            "WilliamQf-AI",
            "WilliamQf-AI"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2019,
        "title": "C++模型加载的时候就报错了python没有问题",
        "body": "\r\n## 环境\r\n  windows11\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/e776fa4b-d0ac-4e86-9e93-ab20d6681bf3)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/c2097e8e-85e7-46c5-8f6d-dd72edb3d2f6)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/19c1e96d-7316-4990-a955-a78eb105ad22)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/fbd1f2e1-8253-4d4c-b061-6faceb370245)\r\n\r\n- 【硬件】： CUDA 11.7  \r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/53092376-a9c5-4f3e-be73-44b5ee52dc6a)\r\n## python代码样例\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/37928107/f54e5aea-7414-4b05-8244-367e85d3eafa)\r\n\r\n\r\n",
        "state": "open",
        "user": "LIUSHUAI2018",
        "closed_by": null,
        "created_at": "2023-06-08T09:31:24+00:00",
        "updated_at": "2023-06-09T05:56:01+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "LIUSHUAI2018",
            "jiangjiajun",
            "LIUSHUAI2018",
            "LIUSHUAI2018",
            "LIUSHUAI2018"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2021,
        "title": "不改变代码的情况下直接输入yolov5s6模型是否支持？",
        "body": "我 如果直接使用1280分辨率训练的yolov5s6模型，前处理还是用编译好的库，看起来可以直接运行，推理时间也有所增加，但是前处理代码没做任何修改，默认yolov5原始代码使用32倍下采样，不是64倍，这样会对精度产生一定影响？但是我比较下来比640分辨率结果有所提升，如果修改前处理代码中32倍下采样，和init中分辨率默认值，是否还有所提升？或者直接问支持1280分辨率下yolov5s6推理吗？",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "jia0511",
        "created_at": "2023-06-08T13:52:20+00:00",
        "updated_at": "2023-06-13T02:44:56+00:00",
        "closed_at": "2023-06-13T02:44:56+00:00",
        "comments_count": [
            "jiangjiajun",
            "jia0511",
            "jiangjiajun",
            "jia0511",
            "jiangjiajun",
            "jia0511",
            "jia0511"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2025,
        "title": "The output index: 93 is larger than the size of label_list: 14. Please check the label file!",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.6\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】：CPU\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/53334346/ad4cf6a4-db1e-4758-85ed-2ea256eeda23)\r\n模型是我自己下载ppocr_v3预训练模型在自己的数据集上训练导出的，我在yml文件中改了label的输出，因为我不需要那么多字符，如下。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/53334346/1824bdda-1719-41ef-9e32-abdfeb6f719a)\r\n其他的没变。\r\n然后测试过程报错如下。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/53334346/145012f8-de3a-409e-9def-15eeca3be886)\r\n\r\n请问目前是不支持自己修改yml文件训练后的模型吗？？\r\n",
        "state": "open",
        "user": "kingkingpang",
        "closed_by": null,
        "created_at": "2023-06-11T13:56:59+00:00",
        "updated_at": "2023-06-11T13:56:59+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2024,
        "title": "在CUDA 12(Windows) 上构建失败:nvcc fatal   : Unsupported gpu architecture 'compute_35'",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：73f42e36beb959717f8a901251a12b0ff162e8d3\r\n- 【编译命令】: \r\n```\r\ncmake .. -G \"Visual Studio 17 2022\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON -DWITH_GPU=ON -DTRT_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\" -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\" -DCMAKE_INSTALL_PREFIX=\"C:\\Users\\MWX\\FastDeploy\\compiled_fastdeploy\"\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\n```\r\n- 【系统平台】:Windows x64(Windows11)\r\n- 【硬件】： Nvidia GPU 3070TI Laptop， CUDA 12.1 CUDNN 8.9.2\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【编译失败】\r\n```\r\n> msbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\n...\r\n 23>CudaBuildCore:\r\n         nvcc fatal   : Unsupported gpu architecture 'compute_35'\r\n    23>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.1.\r\n       targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use\r\n       -local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n       \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\\r\n       eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n       rd_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC\r\n       :\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Compu\r\n       ting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\U\r\n       sers\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cp\r\n       p\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n       uild\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\pa\r\n       ddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDep\r\n       loy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\open\r\n       vino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --kee\r\n       p-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -\r\n       gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -genc\r\n       ode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode\r\n       arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n       a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fast\r\n       deploy/vision/common/processors/normalize_and_permute.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML\r\n       _CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_\r\n       PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABL\r\n       E_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xco\r\n       mpiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\\r\n       Release\\/fastdeploy/vision/common/processors/normalize_and_permute.cu.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vi\r\n       sion\\common\\processors\\normalize_and_permute.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 个目标)的操作 - 失败。\r\n    23>CudaBuildCore:\r\n         nvcc fatal   : Unsupported gpu architecture 'compute_35'\r\n    23>CudaBuildCore:\r\n         nvcc fatal   : Unsupported gpu architecture 'compute_35'\r\n    23>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.1.\r\n       targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use\r\n       -local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n       \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\\r\n       eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n       rd_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC\r\n       :\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Compu\r\n       ting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\U\r\n       sers\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cp\r\n       p\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n       uild\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\pa\r\n       ddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDep\r\n       loy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\open\r\n       vino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --kee\r\n       p-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -\r\n       gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -genc\r\n       ode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode\r\n       arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n       a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fast\r\n       deploy/runtime/backends/common/cuda/adaptive_pool2d_kernel.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -\r\n       DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DEN\r\n       ABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -D\r\n       ENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS\r\n        -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy\r\n       .dir\\Release\\adaptive_pool2d_kernel.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\runtime\\backends\\common\\cuda\\adaptiv\r\n       e_pool2d_kernel.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n    23>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.1.\r\n       targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use\r\n       -local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n       \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\\r\n       eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n       rd_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC\r\n       :\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Compu\r\n       ting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\U\r\n       sers\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cp\r\n       p\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n       uild\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\pa\r\n       ddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDep\r\n       loy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\open\r\n       vino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --kee\r\n       p-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -\r\n       gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -genc\r\n       ode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode\r\n       arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n       a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fast\r\n       deploy/vision/common/processors/normalize.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DF\r\n       ASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKE\r\n       ND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENA\r\n       BLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHs\r\n       c /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\/fas\r\n       tdeploy/vision/common/processors/normalize.cu.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\common\\processors\\n\r\n       ormalize.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 个目标)的操作 - 失败。\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 个目标)的操作 - 失败。\r\n    23>CudaBuildCore:\r\n         cmd.exe /C \"C:\\Users\\MWX\\AppData\\Local\\Temp\\tmp3f74f4dfd91c406496900b81aaa10ee5.cmd\"\r\n         \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use-local-env -ccbin \"C:\\Program Fi\r\n         les\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\\x64\" -x cu   -IC:\\Users\\MWX\\F\r\n         astDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\eigen -IC:\\Users\\MWX\\FastDep\r\n         loy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\paddle_inf\r\n         erence -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n         uild\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.\r\n         1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\Users\\MWX\\FastDeploy\\\r\n         .\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cpp\\include\" -IC:\\Us\r\n         ers\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\\r\n         install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\paddle2onnx\\incl\r\n         ude -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDeploy\\build\\th\r\n         ird_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runti\r\n         me\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --keep-dir x6\r\n         4\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencod\r\n         e arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode\r\n         arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode ar\r\n         ch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n         a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n         EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND\r\n          -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENA\r\n         BLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE_\r\n         _=\\\"fastdeploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL\r\n          -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE\r\n         _BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TE\r\n         XT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcomp\r\n         iler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\\r\n         Release\\yolo_preprocess.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\utils\\yolo_preprocess.cu\"\r\n    23>CudaBuildCore:\r\n         cmd.exe /C \"C:\\Users\\MWX\\AppData\\Local\\Temp\\tmp1a6ff40ea2a641169ed293d39d93eee7.cmd\"\r\n         \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use-local-env -ccbin \"C:\\Program Fi\r\n         les\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\\x64\" -x cu   -IC:\\Users\\MWX\\F\r\n         astDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\eigen -IC:\\Users\\MWX\\FastDep\r\n         loy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\paddle_inf\r\n         erence -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n         uild\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.\r\n         1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\Users\\MWX\\FastDeploy\\\r\n         .\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cpp\\include\" -IC:\\Us\r\n         ers\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\\r\n         install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\paddle2onnx\\incl\r\n         ude -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDeploy\\build\\th\r\n         ird_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runti\r\n         me\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --keep-dir x6\r\n         4\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencod\r\n         e arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode\r\n         arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode ar\r\n         ch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n         a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n         EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND\r\n          -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENA\r\n         BLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE_\r\n         _=\\\"fastdeploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n         LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n         DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABL\r\n         E_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHs\r\n         c /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\cu\r\n         da_cast.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\function\\cuda_cast.cu\"\r\n    23>CudaBuildCore:\r\n\r\n         C:\\Users\\MWX\\FastDeploy\\build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use-l\r\n         ocal-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n         \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n         y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\r\n         \\third_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\inclu\r\n         de -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA G\r\n         PU Computing Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\inclu\r\n         de\" -IC:\\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_p\r\n         arty\\yaml-cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MW\r\n         X\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third\r\n         _libs\\install\\paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\r\n         \\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n         rd_libs\\install\\openvino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v1\r\n         2.1\\include\"     --keep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch\r\n         =compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=c\r\n         ompute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=com\r\n         pute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-c\r\n         onstexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -\r\n         DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_B\r\n         ACKEND -DENABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENA\r\n         BLE_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfas\r\n         tdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_W\r\n         INDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENAB\r\n         LE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKE\r\n         ND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\r\n         \"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\r\n         \\vc143.pdb\" -o fastdeploy.dir\\Release\\yolo_preprocess.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\utils\\yol\r\n         o_preprocess.cu\"\r\n         nvcc fatal   : Unsupported gpu architecture 'compute_35'\r\n    23>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.1.\r\n       targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use\r\n       -local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n       \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\\r\n       eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n       rd_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC\r\n       :\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Compu\r\n       ting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\U\r\n       sers\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cp\r\n       p\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n       uild\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\pa\r\n       ddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDep\r\n       loy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\open\r\n       vino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --kee\r\n       p-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -\r\n       gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -genc\r\n       ode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode\r\n       arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n       a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fast\r\n       deploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0\r\n       /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\yolo_prepr\r\n       ocess.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\utils\\yolo_preprocess.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDe\r\n       ploy\\build\\fastdeploy.vcxproj]\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 个目标)的操作 - 失败。\r\n    23>CudaBuildCore:\r\n\r\n         C:\\Users\\MWX\\FastDeploy\\build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use-l\r\n         ocal-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n         \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n         y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\r\n         \\third_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\inclu\r\n         de -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA G\r\n         PU Computing Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\inclu\r\n         de\" -IC:\\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_p\r\n         arty\\yaml-cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MW\r\n         X\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third\r\n         _libs\\install\\paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\r\n         \\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n         rd_libs\\install\\openvino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v1\r\n         2.1\\include\"     --keep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch\r\n         =compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=c\r\n         ompute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=com\r\n         pute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-c\r\n         onstexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -\r\n         DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_B\r\n         ACKEND -DENABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENA\r\n         BLE_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfas\r\n         tdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DN\r\n         DEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BAC\r\n         KEND -DENABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABL\r\n         E_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastd\r\n         eploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\r\n         \" -o fastdeploy.dir\\Release\\cuda_cast.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\function\\cuda_cast.cu\"\r\n         nvcc fatal   : Unsupported gpu architecture 'compute_35'\r\n    23>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.1.\r\n       targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --use\r\n       -local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\r\n       \\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_party\\\r\n       eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\thi\r\n       rd_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -IC\r\n       :\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Compu\r\n       ting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\\U\r\n       sers\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-cp\r\n       p\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\\b\r\n       uild\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\pa\r\n       ddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastDep\r\n       loy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\open\r\n       vino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --kee\r\n       p-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -\r\n       gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -genc\r\n       ode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode\r\n       arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambd\r\n       a -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEP\r\n       LOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DE\r\n       NABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PA\r\n       DDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fast\r\n       deploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -D\r\n       CMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DENABLE_OPEN\r\n       VINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONNX\r\n       -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O\r\n       2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\cuda_cast.obj \"C:\\Us\r\n       ers\\MWX\\FastDeploy\\fastdeploy\\function\\cuda_cast.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxp\r\n       roj]\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 个目标)的操作 - 失败。\r\n    23>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标)的操作 - 失败。\r\n     4>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标)的操作 - 失败。\r\n     3>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标)的操作 - 失败。\r\n     2>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标)的操作 - 失败。\r\n     1>已完成生成项目“C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标)的操作 - 失败。\r\n\r\n生成失败。\r\n\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标) (3) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标) (4) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (23) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 目标) (23:4) ->\r\n       (CudaBuildCore 目标) ->\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.\r\n       1.targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --u\r\n       se-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX\r\n       64\\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n       y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\t\r\n       hird_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -\r\n       IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Com\r\n       puting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\r\n       \\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-\r\n       cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\r\n       \\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\\r\n       paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastD\r\n       eploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\op\r\n       envino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --k\r\n       eep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35\r\n        -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -ge\r\n       ncode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencod\r\n       e arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lam\r\n       bda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fa\r\n       stdeploy/vision/common/processors/normalize_and_permute.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYA\r\n       ML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABL\r\n       E_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENA\r\n       BLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -X\r\n       compiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.di\r\n       r\\Release\\/fastdeploy/vision/common/processors/normalize_and_permute.cu.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\\r\n       vision\\common\\processors\\normalize_and_permute.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxpro\r\n       j]\r\n\r\n\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标) (3) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标) (4) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (23) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 目标) (23:3) ->\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.\r\n       1.targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --u\r\n       se-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX\r\n       64\\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n       y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\t\r\n       hird_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -\r\n       IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Com\r\n       puting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\r\n       \\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-\r\n       cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\r\n       \\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\\r\n       paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastD\r\n       eploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\op\r\n       envino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --k\r\n       eep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35\r\n        -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -ge\r\n       ncode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencod\r\n       e arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lam\r\n       bda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fa\r\n       stdeploy/runtime/backends/common/cuda/adaptive_pool2d_kernel.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG\r\n        -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -D\r\n       ENABLE_PADDLE_BACKEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION\r\n       -DENABLE_TEXT -DENABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPOR\r\n       TS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdepl\r\n       oy.dir\\Release\\adaptive_pool2d_kernel.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\runtime\\backends\\common\\cuda\\adapt\r\n       ive_pool2d_kernel.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n\r\n\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标) (3) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标) (4) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (23) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 目标) (23:5) ->\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.\r\n       1.targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --u\r\n       se-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX\r\n       64\\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n       y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\t\r\n       hird_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -\r\n       IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Com\r\n       puting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\r\n       \\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-\r\n       cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\r\n       \\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\\r\n       paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastD\r\n       eploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\op\r\n       envino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --k\r\n       eep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35\r\n        -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -ge\r\n       ncode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencod\r\n       e arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lam\r\n       bda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fa\r\n       stdeploy/vision/common/processors/normalize.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -\r\n       DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BAC\r\n       KEND -DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DE\r\n       NABLE_PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/E\r\n       Hsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\/f\r\n       astdeploy/vision/common/processors/normalize.cu.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\common\\processors\r\n       \\normalize.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n\r\n\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标) (3) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标) (4) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (23) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 目标) (23:6) ->\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.\r\n       1.targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --u\r\n       se-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX\r\n       64\\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n       y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\t\r\n       hird_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -\r\n       IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Com\r\n       puting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\r\n       \\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-\r\n       cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\r\n       \\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\\r\n       paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastD\r\n       eploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\op\r\n       envino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --k\r\n       eep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35\r\n        -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -ge\r\n       ncode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencod\r\n       e arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lam\r\n       bda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fa\r\n       stdeploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W\r\n       0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\yolo_pre\r\n       process.obj \"C:\\Users\\MWX\\FastDeploy\\fastdeploy\\vision\\utils\\yolo_preprocess.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\Fast\r\n       Deploy\\build\\fastdeploy.vcxproj]\r\n\r\n\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\copy_yaml_include.vcxproj.metaproj”(默认目标) (3) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj.metaproj”(默认目标) (4) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (23) ->\r\n       “C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vcxproj”(CudaBuildCore 目标) (23:2) ->\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.\r\n       1.targets(799,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin\\nvcc.exe\"  --u\r\n       se-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX\r\n       64\\x64\" -x cu   -IC:\\Users\\MWX\\FastDeploy\\. -IC:\\Users\\MWX\\FastDeploy\\build -IC:\\Users\\MWX\\FastDeploy\\third_part\r\n       y\\eigen -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\onnxruntime\\include -IC:\\Users\\MWX\\FastDeploy\\build\\t\r\n       hird_libs\\install\\paddle_inference -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include -\r\n       IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\openvino\\runtime\\include\\ie -I\"C:\\Program Files\\NVIDIA GPU Com\r\n       puting Toolkit\\CUDA\\v12.1\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\TensorRT\\8.6.1\\include\" -IC:\r\n       \\Users\\MWX\\FastDeploy\\.\\fastdeploy\\runtime\\backends\\tensorrt\\common -I\"C:\\Users\\MWX\\FastDeploy\\third_party\\yaml-\r\n       cpp\\include\" -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\fast_tokenizer\\include -IC:\\Users\\MWX\\FastDeploy\r\n       \\build\\third_libs\\install\\fast_tokenizer\\third_party\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\\r\n       paddle2onnx\\include -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\opencv\\build\\include -IC:\\Users\\MWX\\FastD\r\n       eploy\\build\\third_libs\\install\\opencv\\build\\include\\opencv -IC:\\Users\\MWX\\FastDeploy\\build\\third_libs\\install\\op\r\n       envino\\runtime\\3rdparty\\tbb\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\include\"     --k\r\n       eep-dir x64\\Release  -maxrregcount=0   --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35\r\n        -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -ge\r\n       ncode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencod\r\n       e arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lam\r\n       bda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTD\r\n       EPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -\r\n       DENABLE_OPENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_\r\n       PADDLE2ONNX -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fa\r\n       stdeploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB\r\n       -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DENABLE_ORT_BACKEND -DENABLE_PADDLE_BACKEND -DENABLE_OP\r\n       ENVINO_BACKEND -DWITH_GPU -DENABLE_NVJPEG -DENABLE_TRT_BACKEND -DENABLE_VISION -DENABLE_TEXT -DENABLE_PADDLE2ONN\r\n       X -D__TBB_NO_IMPLICIT_LINKAGE=1 -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo\r\n       /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\cuda_cast.obj \"C:\\\r\n       Users\\MWX\\FastDeploy\\fastdeploy\\function\\cuda_cast.cu\"”已退出，返回代码为 1。 [C:\\Users\\MWX\\FastDeploy\\build\\fastdeploy.vc\r\n       xproj]\r\n\r\n    0 个警告\r\n    5 个错误\r\n\r\n已用时间 00:00:02.01\r\n```",
        "state": "open",
        "user": "justghostof",
        "closed_by": null,
        "created_at": "2023-06-09T08:03:00+00:00",
        "updated_at": "2024-12-12T07:38:59+00:00",
        "closed_at": null,
        "comments_count": [
            "weiweijeff",
            "justghostof",
            "lym169",
            "LateLinux",
            "smalie2222",
            "LateLinux",
            "chenbinghui1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2026,
        "title": "python版PPYOLOER模板程序返回数据不完整，格式不正确。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop \r\n- 【编译命令】按照GPU部署库编译文档执行\r\n- 【系统平台】: win11\r\n- 【硬件】：  Nvidia GPU RTX4090移动版， CUDA 12.1 CUDNN 8.9\r\n- 【编译语言】：  Python3.10\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- PPYOLOER推理结果数据格式与PaddleDetection2.6数据格式不一样\r\n- - 先执行`examples`下的部署示例，https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/python/infer_ppyoloe_r.py\r\n结果是：\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n317.424927,414.574188, 290.774170, 238.207764, 0.970878, 0\r\n457.598633,393.369568, 0.000000, 0.962562, 217.003098, 431\r\n\r\n用PaddleDetection2.6的程序\r\nhttps://github.com/PaddlePaddle/PaddleDetection/blob/release/2.6/deploy/python/infer.py\r\n结果是\r\n{'boxes': array([[  0.        ,   0.95843047, 588.8065    , 354.3309    ,\r\n        518.5979    , 189.9114    , 648.1887    , 134.57492   ,\r\n        718.39734   , 298.99442   ],\r\n       [  0.        ,   0.93819976, 453.2648    , 422.79578   ,\r\n        372.95996   , 262.75952   , 495.3757    , 201.33232   ,\r\n        575.68054   , 361.36856   ]], dtype=float32), 'boxes_num': array([2])}\r\n\r\nFastDeploy得到的结果明显不对，PPYOLOER的结果应该是4个点，不是两个对角点。\r\n第三行的0.000000, 0.962562, 其实应该是第二个目标的label_id编号0，以及score得分0.962562\r\n怎么才能得到完整且正确的数据？\r\n",
        "state": "closed",
        "user": "weiweijeff",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-11T13:58:19+00:00",
        "updated_at": "2025-06-17T06:49:52+00:00",
        "closed_at": "2025-06-17T06:49:52+00:00",
        "comments_count": [
            "zjx424",
            "weiweijeff",
            "hxuaj",
            "zjx424"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2031,
        "title": "c++GPU多线程推理 程序被挂起.--CPU多线程推理正常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.6\r\n- 【编译命令】预编译库\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU1660s， CUDA 11.2\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- c++ gpu推理，设置线程3 ，当图片只有一张的时候，后台知道看Thread 2 在运行\r\n- \r\n![11](https://github.com/PaddlePaddle/FastDeploy/assets/44670105/f332d57f-d058-4a65-a04d-5fc94cbb3253)\r\n - 当在图片集多个图片时，程序会卡住 挂起\r\n - \r\n\r\n![22](https://github.com/PaddlePaddle/FastDeploy/assets/44670105/a0ad8559-584e-4a75-9c99-e0aab6cbab36)\r\n",
        "state": "open",
        "user": "zzaozhuang",
        "closed_by": null,
        "created_at": "2023-06-14T09:20:27+00:00",
        "updated_at": "2023-06-14T09:30:26+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2027,
        "title": "能否提供java调用grpc 服务端pp-ocrv3的示例",
        "body": "能否提供java调用grpc 服务端pp-ocrv3的示例",
        "state": "closed",
        "user": "polarisunny",
        "closed_by": "polarisunny",
        "created_at": "2023-06-11T14:42:43+00:00",
        "updated_at": "2023-07-13T04:40:30+00:00",
        "closed_at": "2023-07-13T04:40:30+00:00",
        "comments_count": [
            "polarisunny"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2030,
        "title": "PaddleOCR通过FastDeploy部署到rv1126上遇到的问题",
        "body": "目标：使用PaddleOCR部署到瑞芯微rv1126上面\r\n\r\n- 【FastDeploy版本】： master\r\n- 【编译命令】\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)    交叉编译到rv1126平台\r\n- 【硬件】：rv1126\r\n- 【编译语言】： C++\r\n\r\n- 【模型跑不通】\r\n我用的是rv1126，它不是rknpu2，如果使用FastDeploy 以-DWITH_TIMVX=ON的方式进行编译。模型格式该是什么呢(不是rknn或者onnx吧？)？如何转换？\r\n",
        "state": "open",
        "user": "tonglingwen",
        "closed_by": null,
        "created_at": "2023-06-14T07:08:25+00:00",
        "updated_at": "2023-06-14T07:08:25+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2028,
        "title": "  fd_serving 启动服务后，为什么CPU会有占用？是哪里来的占用？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy:1.0.2-gpu-cuda11.4-trt8.4-21.10   官方镜像\r\n- 【系统平台】: linux Ubuntu 18.04\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1060I， CUDA 11.4\r\n\r\n启动服务后，CPU持续占用8%。\r\n就是不明白，还没开始预测为什么会持续占用\r\n\r\n\r\n",
        "state": "closed",
        "user": "teymur-git",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-13T03:04:54+00:00",
        "updated_at": "2024-07-09T06:40:40+00:00",
        "closed_at": "2024-07-09T06:40:40+00:00",
        "comments_count": [
            "teymur-git",
            "fuloong",
            "teymur-git",
            "teymur-git",
            "fuloong",
            "teymur-git",
            "teymur-git",
            "fuloong",
            "polarisunny",
            "teymur-git",
            "polarisunny"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2029,
        "title": "How to run ocr detection or recognition on AWS Lambda?",
        "body": "👋 Hello there,\r\nI found this amazing ml work done by you and it seems incredible!! 🎊\r\n\r\nI'm going to experiment it on a full backend service, like a AWS Lambda. Is it possible?\r\n\r\nStarting a basic Node JS app, I get some **webgl errors**.\r\n\r\n`\r\nLoading ocr_detect model...\r\n[conv2d]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: conv2d -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n[batchnorm]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: batchnorm -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n[hard_swish]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: hard_swish -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n[conv2d]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: conv2d -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n[batchnorm]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: batchnorm -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n[conv2d_depthwise]: TypeError: Cannot read property 'frameBufferSupportFloat' of undefined\r\nwebgl createProgram: conv2d_depthwise -- Error: TypeError: Cannot read property 'FRAGMENT_SHADER' of null\r\n`\r\n\r\nHere my files:\r\n\r\n**index.ts**\r\n`\r\n\r\n\r\n              import '@paddlejs/paddlejs-backend-webgl';\r\n\r\n              const { Canvas, createCanvas, loadImage, Image2, ImageData2 } = require('canvas');\r\n              const { JSDOM } = require('jsdom');\r\n\r\n              \r\n              const installDOM = (): void => {\r\n                  const dom = new JSDOM();\r\n                  global.document = dom.window.document;\r\n              \r\n                  // The rest enables DOM image and canvas and is provided by node-canvas\r\n                  global.Image = Image2;\r\n                  global.HTMLCanvasElement = Canvas;\r\n                  global.ImageData = ImageData2;\r\n                  global.HTMLImageElement = Image;\r\n                  global.Headers = dom.window.Headers;\r\n                  global.window = dom.window;\r\n              \r\n                  global.window.fetch = require('node-fetch')\r\n              };\r\n\r\n              (async () => {\r\n                  installDOM();\r\n              \r\n                  const ocr = require('@paddlejs-models/ocrdet');\r\n                  \r\n                  console.log(\"Loading Model...\");\r\n                  await ocr.load();\r\n                  console.log(\"Model loaded.\");\r\n              })();\r\n`\r\n\r\n**package.json**\r\n`\r\n\r\n\r\n              {\r\n                \"name\": \"ocrdet\",\r\n                \"version\": \"1.0.0\",\r\n                \"description\": \"\",\r\n                \"main\": \"index.js\",\r\n                \"scripts\": {\r\n                  \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\r\n                  \"dev\": \"tsc && node dist/index.js\"\r\n                },\r\n                \"keywords\": [],\r\n                \"author\": \"\",\r\n                \"license\": \"ISC\",\r\n                \"dependencies\": {\r\n                  \"@paddlejs-models/ocrdet\": \"^1.1.1\",\r\n                  \"@paddlejs/paddlejs-backend-webgl\": \"^1.2.9\",\r\n                  \"@paddlejs/paddlejs-core\": \"^2.2.0\",\r\n                  \"@types/node\": \"^20.3.1\",\r\n                  \"canvas\": \"^2.11.0\",\r\n                  \"jsdom\": \"^21.1.0\",\r\n                  \"typescript\": \"^4.3.5\"\r\n                },\r\n                \"devDependencies\": {}\r\n              }\r\n\r\n\r\n\r\n`\r\n\r\nAny idea how to solve it?\r\n\r\nThank you in advance 🙏",
        "state": "closed",
        "user": "pepperav",
        "closed_by": "chenqianhe",
        "created_at": "2023-06-13T10:26:29+00:00",
        "updated_at": "2023-06-26T05:07:48+00:00",
        "closed_at": "2023-06-26T05:07:48+00:00",
        "comments_count": [
            "chenqianhe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2032,
        "title": "Build failed on jetson nano with paddle inference backend.",
        "body": "## Environment\r\n\r\nOS Platform: e.g. Linux x64 aarch64 - Jetson Nano - Ubuntu 18.04.6\r\nHardware: e.g. Jetson Nano  CUDA 10.2 CUDNN 8.2.1\r\nProgram Language: e.g. Python 3.6.9\r\n\r\n## Problem description\r\nHi, I followed this [link](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/jetson.md) to build fastdeploy library on jetson nano with Jetpack 4.6.1 image. I'm able to install the library with ENABLE_PADDLE_BACKEND=OFF, and able to run the code infer.py with device cpu and get the results. But I want to run it on GPU. So, I downloaded the pre-built paddle inference library from here - https://www.paddlepaddle.org.cn/inference/v2.4/guides/install/download_lib.html and extracted it and set the folder on PADDLEINFERENCE_DIRECTORY. But when I try to build the package for Python, I get below issue:\r\n\r\n[ 23%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/utils/utils.cc.o\r\n[ 24%] Building CUDA object CMakeFiles/fastdeploy.dir/fastdeploy/function/cuda_cast.cu.o\r\n[ 24%] Building CUDA object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/common/cuda/adaptive_pool2d_kernel.cu.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/ort/ops/adaptive_pool2d.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/ort/ops/multiclass_nms.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/ort/ort_backend.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/ort/utils.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/paddle/paddle_backend.cc.o\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc: In member function ‘void fastdeploy::PaddleBackend::BuildOption(const fastdeploy::PaddleBackendOption&)’:\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc:53:15: error: ‘using Config = struct paddle::AnalysisConfig {aka struct paddle::AnalysisConfig}’ has no member named ‘Exp_EnableUseCutlass’\r\n       config_.Exp_EnableUseCutlass();\r\n               ^~~~~~~~~~~~~~~~~~~~\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc: In member function ‘bool fastdeploy::PaddleBackend::InitFromPaddle(const string&, const string&, bool, const fastdeploy::PaddleBackendOption&)’:\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc:293:36: error: ‘using element_type = class paddle_infer::Predictor {aka class paddle_infer::Predictor}’ has no member named ‘GetOutputTypes’; did you mean ‘GetInputTypes’?\r\n   auto output_dtypes = predictor_->GetOutputTypes();\r\n                                    ^~~~~~~~~~~~~~\r\n                                    GetInputTypes\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc:294:35: error: ‘using element_type = class paddle_infer::Predictor {aka class paddle_infer::Predictor}’ has no member named ‘GetInputTensorShape’; did you mean ‘GetInputTypes’?\r\n   auto input_shapes = predictor_->GetInputTensorShape();\r\n                                   ^~~~~~~~~~~~~~~~~~~\r\n                                   GetInputTypes\r\n/home/kamerai/Documents/fd/FastDeploy/fastdeploy/runtime/backends/paddle/paddle_backend.cc:295:36: error: ‘using element_type = class paddle_infer::Predictor {aka class paddle_infer::Predictor}’ has no member named ‘GetOutputTensorShape’; did you mean ‘GetOutputHandle’?\r\n   auto output_shapes = predictor_->GetOutputTensorShape();\r\n                                    ^~~~~~~~~~~~~~~~~~~~\r\n                                    GetOutputHandle\r\nCMakeFiles/fastdeploy.dir/build.make:579: recipe for target 'CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/paddle/paddle_backend.cc.o' failed\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/fastdeploy/runtime/backends/paddle/paddle_backend.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n\r\nKindly help fix the issue. I need to run PaddleOCR on Jetson Nano with GPU support.\r\n",
        "state": "open",
        "user": "ArivCR7",
        "closed_by": null,
        "created_at": "2023-06-14T11:03:02+00:00",
        "updated_at": "2023-06-16T10:11:06+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "ArivCR7",
            "jiangjiajun",
            "ArivCR7",
            "jiangjiajun",
            "ArivCR7"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2033,
        "title": "Deprecated APIs in Onnxruntime backend ",
        "body": "Recently Onnxruntime had `CustomOp` API removed from its main branch (ref: https://github.com/microsoft/onnxruntime/commit/908e94066006734828e449ebd16371029ff814d1) , which breaks compilation.\r\n\r\n```\r\n  In file included from /src/FastDeploy/fastdeploy/backends/ort/ops/adaptive_pool2d.cc:17:\r\n  /src/FastDeploy/fastdeploy/backends/ort/ops/adaptive_pool2d.h:36:8: error: ‘CustomOpApi’ in namespace ‘Ort’ does not name a type; did you mean ‘CustomOpBase’?\r\n     36 |   Ort::CustomOpApi ort_;\r\n        |        ^~~~~~~~~~~\r\n        |        CustomOpBase\r\n```\r\nIt seems working fine after turning to new APIs https://github.com/MaaAssistantArknights/FastDeploy/commit/e391a9cd47a6da1db8eede1b67ddcb549f797a86.",
        "state": "closed",
        "user": "horror-proton",
        "closed_by": "jiangjiajun",
        "created_at": "2023-06-14T13:46:30+00:00",
        "updated_at": "2023-06-15T13:36:01+00:00",
        "closed_at": "2023-06-15T13:36:01+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2035,
        "title": "docker pull error",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\ndocker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.4-21.10\r\n\r\nError response from daemon: manifest for registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.4-21.10 not found: manifest unknown: manifest unknown\r\n\r\n",
        "state": "closed",
        "user": "jo-dean",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-15T02:49:09+00:00",
        "updated_at": "2024-07-02T06:40:19+00:00",
        "closed_at": "2024-07-02T06:40:19+00:00",
        "comments_count": [
            "teymur-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2037,
        "title": "PaddleOCR not able to recognize a simple preprocessed image. ",
        "body": "## Environment\r\n\r\nFastDeploy version: latest code in develop branch\r\nOS Platform: e.g. Linux x64\r\nHardware: e.g. Nvidia GPU 3070Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nI used fast delpoy to deploy the \r\n![white_on_blkbg](https://github.com/PaddlePaddle/FastDeploy/assets/28477568/672f9a46-af16-480a-832d-30a0ccf8ec9d)\r\nPaddle OCR model on my x86 Linux machine. PaddleOCR works out of the box for the tested label receipts. However, for the attached simple preprocessed image, paddleOCR doesn't work. The OCR result is as below for the attached image: \r\ndet boxes: [[152,174],[376,163],[378,207],[154,218]]rec text: NERPI6IZLIL rec score:0.463849 cls label: 0 cls score: 0.947102\r\n\r\nDoes paddleOCR has any limitation on the image to be processed? If so, can something be done to this pre-processed image to improve the recognition accuracy?\r\n",
        "state": "closed",
        "user": "ArivCR7",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-16T13:17:54+00:00",
        "updated_at": "2024-06-25T06:41:02+00:00",
        "closed_at": "2024-06-25T06:41:02+00:00",
        "comments_count": [
            "ArivCR7",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2036,
        "title": "使用example中的 rkyolo/cpp  在瑞星微板端编译、推理，Score远大于1 ",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n## 环境\r\n- 【FastDeploy版本】： git clone https://github.com/PaddlePaddle/FastDeploy.git 最新代码\r\n- 【编译命令】板端编译FastDeploy C++ SDK\r\n- git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\ngit checkout develop\r\nmkdir build && cd build\r\ncmake ..  -DENABLE_ORT_BACKEND=OFF \\\r\n\t      -DENABLE_RKNPU2_BACKEND=ON \\\r\n\t      -DENABLE_VISION=ON \\\r\n\t      -DRKNN2_TARGET_SOC=RK3588 \\\r\n          -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.3\r\nmake -j8\r\nmake install\r\ncd  ../examples/vision/detection/rkyolo/cpp\r\nmkdir build\r\nmkdir images\r\nmkdir model\r\nmkdir thirdpartys\r\ncopy fastdeploy-0.0.3 至  thirdpartys\r\ncd build\r\ncmake  ..\r\nmake -j8 \r\nmake  install\r\ncd install\r\nsource ../../thirdpartys/fastdeploy-0.0.3\r\n./infer_rkyolov5  model/0616.rknn  images/bus.jpg\r\n- \r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【硬件】： Rockchip  rk3588  debian 11\r\n- 【编译语言】： C++\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型精度问题】\r\n`examples`下的rkyolo/cpp 可以正确执行   使用自己的onnx转rknn模型  score远大于1\r\nrknn模型使用 瑞芯微 官方的  rknpu2 在 设备端可以正常推理识别\r\n\r\n**rkyolo 推理日志如下：\r\nlinaro@linaro-alip:~/rknn/FastDeploy/examples/vision/detection/rkyolo/cpp/build/install$ ./infer_rkyolov5 model/0616.rknn images/bus.jpg \r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output, n_dims=4, dims=[1, 24, 80, 80], n_elems=153600, size=153600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=63, scale=0.131856, pass_through=0\r\nindex=1, name=273, n_dims=4, dims=[1, 24, 40, 40], n_elems=38400, size=38400, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=58, scale=0.130438, pass_through=0\r\nindex=2, name=274, n_dims=4, dims=[1, 24, 20, 20], n_elems=9600, size=9600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=12, scale=0.163191, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(341)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[FastDeploy] RKYOLOV5 in RKNN duration = 0.02885s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n187.604065,238.120667, 293.092041, 241.183228, 8.677237, 1\r\n233.081009,242.554581, 242.260818, 282.542419, 8.293127, 1\r\n235.190704,214.689728, 244.370514, 254.677551, 7.823704, 1\r\n239.327194,220.970367, 241.368896, 258.333527, 7.809513, 1\r\n193.487061,160.840317, 277.635376, 364.256653, 7.510757, 1\r\n190.156189,41.232483, 290.539917, 438.071411, 7.503257, 1\r\n226.659576,253.160049, 244.462860, 271.936951, 7.371669, 1\r\n228.769272,230.511002, 246.572540, 238.856293, 6.954404, 1\r\n183.430038,311.306580, 288.917999, 311.306580, 6.550463, 1\r\n239.327194,292.625000, 241.368896, 329.988159, 5.614682, 1\r\n164.519394,231.349030, 164.519394, 247.954880, 5.308427, 1\r\n114.327538,93.874374, 214.711243, 385.429504, 5.206342, 1\r\n111.775406,239.651947, 217.263367, 239.651947, 5.206342, 1\r\n9.760071,123.748032, 318.681946, 210.033112, 2.237020, 0\r\n129.534256,263.860962, 167.509918, 266.923523, 2.041703, 0\r\n144.742828,181.828308, 147.037781, 284.197144, 1.999391, 0\r\n100.825768,216.322159, 190.954834, 249.703308, 1.825531, 0\r\n245.297852,256.266144, 273.115479, 264.611420, 1.808145, 1\r\n254.616760,240.444870, 263.796570, 280.432678, 1.808145, 1\r\n0.000000,237.966888, 308.237732, 324.251984, 1.731028, 0\r\n133.708282,208.902451, 171.683960, 211.965012, 1.701419, 0\r\n144.438690,263.316498, 152.605499, 267.467957, 1.684405, 0\r\n148.612717,208.358002, 156.779526, 212.509460, 1.684405, 0\r\n130.453033,119.614670, 166.591171, 411.169800, 1.531277, 0**\r\n\r\n\r\n",
        "state": "closed",
        "user": "cafreyyao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-16T10:54:21+00:00",
        "updated_at": "2024-07-16T06:40:55+00:00",
        "closed_at": "2024-07-16T06:40:55+00:00",
        "comments_count": [
            "tongxiaohua"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2039,
        "title": "编译fastdeploy的C#库,出现报错",
        "body": "【FastDeploy】https://github.com/PaddlePaddle/FastDeploy/tree/develop\r\n【C++ SDK安装 Release版本】 fastdeploy-win-x64-gpu-1.0.7\r\n【系统】:win10\r\n【显卡】:NVIDIA GeForce RTX 3050 Laptop GPU\r\n【CUDA版本】11.2\r\n【CUDNN版本】11.3\r\n【VS版本】:VS2019\r\n\r\n\r\n【具体报错信息】\r\n----------------------------------------------------------------------------------------------------------------------------------\r\n【严重性\t代码\t说明\t项目\t文件\t行\t禁止显示状态】\r\n错误\tCS1617\t/langversion 的选项“10”无效。使用 \"/langversion:?\" 列出支持的值。\tfastdeploy_csharp\tD:\\PaddleX\\PaddleDeploy\\fastdeploy_develop\\build\\csharp\\CSC\t1\t活动\r\n\r\n【严重性\t代码\t说明\t项目\t文件\t行\t禁止显示状态】\r\n错误\tLNK1181\t无法打开输入文件“CUDA_LIB-NOTFOUND.lib”\tfastdeploy\tD:\\PaddleX\\PaddleDeploy\\fastdeploy_develop\\build\\LINK\t1\t\r\n---------------------------------------------------------------------------------------------------------------------------------\r\n【参考编译的文档路径：https://github.com/PaddlePaddle/FastDeploy/tree/develop/csharp/】\r\n![f78200381abee5d61a63353d9aa9b59](https://github.com/PaddlePaddle/FastDeploy/assets/134047873/53d32b25-478d-4888-a87c-32afc93def60)\r\n【出错图片】\r\n![8bc8ae099fdc8468cd3800d932c8214](https://github.com/PaddlePaddle/FastDeploy/assets/134047873/31f03895-2e56-4f91-9333-b4b751f950c1)\r\n\r\n![0aad6b28fd7304713c680a73e2d61cf](https://github.com/PaddlePaddle/FastDeploy/assets/134047873/2afb3e0d-06a8-45ed-acec-029cc8534ab1)\r\n![688a2afe4245f5cc2e5815c3ea47398](https://github.com/PaddlePaddle/FastDeploy/assets/134047873/ba3baacf-05a9-4d21-aa3e-836eceb97e3e)\r\n\r\n",
        "state": "closed",
        "user": "dilsm56-github",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-17T09:08:17+00:00",
        "updated_at": "2024-10-29T06:43:36+00:00",
        "closed_at": "2024-10-29T06:43:36+00:00",
        "comments_count": [
            "Duhanfeng",
            "guoyunqingyue",
            "pcycccccc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2038,
        "title": "【ppocrv3_cls.yaml】转换rknn模型报错【Field 'shape' of 'type' is required but missing.】",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： rknn-toolkit2==1.5.0+1fa95b5c  paddle2onnx==1.0.6\r\n- 【编译命令】python FastDeploy/tools/rknpu2/export.py --config_path FastDeploy/tools/rknpu2/config/ppocrv3_cls.yaml --target_platform rk3588\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： Nvidia GPU 3090， CUDA 11.7\r\n- 【编译语言】： Python 3.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型转换出错】\r\n```bash\r\n(rknn) ➜  paddle git:(master) python FastDeploy/tools/rknpu2/export.py --config_path FastDeploy/tools/rknpu2/config/ppocrv3_cls.yaml --target_platform rk3588\r\n{'mean': [[127.5, 127.5, 127.5]], 'std': [[127.5, 127.5, 127.5]], 'model_path': './ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.onnx', 'outputs_nodes': None, 'do_quantization': False, 'dataset': None, 'output_folder': './ch_ppocr_mobile_v2.0_cls_infer'}\r\nW __init__: rknn-toolkit2 version: 1.5.0+1fa95b5c\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 9!\r\nE load_onnx: Catch exception when loading onnx model: /srv/ssd/github/rockchip/rknn_model_foo/paddle/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1382, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 658, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 58, in rknn.api.ir_graph.IRGraph.__init__\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 503, in rknn.api.ir_graph.IRGraph.rebuild\r\nE load_onnx:   File \"/srv/ssd/anaconda3/envs/rknn/lib/python3.10/site-packages/onnx/checker.py\", line 119, in check_model\r\nE load_onnx:     C.check_model(protobuf_string, full_check)\r\nE load_onnx: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of 'type' is required but missing.\r\nW If you can't handle this error, please try updating to the latest version of the toolkit2 and runtime from:\r\n  https://eyun.baidu.com/s/3eTDMk6Y (Pwd: rknn)  Path: RK_NPU_SDK / RK_NPU_SDK_1.X.0 / develop /\r\n  If the error still exists in the latest version, please collect the corresponding error logs and the model,\r\n  convert script, and input data that can reproduce the problem, and then submit an issue on:\r\n  https://redmine.rock-chips.com (Please consult our sales or FAE for the redmine account)\r\nTraceback (most recent call last):\r\n  File \"/srv/ssd/github/rockchip/rknn_model_foo/paddle/FastDeploy/tools/rknpu2/export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n```\r\n",
        "state": "closed",
        "user": "autoexpect",
        "closed_by": "autoexpect",
        "created_at": "2023-06-16T17:35:38+00:00",
        "updated_at": "2024-06-27T14:32:22+00:00",
        "closed_at": "2024-06-27T14:32:22+00:00",
        "comments_count": [
            "liuzong616"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2040,
        "title": "ARM 1.0.7版预编译库无法下载",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\n[fastdeploy-linux-aarch64-1.0.7.tgz](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/download_prebuilt_libraries.md)\r\n{\"code\":\"NoSuchKey\",\"message\":\"The specified key does not exist.\",\"requestId\":\"68d2612c-ee2f-4fa0-83f7-7e762a9e07e4\"}\r\n",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "jia0511",
        "created_at": "2023-06-18T12:57:21+00:00",
        "updated_at": "2023-09-03T07:57:45+00:00",
        "closed_at": "2023-09-03T07:57:45+00:00",
        "comments_count": [
            "jia0511",
            "jia0511",
            "jia0511",
            "jia0511",
            "jiangjiajun",
            "jia0511"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2041,
        "title": "FastDeploy下的YoloV5服务化后，客户端怎么获得推理后有标注的图片？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.--gpu-cuda11.7-trt8.5-21.10\r\n- 【编译命令】: 参考这篇文档->https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN/compile.md , \r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】：  Nvidia GPU TITIAN X， CUDA 11.7\r\n- 【编译语言】： Python 3.8\r\n\r\n## 问题\r\n- 我参考了https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/yolov5/serving/README_CN.md      这份文档在docker里部署了一个yolov5的服务后，我调用 python3 yolov5_grpc_client.py 发送了一个图片目标检测的请求。\r\n以下是我得到的输出，我很想知道的是，这只是输出，但我怎么才能够获得有标注的图片呢？\r\nFastDeploy/examples/vision/detection/yolov5/serving$ python3 yolov5_grpc_client.py\r\ntm: name: \"INPUT\"\r\ndatatype: \"UINT8\"\r\nshape: -1\r\nshape: -1\r\nshape: -1\r\nshape: 3\r\n\r\noutput_name: detction_result\r\n{'boxes': [[188.49755859375, 331.054931640625, 418.91778564453125, 351.24371337890625], [436.3992614746094, 308.8633117675781, 535.6851196289062, 332.1841735839844], [138.71661376953125, 268.9527587890625, 373.19927978515625, 288.5521240234375], [99.02114868164062, 329.9296875, 151.0340576171875, 351.8544921875], [132.05865478515625, 458.33148193359375, 554.5450439453125, 497.2388916015625]], 'scores': [0.7492504715919495, 0.7485949993133545, 0.6756349205970764, 0.6652286052703857, 0.25799834728240967], 'label_ids': [0, 0, 0, 0, 0], 'masks': [], 'contain_masks': False}\r\n",
        "state": "closed",
        "user": "19245222",
        "closed_by": "19245222",
        "created_at": "2023-06-19T04:30:49+00:00",
        "updated_at": "2024-04-02T07:16:10+00:00",
        "closed_at": "2023-06-20T02:03:55+00:00",
        "comments_count": [
            "njflove"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2042,
        "title": "  Paddle Backend doesn't support linux aarch64 now.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n-- cmake-3.26 g++ 8.3\r\n- 【FastDeploy版本】： FastDeploy\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/home/huilang/jgq/gansu/rebuild/opencv-3.4.12 \\\r\n         -DENABLE_TEXT=ON\r\n\r\n- 【系统平台】: uos\r\n- 【硬件】：ft-2000\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/huilang/hezuo/FastDeploy/build/third_libs/install/onnxruntime\r\nCMake Warning (dev) at /usr/local/share/cmake-3.26/Modules/ExternalProject.cmake:3091 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /usr/local/share/cmake-3.26/Modules/ExternalProject.cmake:4208 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:109 (ExternalProject_Add)\r\n  CMakeLists.txt:223 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) in cmake/paddle_inference.cmake:\r\n  A logical block opening on the line\r\n\r\n    /home/huilang/hezuo/FastDeploy/cmake/paddle_inference.cmake:66 (if)\r\n\r\n  closes on the line\r\n\r\n    /home/huilang/hezuo/FastDeploy/cmake/paddle_inference.cmake:122 (endif)\r\n\r\n  with mis-matching arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:238 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Error at cmake/paddle_inference.cmake:91 (message):\r\n  Paddle Backend doesn't support linux aarch64 now.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:238 (include)",
        "state": "open",
        "user": "dengmingD",
        "closed_by": null,
        "created_at": "2023-06-19T07:33:23+00:00",
        "updated_at": "2023-10-07T01:26:02+00:00",
        "closed_at": null,
        "comments_count": [
            "zaicyxu",
            "dengmingD"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2043,
        "title": "Combining Object Detection and OCR into a Network-Accessible Service",
        "body": "Hello, I want to combine the functionalities of object detection and OCR into a new feature. For instance, after object detection, I get several bounding boxes, and then I use OCR to extract the text within these boxes. Moreover, I want to package this functionality into a service interface that can be used by other machines in the local area network. How should I go about doing this?",
        "state": "closed",
        "user": "19245222",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-20T02:22:33+00:00",
        "updated_at": "2024-07-02T06:40:20+00:00",
        "closed_at": "2024-07-02T06:40:20+00:00",
        "comments_count": [
            "leiqing1",
            "MikeLud"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2045,
        "title": "在AIxBoard爱克斯板 无法调用fastdeploy",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-1.0.7\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： CPU Intel(R) Celeron(R) N5105 @ 2.00GHz\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【exe运行报错】\r\n- - 在AIxBoard上运行生成的exe报0xc0000142\r\n- \r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/88479008/15395075-92bb-4b2c-b349-5f4742b92d5d)\r\n\r\n- 【在vs报错】\r\n- - 在vs里面报  没有为 paddle_inference.dll 加载的符号文件\r\n- 是CPU的指令集不支持吗？\r\n- \r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/88479008/85c85ef3-3fa5-4da1-b9ab-4949e211d770)\r\n\r\n- 【在其他电脑正常】\r\n- - 其他电脑测试正常 i7-8700k  i3-7100\r\n- 【其他测试】\r\n- 在AIxBoard上只要不调用paddle_inference.dll，opencv openvino都可以正常调用",
        "state": "closed",
        "user": "534114658",
        "closed_by": "534114658",
        "created_at": "2023-06-20T04:17:59+00:00",
        "updated_at": "2023-07-08T02:45:57+00:00",
        "closed_at": "2023-07-08T02:45:57+00:00",
        "comments_count": [
            "jiangjiajun",
            "534114658",
            "534114658",
            "jiangjiajun",
            "534114658",
            "jiangjiajun",
            "534114658",
            "534114658",
            "534114658"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2047,
        "title": "VS2019 编译错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】：  Nvidia GPU 3090， CUDA 12.1\r\n- 【编译语言】：Python3.10 （C++）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n    正在创建库 D:/OCR/FastDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.lib 和对象 D:/OCR/Fas\r\n  tDeploy/python/.setuptools-cmake-build/Release/fastdeploy_main.exp\r\n  fastdeploy_main.vcxproj -> D:\\OCR\\FastDeploy\\python\\.setuptools-cmake-build\\Release\\fastdeploy_main\r\n  .cp310-win_amd64.pyd\r\n  Error copying directory from \"D:/OCR/FastDeploy/python/.setuptools-cmake-build/third_libs/install\"\r\n  to \"D:/OCR/FastDeploy/python/fastdeploy/libs/third_libs\".\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Microsoft\\VC\\v160\\Microsoft.Cpp\r\nCommon.targets(241,5): error MSB8066: “D:\\OCR\\FastDeploy\\python\\.setuptools-cmake-build\\CMakeFiles\\d1\r\nb01e200c73bd69209e24f7896224e0\\copy_third_libraries.rule”的自定义生成已退出，代码为 1。 [D:\\OCR\\FastDeploy\\python\\.\r\nsetuptools-cmake-build\\copy_third_libraries.vcxproj]\r\n  Building Custom Rule D:/OCR/FastDeploy/CMakeLists.txt\r\nTraceback (most recent call last):\r\n  File \"D:\\OCR\\FastDeploy\\python\\setup.py\", line 449, in <module>\r\n    setuptools.setup(\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 108, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\r\n    self.run_command(cmd)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 1221, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 131, in run\r\n    self.run_command(cmd_name)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 1221, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\OCR\\FastDeploy\\python\\setup.py\", line 312, in run\r\n    self.run_command('cmake_build')\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 1221, in run_command\r\n    super().run_command(command)\r\n  File \"D:\\anaconda\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"D:\\OCR\\FastDeploy\\python\\setup.py\", line 306, in run\r\n    subprocess.check_call(build_args)\r\n  File \"D:\\anaconda\\lib\\subprocess.py\", line 369, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Community\\\\Common7\\\\IDE\\\\CommonExtensions\\\\Microsoft\\\\CMake\\\\CMake\\\\bin\\\\cmake.exe', '--build', '.', '--config', 'Release', '--', '/maxcpucount:32']' returned non-zero exit status 1.\r\n\r\n\r\n您好，我按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/gpu.md 下Windows python的方法进行编译 在VS2019下运行\r\nset ENABLE_ORT_BACKEND=ON\r\nset ENABLE_PADDLE_BACKEND=ON\r\nset ENABLE_OPENVINO_BACKEND=ON\r\nset ENABLE_VISION=ON\r\nset ENABLE_TEXT=ON\r\nset ENABLE_TRT_BACKEND=ON\r\nset WITH_GPU=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\n但是出现了以上错误 我不知道具体哪里出现了问题 还请帮我检查检查",
        "state": "open",
        "user": "LLLYF",
        "closed_by": null,
        "created_at": "2023-06-20T06:33:10+00:00",
        "updated_at": "2023-06-20T06:33:10+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2052,
        "title": "量化时loss为NaN",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-tools 0.0.1  paddlepaddle-gpu2.4.2  paddleslim2.4.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】：  Nvidia GPU 4090、3090Ti， CUDA 11.2 CUDNN 8.2.1\r\n- 【编译语言】： C++ / Python3.8\r\n\r\n## 问题\r\n###  在进行模型QAT量化时，经常出现如下问题，前几轮还算正常，但后面的loss就变成了nan，最后导致量化失败。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/57162039/a1de30d6-1dac-4cce-84b4-cb4b3d3410b0)\r\n\r\n使用的命令:`fastdeploy compress --config_path=./configs/detection/yolov7_quant.yaml --method='QAT' --save_dir='./yolov7_qat_model/' `\r\n配置文件cofig：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/57162039/b1b44380-363e-4e28-a523-f889bc283618)\r\n\r\n\r\n",
        "state": "open",
        "user": "xiaohui0225",
        "closed_by": null,
        "created_at": "2023-06-21T06:18:27+00:00",
        "updated_at": "2023-06-21T06:20:00+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2051,
        "title": "支持GPT2",
        "body": "支持GPT2",
        "state": "closed",
        "user": "SunYanCN",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-21T02:59:29+00:00",
        "updated_at": "2024-06-25T06:41:02+00:00",
        "closed_at": "2024-06-25T06:41:02+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2054,
        "title": "onnx转rknn遇到的疑问。YOLOV5训练出来的权重怎么知道normalize中的std和mean是多少？",
        "body": "如下图，需要我填写std和mean，我不知道该怎么填这里面的数字啊。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/129148313/6caea79f-a23f-43d7-9662-7325617132c7)",
        "state": "closed",
        "user": "ijiami-01",
        "closed_by": "ijiami-01",
        "created_at": "2023-06-21T10:01:44+00:00",
        "updated_at": "2023-10-19T08:22:05+00:00",
        "closed_at": "2023-10-19T08:22:05+00:00",
        "comments_count": [
            "zg651413411"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2053,
        "title": "gpu环境，T4的卡，进行OCR推理，显存持续增加导致程序崩溃",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.6\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令），参数命令完全按照官方给的指导编译的。\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU T4， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n- 使用官方的推理例子，https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/cpu-gpu/cpp/infer.cc\r\n\r\n如果模型初始化后，循环进行图片的推理调用，显存会持续增加，导致程序最终崩溃，不能使用\r\n",
        "state": "closed",
        "user": "ytzhang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-21T07:12:17+00:00",
        "updated_at": "2024-06-25T06:41:03+00:00",
        "closed_at": "2024-06-25T06:41:03+00:00",
        "comments_count": [
            "Cute-Yang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2066,
        "title": "算能-se5-bm1684-aarch64编译python sdk失败!",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-1.0.7-CPU-TPU\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(bm1684)\r\n- 【硬件】： 算能SE5-bm1684-aarch64\r\n- 【编译语言】： Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【编译算能的PythonSDK失败】\r\n- 具体的命令执行如下：\r\n- \r\n(fastdeploy) @:~/fastdeploy/python$ export ENABLE_SOPHGO_BACKEND=ON           \r\n(fastdeploy) @:~/fastdeploy/python$ export ENABLE_VISION=ON         \r\n(fastdeploy) @:~/fastdeploy/python$ python setup.py build          \r\nIllegal instruction          ",
        "state": "open",
        "user": "ijiami-01",
        "closed_by": null,
        "created_at": "2023-06-27T01:30:44+00:00",
        "updated_at": "2023-06-27T01:33:28+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2056,
        "title": "使用FastDeploy搭建PP-OCRv3模型服务后怎么配置参数",
        "body": "系统环境/System Environment：docker\r\n版本号/Version： fastdeploy:1.0.4-cpu-only-21.10\r\n\r\n我需要配置最大只识别两行文字，看文档是有max_candidates这么个配置是符合我要求的。\r\n我能通过下面的命令使用工具获取只有两个文本框的图片\r\npython3 infer_det.py -c ch_PP-OCRv3_det_student.yml -o Global.infer_img=\"12.jpg\" Global.pretrained_model=\"student\" Global.use_gpu=False PostProcess.max_candidates=2\r\n![12](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/c141d159-6328-4aa6-8764-e4263b8eddbb)\r\n\r\n但是这个配置我怎么在FastDeploy服务里配置呢？我想在java上面调用，所以只能调用服务。我看着服务里的文件也没有长得像yml的东西啊\r\n![QQ图片20230622034214](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/a2b6f297-5a24-4661-b60e-d770e7009f3f)\r\n",
        "state": "open",
        "user": "zhouyiminga",
        "closed_by": null,
        "created_at": "2023-06-21T19:43:06+00:00",
        "updated_at": "2023-06-26T12:29:28+00:00",
        "closed_at": null,
        "comments_count": [
            "teymur-git",
            "zhouyiminga"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2065,
        "title": "python 调用fastdeploy返回的result内存无法释放？ del result也不行？",
        "body": "# For deployment of GPU/TensorRT, please refer to examples/vision/detection/paddledetection/python\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nim = cv2.imread(\"000000014439.jpg\")\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\")\r\n\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)",
        "state": "closed",
        "user": "moyans",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-26T06:45:16+00:00",
        "updated_at": "2025-05-27T06:44:31+00:00",
        "closed_at": "2025-05-27T06:44:31+00:00",
        "comments_count": [
            "ChaoII",
            "moyans",
            "ChaoII",
            "moyans",
            "jiangjiajun",
            "moyans",
            "fastislow",
            "youzeFix"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2068,
        "title": "推理的时候可以实现多卡推理，提供web服务吗",
        "body": "目前问题：一个卡的显存不够用，需要把一个模型拆分到2个卡中进行推理。。提供web服务。\r\n看到paddlenlp推荐采用fastdeploy进行推理。",
        "state": "closed",
        "user": "liuzhipengchd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-27T03:33:10+00:00",
        "updated_at": "2024-07-02T06:40:20+00:00",
        "closed_at": "2024-07-02T06:40:20+00:00",
        "comments_count": [
            "jiangjiajun",
            "liuzhipengchd",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2073,
        "title": "fastdeploy 部署paddleocr，压力测试异常，进程退出",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n【FastDeploy版本】： docker镜像 registry.baidubce.com/paddlepaddle/fastdeploy:x.y.z-gpu-cuda11.4-trt8.4-21.10\r\n【系统平台】: Linux x64(Ubuntu 22.04)\r\n【硬件】： Nvidia GPU P40， CUDA 11.7 CUDNN 8.4\r\n【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\ncall ocr\r\n 0# 0x00005650067998A9 in fastdeployserver\r\n 1# 0x00007FAF577AF210 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >) in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 3# void phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::KernelCallHelper<paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::TypeTag<int> >::Compute<1, 3, 0, 0, phi::GPUContext const, phi::DenseTensor const, phi::DenseTensor const, phi::DenseTensor const>(phi::KernelContext*, phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&) in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 4# paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&, paddle::framework::RuntimeContext*) const in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 5# paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 6# paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&) in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 7# 0x00007FAE3D72085D in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 8# paddle::AnalysisPredictor::ZeroCopyRun() in /opt/fastdeploy/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so\r\n 9# fastdeploy::PaddleBackend::Infer(std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >&, std::vector<fastdeploy::FDTensor, std::allocator<fastdeploy::FDTensor> >*, bool) in /opt/fastdeploy/lib/libfastdeploy_runtime.so.0.0.0\r\n10# fastdeploy::Runtime::Infer() in /opt/fastdeploy/lib/libfastdeploy_runtime.so.0.0.0\r\n11# 0x00007FAEFC1B0FA4 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n12# 0x00007FAEFC1B4826 in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n13# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\r\n14# 0x00007FAF5833B83A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n15# 0x00007FAF5833C04D in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n16# 0x00007FAF581F0801 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n17# 0x00007FAF58335DC7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n18# 0x00007FAF57B9DDE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n19# 0x00007FAF5801B609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n20# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\n[StatusCode.UNAVAILABLE] recvmsg:Connection reset by peer\r\n[StatusCode.UNAVAILABLE] recvmsg:Connection reset by peer\r\n[StatusCode.UNAVAILABLE] recvmsg:Connection reset by peer\r\n[StatusCode.UNAVAILABLE] recvmsg:Connection reset by peer\r\n[StatusCode.UNAVAILABLE] recvmsg:Connection reset by peer",
        "state": "open",
        "user": "EasyIsAllYouNeed",
        "closed_by": null,
        "created_at": "2023-06-29T14:33:57+00:00",
        "updated_at": "2023-06-29T14:33:57+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2072,
        "title": "复现FastDeploy Serving中的PaddleDetection 服务化部署失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： docker镜像  registry.baidubce.com/paddlepaddle/fastdeploy:x.y.z-gpu-cuda11.4-trt8.4-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Nvidia GPU 3050TI， CUDA 11.7  CUDNN 8.4\r\n- 【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n我按照[此处](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README_CN.md)进行FastDeploy Serving中的PaddleDetection 服务化部署，进入docker镜像后运行如下命令\r\n```\r\nCUDA_VISIBLE_DEVICES=0 fastdeployserver --model-repository=/serving/models\r\n```\r\n无法启动模型，具体日志如下\r\n```\r\nI0629 04:47:53.767341 117 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3050\r\nI0629 04:47:53.888306 117 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fe9ac000000' with size 268435456\r\nI0629 04:47:53.888427 117 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0629 04:47:53.889543 117 model_repository_manager.cc:1022] loading: postprocess:1\r\nI0629 04:47:53.990155 117 model_repository_manager.cc:1022] loading: preprocess:1\r\nI0629 04:47:53.996255 117 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: postprocess_0 (CPU device 0)\r\nI0629 04:47:54.090382 117 model_repository_manager.cc:1022] loading: runtime:1\r\nmodel_config: {'name': 'postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 0, 'input': [{'name': 'post_input1', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'post_input2', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [-1], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'post_output', 'data_type': 'TYPE_STRING', 'dims': [-1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['post_input1', 'post_input2']\r\npostprocess output names: ['post_output']\r\nI0629 04:47:54.203991 117 model_repository_manager.cc:1183] successfully loaded 'postprocess' version 1\r\nI0629 04:47:54.335351 117 fastdeploy_runtime.cc:1173] TRITONBACKEND_Initialize: fastdeploy\r\nI0629 04:47:54.335382 117 fastdeploy_runtime.cc:1182] Triton TRITONBACKEND API version: 1.6\r\nI0629 04:47:54.335386 117 fastdeploy_runtime.cc:1187] 'fastdeploy' TRITONBACKEND API version: 1.6\r\nI0629 04:47:54.335388 117 fastdeploy_runtime.cc:1216] backend configuration:\r\n{}\r\nI0629 04:47:54.335410 117 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: preprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 0, 'input': [{'name': 'preprocess_input', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'preprocess_output1', 'data_type': 'TYPE_FP32', 'dims': [-1, 3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'preprocess_output2', 'data_type': 'TYPE_FP32', 'dims': [-1, 2], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'preprocess_output3', 'data_type': 'TYPE_FP32', 'dims': [-1, 2], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['preprocess_input']\r\npreprocess output names: ['preprocess_output1', 'preprocess_output2', 'preprocess_output3']\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nI0629 04:47:54.523955 117 fastdeploy_runtime.cc:1246] TRITONBACKEND_ModelInitialize: runtime (version 1)\r\nI0629 04:47:54.524059 117 model_repository_manager.cc:1183] successfully loaded 'preprocess' version 1\r\nI0629 04:47:54.524807 117 fastdeploy_runtime.cc:1285] TRITONBACKEND_ModelInstanceInitialize: runtime_0 (GPU device 0)\r\n[INFO] fastdeploy/runtime.cc(517)::Init\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nI0629 04:47:56.292450 117 model_repository_manager.cc:1183] successfully loaded 'runtime' version 1\r\nI0629 04:47:56.292627 117 model_repository_manager.cc:1022] loading: ppdet:1\r\nI0629 04:47:56.393060 117 model_repository_manager.cc:1183] successfully loaded 'ppdet' version 1\r\nI0629 04:47:56.393241 117 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0629 04:47:56.393341 117 server.cc:549] \r\n+------------+---------------------------------------------------------------+--------+\r\n| Backend    | Path                                                          | Config |\r\n+------------+---------------------------------------------------------------+--------+\r\n| python     | /opt/tritonserver/backends/python/libtriton_python.so         | {}     |\r\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\r\n+------------+---------------------------------------------------------------+--------+\r\n\r\nI0629 04:47:56.393434 117 server.cc:592] \r\n+-------------+---------+--------+\r\n| Model       | Version | Status |\r\n+-------------+---------+--------+\r\n| postprocess | 1       | READY  |\r\n| ppdet       | 1       | READY  |\r\n| preprocess  | 1       | READY  |\r\n| runtime     | 1       | READY  |\r\n+-------------+---------+--------+\r\n\r\nI0629 04:47:56.393658 117 tritonserver.cc:1920] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                 |\r\n| server_version                   | 2.15.0                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory  |\r\n|                                  | cuda_shared_memory binary_tensor_data statistics                                                                                       |\r\n| model_repository_path[0]         | /serving/models                                                                                                                        |\r\n| model_control_mode               | MODE_NONE                                                                                                                              |\r\n| strict_model_config              | 1                                                                                                                                      |\r\n| rate_limit                       | OFF                                                                                                                                    |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                              |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                               |\r\n| response_cache_byte_size         | 0                                                                                                                                      |\r\n| min_supported_compute_capability | 6.0                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nE0629 12:47:56.394712287     117 server_chttp2.cc:40]        {\"created\":\"@1688014076.394608102\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":394,\"referenced_errors\":[{\"created\":\"@1688014076.394604893\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":340,\"referenced_errors\":[{\"created\":\"@1688014076.394584447\",\"description\":\"Unable to configure socket\",\"fd\":31,\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":207,\"referenced_errors\":[{\"created\":\"@1688014076.394580766\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":181,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1688014076.394604329\",\"description\":\"Unable to configure socket\",\"fd\":31,\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":207,\"referenced_errors\":[{\"created\":\"@1688014076.394601805\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":181,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\nSignal (11) received.\r\n 0# 0x000055D9456968A9 in fastdeployserver\r\n 1# 0x00007FEA03C6D210 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# 0x000055D94599ED78 in fastdeployserver\r\n 3# 0x000055D94577B341 in fastdeployserver\r\n 4# 0x000055D945785998 in fastdeployserver\r\n 5# 0x000055D9457981B2 in fastdeployserver\r\n 6# 0x00007FEA0405BDE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# 0x00007FEA044D9609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n 8# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\nSegmentation fault (core dumped)\r\n```\r\n",
        "state": "closed",
        "user": "LUOQING1994",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-29T08:22:37+00:00",
        "updated_at": "2024-09-29T08:55:30+00:00",
        "closed_at": "2024-07-09T06:40:42+00:00",
        "comments_count": [
            "teymur-git",
            "youqugit"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2071,
        "title": "fastdeploy使用web_demo上传图片进行文字识别，识别正确率低于paddleocr中推理的精度，而且速度很慢",
        "body": "使用web_demo，安装npm套件，上传图片进行文字识别，识别正确率低于paddleocr中推理的精度，而且速度很慢。\r\n是不是因为fastdeploy集成的前端paddle-js-models/ocr模型包跟paddleocr中paddleocr>=2.0.1使用的模型PP-OCRv3版本不一样，精度下降造成的？怎么解决？\r\n而且我安装的是fastdeploy-gpu-python，图片识别的时候，没有看到后台使用gpu显存，速度很慢是否是因为没有用gpu？怎么解决",
        "state": "closed",
        "user": "dizhenx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-06-28T05:57:01+00:00",
        "updated_at": "2024-07-09T06:40:41+00:00",
        "closed_at": "2024-07-09T06:40:41+00:00",
        "comments_count": [
            "chenqianhe",
            "dizhenx",
            "chenqianhe",
            "dizhenx",
            "chenqianhe",
            "dizhenx",
            "chenqianhe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2074,
        "title": "自训练PPYOLOv3模型转RKNN格式模型过程中遇到的问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【模型转换环境】: rknn-toolkit2版本为1.7.3\r\n- 【编译语言】： Python(3.6）\r\n\r\n1. 【所用的自训练模型文件】\r\n\r\n- 使用项目训练好的模型动转静导出Paddle模型文件，导出时设置export.nms=True，运行代码如下所示\r\n\r\n- `python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model/0630 -o weights=output/yolov3_darknet53_270e_coco/best_model.pdparams export.nms=True`\r\n\r\n2. Paddle模型转ONNX模型\r\n\r\n- 导出Paddle模型后，按照[Paddle模型转换为ONNX模型文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md),静态图转ONNX、固定Shape操作，运行代码如下所示\r\n\r\n- 静态图转ONNX：` paddle2onnx --model_dir inference_model/0630/yolov3_darknet53_270e_coco --model_filename model.pdmodel --params_filename model.pdiparams --save_file inference_model/0630/yolov3.onnx --enable_dev_version True`\r\n\r\n- 固定shape：` python -m paddle2onnx.optimize --input_model ./inference_model/0630/yolov3.onnx --output_model ./inference_model/0630/new_yolov3.onnx --input_shape_dict \"{'image':[1,3,416,416],'scale_factor':[1,2]}\"`\r\n\r\n以上过程均能正常执行\r\n\r\n3. ONNX转RKNN\r\n\r\n- 转换前准备：将模型文件拷贝到FastDeploy/tools/rknpu2下，根据模型netron可视化结构图、模型配置文件infer_cfg.yml，修改config文件，按照按照[编写yaml文件文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md)修改、normaliz参数和output参数.\r\n- 设置do_quantization为False并执行下面命令转换模型后出现错误\r\n- `python export.py --config_path=./infer_cfg.yml --target_platform rk3588`\r\n- 错误信息提示：`E Loading dynamic inputs model is not support, please fix it first\r\n  E Catch exception when loading onnx model: ./new_yolov3.onnx!`\r\n- 设置do_quantization为True并执行下面命令转换模型后出现错误：\r\n- `python export.py --config_path=./infer_cfg.yml --target_platform rk3588`\r\n\r\n- `Loading dynamic inputs model is not support, please fix it first\r\n  E Catch exception when loading onnx model: ./new_yolov3.onnx!`\r\n",
        "state": "closed",
        "user": "zg651413411",
        "closed_by": "zg651413411",
        "created_at": "2023-06-30T03:26:59+00:00",
        "updated_at": "2023-06-30T03:36:29+00:00",
        "closed_at": "2023-06-30T03:36:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2075,
        "title": "自训练PPYOLOv3模型转RKNN格式模型过程中遇到的问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-develop\r\n- 【系统平台】:Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 1060， CUDA 10.2 CUDNN 8.3\r\n- 【模型转换环境】: rknn-toolkit2版本为1.4.3b4+48391b8f\r\n- 【编译语言】： Python3.6\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 1. 【所用的自训练模型文件】\r\n- 使用项目训练好的模型动转静导出Paddle模型文件，导出时设置export.nms=True，运行代码如下所示\r\n`python tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model/0630 -o weights=output/yolov3_darknet53_270e_coco/best_model.pdparams export.nms=True`\r\n\r\n- 2. Paddle模型转ONNX模型\r\n- 导出Paddle模型后，按照[Paddle模型转换为ONNX模型文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md),静态图转ONNX、固定Shape操作，运行代码如下所示\r\n- 静态图转ONNX：` paddle2onnx --model_dir inference_model/0630/yolov3_darknet53_270e_coco --model_filename model.pdmodel --params_filename model.pdiparams --save_file inference_model/0630/yolov3.onnx --enable_dev_version True`\r\n- 固定shape：` python -m paddle2onnx.optimize --input_model ./inference_model/0630/yolov3.onnx --output_model ./inference_model/0630/new_yolov3.onnx --input_shape_dict \"{'image':[1,3,416,416],'scale_factor':[1,2]}\"`\r\n以上过程均能正常执行\r\n\r\n- 3. ONNX转RKNN\r\n- 转换前准备：将模型文件拷贝到FastDeploy/tools/rknpu2下，根据模型netron可视化结构图、模型配置文件infer_cfg.yml，修改config文件，按照按照[编写yaml文件文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/rknpu2/README_CN.md)修改、normaliz参数和output参数.\r\n- 设置do_quantization为False并执行下面命令转换模型后出现错误\r\n- `python export.py --config_path=./infer_cfg.yml --target_platform rk3588`\r\n- 错误信息提示：`E Loading dynamic inputs model is not support, please fix it first\r\n  E Catch exception when loading onnx model: ./new_yolov3.onnx!`\r\n- 设置do_quantization为True并执行下面命令转换模型后出现错误：\r\n- `python export.py --config_path=./infer_cfg.yml --target_platform rk3588`\r\n\r\n- `Loading dynamic inputs model is not support, please fix it first\r\n  E Catch exception when loading onnx model: ./new_yolov3.onnx!`\r\n",
        "state": "open",
        "user": "zg651413411",
        "closed_by": null,
        "created_at": "2023-06-30T03:44:50+00:00",
        "updated_at": "2023-06-30T03:44:50+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2076,
        "title": "option.set_trt_cache_file()API报错",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： jeston\r\n- 【编译命令】教程命令\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： jeston nano\r\n- 【编译语言】： python3.6\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用    option.set_trt_cache_file()API报错\r\n错误信息 ：\r\n```\r\nWARNING:root:`RuntimeOption.set_trt_input_shape` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.set_shape()` instead.\r\nWARNING:root:`RuntimeOption.set_trt_cache_file` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.serialize_file = cache` instead.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(703)::CreateTrtEngineFromOnnx        Detect serialized TensorRT Engine file in cache, will load it directly.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 39, in <module>\r\n    'img/ppyoloe_plus_crn_l_80e_coco/infer_cfg.yml',runtime_option=runtime_option)\r\n  File \"/home/paddle/.local/lib/python3.6/site-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 115, in __init__\r\n    model_format)\r\nMemoryError: std::bad_alloc\r\n```\r\n具体代码：\r\n```\r\nimport cv2\r\nfrom fastdeploy.vision import detection\r\nfrom fastdeploy import vision\r\nimport fastdeploy as fd\r\n\r\noption = fd.RuntimeOption()\r\n\r\ndef build_option(device = 'cpu',backend = 'paddle'):\r\n    \r\n    # 创建Runtimeoption\r\n    option = fd.RuntimeOption()\r\n    option.use_cpu()\r\n    if device.lower() == \"gpu\":\r\n        option.use_gpu(0)\r\n\r\n    if backend.lower() == \"trt\":\r\n        assert device.lower(\r\n        ) == \"gpu\", \"TensorRT backend require inference on device GPU.\"\r\n        option.use_trt_backend()\r\n    elif backend.lower() == \"pptrt\":\r\n        assert device.lower(\r\n        ) == \"gpu\", \"TensorRT backend require inference on device GPU.\"\r\n        option.use_trt_backend()\r\n        option.enable_paddle_to_trt()\r\n    elif backend.lower() == \"ort\":\r\n        option.use_ort_backend()\r\n    elif backend.lower() == \"paddle\":\r\n        option.use_paddle_backend()\r\n    elif backend.lower() == \"openvino\":\r\n        option.use_openvino_backend()\r\n    option.set_trt_input_shape('input',[1,3,640,640])\r\n    option.set_trt_cache_file('cache')\r\n    return option\r\nruntime_option = build_option(device = 'gpu',backend = 'trt')\r\n#runtime_option.set_trt_input_shape('inpu',[1,3,640,640])\r\n#runtime_option.set_trt_cache_file('model/ppyoloe_plus_crn_l_80e_coco')\r\nmodel = detection.PPYOLOE('img/ppyoloe_plus_crn_l_80e_coco/model.pdmodel',\r\n                          'img/ppyoloe_plus_crn_l_80e_coco/model.pdiparams',\r\n                          'img/ppyoloe_plus_crn_l_80e_coco/infer_cfg.yml',runtime_option=runtime_option)\r\n# model.preprocessor.use_cuda(True, 0)\r\n\r\ncpa = cv2.VideoCapture(0)\r\nret, frame = cpa.read()  # 预先读取一帧用于初始化\r\n\r\nwhile True:\r\n    if ret:\r\n        result = model.predict(frame)\r\n        vis_im = vision.vis_detection(frame, result, score_threshold=0.699)\r\n        cv2.imshow(\"vis_img.jpg\", vis_im)\r\n        print(result)\r\n    \r\n    key = cv2.waitKey(1)\r\n    if key == 27:  # 按下 ESC 键退出循环\r\n        break\r\n    \r\n    ret, frame = cpa.read()\r\n\r\ncpa.release()\r\ncv2.destroyAllWindows()```",
        "state": "open",
        "user": "kewuyu",
        "closed_by": null,
        "created_at": "2023-06-30T07:30:08+00:00",
        "updated_at": "2023-07-05T15:34:03+00:00",
        "closed_at": null,
        "comments_count": [
            "teymur-git",
            "kewuyu",
            "teymur-git",
            "kewuyu",
            "teymur-git",
            "kewuyu",
            "teymur-git",
            "kewuyu",
            "teymur-git",
            "kewuyu",
            "teymur-git",
            "kewuyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2077,
        "title": "RK3588上测试paddle-fastdeploy-picodet样例（cpu可以推理，npu无法推理）",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n- 【FastDeploy版本】：git clone https://github.com/PaddlePaddle/FastDeploy.git\r\n\r\n- 【编译命令】自行编译\r\n- -安装步骤\r\nSTEP1：\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n\t      -DENABLE_RKNPU2_BACKEND=ON \\\r\n\t      -DENABLE_VISION=ON \\\r\n\t      -DRKNN2_TARGET_SOC=RK3588 \\\r\n          -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy\r\nmake -j8\r\nmake install\r\n\r\nSTEP2：\r\n永久配置\r\nsource /home/neardi/ruida/work/FastDeploy/build/fastdeploy/fastdeploy_init.sh\r\nsudo cp /home/neardi/ruida/work/FastDeploy/build/fastdeployfastdeploy_libs.conf /etc/ld.so.conf.d/\r\nsudo ldconfig\r\n\r\nSTEP3：\r\ncd FastDeploy/examples/vision/detection/paddledetection/rknpu2/cpp/\r\nmkdir build\r\ncd build\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=~/ruida/work/FastDeploy/build/fastdeploy\r\nmake -j8\r\n\r\n- 【系统平台】: Debian11\r\n- 【硬件】： RK3588\r\n- 【编译语言】： C++ \r\n\r\n- 【模型跑不通】\r\n[CPU可以跑通，NPU跑不通]\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/52811556/25fa86b1-e3d4-4673-9b39-e4f194fa9b2a)\r\n<img width=\"708\" alt=\"d65e5aeab01b512febd5972ea85c1d6\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/52811556/1a53c940-6147-4646-830f-d0a4ab943754\">\r\n\r\n- 【文件目录】\r\n<img width=\"373\" alt=\"322a0c66b2b9063ab298b718d29b6a6\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/52811556/20084010-26e0-4f06-b626-353022f00592\">\r\n<img width=\"351\" alt=\"1ac0bcd47d39b534ed19639e87a9d7a\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/52811556/57b0f95e-f2ba-4b08-a3eb-753c01197961\">\r\n\r\n\r\n",
        "state": "open",
        "user": "srd2018",
        "closed_by": null,
        "created_at": "2023-07-01T08:08:35+00:00",
        "updated_at": "2023-07-03T09:20:55+00:00",
        "closed_at": null,
        "comments_count": [
            "srd2018",
            "Zheng-Bicheng",
            "srd2018",
            "Zheng-Bicheng",
            "srd2018",
            "Zheng-Bicheng",
            "srd2018",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "srd2018"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2079,
        "title": "Jetson nano 编译错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-jetson\r\n- 【编译命令】\r\n```cmake .. -DBUILD_ON_JETSON=ON \\\r\n>          -DENABLE_VISION=ON \\\r\n>          -DENABLE_PADDLE_BACKEND=ON \\\r\n>          -DPADDLEINFERENCE_DIRECTORY=/home/kewuyu/Downloads/paddle_inference_install_dir \\\r\n>          -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\n```\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：jeston nano\r\n- 【编译语言】： Cmake 3.25\r\n- 【错误】\r\n```\r\n-- The C compiler identification is GNU 7.5.0\r\n-- The CXX compiler identification is GNU 7.5.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /home/kewuyu/FastDeploy/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 5% complete]\r\n-- [download 10% complete]\r\n-- [download 15% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 42% complete]\r\n-- [download 47% complete]\r\n-- [download 52% complete]\r\n-- [download 57% complete]\r\n-- [download 63% complete]\r\n-- [download 68% complete]\r\n-- [download 73% complete]\r\n-- [download 78% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/kewuyu/FastDeploy/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/kewuyu/FastDeploy/build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\nCMake Warning (dev) at /home/kewuyu/cmake-3.25.2-linux-aarch64/share/cmake-3.25/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /home/kewuyu/cmake-3.25.2-linux-aarch64/share/cmake-3.25/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:109 (ExternalProject_Add)\r\n  CMakeLists.txt:230 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found Python: /usr/bin/python3.6 (found version \"3.6.9\") found components: Interpreter Development Development.Module Development.Embed \r\n-- Copying /home/kewuyu/Downloads/paddle_inference_install_dir to /home/kewuyu/FastDeploy/build/third_libs/install/paddle_inference ...\r\nCMake Error at cmake/paddle_inference.cmake:271 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\nCMake Error at cmake/paddle_inference.cmake:272 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\nCMake Error at cmake/paddle_inference.cmake:273 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\n-- The CUDA compiler identification is NVIDIA 10.2.300\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /usr/local/cuda-10.2/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- CUDA compiler: /usr/local/cuda-10.2/bin/nvcc, version: NVIDIA 10.2.300\r\n-- CUDA detected: 10.2.300\r\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_53,code=sm_53 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_72,code=sm_72\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/lib/aarch64-linux-gnu/cmake/opencv4/\r\n-- Found OpenCV: /usr (found version \"4.1.1\") \r\nCMake Warning (dev) at /home/kewuyu/cmake-3.25.2-linux-aarch64/share/cmake-3.25/Modules/ExternalProject.cmake:3075 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /home/kewuyu/cmake-3.25.2-linux-aarch64/share/cmake-3.25/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\r\n  cmake/paddle2onnx.cmake:70 (ExternalProject_Add)\r\n  CMakeLists.txt:478 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.25.2\r\n--   CMake command             : /home/kewuyu/cmake-3.25.2-linux-aarch64/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ standard              : 11\r\n--   C++ cuda standard         : 11\r\n--   C++ compiler version      : 7.5.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;WITH_GPU;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /home/kewuyu/FastDeploy/build/installed_fastdeploy\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : \r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : \r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/kewuyu/FastDeploy/build/CMakeFiles/CMakeOutput.log\".\r\n```\r\n\r\n",
        "state": "closed",
        "user": "kewuyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-03T06:33:31+00:00",
        "updated_at": "2024-07-16T06:40:56+00:00",
        "closed_at": "2024-07-16T06:40:56+00:00",
        "comments_count": [
            "kewuyu",
            "xiaoqingzheng",
            "kewuyu",
            "UygarUsta99",
            "AnjanaWijesinghe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2084,
        "title": "如果分割网络有两个分割头的话，在推理时如何获取结果？目前仅能输出一个分割头的结果",
        "body": "## 环境\r\n- 【编译命令】自行编译的FastDeploy : \r\n- cmake .. -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DWITH_GPU=ON -DTRT_DIRECTORY=/home/kzhao/Downloads/Cuda11.6_Cudnn8.4_TensorRT8.4.3/TensorRT-8.4.0.6 -DCUDA_DIRECTORY=/usr/local/cuda -DCMAKE_INSTALL_PREFIX=../../compiled_fastdeploy_sdk -DENABLE_VISION=ON -DOPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 -DENABLE_TEXT=ON\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： Nvidia GPU 2060， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 如果分割网络有两个分割头的话，在推理时如何获取结果？目前仅能输出一个分割头的结果\r\n",
        "state": "closed",
        "user": "kzhao94",
        "closed_by": "kzhao94",
        "created_at": "2023-07-04T14:13:46+00:00",
        "updated_at": "2023-10-08T09:21:19+00:00",
        "closed_at": "2023-10-08T09:21:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2081,
        "title": "PP-OCRv4支持了没？",
        "body": "新出的PP-OCRv4，期待更新后，测试一波",
        "state": "open",
        "user": "xinsuinizhuan",
        "closed_by": null,
        "created_at": "2023-07-04T01:00:43+00:00",
        "updated_at": "2024-12-05T08:09:43+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2080,
        "title": "docker使用vdl",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如docker pull paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) /\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3090\r\n- 【编译语言】： Python(3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n**开放的端口没有FASTDEPLOY_SERVER,和FASTDEPLOY_CLIENT.**\r\n\r\n按照这个文档进行操作的\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN/vdl_management.md\r\n\r\n这是我的docker命令\r\nsudo docker pull paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12\r\nsudo docker run -it --runtime=nvidia --gpus all --net=host --name fd_serving -v `pwd`/:/FastDeploy paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12 bash\r\n\r\n/FastDeploy/examples# visualdl --host 0.0.0.0 --port 8082\r\n![2023-07-04 10-24-50屏幕截图](https://github.com/PaddlePaddle/FastDeploy/assets/38728358/c20f3f35-44a5-417e-83be-93af335a6f15)\r\n\r\n\r\n这是网页端结果\r\n![2023-07-04 10-24-24屏幕截图](https://github.com/PaddlePaddle/FastDeploy/assets/38728358/55290183-dd30-42d8-a8b0-da28b3d43ae6)\r\n\r\n\r\n\r\n这是README的结果\r\n![2023-07-03 17-47-50屏幕截图](https://github.com/PaddlePaddle/FastDeploy/assets/38728358/78e10251-7e11-4967-a22c-280042b52267)\r\n\r\n想问一下为什么我开放的端口没有FASTDEPLOY_SERVER,和FASTDEPLOY_CLIENT.",
        "state": "open",
        "user": "jo-dean",
        "closed_by": null,
        "created_at": "2023-07-03T09:40:50+00:00",
        "updated_at": "2023-07-06T09:26:14+00:00",
        "closed_at": null,
        "comments_count": [
            "teymur-git",
            "teymur-git",
            "jo-dean",
            "teymur-git",
            "teymur-git",
            "jo-dean",
            "jo-dean"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2083,
        "title": "Windows下利用QT编写的包含paddl推理模型的软件发行到其他电脑中软件会闪退。（加载模型正常，使用模型推理会闪退，但发行的软件在开发电脑中正常运行）",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：  Nvidia GPU 1050ti Nvidia GPU 1650， CUDA 10.1 CUDNN 7.6.5\r\n- 【编译语言】： C++ /python\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n利用QT编写的包含paddle推理模型的软件发行到其他电脑中软件会闪退。（加载模型正常，使用模型推理会闪退，但发行的软件在开发电脑中正常运行）\r\n排除了海康SDK、opencv等的问题，要发行的电脑中配置了和开发电脑一致的cuda和cudnn，软件在新电脑中其他功能包括相机取图及显示，opencv处理等功能均正常，但加载模型后调用模型进行推理就会闪退。\r\n发行软件的文件夹中包含的paddle动态库如下：\r\ntest_seg.dll、test_seg.lib、libiomp5md.dll、mkldnn.dll 、mklml.dll 、paddle_inference.dll\r\n其中，test_seg.dll为使用VS生成的模型载入内存的动态库。\r\n是否缺少了其他paddle推理模型必备的动态库导致发行的软件闪退呢？\r\n",
        "state": "closed",
        "user": "elkslanlanlan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-04T05:47:28+00:00",
        "updated_at": "2024-07-09T06:40:43+00:00",
        "closed_at": "2024-07-09T06:40:43+00:00",
        "comments_count": [
            "teymur-git"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2086,
        "title": "安卓1.0.7版预编译库无法下载",
        "body": "访问 https://bj.bcebos.com/fastdeploy/release/android/fastdeploy-android-sdk-1.0.7.aar\r\n提示\r\n{\r\ncode: \"NoSuchKey\",\r\nmessage: \"The specified key does not exist.\",\r\nrequestId: \"22a4f041-928f-48fe-99ec-634d9caf1a39\"\r\n}",
        "state": "closed",
        "user": "uestccokey",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-06T00:25:15+00:00",
        "updated_at": "2025-06-24T06:46:17+00:00",
        "closed_at": "2025-06-24T06:46:17+00:00",
        "comments_count": [
            "jia0511",
            "Yamcanda",
            "loveanao",
            "loveanao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2085,
        "title": "自训练PPYOLOE模型在RK3588板子上推理部署时出现的检测框数目不匹配问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：  FastDeploy-Develop\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： EVM3588开发板\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 根据官方文档对自训练PPYOLOE模型进行Paddle模型导出、ONNX模型转换以及RKNN模型转换，得到适配RK3588的RKNN格式模型，上述过程均没有问题。在板端进行部署时，可以进行正常的推理部署，但是在进行CPU推理的时候，命令窗口出现的DetectionResult中前六个为实际目标位置信息，其余的为多余检测结果；\r\n\r\n> [INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\r\n[FastDeploy] Para Setting duration = 0.465889s.\r\n[FastDeploy] PPDet in ONNX duration = 0.546816s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n455.986206,429.263367, 490.114166, 476.926239, 0.944360, 0\r\n468.794525,718.686218, 503.025879, 764.895142, 0.939238, 0\r\n589.441345,446.637756, 621.391052, 491.914978, 0.933000, 0\r\n425.842194,285.056732, 447.185364, 326.389374, 0.929535, 0\r\n436.465759,587.606995, 463.763275, 630.181458, 0.929394, 0\r\n638.644592,83.125992, 664.410339, 113.388077, 0.922930, 0\r\n410.938873,489.485535, 416.722931, 498.174927, 0.103037, 0\r\n359.797577,463.694458, 370.237579, 468.092560, 0.085985, 0\r\n727.856201,468.820312, 736.446167, 476.910767, 0.076584, 0\r\n238.345276,567.116333, 252.643799, 578.588684, 0.046921, 0\r\n454.324799,134.796417, 477.875275, 194.002335, 0.045907, 0\r\n238.524353,566.503601, 254.267441, 572.611511, 0.044823, 0\r\n0.294706,542.810059, 10.199859, 549.584412, 0.035525, 0\r\n425.673126,284.588470, 459.522614, 327.462799, 0.032669, 0\r\n236.521683,566.825256, 262.060394, 582.616699, 0.031999, 0\r\n746.805542,104.885902, 755.846802, 114.493874, 0.030192, 0\r\n407.737732,283.567169, 449.917511, 327.228058, 0.030156, 0\r\n409.439270,489.150360, 428.874329, 502.156342, 0.027988, 0\r\n238.210510,564.024109, 248.016968, 572.044067, 0.027763, 0\r\n115.589897,567.535950, 137.294540, 573.837524, 0.021952, 0\r\n427.162689,282.369354, 690.431763, 793.372009, 0.020722, 0\r\n357.684021,458.450348, 378.363678, 469.234802, 0.018810, 0\r\n638.479004,80.117393, 665.126587, 132.060730, 0.018268, 0\r\n384.765503,362.874207, 409.335968, 398.499512, 0.018212, 0\r\n236.312271,560.734192, 261.843079, 573.504700, 0.017360, 0\r\n236.125488,566.534058, 263.230408, 591.391846, 0.017274, 0\r\n439.041870,587.318298, 473.213074, 731.087463, 0.017142, 0\r\n408.762115,488.897552, 421.835602, 513.156189, 0.016366, 0\r\n725.005920,469.176483, 741.312500, 499.865540, 0.016227, 0\r\n199.599030,3.692741, 526.446411, 802.198242, 0.015871, 0\r\n264.280762,190.812927, 493.815277, 668.242126, 0.015495, 0\r\n722.991150,469.203949, 736.423096, 477.634705, 0.015429, 0\r\n334.728699,14.428749, 593.289429, 787.247925, 0.015052, 0\r\n579.858337,421.756287, 671.805481, 749.482544, 0.014739, 0\r\n408.830109,488.961609, 416.462585, 496.386597, 0.014693, 0\r\n439.641846,427.205780, 673.205322, 786.821411, 0.014140, 0\r\n555.062927,211.383301, 570.502258, 233.443130, 0.013810, 0\r\n423.592560,474.958496, 655.117493, 706.951904, 0.013428, 0\r\n541.012085,296.483459, 553.715149, 324.761505, 0.013368, 0\r\n417.633362,587.328735, 499.131897, 772.587036, 0.013226, 0\r\n0.205423,542.124939, 11.661577, 555.072327, 0.013153, 0\r\n578.188171,340.772461, 603.350220, 384.800018, 0.012889, 0\r\n115.839096,562.590637, 127.958076, 573.143799, 0.012841, 0\r\n638.829407,82.350456, 679.278381, 115.160980, 0.012707, 0\r\n557.337280,341.781464, 576.426086, 380.147095, 0.012333, 0\r\n389.385132,275.180786, 449.274292, 342.557129, 0.011945, 0\r\n358.656921,463.665131, 371.428650, 471.970062, 0.011891, 0\r\n230.595581,567.300049, 248.143936, 579.325684, 0.011848, 0\r\n351.845520,464.280396, 369.845032, 481.591675, 0.011816, 0\r\n372.600098,-6.409836, 702.137756, 799.286743, 0.011775, 0\r\n569.580505,29.419289, 807.729736, 785.201416, 0.011726, 0\r\n229.839157,560.183594, 246.077026, 571.958130, 0.011725, 0\r\n726.034485,465.855255, 750.102966, 482.702057, 0.011687, 0\r\n226.577011,566.285706, 243.736404, 572.849670, 0.011463, 0\r\n448.994873,423.597839, 653.400574, 658.237244, 0.011206, 0\r\n353.673706,448.639221, 370.566376, 467.538757, 0.010616, 0\r\n229.945816,566.257935, 272.129059, 591.202515, 0.010572, 0\r\n435.647125,429.286102, 482.257690, 477.639496, 0.010543, 0\r\n746.368347,106.118156, 771.953796, 123.264496, 0.010359, 0\r\n237.617203,549.077698, 250.345825, 571.559814, 0.010258, 0\r\n402.174042,473.342773, 431.851074, 497.778015, 0.010059, 0\r\n407.922363,585.244507, 464.583344, 631.346985, 0.010055, 0\r\n\r\nVisualized result saved in ./infer_onnx.jpg\r\n- 另外，在进行NPU部署时存在诸多冗余检测目标框，具体结果如下所示：\r\n\r\n> [INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=image, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=p2o.Mul.224, n_dims=4, dims=[1, 8400, 4, 1], n_elems=33600, size=33600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-88, scale=3.794765, pass_through=0\r\nindex=1, name=p2o.Concat.29, n_dims=4, dims=[1, 1, 8400, 1], n_elems=8400, size=8400, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003705, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[FastDeploy] Para Setting duration = 0.240989s.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[FastDeploy] PPDet in RKNPU2 duration = 0.070465s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n469.602173,450.628357, 469.602173, 450.628357, 0.937446, 0\r\n464.858704,721.005310, 498.062897, 763.696411, 0.937446, 0\r\n602.418945,464.858704, 602.418945, 460.115234, 0.933741, 0\r\n431.654510,298.837738, 436.397980, 298.837738, 0.926330, 0\r\n450.628357,602.418945, 450.628357, 602.418945, 0.926330, 0\r\n654.596924,94.869125, 654.596924, 90.125664, 0.922625, 0\r\n445.884888,602.418945, 445.884888, 602.418945, 0.915214, 0\r\n602.418945,469.602173, 602.418945, 469.602173, 0.911509, 0\r\n431.654510,313.068085, 436.397980, 313.068085, 0.907803, 0\r\n592.932007,464.858704, 592.932007, 460.115234, 0.900393, 0\r\n450.628357,611.905823, 450.628357, 616.649292, 0.900393, 0\r\n450.628357,422.167572, 483.832550, 474.345612, 0.896688, 0\r\n464.858704,450.628357, 460.115234, 450.628357, 0.896688, 0\r\n611.905823,464.858704, 616.649292, 460.115234, 0.896688, 0\r\n445.884888,611.905823, 445.884888, 616.649292, 0.896688, 0\r\n645.109985,94.869125, 645.109985, 90.125664, 0.892982, 0\r\n645.109985,99.612579, 645.109985, 99.612579, 0.889277, 0\r\n654.596924,99.612579, 654.596924, 99.612579, 0.878161, 0\r\n592.932007,469.602173, 592.932007, 469.602173, 0.870750, 0\r\n469.602173,464.858704, 469.602173, 460.115234, 0.867045, 0\r\n483.832550,450.628357, 483.832550, 450.628357, 0.852224, 0\r\n611.905823,469.602173, 616.649292, 469.602173, 0.833697, 0\r\n431.654510,294.094299, 436.397980, 294.094299, 0.829992, 0\r\n445.884888,298.837738, 445.884888, 298.837738, 0.778117, 0\r\n464.858704,464.858704, 460.115234, 460.115234, 0.778117, 0\r\n445.884888,313.068085, 445.884888, 313.068085, 0.744769, 0\r\n445.884888,294.094299, 445.884888, 294.094299, 0.737359, 0\r\n630.879639,80.638748, 664.083862, 113.842941, 0.596557, 0\r\n602.418945,483.832550, 602.418945, 483.832550, 0.563209, 0\r\n450.628357,592.932007, 450.628357, 592.932007, 0.507629, 0\r\n602.418945,450.628357, 602.418945, 450.628357, 0.429817, 0\r\n445.884888,592.932007, 445.884888, 592.932007, 0.418701, 0\r\n483.832550,464.858704, 483.832550, 460.115234, 0.392764, 0\r\n592.932007,483.832550, 592.932007, 483.832550, 0.329773, 0\r\n611.905823,450.628357, 616.649292, 450.628357, 0.303836, 0\r\n\r\nVisualized result saved in ./infer_rknpu2.jpg\r\n出现这种结果检测框和实际目标（6）不匹配是什么原因造成的？应该怎么解决？\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-07-05T09:30:16+00:00",
        "updated_at": "2024-07-08T08:31:48+00:00",
        "closed_at": "2023-07-10T14:08:39+00:00",
        "comments_count": [
            "MrMzl",
            "liuzong616"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2087,
        "title": "YOLOv8  Classification  Segmentation KeyPoint 什么时候能支持",
        "body": "YOLOv8  Classification  Segmentation KeyPoint 什么时候能支持\r\n现在只支持 Detection",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-06T01:05:35+00:00",
        "updated_at": "2024-11-05T06:41:28+00:00",
        "closed_at": "2024-11-05T06:41:28+00:00",
        "comments_count": [
            "lai6964"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2089,
        "title": "有什么办法可以动态检测输入的模型大小，而不是默认写死，如SetSize({640，640})",
        "body": "老师好\r\n目前可知通过model.GetPreprocessor().SetSize({416,416});来改变\r\n但我想请教一下，有自适应适配的接口吗",
        "state": "closed",
        "user": "lvjinjie",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-06T11:47:36+00:00",
        "updated_at": "2025-02-11T06:43:23+00:00",
        "closed_at": "2025-02-11T06:43:23+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2088,
        "title": "mask rcnn算法部署实例分割模型如何获取mask和轮廓contours",
        "body": "使用c++部署mask rcnn算法训练好的模型，无法获取每一个mask和轮廓contours信息",
        "state": "closed",
        "user": "happybear1015",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-06T07:55:15+00:00",
        "updated_at": "2025-02-11T06:43:22+00:00",
        "closed_at": "2025-02-11T06:43:22+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2091,
        "title": "Win10, 开启ENABLE_ENCRYPTION编译报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 最新develop分支\r\n- 【编译命令】使用教程中的编译命令，设置 ENABLE_ENCRYPTION=ON\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： CPU（i5-13400）\r\n- 【编译语言】： Python(3.10）\r\n\r\n## 问题日志及出现问题的操作流程\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\.\\fastdeploy/encryption/include/encrypt.h(51,42): error C2526: “Encrypt”: C 链接函数无法返回 C++\r\n 类“std::vector<std::string,std::allocator<std::string>>” [C:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\r\n\\fastdeploy.vcxproj]\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\.\\fastdeploy/encryption/include/encrypt.h(51,22): message : 参见“std::vector<std::string,s\r\ntd::allocator<std::string>>”的声明 [C:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\fastdeploy\\encryption\\src\\encrypt.cc(61,21): warning C4267: “参数”: 从“size_t”转换到“const int\r\n”，可能丢失数据 [C:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\fastdeploy\\encryption\\src\\encrypt.cc(91,43): error C2556: “std::vector<std::string,std::\r\nallocator<std::string>> fastdeploy::Encrypt(const std::string &,const std::string &)”: 重载函数与“void fastdeploy::Encrypt(c\r\nonst std::string &,const std::string &)”只是在返回类型上不同 [C:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\\fastd\r\neploy.vcxproj]\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\.\\fastdeploy/encryption/include/encrypt.h(51,42): message : 参见“fastdeploy::Encrypt”的声明 [\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\r\nC:\\Users\\ht\\Desktop\\FastDeploy\\fastdeploy\\encryption\\src\\encrypt.cc(90,26): error C2371: “fastdeploy::Encrypt”: 重定义；不同的\r\n基类型 [C:\\Users\\ht\\Desktop\\FastDeploy\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\r\n",
        "state": "closed",
        "user": "elky98",
        "closed_by": "elky98",
        "created_at": "2023-07-08T03:32:44+00:00",
        "updated_at": "2023-12-22T13:35:36+00:00",
        "closed_at": "2023-12-22T13:22:32+00:00",
        "comments_count": [
            "zhoujun0715",
            "elky98",
            "zhoujun0715",
            "elky98",
            "zhoujun0715",
            "elky98",
            "zhoujun0715",
            "elky98",
            "zhoujun0715",
            "zhoujun0715",
            "zhoujun0715",
            "elky98",
            "elky98",
            "elky98"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2093,
        "title": "为什么使用GPU编译库编译完成后出现error while loading shared libraries: libonnxruntime.so.1.12.0:",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 自行编译环境\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DTRT_DIRECTORY=/home/xx/dev-file/deploy-file/TensorRT-8.6.0.12 \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda-11.8 \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \\\r\n         -DORT_DIRECTORY=/home/xx/dev-file/deploy-file/onnxruntime-linux-x64-1.12.0 \\\r\n         -DENABLE_TEXT=ON\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】：  Nvidia GPU 3070， CUDA 11.8 CUDNN 8.6\r\n- 【编译语言】： C++ \r\n\r\n编译正常进行，但运行demo提示：error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory\r\n\r\nCMakeLists结果:\r\n-- Found OpenCV: /home/chenzhen/dev-file/fastdeploy-linux-x64-gpu-1.0.7/third_libs/install/opencv (found version \"3.4.16\") \r\n-- The path of ONNXRuntime is /home/chenzhen/dev-file/deploy-file/onnxruntime-linux-x64-1.12.0/lib.\r\n-- OPENVINO_LIBS = /home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/openvino/runtime/lib/libopenvino.so;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy\r\n-- The path of OpenCV is /usr/lib/x86_64-linux-gnu/cmake/opencv4.\r\n-- Found OpenCV: /usr (found version \"4.2.0\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.24.2\r\n--   CMake command             : /home/chenzhen/dev-file/clion-2022.3.3/bin/cmake/linux/x64/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/g++\r\n--   C++ standard              : 11\r\n--   C++ cuda standard         : \r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : \r\n--   Paddle Inference version  : 2.5.0.558ae9cd11\r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   OpenVINO version          : dev.2023.03.2\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : \r\n--   DEPENDENCY_LIBS           : /home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.so;/home/chenzhen/dev-file/deploy-file/onnxruntime-linux-x64-1.12.0/lib/libonnxruntime.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/paddle_inference/third_party/install/mkldnn/lib/libmkldnn.so.0;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/paddle_inference/third_party/install/mklml/lib/libiomp5.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/openvino/runtime/lib/libopenvino.so;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy;/usr/local/cuda/lib64/libcudart.so;/usr/local/cuda/lib64/libnvjpeg.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/tensorrt/lib/libnvinfer.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/tensorrt/lib/libnvonnxparser.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/tensorrt/lib/libnvinfer_plugin.so;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;opencv_aruco;opencv_bgsegm;opencv_bioinspired;opencv_ccalib;opencv_datasets;opencv_dnn_objdetect;opencv_dnn_superres;opencv_dpm;opencv_face;opencv_freetype;opencv_fuzzy;opencv_hdf;opencv_hfs;opencv_img_hash;opencv_line_descriptor;opencv_optflow;opencv_phase_unwrapping;opencv_plot;opencv_quality;opencv_reg;opencv_rgbd;opencv_saliency;opencv_stereo;opencv_structured_light;opencv_surface_matching;opencv_text;opencv_tracking;opencv_viz;opencv_ximgproc;opencv_xobjdetect;opencv_xphoto;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.so;/home/chenzhen/work-code/Paddle/FastDeploy/build/compiled_fastdeploy_sdk/third_libs/install/paddle2onnx/lib/libpaddle2onnx.so\r\n-- Configuring done\r\n-- Generating done\r\n\r\n",
        "state": "closed",
        "user": "vivienness",
        "closed_by": "jiangjiajun",
        "created_at": "2023-07-10T06:39:51+00:00",
        "updated_at": "2024-02-22T07:43:28+00:00",
        "closed_at": "2024-02-22T07:43:28+00:00",
        "comments_count": [
            "rainyfly",
            "chlg",
            "vivienness",
            "chlg",
            "chlg"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2092,
        "title": "自训练分类器模型转rknn失败",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：develop分支，最后更新Fri Jul 7 17:43:08 2023 +0800，\r\n         提交  4c1e80b7231a81e898b2bbaf1df07cce136de38f\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： wsl\r\n- 【编译语言】：Python 3.10.6\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【自训练分类器模型转rknn流程】\r\n- - 在PaddleClas Release 2.5 训练Resnet50_vd模型，配置文件./ppcls/configs/ImageNet/ResNet/ResNet50_vd.yaml，修改数据集为CIFAR100.tar方便快速验证\r\n- - 训练模型导出推理模型\r\n- - 使用paddle2onnx转换为onnx模型并固定shape。paddle2onnx (1.0.6)\r\n- - 转rknn模型失败，错误日志如下：\r\n- (rknn2_1.5) xxx@xxxxxx:~/PaddlePadle/FastDeploy$ python ./tools/rknpu2/export.py \\\r\n        --config_path ./tools/rknpu2/config/ResNet50_vd_infer_rknn.yaml \\\r\n        --target_platform rk3568\r\n{'model_path': './ResNet50_vd_infer/ResNet50_vd_infer.onnx', 'output_folder': './ResNet50_vd_infer', 'mean': [[123.675, 116.28, 103.53]], 'std': [[58.395, 57.12, 57.375]], 'outputs_nodes': None, 'do_quantization': False, 'dataset': './ResNet50_vd_infer/dataset.txt'}\r\nW __init__: rknn-toolkit2 version: 1.5.0+1fa95b5c\r\nE load_onnx: Catch exception when loading onnx model: /home/xxx/PaddlePadle/FastDeploy/ResNet50_vd_infer/ResNet50_vd_infer.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1382, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 658, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 58, in rknn.api.ir_graph.IRGraph.__init__\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 503, in rknn.api.ir_graph.IRGraph.rebuild\r\nE load_onnx:   File \"/home/xxx/.local/lib/python3.10/site-packages/onnx/checker.py\", line 119, in check_model\r\nE load_onnx:     C.check_model(protobuf_string, full_check)\r\nE load_onnx: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of 'type' is required but missing.\r\nW If you can't handle this error, please try updating to the latest version of the toolkit2 and runtime from:\r\n  https://eyun.baidu.com/s/3eTDMk6Y (Pwd: rknn)  Path: RK_NPU_SDK / RK_NPU_SDK_1.X.0 / develop /\r\n  If the error still exists in the latest version, please collect the corresponding error logs and the model,\r\n  convert script, and input data that can reproduce the problem, and then submit an issue on:\r\n  https://redmine.rock-chips.com (Please consult our sales or FAE for the redmine account)\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/PaddlePadle/FastDeploy/./tools/rknpu2/export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n\r\n- 【已做的分析与验证工作】\r\n- - 确认环境正常。\r\n       PaddleClas Release 2.5 训练模型无异常，训练模型导出推理模型过程无异常\r\n      完全按照https://github.com/PaddlePaddle/PaddleClas/tree/develop/deploy/fastdeploy/rockchip/rknpu2\r\n      使用官方模型，则转为onnx模型并固定shape，一直到转rknn模型完全正常。\r\n- - 排除rknn-toolkit2版本问题\r\n      更换版本至rknn_toolkit2-1.4.2b3+0bdd72ff-cp36-cp36m-linux_x86_64.whl进行测试，结果是一样的报错。\r\n- - 排除自定义训练模型问题\r\n      下载预训练模型[Resnet50_vd](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/ResNet50_vd_pretrained.pdparams)，并进行导出与转换，也在最后转rknn时报相同的错误。\r\n\r\n- 【分析与猜测】\r\n下面是我的分析与猜测，不一定正确仅供参考\r\n- - 怀疑是PaddleClas 2.5 所用的模型定义发生了变化触发的问题。\r\n- 比较了可用模型 https://bj.bcebos.com/paddlehub/fastdeploy/ResNet50_vd_infer.tgz 和训练模型，同为Resnet50_vd模型却在最后几个节点存在差异，一个是pool2d --> reshape2 --> matmul  -->  elementwise_add --> softmax  --> scale --> 0，另一个则是pool2d --> flatten_contiguous_range --> matmul_v2  -->  elementwise_add --> softmax  --> 0\r\n\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/12209617/0a086413-6966-43c2-b3ff-8e369ff73472)\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "fulsz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-10T02:55:16+00:00",
        "updated_at": "2025-04-09T06:20:26+00:00",
        "closed_at": "2025-02-11T06:43:24+00:00",
        "comments_count": [
            "fulsz",
            "fulsz",
            "rainyfly",
            "riskeverything"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2094,
        "title": "编译C# API 报错未能找到类型或命名空间名int64_t",
        "body": "- 【编译命令】\r\ncd FastDeploy-develop\r\nmkdir build && cd build\r\ncmake .. -G \"Visual Studio 17 2022\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_VISION=ON -DENABLE_TEXT=ON -DWITH_CAPI=ON -DWITH_CSHARPAPI=ON -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy\"\r\nnuget restore\r\nmsbuild Project.sln /m /p:Configuration=Release /p:Platform=x64\r\n- 【系统平台】:  Windows x64(Windows10) cpu\r\n- 【编译语言】：C#\r\n执行 msbuild Project.sln /m /p:Configuration=Release /p:Platform=x64后：\r\n- 【报错】：\r\n       “C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\fastdeploy.sln”(默认目标) (1) ->\r\n       “C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\ALL_BUILD.vcxproj.metaproj”(默认目标) (2) ->\r\n       “C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\csharp\\fastdeploy_csharp.csproj.metaproj”(默认目标) (11) ->\r\n       “C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\csharp\\fastdeploy_csharp.csproj”(默认目标) (16) ->\r\n       (CoreCompile 目标) ->\r\n         C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\csharp\\fastdeploy\\runtime_option.cs(113,16): error CS0246: 未能找到类 型或命名空\r\n       间名“int64_t”(是否缺少 using 指令或程序集引用?) [C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\csharp\\fastdeploy_csharp.cspro\r\n       j]\r\n         C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\csharp\\fastdeploy\\runtime_option.cs(113,24): error CS1750: 不能将“int”类型\r\n       的值用作默认参数，因为没有到类型“int64_t”的标准转换 [C:\\Users\\xxl\\Desktop\\FastDeploy-develop\\build\\csharp\\fastdeploy_csharp.csproj]\r\n\r\n    1189 个警告\r\n    2 个错误\r\n\r\n",
        "state": "closed",
        "user": "liang-stu",
        "closed_by": "rainyfly",
        "created_at": "2023-07-10T08:29:09+00:00",
        "updated_at": "2024-02-06T09:54:23+00:00",
        "closed_at": "2024-02-06T09:54:23+00:00",
        "comments_count": [
            "903142301",
            "liang-stu",
            "903142301"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2096,
        "title": "[WARNING] fastdeploy/runtime/runtime_option.cc(58)::fastdeploy::RuntimeOption::UseGpu\tThe FastDeploy didn't compile with GPU, will force to use CPU.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7 \r\n- 【编译命令】另外我也尝试了自行编译，错误提示一致。\r\ncmake .. -G \"Visual Studio 17 2022\" -A x64 ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_PADDLE_BACKEND=ON ^\r\n         -DENABLE_OPENVINO_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DWITH_GPU=ON ^\r\n         -DCUDA_DIRECTORY=\"D:\\cuda_manger\\cuda_11_7\" ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy\"\r\n编译过程顺利且无错误与警告。\r\n- 【系统平台】: Windows x64(Windows11)/Qt\r\n- 【硬件】：  Nvidia GPU 4060， CUDA 11.7 CUDNN 8.8\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 运行如下代码\r\n<img width=\"914\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/53334346/f9cafc54-d2af-43f4-a4e7-71b8645baab2\">\r\n-输出提示\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(58)::fastdeploy::RuntimeOption::UseGpu\tThe FastDeploy didn't compile with GPU, will force to use CPU.\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0710 22:20:09.210850  8372 analysis_config.cc:971] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::fastdeploy::Runtime::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\n\r\n在pro文件中配置了opencv/cuda/paddleinference等相关库\r\n<img width=\"922\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/53334346/d615f414-da2e-47c8-836a-551d4f950a34\">\r\n\r\n电脑自带GPU 情况如下\r\n<img width=\"340\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/53334346/e22e49ac-e73f-43ad-84a8-37a2ed5b886f\">\r\n\r\n\r\n编译应该是没有问题，应该是配置问题？？但是不知道具体哪里错了？\r\n\r\n",
        "state": "closed",
        "user": "kingkingpang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-10T14:49:22+00:00",
        "updated_at": "2025-02-11T06:43:25+00:00",
        "closed_at": "2025-02-11T06:43:25+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2097,
        "title": "which runtime framework is the best for raspberry pi 4 ?  ",
        "body": "Hi\r\n\r\nThanks a bunch for sharing this project ? \r\n\r\nI have a general question. I want to know which framework is the best to run deep learning models on raspberry pi in python? onnxruntime, tensorflow lite  or othee framework ? \r\nI want to use a framework that has a set of features such as int8 quantization . \r\n\r\nbest regards\r\n",
        "state": "open",
        "user": "saeedkhanehgir",
        "closed_by": null,
        "created_at": "2023-07-11T08:37:32+00:00",
        "updated_at": "2023-07-17T07:12:59+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "saeedkhanehgir"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2098,
        "title": "PaddleClasModel怎么得到feature_map呀？",
        "body": "\r\nPaddleClasModel怎么得到feature_map呀？",
        "state": "closed",
        "user": "Firestick-Xia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-12T05:22:08+00:00",
        "updated_at": "2025-02-11T06:43:26+00:00",
        "closed_at": "2025-02-11T06:43:26+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2102,
        "title": "capi问题",
        "body": "capi没办法指定yolov5的imagesize  只能修改源码",
        "state": "open",
        "user": "taojishou",
        "closed_by": null,
        "created_at": "2023-07-13T01:36:53+00:00",
        "updated_at": "2023-07-13T02:11:39+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "taojishou"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2104,
        "title": "Error in launch Streamer PP-YOLOE C++ Example",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： release/1.0.7 分支\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： Nvidia GPU 3080， 环境为 docker `nvcr.io/nvidia/deepstream:6.1.1-devel`\r\n- 【编译语言】： C++\r\n- 【编译命令】\r\n```\r\n# 编译 FastDeploy （opencv 版本 4.6.0）\r\ncd FastDeploy\r\nmkdir -p build && cd build\r\n\r\ncmake .. \\\r\n-D WITH_GPU=ON \\\r\n-D ENABLE_ORT_BACKEND=ON \\\r\n-D ENABLE_TRT_BACKEND=ON \\\r\n-D ENABLE_VISION=ON \\\r\n-D CUDA_DIRECTORY=/usr/local/cuda \\\r\n-D TRT_DIRECTORY=/root/workspace/TensorRT-8.4.1.5 \\\r\n-D OPENCV_DIRECTORY=/root/workspace/opencv/lib/cmake/opencv4 \\\r\n-D CMAKE_INSTALL_PREFIX=./install\r\n\r\nmake -j$(nproc) && make install\r\n\r\n# 编译 fdstreamer\r\ncd FastDeploy/streamer/\r\nmkdir -p build && cd build\r\n\r\ncmake ..\\\r\n-D ENABLE_DEEPSTREAM=ON \\\r\n-D FASTDEPLOY_INSTALL_DIR=../build/install\r\n\r\nmake -j$(nproc)\r\n\r\n# 编译 ppyoloe example\r\ncd FastDeploy/streamer/examples/ppyoloe/cpp/\r\nmkdir -p build && cd build\r\n\r\ncmake -D FASTDEPLOY_INSTALL_DIR=/root/workspace/FastDeploy/build/install ..\r\n\r\nmake -j$(nproc)\r\n\r\n# 执行\r\ncd FastDeploy/streamer/examples/ppyoloe/cpp/\r\nmv ./build/streamer_demo .\r\n./streamer_demo\r\n```\r\n\r\n- 【报错内容】\r\n```\r\nroot@c560e41fc7fc:~/workspace/FastDeploy/streamer/examples/ppyoloe/cpp# ./streamer_demo \r\napp\r\nnvurisrcbin_list\r\nnvstreammux\r\nnvinfer\r\nnvtracker\r\nnvmultistreamtiler\r\nnvosdbin\r\nnvvideoencfilesinkbin\r\n[INFO] /root/workspace/FastDeploy/streamer/src/gstreamer/utils.cc(60)::CreatePipeline   Trying to launch pipeline: nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_0  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_1  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_2  nvurisrcbin uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_ride_bike.mov gpu-id=0 ! mux.sink_3  nvstreammux name=mux gpu-id=0 batch-size=4 width=1920 height=1080 batched-push-timeout=40000 ! nvinfer gpu-id=0 config-file-path=nvinfer_config.txt ! nvtracker gpu-id=0 tracker-width=640 tracker-height=640 ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so ll-config-file=/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/config_tracker_NvDCF_perf.yml enable-batch-process=true ! nvmultistreamtiler gpu-id=0 rows=2 columns=2 ! nvosdbin gpu-id=0 ! nvvideoencfilesinkbin gpu-id=0 bitrate=4000 output-file=out.mp4 \r\ndsnvvideoencfilesinkbin0\r\ngstnvtracker: Loading low-level lib at /opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so\r\ngstnvtracker: Batch processing is ON\r\ngstnvtracker: Past frame output is OFF\r\n\r\n!![ERROR] yaml-cpp: error at line 25, column 26: bad conversion\r\nAn exception occurred. yaml-cpp: error at line 25, column 26: bad conversion\r\ngstnvtracker: Failed to initialize tracker context!\r\ngstnvtracker:: Failed to create batch context. Shutting down processing.\r\nRunning...\r\n^C\r\n```\r\n\r\n",
        "state": "open",
        "user": "ThinkWD",
        "closed_by": null,
        "created_at": "2023-07-13T07:17:07+00:00",
        "updated_at": "2023-09-21T06:58:15+00:00",
        "closed_at": null,
        "comments_count": [
            "ThinkWD",
            "HGD-ai",
            "ThinkWD"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2106,
        "title": "fastdeploy UIEmodel，尝试线程池、异步，并行运行 -->结果全是串行",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.6\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060， CUDA 11.8  CUDNN 8.6\r\n- 【编译语言】： python 3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n代码如下：\r\ndef func_information_extract(item_dict):\r\n    print('fun1 start')\r\n    information_extraion_model.set_schema(item_dict['prompt_ie'])\r\n    ie_result = information_extraion_model.predict(item_dict['data'],return_dict=True)\r\n    return ie_result\r\n\r\ndef func_sentiment(item_dict):\r\n    print('fun2 start')\r\n    sentiment_analysis_model.set_schema(item_dict['prompt_se'])\r\n    se_result = sentiment_analysis_model.predict(item_dict['data'],return_dict=True)\r\n    return se_result\r\n    \r\n\r\ndef func_cls(item_dict):\r\n    print('fun3 start')\r\n    cls_model.set_schema(item_dict['prompt_cls'])\r\n    cls_result = cls_model.predict(item_dict['data'],return_dict=True)\r\n    return cls_result\r\n\r\n\r\n@app.post('/test/')\r\nasync def parallel_run(item: Item_merge):\r\n    try:\r\n        item_dict = item.dict()\r\n        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\r\n            futures = [\r\n                executor.submit(func_information_extract, item_dict),\r\n                executor.submit(func_sentiment, item_dict),\r\n                executor.submit(func_cls, item_dict)\r\n            ]\r\n\r\n            results = []\r\n            for future in concurrent.futures.as_completed(futures):\r\n                result = future.result()\r\n                results.append(result)\r\n\r\n        # 封装成 FastAPI 可以返回的格式\r\n        response = {\r\n            'information_extraction': results[0][0],\r\n            'sentiment_analysis': results[1][0],\r\n            'classification': results[2][0]\r\n        }\r\n        return JSONResponse(status_code=200, content={'status': 200, 'msg': '执行成功', 'code': 200, 'data': response})\r\n    \r\n    except Exception as e:\r\n        return JSONResponse(status_code=418, content={'status': 500, 'msg': '程序运行错误', 'code': 500, 'data': str(e)})\r\n\r\n三个model 是加载自fastdeploy.text 的UIEModel,是三个不同的model，分别加载不同的权重。我在尝试在一个函数里并行运行三个model的推理，我尝试了线程池和python 异步，结果都是串行执行，不起作用。请问是不是fastdeploy uie的底层实现限制了无法并行运行？\r\n \r\n",
        "state": "open",
        "user": "lzh1998-jansen",
        "closed_by": null,
        "created_at": "2023-07-13T10:48:34+00:00",
        "updated_at": "2024-12-09T12:44:57+00:00",
        "closed_at": null,
        "comments_count": [
            "leiqing1",
            "lzh1998-jansen",
            "NyquistBodeTu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2109,
        "title": "中文版面分析模型部署预测结果错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\nexamples/vision/ocr/PP-OCR/cpu-gpu/python/infer_structurev2_layout.py\r\n执行脚本，仅将路径改为中文的版面分析模型picodet_lcnet_x1_0_fgd_layout_cdla_infer，结果预测和PPStructure的结果不一样，这个示例脚本不适用中文版面模型picodet_lcnet_x1_0_fgd_layout_cdla_infer吗？\r\n\r\n- 原页面：\r\n![原页面](https://github.com/PaddlePaddle/FastDeploy/assets/25731261/941f57f3-e446-43d4-9262-e4a56a1a5f90)\r\n\r\n- 原PPStructure载入中文版面分析模型picodet_lcnet_x1_0_fgd_layout_cdla_infer的预测结果：\r\n![PPStructure结果](https://github.com/PaddlePaddle/FastDeploy/assets/25731261/64f53b8a-9291-41d7-bbb3-077a82194731)\r\n\r\n- infer_structurev2_layout.py预测结果:\r\n![Fastdeploy结果](https://github.com/PaddlePaddle/FastDeploy/assets/25731261/c72bd0fe-13a7-4b8e-a594-9da83bf57347)\r\n",
        "state": "closed",
        "user": "lycfight",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-14T07:32:39+00:00",
        "updated_at": "2024-09-17T06:42:44+00:00",
        "closed_at": "2024-09-17T06:42:44+00:00",
        "comments_count": [
            "tim7-m",
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2107,
        "title": "链接失效，编译时，setup.py里的paddle2onnx包的链接失效了",
        "body": "\r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/paddle2onnx-linux-aarch64-1.0.8rc.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n",
        "state": "closed",
        "user": "LXXIANG12",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-13T11:02:52+00:00",
        "updated_at": "2024-08-13T06:42:47+00:00",
        "closed_at": "2024-08-13T06:42:47+00:00",
        "comments_count": [
            "leiqing1",
            "jiangjiajun",
            "LXXIANG12",
            "LXXIANG12",
            "LXXIANG12",
            "jiangjiajun",
            "LXXIANG12",
            "jryxxx",
            "LXXIANG12",
            "jryxxx",
            "LXXIANG12",
            "jryxxx",
            "jiangjiajun",
            "jryxxx",
            "LXXIANG12",
            "jryxxx",
            "jryxxx",
            "jryxxx",
            "jiangjiajun",
            "jryxxx",
            "jiangjiajun",
            "jryxxx",
            "jryxxx",
            "Miller-em",
            "jiangjiajun",
            "Miller-em"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2110,
        "title": "jetson nano部署python环境报错",
        "body": "\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103343903/f2a2e9ee-ba54-48ab-bd2d-5d47a9246e07)\r\n按照以上官方教程编译报错：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103343903/96285a67-e22e-455d-9bb7-16b3c92cbd8b)\r\n环境：jetpack=4.6.1    cmake=3.26.4   gcc=7.3.1 python=3.10",
        "state": "closed",
        "user": "LXXIANG12",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-14T07:51:08+00:00",
        "updated_at": "2024-07-30T06:41:51+00:00",
        "closed_at": "2024-07-30T06:41:51+00:00",
        "comments_count": [
            "jiangjiajun",
            "bianyiJetsonnano"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2113,
        "title": "能不能和 Nuitka 打包的合作 , 实现FastDeploy打包部署",
        "body": "https://github.com/Nuitka/Nuitka\r\n\r\n能不能和 Nuitka 打包的合作\r\n实现FastDeploy打包部署\r\n\r\n人家提供特定框架的解决\r\n\r\nNuitka\r\n打包比Pyinstaller 好多了",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-15T02:18:08+00:00",
        "updated_at": "2025-02-11T06:43:26+00:00",
        "closed_at": "2025-02-11T06:43:26+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2114,
        "title": "RK3588s 上使用 PP-YOLOE-SOD 导出的 RKNN 模型输出结果始终为空，ONNX输出结果正常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-develop\r\n- 【系统平台】:Ubuntu 20.04.6 LTS\r\n- 【硬件】： RK3588s\r\n- 【模型转换环境】: rknn-toolkit2版本为1.5.1b13\r\n- 【编译语言】： Python3.8.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n- 【模型跑不通】\r\n- - 执行examples下/vision/detection/paddledetection/rknpu2中的代码，都能跑通，并且使用官方提供的ppyoloe_plus_crn_s_80e_coco模型，ONNX和RKNN都能跑通出结果；\r\n- - 但使用PP-YOLOE-SOD的官方模型，通过 rknn-toolkit2 1.5.1b13转换成RKNN后，RKNN模型能在NPU上运行，有运行时间的提示，但没有输出结果始终为空，整个过程中也没有任何报错，同时，使用转换RKNN模型前的ONNX在CPU上运行，能正常输出结果。\r\n- - - 使用的代码是官方原版代码，模型未做任何改动，推理代码也是官方原版代码，只更改了模型名称\r\n\r\n- - - ONNX模型运行log：\r\n- - - -$ ./infer_ppyoloe_plus_sod_crn_s_80e_visdrone_w_nms ../models/ppyoloe_plus_sod_crn_s_80e_visdrone_w_nms ../images/20230420062143008.bmp 0\r\n- - - -[INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend     Runtime initialized with Backend::ORT in Device::CPU.\r\n- - - -[FastDeploy] PPDet in ONNX duration = 0.944609s.\r\n- - - -DetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n- - - -1764.440186,1963.194946, 1825.802490, 2014.900269, 0.874629, 3\r\n- - - -1879.012207,1586.902832, 1924.154785, 1626.965576, 0.835925, 3\r\n- - - -1617.314331,1601.050659, 1662.620850, 1645.246094, 0.815566, 3\r\n- - - -1799.939575,2067.222168, 1870.940186, 2131.863281, 0.800500, 3\r\n- - - -1609.115112,1834.134155, 1662.692627, 1878.682739, 0.793823, 3\r\n- - - -1922.314819,1568.100830, 1969.835449, 1606.360718, 0.783511, 3\r\n- - - -1734.748901,1883.613403, 1797.476196, 1938.679199, 0.781813, 3\r\n- - - -1559.900391,1466.017700, 1604.416992, 1505.471924, 0.781790, 3\r\n- - - -Visualized result saved in ./infer_onnx.jpg\r\n\r\n- - -RKNN模型运行log：\r\n- - - -$ ./infer_ppyoloe_plus_sod_crn_s_80e_visdrone_w_nms ../models/ppyoloe_plus_sod_crn_s_80e_visdrone_w_nms ../images/20230420062143008.bmp 1\r\n- - - -[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.5.1b13 (4a7483d6e@2023-06-16T17:28:52)\r\n- - - -[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.8.2\r\n- - - -index=0, name=image, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\n- - - -index=0, name=p2o.Mul.195, n_dims=4, dims=[1, 8400, 4, 1], n_elems=33600, size=33600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-110, scale=2.873857, pass_through=0\r\n- - - -index=1, name=p2o.Concat.32, n_dims=4, dims=[1, 10, 8400, 1], n_elems=84000, size=84000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.000223, pass_through=0\r\n- - - -[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n- - - -[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory       The input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n- - - -[FastDeploy] PPDet in RKNPU2 duration = 0.645644s.\r\n- - - -DetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n- - - -\r\n- - - -Visualized result saved in ./infer_rknpu2.jpg",
        "state": "open",
        "user": "clintonfang",
        "closed_by": null,
        "created_at": "2023-07-16T03:54:27+00:00",
        "updated_at": "2023-08-21T02:53:06+00:00",
        "closed_at": null,
        "comments_count": [
            "Miller-em",
            "clintonfang",
            "Miller-em",
            "clintonfang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2115,
        "title": "MCFairMOT多类别目标追踪模型推理问题，识别效果差，检测框固定在图像左上角",
        "body": "*********************************************\r\nMCFairMOT多类别目标追踪模型推理问题\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 1050TI， CUDA 11.2 CUDNN 8.2\r\n- 【编译语言】： Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n推理MCFairMOT模型只能识别一两个目标，而且检测框的左上角会固定在图像左上角。\r\n确定模型没有问题，使用PaddleDetection的推理代码结果是正常的。\r\n",
        "state": "closed",
        "user": "fmy077",
        "closed_by": "fmy077",
        "created_at": "2023-07-16T08:21:34+00:00",
        "updated_at": "2023-10-30T14:20:07+00:00",
        "closed_at": "2023-09-03T08:09:44+00:00",
        "comments_count": [
            "fmy077",
            "jack00000",
            "fmy077"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2117,
        "title": "PP-MobileSeg使用FastDeploy部署问题",
        "body": "\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-window-cpu-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： cpu\r\n- 【编译语言】： python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n    PP-MobileSeg 训练完导出的模型使用下面代码运行报错：\r\n  ```\r\n    model = fd.vision.segmentation.PaddleSegModel(\r\n    \"model.pdmodel\", \"model.pdiparams\", \"deploy.yaml\", runtime_option=option)\r\n    im = cv2.imread(\"1.jpg\")\r\n    result = model.predict(im)\r\n    print(result)\r\n 报错：\r\n```\r\nRuntimeError: Check 'false' failed at C:\\Jenkins\\workspace\\private-ci\\ie\\build-windows-vs2019\\b\\repos\\openvino\\src\\frontends\\paddle\\src\\frontend.cpp:60:\r\nFrontEnd API failed with OpConversionFailure: :\r\nFail to convert linear_interp_v2 Exception Check 'scale.size() > 0 && scale.size() <= scale_size' failed at C:\\Jenkins\\workspace\\private-ci\\ie\\build-windows-vs2019\\b\\repos\\openvino\\src\\frontends\\paddle\\src\\op\\interp.cpp:19:\r\nFrontEnd API failed with GeneralFailure: \r\n```\r\n\r\n\r\n- 【直接使用PP-MobileSeg 导出的模型】\r\n   直接使用PP-MobileSeg 导出的模型运行报错：\r\n  ```\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site- \r\n  packages\\fastdeploy\\vision\\segmentation\\ppseg\\__init__.py\", line 40, in __init__\r\n   self._model = C.vision.segmentation.PaddleSegModel(RuntimeError: Check 'm_fw_ptr->ParseFromIstream(streams[0])' failed at \r\n   C:\\Jenkins\\workspace\\private-ci\\ie\\build-windows-vs2019\\b\\repos\\openvino\\src\\frontends\\paddle\\src\\input_model.cpp:342:\r\n  FrontEnd API failed with GeneralFailure: :\r\n  Model can't be parsed\r\n  ```\r\n\r\n- 【直接使用PP-MobileSeg 导出的模型】\r\n ```\r\nmodel = fd.vision.matting.PPMatting(\r\n    \"model.pdmodel\", \"model.pdiparams\", \"deploy.yaml\", runtime_option=option)\r\n\r\n```\r\n可得到MattingResult，正常运行，但和预期不符合",
        "state": "closed",
        "user": "ashe2333",
        "closed_by": "ashe2333",
        "created_at": "2023-07-17T11:46:29+00:00",
        "updated_at": "2023-07-17T14:02:46+00:00",
        "closed_at": "2023-07-17T14:02:46+00:00",
        "comments_count": [
            "jiangjiajun",
            "ashe2333"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2116,
        "title": "何时能够支持PaddleX导出的模型",
        "body": "现在FastDeploy支持PaddleSeg、PaddleCls和PaddleDetection三大组件导出的模型，但何时能支持PaddleX导出的模型？",
        "state": "closed",
        "user": "tsing-luo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-17T03:17:10+00:00",
        "updated_at": "2025-02-11T06:43:27+00:00",
        "closed_at": "2025-02-11T06:43:27+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2119,
        "title": "使用c#api的例程链接是失效的",
        "body": null,
        "state": "closed",
        "user": "guoyunqingyue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-18T02:52:09+00:00",
        "updated_at": "2025-02-11T06:43:28+00:00",
        "closed_at": "2025-02-11T06:43:28+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2120,
        "title": "ppyoloe_plus模型onnx转rknn后cpp预测位置有误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【编译命令】交叉编译FastDeploy C++ SDK\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 迅为i-Top RK3588开发板\r\n- 【编译语言】： C++ / Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型精度问题】\r\n- - 下载ppyoloe_plus_crn_s_80e_coco.zip压缩包，在板子上利用infer_ppyoloe_demo.cc编译后可执行程序调用.rknn文件推理可以得到正确结果，但是如果在虚拟机对ppyoloe_plus_crn_s_80e_coco.zip压缩包下的ppyoloe_plus_crn_s_80e_coco.onnx文件，利用FastDeploy/tools/rknpu2/export.py转换成.rknn文件，在开发板下推理则得到错误推理结果。\r\n- 正确打印信息为：\r\n- [INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.7.2\r\nindex=0, name=image, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=p2o.Mul.224, n_dims=4, dims=[1, 8400, 4, 1], n_elems=33600, size=33600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-72, scale=4.541211, pass_through=0\r\nindex=1, name=p2o.Concat.29, n_dims=4, dims=[1, 80, 8400, 1], n_elems=672000, size=672000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003515, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory       The input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[FastDeploy] PPDet in RKNPU2 duration = 0.146818s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n404.167786,118.071487, 508.615662, 581.275024, 0.790784, 0\r\n499.533234,154.401184, 581.275024, 558.568970, 0.790784, 0\r\n295.178711,168.024811, 413.250214, 581.275024, 0.790784, 0\r\n485.909576,213.436920, 640.310791, 508.615662, 0.727521, 2\r\n254.307831,172.566025, 286.096313, 245.225403, 0.639656, 9\r\n281.555084,9.082422, 308.802368, 90.824219, 0.492043, 9\r\n277.013885,13.623633, 322.425995, 136.236328, 0.463926, 9\r\n249.766617,54.494534, 267.931458, 140.777542, 0.302255, 9\r\n431.415070,190.730865, 485.909576, 331.508423, 0.604510, 26\r\n\r\nVisualized result saved in ./infer_rknpu2.jpg\r\n\r\n错误打印信息为：\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.4.2b0 (c5d79ccf9@2023-02-14T17:55:39)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.7.2\r\nindex=0, name=image, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=p2o.Mul.224, n_dims=4, dims=[1, 8400, 4, 1], n_elems=33600, size=33600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-72, scale=4.541211, pass_through=0\r\nindex=1, name=p2o.Concat.29, n_dims=4, dims=[1, 80, 8400, 1], n_elems=672000, size=672000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003515, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory       The input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\n[FastDeploy] PPDet in RKNPU2 duration = 0.128157s.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n45.412106,-118.071480, 808.335510, 640.310730, 0.790784, 0\r\n199.813263,-22.706053, 903.700928, 735.676147, 0.731036, 2\r\n59.035740,-13.623632, 481.368317, 413.250183, 0.646685, 9\r\n267.931427,199.813263, 267.931427, 199.813263, 0.632627, 9\r\n267.931427,195.272064, 267.931427, 195.272064, 0.579908, 9\r\n277.013855,4.541211, 304.261108, 90.824211, 0.495558, 9\r\n267.931427,208.895691, 267.931427, 208.895691, 0.463927, 9\r\n90.824211,-122.612686, 508.615601, 236.142960, 0.425266, 9\r\n-118.071480,-249.766586, 640.310730, 449.579865, 0.411208, 9\r\n272.472626,195.272064, 272.472626, 195.272064, 0.397149, 9\r\n272.472626,199.813263, 272.472626, 199.813263, 0.390120, 9\r\n254.307800,199.813263, 254.307800, 199.813263, 0.369032, 9\r\n267.931427,186.189636, 267.931427, 181.648422, 0.362003, 9\r\n254.307800,195.272064, 254.307800, 195.272064, 0.323343, 9\r\n254.307800,208.895691, 254.307800, 208.895691, 0.302255, 9\r\n267.931427,59.035740, 690.264038, 481.368317, 0.604510, 26\r\n\r\nVisualized result saved in ./infer_rknpu2.jpg\r\n\r\n从打印信息上看，前面推理过程都一样，但是自己转的rknn结果有问题，请问onnx转rknn过程是否有问题？\r\n\r\n",
        "state": "closed",
        "user": "maqun8358",
        "closed_by": "maqun8358",
        "created_at": "2023-07-18T03:37:45+00:00",
        "updated_at": "2023-07-18T07:30:05+00:00",
        "closed_at": "2023-07-18T07:30:05+00:00",
        "comments_count": [
            "maqun8358",
            "maqun8358",
            "maqun8358"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2121,
        "title": "咨询PaddleOCR在RKNPU2上的量化精度",
        "body": "1. 我们测试 [PaddleOCR模型](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/rockchip/README.md#paddleocr-%E6%A8%A1%E5%9E%8B%E5%9C%A8rknpu2%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88-fastdeploy) 发现只有fp16的模型，并没有int8模型，想了解一下模型在int8量化情况的精度问题，望分享。\r\n2. 我们想在端侧NPU上尝试部署OCR算法，但不知是否可行，所以想了解业界相关ocr量化精度，确定可行性。",
        "state": "closed",
        "user": "XiaotaoChen",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-18T06:14:21+00:00",
        "updated_at": "2024-07-30T06:41:52+00:00",
        "closed_at": "2024-07-30T06:41:52+00:00",
        "comments_count": [
            "xuxiaolon"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2122,
        "title": "没有为 openvino.dll 加载的符号文件",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】使用Visual Studio 2019创建sln工程\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】：CUDA 11.4 CUDNN 8.4.1\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n编译成功，在执行可执行文件时，出错：\r\n<img width=\"870\" alt=\"88d78dd47678bd2a2d8f3e55d2feedd\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/73320203/9dfecfb8-2099-400d-ba90-02ee7e4dbfdc\">\r\n\r\n",
        "state": "closed",
        "user": "WYQ-Github",
        "closed_by": "WYQ-Github",
        "created_at": "2023-07-18T09:52:28+00:00",
        "updated_at": "2023-07-25T01:10:46+00:00",
        "closed_at": "2023-07-19T08:42:11+00:00",
        "comments_count": [
            "WYQ-Github",
            "alexHxun",
            "WYQ-Github"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2123,
        "title": "创建c#api时遇到问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-cpu\r\n- 【编译命令】采用官方的编译命令\r\n-git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64 ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_PADDLE_BACKEND=ON ^\r\n         -DENABLE_OPENVINO_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DENABLE_TEXT=ON ^\r\n         -DWITH_CAPI=ON ^\r\n         -DWITH_CSHARPAPI=ON ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy\"\r\nnuget restore  #（please execute it when WITH_CSHARPAPI=ON to prepare dependencies in C#)\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\nmsbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 经过一些修改：\r\n- 1.set(CMAKE_CSharp_FLAGS \"/langversion:10\")改为9\r\n- 2.OpenCvSharp4版本改为4.2.0.20200108\r\n- 3.int64_t改为long\r\n\r\n在编译时产生错误：LINK : fatal error LNK1181: 无法打开输入文件“OPENVINO_LIB-NOTFOUND.lib” [D:\\vs2019\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n\r\n错误日志如下：\r\n10>LINK : fatal error LNK1181: 无法打开输入文件“OPENVINO_LIB-NOTFOUND.lib” [D:\\vs2019\\FastDeploy\\build\\fastdeploy.vcxproj]\r\n    10>已完成生成项目“D:\\vs2019\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标)的操作 - 失败。\r\n     5>已完成生成项目“D:\\vs2019\\FastDeploy\\build\\copy_yaml_library.vcxproj”(默认目标)的操作 - 失败。\r\n     3>已完成生成项目“D:\\vs2019\\FastDeploy\\build\\ALL_BUILD.vcxproj”(默认目标)的操作 - 失败。\r\n     1>已完成生成项目“D:\\vs2019\\FastDeploy\\build\\INSTALL.vcxproj”(默认目标)的操作 - 失败。\r\n\r\n生成失败。\r\n\r\n       “D:\\vs2019\\FastDeploy\\build\\INSTALL.vcxproj”(默认目标) (1) ->\r\n       “D:\\vs2019\\FastDeploy\\build\\ALL_BUILD.vcxproj”(默认目标) (3) ->\r\n       “D:\\vs2019\\FastDeploy\\build\\copy_yaml_library.vcxproj”(默认目标) (5) ->\r\n       “D:\\vs2019\\FastDeploy\\build\\fastdeploy.vcxproj”(默认目标) (10) ->\r\n       (Link 目标) ->\r\n         LINK : fatal error LNK1181: 无法打开输入文件“OPENVINO_LIB-NOTFOUND.lib” [D:\\vs2019\\FastDeploy\\build\\fastdeploy.vcxproj\r\n       ]\r\n\r\n    0 个警告\r\n    1 个错误\r\n",
        "state": "closed",
        "user": "guoyunqingyue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-19T01:24:12+00:00",
        "updated_at": "2025-05-13T06:50:18+00:00",
        "closed_at": "2025-05-13T06:50:18+00:00",
        "comments_count": [
            "guoyunqingyue",
            "BigerPatax",
            "smalie2222"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2124,
        "title": "运行demo或者编译时，会有libcudart.so.11.0 找不到的报错",
        "body": "错误log如下，我的cuda环境时10.2.89，这个cuda版本不能更换，请问下作者，怎么适配10.2.89的cuda？谢谢\r\n\r\nroot@e49e02585200:/home/FastDeploy/python_demo# python3 paddledetection.py \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"paddledetection.py\", line 3, in <module>\r\n    import fastdeploy.vision as vision\r\n  File \"/usr/local/lib/python3.7/site-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/usr/local/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "Sooguo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-19T08:58:15+00:00",
        "updated_at": "2024-07-23T06:41:02+00:00",
        "closed_at": "2024-07-23T06:41:02+00:00",
        "comments_count": [
            "tingaicompass"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2126,
        "title": "有考虑 Rust 吗",
        "body": "高性能\r\n低资源占用\r\n可以部署在 桌面端   WebAssembly   嵌入式开发板  服务器\r\n服务器合适大规模并发推理\r\n\r\n有深度学习的框架示例等等\r\nhttps://github.com/LaurentMazare/tch-rs\r\nhttps://github.com/burn-rs/burn\r\n\r\nhttps://github.com/pykeio/ort\r\nhttps://github.com/microsoft/onnxruntime/tree/main/rust\r\n\r\n\r\nWebAssembly   演示\r\nhttps://burn-rs.github.io/demo\r\n\r\nRust paddle_inference库  是对百度飞浆推理库C接口的封装\r\nhttps://docs.rs/paddle_inference/latest/paddle_inference\r\nhttps://github.com/ZB94/paddle_inference",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "monkeycc",
        "created_at": "2023-07-20T06:53:50+00:00",
        "updated_at": "2023-07-26T16:28:07+00:00",
        "closed_at": "2023-07-26T16:28:07+00:00",
        "comments_count": [
            "leiqing1",
            "monkeycc",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2127,
        "title": "FastDeploy是否支持在华为昇腾Atlas 200I DK A2开发板上的编译与模型推理部署",
        "body": "FastDeploy是否支持板端编译部署FastDeploy？\r\n我这边在编译过程中直接会在编译到26%多的时候卡住，不报错也不会停止编译，查看板端CPU使用情况，依然是正在占用中。\r\n(base) root@davinci-mini:/home/data/FastDeploy-develop/build# cmake .. -DWITH_ASCEND=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-ascend -DENABLE_VISION=ON\r\n-- The C compiler identification is GNU 12.1.0\r\n-- The CXX compiler identification is GNU 12.1.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /home/data/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 16% complete]\r\n-- [download 21% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 42% complete]\r\n-- [download 47% complete]\r\n-- [download 52% complete]\r\n-- [download 57% complete]\r\n-- [download 63% complete]\r\n-- [download 68% complete]\r\n-- [download 73% complete]\r\n-- [download 78% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 98% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/data/FastDeploy-develop/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Build FastDeploy Ascend C++ library on aarch64 platform.\r\n-- Use the default OpenCV lib from: https://bj.bcebos.com/paddle2onnx/libs/opencv-linux-aarch64-3.4.14.tgz\r\nDownloading file from https://bj.bcebos.com/paddle2onnx/libs/opencv-linux-aarch64-3.4.14.tgz to /home/data/FastDeploy-develop/build/opencv-linux-aarch64-3.4.14.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 2% complete]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 7% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 13% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\n-- [download 17% complete]\r\n-- [download 18% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 23% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 29% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 34% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- [download 40% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 45% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 51% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 56% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 61% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 67% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 72% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 78% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 89% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 94% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/data/FastDeploy-develop/build/opencv-linux-aarch64-3.4.14.tgz ...\r\n-- Found OpenCV: /home/data/FastDeploy-develop/build/third_libs/install/opencv (found version \"3.4.14\")\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.22.1\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ standard              : 11\r\n--   C++ cuda standard         :\r\n--   C++ compiler version      : 12.1.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          :\r\n--   Shared linker flags       :\r\n--   Build type                :\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_LITE_BACKEND;ENABLE_VISION\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /home/data/FastDeploy-develop/build/fastdeploy-ascend\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : ON\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : ON\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   Paddle Lite version       :\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/data/FastDeploy-develop/build\r\n(base) root@davinci-mini:/home/data/FastDeploy-develop/build# make -j8\r\n[  0%] Creating directories for 'extern_paddlelite'\r\n[  0%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  3%] Performing download step (download, verify and extract) for 'extern_paddlelite'\r\n-- Downloading...\r\n   dst='/home/data/FastDeploy-develop/build/third_libs/paddlelite/src/FastDeploy.CPP.inference_lite_lib.ubuntu.armv8.huawei_ascend_npu.CANN5.1.RC2.alpha001.tar.gz'\r\n   timeout='none'\r\n   inactivity timeout='none'\r\n-- Using src='https://paddle-qa.bj.bcebos.com/Paddle-Lite/DevelopDailyBuild/FastDeploy.CPP.inference_lite_lib.ubuntu.armv8.huawei_ascend_npu.CANN5.1.RC2.alpha001.tar.gz'\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/data/FastDeploy-develop/build/third_libs/paddlelite/src/FastDeploy.CPP.inference_lite_lib.ubuntu.armv8.huawei_ascend_npu.CANN5.1.RC2.alpha001.tar.gz'\r\n     dst='/home/data/FastDeploy-develop/build/third_libs/paddlelite/src/extern_paddlelite'\r\n-- extracting... [tar xfz]\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[ 12%] No update step for 'extern_paddlelite'\r\n[ 12%] No patch step for 'extern_paddlelite'\r\n[ 13%] No configure step for 'extern_paddlelite'\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 14%] No build step for 'extern_paddlelite'\r\n[ 15%] Performing install step for 'extern_paddlelite'\r\n[ 15%] Completed 'extern_paddlelite'\r\n[ 15%] Built target extern_paddlelite\r\n[ 15%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 16%] Linking CXX static library libyaml-cpp.a\r\n[ 16%] Built target yaml-cpp\r\n[ 16%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-sandbox.dir/sandbox.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-read.dir/read.cpp.o\r\n[ 16%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-parse.dir/parse.cpp.o\r\n[ 17%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/allocate.cc.o\r\n[ 17%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_type.cc.o\r\n[ 17%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/benchmark/utils.cc.o\r\n[ 17%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/core/fd_tensor.cc.o\r\n[ 18%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/fastdeploy_model.cc.o\r\n[ 18%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cast.cc.o\r\n[ 19%] Linking CXX executable parse\r\n[ 19%] Built target yaml-cpp-parse\r\n[ 20%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/clip.cc.o\r\n[ 20%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/concat.cc.o\r\n[ 21%] Linking CXX executable read\r\n[ 22%] Linking CXX executable sandbox\r\n[ 22%] Built target yaml-cpp-read\r\n[ 22%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/cumprod.cc.o\r\n[ 22%] Built target yaml-cpp-sandbox\r\n[ 23%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/eigen.cc.o\r\n[ 23%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/elementwise.cc.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/full.cc.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gather_scatter_along_axis.cc.o\r\n[ 24%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/gaussian_random.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/isfinite.cc.o\r\n[ 25%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/linspace.cc.o\r\n[ 26%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/math.cc.o\r\n[ 26%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/pad.cc.o\r\n[ 26%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/quantile.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/reduce.cc.o\r\n[ 27%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/slice.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/softmax.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/sort.cc.o\r\n[ 28%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/function/split.cc.o\r\n^Cmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:216: CMakeFiles/fastdeploy.dir/fastdeploy/function/elementwise.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:300: CMakeFiles/fastdeploy.dir/fastdeploy/function/math.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:314: CMakeFiles/fastdeploy.dir/fastdeploy/function/pad.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:342: CMakeFiles/fastdeploy.dir/fastdeploy/function/reduce.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:356: CMakeFiles/fastdeploy.dir/fastdeploy/function/slice.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:370: CMakeFiles/fastdeploy.dir/fastdeploy/function/softmax.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:384: CMakeFiles/fastdeploy.dir/fastdeploy/function/sort.cc.o] Interrupt\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:398: CMakeFiles/fastdeploy.dir/fastdeploy/function/split.cc.o] Interrupt\r\nmake[1]: *** [CMakeFiles/Makefile2:195: CMakeFiles/fastdeploy.dir/all] Interrupt\r\nmake: *** [Makefile:156: all] Interrupt\r\n\r\n(base) root@davinci-mini:/home/data/FastDeploy-develop/build#\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-21T01:57:18+00:00",
        "updated_at": "2024-07-30T06:41:53+00:00",
        "closed_at": "2024-07-30T06:41:53+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2128,
        "title": "请问一下，是否支持多张图片以batch形式传入网络做检测任务？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "zhDai",
        "closed_by": "zhDai",
        "created_at": "2023-07-21T08:38:02+00:00",
        "updated_at": "2023-07-21T08:38:36+00:00",
        "closed_at": "2023-07-21T08:38:36+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2125,
        "title": "[Bug]: UTC模型部署：Fastploy有错误，暂未解决 Segmentation fault",
        "body": "### 软件环境\r\n\r\n* fastploy版本 1.0.7最新版\r\n\r\n* GPU版本\r\n  - paddlepaddle:2.4.2\r\n  - paddlepaddle-gpu: 11.2\r\n  - paddlenlp: 2.5.2\r\n  - cuda 11.2\r\n  - cudnn 8.1.1\r\n\r\n**除使用`fastploy`外，所有程序均正常**\r\n\r\n[cudnn下载官网](https://developer.nvidia.com/rdp/cudnn-archive)\r\n\r\n![image](https://github.com/PaddlePaddle/PaddleNLP/assets/34392683/e66c8e93-0295-453b-a9d2-a01abff866e7)\r\n\r\n\r\n```\r\nfastploy官方环境要求:\r\nCUDA >= 11.2\r\ncuDNN >= 8.0\r\npython >= 3.6\r\nOS: Linux(x64)/Windows 10(x64)\r\n```\r\n```\r\npaddle-gpu 要求\r\nCUDA 工具包 10.2 配合 cuDNN v7.6.5, 如需使用 PaddleTensorRT 推理，需配合 TensorRT7.0.0.11\r\nCUDA 工具包 11.2 配合 cuDNN v8.2.1, 如需使用 PaddleTensorRT 推理，需配合 TensorRT8.0.3.4\r\nCUDA 工具包 11.6 配合 cuDNN v8.4.0, 如需使用 PaddleTensorRT 推理，需配合 TensorRT8.4.0.6\r\nCUDA 工具包 11.7 配合 cuDNN v8.4.1, 如需使用 PaddleTensorRT 推理，需配合 TensorRT8.4.2.4\r\nCUDA 工具包 11.8 配合 cuDNN v8.6.0, 如需使用 PaddleTensorRT 推理，需配合 TensorRT8.5.1.7\r\nCUDA 工具包 12.0 配合 cuDNN v8.9.1, 如需使用 PaddleTensorRT 推理，需配合 TensorRT8.6.1.6\r\n```\r\n\r\n\r\n>看过别的issue发现paddle-gpu貌似要求在cudnn 8.2.1上，但在官网上看到支持cuda 11.2的加速库版本只有<=8.1.1；8.2以上支持11.x 下载下来看都是cuda 11.3  不确定能否在paddle-gpu 11.2版本使用。目前官网上看到只有12.0 11.8 11.7 11.6 11.2以及10.2\r\n\r\n* Cpu版本\r\n  - paddlepaddle:2.4.2\r\n  - paddlenlp: 2.5.2\r\n\r\n操作系统：centos\r\npython版本：3.8\r\n\r\n\r\n\r\n### 重复问题\r\n\r\n- [X] I have searched the existing issues\r\n\r\n### 错误描述\r\n\r\n\r\n1.Fastploy部署时报错（仅报错）\r\n\r\n![image](https://github.com/PaddlePaddle/PaddleNLP/assets/34392683/4aa188d9-8e63-4b03-8029-330e02859619)\r\n\r\nSegmentation fault\r\n\r\n检查了一下：在导入from paddlenlp.prompt import PromptDataCollatorWithPadding, UTCTemplate 这里出的错\r\n\r\n>但是在不适用部署情况下（fastploy方式）可以正常预测；同时使用serving方式也是可以正常预测的\r\n\r\n\r\n### 稳定复现步骤 & 代码\r\n\r\n[https://github.com/PaddlePaddle/PaddleNLP/blob/develop/applications/zero_shot_text_classification](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/applications/zero_shot_text_classification)\r\n\r\n\r\n补充一下：在Windows系统下，没发现上述问题，可以正常执行，在linux centos下GPU V100 T4 以及CPU都会报错`Segmentation fault`\r\n",
        "state": "open",
        "user": "tingaicompass",
        "closed_by": null,
        "created_at": "2023-07-19T12:10:53+00:00",
        "updated_at": "2025-01-08T09:32:56+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "watertianyi",
            "Irisnotiris"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2131,
        "title": "FastDeploy在Jetson AGX Orin上部署时遇到的问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【硬件】： Jetson AGX Orin\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n编译命令：\r\ncmake .. -DBUILD_ON_JETSON=ON -DENABLE_VISION=ON -DENABLE_PADDLE_BACKEND=OFF -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy -DENABLE_FLYCV=ON -DOPENCV_DIRECTORT=/usr/lib/aarch64-linux-gnu/cmake/opencv4 -DBUILD_PADDLE2ONNX=ON\r\nmake -j8\r\nmake install\r\n以上均可正常编译。\r\n进入example中进行ppyoloe模型推理，报下面错误\r\ntest@tegra-ubuntu:~/FastDeploy/Fastdeploy_0724/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build$ ./infer_ppyoloe_demo ./ppyoloe_crn_l_300e_coco 000000014439.jpg 0\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\r\nterminate called after throwing an instance of 'cv::Exception'\r\n  what():  OpenCV(4.5.3) /home/test/opencv/opencv-4.5.3/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\r\nAborted (core dumped)\r\n\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "MrMzl",
        "created_at": "2023-07-24T09:43:08+00:00",
        "updated_at": "2023-07-24T09:45:46+00:00",
        "closed_at": "2023-07-24T09:45:46+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2129,
        "title": "模型推断有关问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n\r\n请问一下，是否支持多张图片以batch形式传入网络做检测任务？\r\n",
        "state": "closed",
        "user": "zhDai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-21T08:40:50+00:00",
        "updated_at": "2025-02-11T06:43:29+00:00",
        "closed_at": "2025-02-11T06:43:29+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2130,
        "title": "infer_yolox.py报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nfastdeploy: 1.0.7\r\npaddledetection release/2.6\r\n\r\n## 问题日志及出现问题的操作流程\r\nyolox_l_300e_coco模型是通过ppdet: python tools/export_model.py -c configs/yolox/yolox_l_300e_coco.yml --output_dir=./inference_model  -o weights=output/yolox_l_300e_coco/best_model 导出的\r\n\r\npython infer_yolox.py --model_dir yolox_l_300e_coco --image road554.png --device cpu 报错如下：\r\nTraceback (most recent call last):\r\n  File \"/home/ai/robert/FastDeploy/examples/vision/detection/paddledetection/python/infer_yolox.py\", line 56, in <module>\r\n    model = fd.vision.detection.PaddleYOLOX(\r\n  File \"/home/ai/anaconda3/envs/paddle242/lib/python3.10/site-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 223, in __init__\r\n    self._model = C.vision.detection.PaddleYOLOX(\r\nRuntimeError: Check 'creator_it != CREATORS_MAP.end()' failed at src/frontends/paddle/src/frontend.cpp:46:\r\nFrontEnd API failed with OpConversionFailure: :\r\nNo creator found for flip node.\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "zouxiaodong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-21T09:42:34+00:00",
        "updated_at": "2024-07-30T06:41:54+00:00",
        "closed_at": "2024-07-30T06:41:54+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2134,
        "title": "文本分类模型部署至Android",
        "body": "使用paddleNLP基于ernie-3.0-tiny-medium-v2-zh 训练的模型文本分类模型，是否支持部署在Android侧，在github和官方论坛上都没有找到相关文档，但是看SDK好像又提供了文本相关操作的版本，如果支持的话能否提供相关示例或者文档？",
        "state": "open",
        "user": "wslin1994",
        "closed_by": null,
        "created_at": "2023-07-26T03:02:44+00:00",
        "updated_at": "2023-07-28T10:09:15+00:00",
        "closed_at": null,
        "comments_count": [
            "flyrainer",
            "wslin1994"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2132,
        "title": "FastDeploy在Jetson AGX Orin上部署时遇到的问题？",
        "body": "环境\r\n【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n【硬件】： Jetson AGX Orin\r\n【编译语言】： C++\r\n\r\n编译命令：\r\ncmake .. -DBUILD_ON_JETSON=ON -DENABLE_VISION=ON -DENABLE_PADDLE_BACKEND=OFF -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy -DENABLE_FLYCV=ON -DOPENCV_DIRECTORT=/usr/lib/aarch64-linux-gnu/cmake/opencv4 -DBUILD_PADDLE2ONNX=ON\r\nmake -j8\r\nmake install\r\n以上均可正常编译。\r\n进入example中进行ppyoloe模型推理，报下面错误\r\ntest@tegra-ubuntu:~/FastDeploy/Fastdeploy_0724/FastDeploy-develop/examples/vision/detection/paddledetection/cpp/build$ ./infer_ppyoloe_demo ./ppyoloe_crn_l_300e_coco 000000014439.jpg 0\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW Normalize and HWC2CHW are fused to NormalizeAndPermute in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend Runtime initialized with Backend::ORT in Device::CPU.\r\nterminate called after throwing an instance of 'cv::Exception'\r\nwhat(): OpenCV(4.5.3) /home/test/opencv/opencv-4.5.3/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\r\nAborted (core dumped)",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-24T09:46:31+00:00",
        "updated_at": "2024-07-30T06:41:55+00:00",
        "closed_at": "2024-07-30T06:41:55+00:00",
        "comments_count": [
            "leiqing1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2135,
        "title": "PP-OCRv3 GPU下使用trt 后端跑不通",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python     1.0.7\r\n- 【编译命令】pip install numpy opencv-python fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n- 【系统平台】: WSL2 Linux x64(Ubuntu 18.04)\r\n- 【硬件】：  Nvidia GPU 3050， CUDA 11.2 CUDNN 8.2（使用conda安装）\r\n- 【编译语言】： Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【PP-OCRv3 GPU下使用trt 后端跑不通】\r\n- - 执行`examples`下的部署示例，默认的GPU版本可以运行\r\n- - 修改backend为trt后（已经加上保存 TRT cache 文件的函数），运行示例代码直接卡在下面，也没有任何报错信息：\r\n- - `[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(556)::BuildTrtEngine Start to building TensorRT Engine...`\r\n- - 经过几个小时依旧是这样\r\n- - tensorrt版本8.4.1.5 和 8.5.2.2 都尝试了，tensorrt安装方式按照官网的guide，安装的 tar 文件\r\n\r\n",
        "state": "closed",
        "user": "wgshun",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-27T02:33:32+00:00",
        "updated_at": "2025-06-24T06:46:18+00:00",
        "closed_at": "2025-06-24T06:46:18+00:00",
        "comments_count": [
            "EasyIsAllYouNeed",
            "wgshun",
            "EasyIsAllYouNeed",
            "wgshun",
            "Hold-on-li"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2136,
        "title": "Triton backend如何选择",
        "body": "[FastDeploy 服务化部署](https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/README_CN.md)中提到的[PaddleDetection](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README.md)、[PaddleClas](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/classification/paddleclas/serving/README.md)等使用的是Model Ensembles方案，[PaddleSpeech/PP-TTS](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/audio/pp-tts/serving/README.md)和[PaddleNLP/UIE](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/text/uie/serving/README.md)使用是python backend方案。\r\n\r\n请问：\r\n1. 示例中使用了两种不同的部署方式：Model Ensembles部署和python backend部署，这是出于哪些方面的考量？\r\n2. 使用Model Ensembles部署的示例中的`models/runtime`模型使用的backend是fastdeploy。那么，直接使用fastdeploy_backend相较于[triton-inference-server/backend](https://github.com/triton-inference-server/backend)提到的backend（比如paddlepaddle_backend或onnxruntime_backend，同样支持TRT、CUDA、CPU、OpenVINO）有那些好处？\r\n",
        "state": "closed",
        "user": "firedent",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-27T08:11:53+00:00",
        "updated_at": "2025-02-11T06:43:30+00:00",
        "closed_at": "2025-02-11T06:43:30+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2137,
        "title": "日志怎么关闭",
        "body": "I0726 16:11:18.357290 1111698 stats.h:78] HostMemoryStatAllocated0: Update current_value with -8, after update, current value = 469423112\r\nuie预测时这种日志怎么取消掉",
        "state": "closed",
        "user": "ericperfect",
        "closed_by": "rainyfly",
        "created_at": "2023-07-27T09:24:26+00:00",
        "updated_at": "2024-02-06T09:34:42+00:00",
        "closed_at": "2024-02-06T09:34:42+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2139,
        "title": "一个sdk同时用fastdeploy和paddlelite加载两个模型实例，fastdeploy 模型实例初始化失败，lite 没问题",
        "body": "A/libc: Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 13034 (Thread-2), pid 12915 ",
        "state": "open",
        "user": "EricHuiK",
        "closed_by": null,
        "created_at": "2023-07-28T12:18:09+00:00",
        "updated_at": "2023-07-28T12:18:35+00:00",
        "closed_at": null,
        "comments_count": [
            "EricHuiK"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2138,
        "title": "Rust win  编译错误",
        "body": "https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/application/rust/yolov8\r\n\r\n系统\r\nwin11\r\n\r\nllvm\r\nMSYS2\r\n已安装\r\n\r\n建议弄win版教程\r\n或者win msvc教程\r\n\r\nhttps://crates.io/\r\n建议crates也弄一个\r\n\r\n\r\nfastdeploy-win-x64-gpu-0.0.0\r\nfastdeploy_capi\r\n放根目录了\r\n\r\nbuild.rs\r\n```\r\nfn main() {\r\n    println!(\"cargo:rustc-link-search=./fastdeploy-win-x64-gpu-0.0.0/lib\");\r\n    println!(\"cargo:rustc-link-lib=fastdeploy\");\r\n    println!(\"cargo:rerun-if-changed=wrapper.h\");\r\n\r\n    let headers_dir = PathBuf::from(\"./fastdeploy-win-x64-gpu-0.0.0/include\");\r\n\r\n```\r\n\r\nwrapper.h\r\n```\r\n#include <fastdeploy_capi/vision.h>\r\n```\r\n\r\n```\r\n[ERROR rust_analyzer::main_loop] FetchBuildDataError:\r\nerror: failed to run custom build command for `infer v0.1.0 (E:\\2023_Code_Paddle\\FastDeploy\\examples\\application\\rust\\yolov8)`\r\nnote: To improve backtraces for build dependencies, set the CARGO_PROFILE_DEV_BUILD_OVERRIDE_DEBUG=true environment variable to enable debug information generation.\r\n\r\nCaused by:\r\n  process didn't exit successfully: `E:\\2023_Code_Paddle\\FastDeploy\\examples\\application\\rust\\yolov8\\target\\debug\\build\\infer-4d99eefc0e05ce0c\\build-script-build` (exit code: 101)\r\n  --- stdout\r\n  cargo:rustc-link-search=./fastdeploy-win-x64-gpu-0.0.0/lib\r\n  cargo:rustc-link-lib=fastdeploy\r\n  cargo:rerun-if-changed=wrapper.h\r\n\r\n  --- stderr\r\n  wrapper.h:1:10: error: 'fastdeploy_capi/vision.h' file not found with <angled> include; use \"quotes\" instead\r\n  ./fastdeploy_capi/vision.h:16:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/config.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/classification/ppcls/model.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/core/fd_type.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/enum_variables.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/core/fd_type.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/runtime/runtime_option.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/result.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/result.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/ppdet/model.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/ppdet/base_define.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/contrib/yolo/model.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/contrib/yolo/base_define.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:22:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/ocr/ppocr/model.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/ocr/ppocr/base_define.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:23:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/segmentation/ppseg/model.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:24:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:25:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/visualize.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/visualize.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/visualize.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision/visualize.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:28:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include]\r\n  ./fastdeploy_capi/vision.h:29:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include]\r\n  wrapper.h:1:10: error: 'fastdeploy_capi/vision.h' file not found with <angled> include; use \"quotes\" instead, err: true\r\n  ./fastdeploy_capi/vision.h:16:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/config.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/classification/ppcls/model.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/core/fd_type.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/enum_variables.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/core/fd_type.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/runtime/runtime_option.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/classification/ppcls/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/result.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/result.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/ppdet/model.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/ppdet/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/ppdet/base_define.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/contrib/yolo/model.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/detection/contrib/yolo/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/detection/contrib/yolo/base_define.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:22:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/ocr/ppocr/model.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/ocr/ppocr/model.h:21:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/ocr/ppocr/base_define.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:23:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/segmentation/ppseg/model.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/segmentation/ppseg/model.h:20:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:24:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:25:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/visualize.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/visualize.h:17:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_common.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/visualize.h:18:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision/visualize.h:19:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/vision/result.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:28:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/core/fd_type.h [-Wmicrosoft-include], err: false\r\n  ./fastdeploy_capi/vision.h:29:10: warning: #include resolved using non-portable Microsoft search rules as: ./fastdeploy_capi/runtime/runtime_option.h [-Wmicrosoft-include], err: false\r\n  thread 'main' panicked at 'Unable to generate bindings: ()', build.rs:20:10\r\n  stack backtrace:\r\n     0: std::panicking::begin_panic_handler\r\n               at /rustc/8ede3aae28fe6e4d52b38157d7bfe0d3bceef225/library\\std\\src\\panicking.rs:593\r\n     1: core::panicking::panic_fmt\r\n               at /rustc/8ede3aae28fe6e4d52b38157d7bfe0d3bceef225/library\\core\\src\\panicking.rs:67\r\n     2: core::result::unwrap_failed\r\n               at /rustc/8ede3aae28fe6e4d52b38157d7bfe0d3bceef225/library\\core\\src\\result.rs:1651\r\n     3: core::result::Result<T,E>::expect\r\n  note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n\r\n\r\n\r\n```\r\n\r\n@wanziyu",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-28T07:31:45+00:00",
        "updated_at": "2024-09-03T06:41:53+00:00",
        "closed_at": "2024-09-03T06:41:53+00:00",
        "comments_count": [
            "chaosgoo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2140,
        "title": "ERROR：The FastDeploy didn't compile with TrtBackend.",
        "body": "[ERROR] fastdeploy/runtime/runtime_option.cc(168)::fastdeploy::RuntimeOption::UseTrtBackend     The FastDeploy didn't compile with TrtBackend.",
        "state": "open",
        "user": "yuanshen24",
        "closed_by": null,
        "created_at": "2023-07-29T07:28:50+00:00",
        "updated_at": "2024-08-16T05:49:07+00:00",
        "closed_at": null,
        "comments_count": [
            "SpringSan",
            "jiauy",
            "huangfujue"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2141,
        "title": "【jetson nano】【python3.10】编译时报错 Installing 'fastdeploy.libs.third_libs.onnxruntime.lib' as data is deprecated, please list it in `packages`.（执行时也报错）",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n**-【环境信息】**\r\nnvidia@nvidia-desktop:~/FastDeploy/python$ jetson_release -v\r\nSoftware part of jetson-stats 4.2.1 - (c) 2023, Raffaello Bonghi\r\nModel: NVIDIA Jetson Nano Developer Kit - Jetpack 4.6.1 [L4T 32.7.1]\r\nNV Power Mode[0]: MAXN\r\nSerial Number: [XXX Show with: jetson_release -s XXX]\r\nHardware:\r\n - 699-level Part Number: 699-13448-0002-401 G.0\r\n - P-Number: p3448-0002\r\n - BoardIDs: p3448\r\n - Module: NVIDIA Jetson Nano module (16Gb eMMC)\r\n - SoC: tegra210\r\n - CUDA Arch BIN: 5.3\r\n - Codename: Porg\r\nPlatform:\r\n - Machine: aarch64\r\n - System: Linux\r\n - Distribution: Ubuntu 18.04 Bionic Beaver\r\n - Release: 4.9.253-tegra\r\n - Python: 3.10.10\r\njtop:\r\n - Version: 4.2.1\r\n - Service: Active\r\nLibraries:\r\n - CUDA: 10.2.300\r\n - cuDNN: 8.2.1.32\r\n - TensorRT: 8.2.1.8\r\n - VPI: 1.2.3\r\n - Vulkan: 1.2.70\r\n - OpenCV: 4.8.0 - with CUDA: NO\r\n\r\n\r\n**- 【FastDeploy版本】：**  \r\n-    官方安装方式的版本 \r\n- \r\n**- 【编译命令】** \r\n- \r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\n\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\n\r\n**export BUILD_PADDLE2ONNX=ON**  --------也加上了这一项 ，版本也是今天最新获取的\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport PADDLEINFERENCE_DIRECTORY=/usr/local/bin/Paddle/build/paddle_inference_install_dir/ **-----自己编译的paddle**\r\nexport PATH=/usr/local/cuda/bin:$PATH\r\nexport CMAKE_CUDA_ARCHITECTURES=53\r\n\r\nexport CUDA_ARCH_NAME=Maxwell\r\nexport CMAKE_CUDA_COMPILER=\"/usr/local/cuda/bin/nvcc\"\r\nexport  PADDLEINFERENCE_VERSION=2.4\r\n\r\nexport LD_PRELOAD=$LD_PRELOAD:/home/nvidia/mambaforge/lib/python3.10/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0\r\n\r\npython3 setup.py build \r\n\r\n**python setup.py bdist_wheel   ------------报错**\r\n\r\n**- 【硬件】**：  jetson nano\r\n**- 【编译语言】**： Python 3.10\r\n\r\n**【编译报错信息】：**\r\n。。。\r\n之前都OK，没报错\r\n**python setup.py bdist_wheel 一----开始 就 报错：**\r\n遇到报错。。。。\r\nnvidia@nvidia-desktop:~/FastDeploy-develop/python$ python3 setup.py bdist_wheel\r\nfatal: not a git repository (or any of the parent directories): .git\r\nrunning bdist_wheel\r\nrunning build\r\nrunning build_py\r\nrunning egg_info\r\nwriting fastdeploy_gpu_python.egg-info/PKG-INFO\r\nwriting dependency_links to fastdeploy_gpu_python.egg-info/dependency_links.txt\r\nwriting requirements to fastdeploy_gpu_python.egg-info/requires.txt\r\nwriting top-level names to fastdeploy_gpu_python.egg-info/top_level.txt\r\nreading manifest file 'fastdeploy_gpu_python.egg-info/SOURCES.txt'\r\nwriting manifest file 'fastdeploy_gpu_python.egg-info/SOURCES.txt'\r\n/**home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.onnxruntime.lib' as data is deprecated, please list it in `packages`.\r\n    !!**\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.onnxruntime.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.onnxruntime.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.onnxruntime.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.paddle.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.externalError.data.data' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.externalError.data.data' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.externalError.data.data' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.externalError.data.data' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.cryptopp.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.gflags.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.glog.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.cmake.openblas' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.cmake.openblas' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.cmake.openblas' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.cmake.openblas' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.pkgconfig' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.pkgconfig' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.pkgconfig' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.openblas.lib.pkgconfig' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n    Python recognizes 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' as an importable package,\r\n    but it is not listed in the `packages` configuration of setuptools.\r\n\r\n    'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' has been automatically added to the distribution only\r\n    because it may contain data files, but this behavior is likely to change\r\n    in future versions of setuptools (and therefore is considered deprecated).\r\n\r\n    Please make sure that 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf' is included as a package by using\r\n    the `packages` configuration field or the proper discovery methods\r\n    (for example by using `find_namespace_packages(...)`/`find_namespace:`\r\n    instead of `find_packages(...)`/`find:`).\r\n\r\n    You can read more about \"package discovery\" and \"data files\" on setuptools\r\n    documentation page.\r\n\r\n\r\n!!\r\n\r\n  check.warn(importable)\r\n/home/nvidia/mambaforge/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'fastdeploy.libs.third_libs.paddle_inference.third_party.install.protobuf.include.google.protobuf.compiler' as data is deprecated, please list it in `packages`.\r\n    !!\r\n\r\n\r\n    ############################\r\n    # Package would be ignored #\r\n    ############################\r\n\r\n。。。。如下类似上面的异常提示有很多。。。。。。。。。\r\n\r\n最终也能编译完成，并安装上， 但是 \r\n执行相关检测处理的应用代码的时候，也报错，（之前在同样硬件，python3.6 以及官方提供的paddle inference 包编译安装后，都可执行正常）\r\n**------附参考信息----\r\n\r\n。。。。\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  (AlreadyExists) 'unique_consecutive' is registered in operator version more than once.\r\n  [Hint: Expected op_version_map_.find(op_type) == op_version_map_.end(), but received op_version_map_.find(op_type) != op_version_map_.end().] (at /usr/local/bin/Paddle/paddle/fluid/framework/op_version_registry.cc:78)\r\n\r\nAborted (core dumped)\r\n。。。。。**\r\n请帮看下，谢谢。",
        "state": "closed",
        "user": "bianyiJetsonnano",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-07-29T15:42:47+00:00",
        "updated_at": "2025-02-11T06:43:30+00:00",
        "closed_at": "2025-02-11T06:43:30+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2142,
        "title": "人脸识别余弦距离公式疑惑",
        "body": "1 fastdeploy里面faceid，对比头像余弦相似度，公式分母为啥用平方根呢？文件路径是FastDeploy-develop\\examples\\vision\\faceid\\adaface\\python\\infer.py\r\n2 余弦相似度归一化到0和1 ，感觉差异就不敏感了，有啥好办法么？\r\n3 如果用欧式距离，如何转换为百分比呢？\r\n谢谢\r\n\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26616061/6693f28c-f85e-407b-92aa-4d509715a535)\r\n",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-01T03:01:17+00:00",
        "updated_at": "2025-02-11T06:43:31+00:00",
        "closed_at": "2025-02-11T06:43:31+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2143,
        "title": "人脸对齐后续该如何处理？",
        "body": "fastdeploy人脸对齐的代码，我看只有人脸关键点检测，没有后续对齐的代码？请问下，后续对齐该如何处理？将研究对齐到一个水平线或者仿射变换吗？fastdeploy官方是否有对齐后续处理的代码？谢谢",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-01T03:05:51+00:00",
        "updated_at": "2025-02-11T06:43:32+00:00",
        "closed_at": "2025-02-11T06:43:32+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2144,
        "title": "如何给tensorrt添加shape",
        "body": "使用c#api的fastdeploy，如何使用TensorRT？\r\n目前使用的代码：\r\nRuntimeOption runtimeoption = new RuntimeOption();\r\nruntimeoption.UseGpu();\r\nruntimeoption.UseTrtBackend();\r\nruntimeoption.EnablePaddleToTrt();\r\nruntimeoption.EnablePaddleTrtCollectShape();\r\nfastdeploy.vision.classification.PaddleClasModel model = new fastdeploy.vision.classification.PaddleClasModel(model_file, params_file, config_file, runtimeoption, ModelFormat.PADDLE);\r\n运行会报错：\r\n\r\n![捕获](https://github.com/PaddlePaddle/FastDeploy/assets/77528622/706fc732-c162-41a2-9e67-a063f2992665)\r\n显然是需要给tensorrt添加shape，但c#中没有如同c++中的SetTrtshape接口",
        "state": "closed",
        "user": "guoyunqingyue",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-01T08:16:02+00:00",
        "updated_at": "2025-02-11T06:43:33+00:00",
        "closed_at": "2025-02-11T06:43:33+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2145,
        "title": "What's the difference between FastDeploy and PaddleServing?",
        "body": "What's the difference between FastDeploy and PaddleServing?",
        "state": "closed",
        "user": "Awezome",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-01T10:41:36+00:00",
        "updated_at": "2025-02-11T06:43:34+00:00",
        "closed_at": "2025-02-11T06:43:34+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2147,
        "title": "FP32、FP16、INT8速度测试",
        "body": "## 环境\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】cmake .. -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DWITH_GPU=ON -DTRT_DIRECTORY=/home/huangry/project/TensorRT-8.6.1.6 -DCUDA_DIRECTORY=/usr/local/cuda -DCMAKE_INSTALL_PREFIX=/usr/local/compiled_fastdeploy_sdk -DENABLE_VISION=ON -DOPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 -DENABLE_TEXT=ON\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】：  Nvidia GPU 4090， CUDA 11.7 CUDNN 8.9\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【性能问题】使用下面代码测试FP32、FP16、INT8模型速度，FP32的时间为5200，FP16的时间为4600，INT8的时间为4550，差距很小，请问代码是否有问题？\r\n****\r\n\r\n> \r\n\r\n#include \"fastdeploy/vision.h\"\r\n#include <chrono>\r\nint main() \r\n{\r\n  // FP32 及 FP16\r\n  std::string model_file = \"ppyoloe_plus_crn_s_80e_coco/model.pdmodel\";\r\n  std::string params_file = \"ppyoloe_plus_crn_s_80e_coco/model.pdiparams\";\r\n  std::string infer_cfg_file = \"ppyoloe_plus_crn_s_80e_coco/infer_cfg.yml\";\r\n\r\n  // // INT8\r\n  // std::string model_file = \"ppyoloe_plus_crn_s_80e_coco_qat/model.pdmodel\";\r\n  // std::string params_file = \"ppyoloe_plus_crn_s_80e_coco_qat/model.pdiparams\";\r\n  // std::string infer_cfg_file = \"ppyoloe_plus_crn_s_80e_coco_qat/infer_cfg.yml\";\r\n  // 模型推理的配置信息\r\n  fastdeploy::RuntimeOption option;\r\n  option.UseGpu();\r\n  option.UseTrtBackend();\r\n  option.EnableTrtFP16(); //FP16时开启\r\n  // option.trt_option.serialize_file = \"trt_serialized.trt\";\r\n  auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file, infer_cfg_file, option);\r\n\r\n  if(!model.Initialized()) // 判断模型是否初始化成功\r\n  {\r\n    std::cout << \"Failed to initalize\" << std::endl;\r\n    return -1;\r\n  }\r\n\r\n  std::cout << \"******************************************\" << std::endl;\r\n\r\n  cv::Mat im = cv::imread(\"test_det.jpg\");\r\n  fastdeploy::vision::DetectionResult result;\r\n  \r\n  if(!model.Predict(&im, &result)) // 判断是否预测成功\r\n  {\r\n    std::cout << \"Failed to predict\" << std::endl;\r\n    return -1;\r\n  }\r\n  std::cout << result.Str() << std::endl;\r\n\r\n  \r\n  std::cout << \"============================================\" << std::endl;\r\n  auto startTime = std::chrono::high_resolution_clock::now();\r\n  int epo = 1000;\r\n  while(epo-- > 0)\r\n  {\r\n    cv::Mat im = cv::imread(\"test_det.jpg\");\r\n    fastdeploy::vision::DetectionResult result;\r\n    \r\n    if (!model.Predict(&im, &result)) // 判断是否预测成功\r\n    {\r\n      std::cout << \"Failed to predict\" << std::endl;\r\n      return -1;\r\n    }\r\n  }\r\n  auto stopTime = std::chrono::high_resolution_clock::now();\r\n  auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stopTime - startTime);\r\n  std::cout << \"执行的时间为：\" << duration.count() / 1000 << std::endl;\r\n\r\n  return 0;\r\n}\r\n\r\n\r\n",
        "state": "closed",
        "user": "qinxianyuzi",
        "closed_by": "rainyfly",
        "created_at": "2023-08-03T04:44:31+00:00",
        "updated_at": "2024-02-06T09:37:15+00:00",
        "closed_at": "2024-02-06T09:37:15+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2146,
        "title": "FastDeploy/examples/multimodal/stable_diffusion /README_CN.md  中Diffusion预先导出模型缺失",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.1~今\r\n- 【编译命令】all\r\n- 【系统平台】: all\r\n- 【硬件】： all\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\nFastDeploy/examples/multimodal/stable_diffusion /README_CN.md  中Diffusion预先导出模型缺失\r\n\r\n\r\nCompVis/stable-diffusion-v1-4 \r\n[CompVis/stable-diffusion-v1-4](https://bj.bcebos.com/fastdeploy/models/stable-diffusion/CompVis/stable-diffusion-v1-4.tgz)\t\r\n\r\nrunwayml/stable-diffusion-v1-5\r\n[runwayml/stable-diffusion-v1-5](https://bj.bcebos.com/fastdeploy/models/stable-diffusion/runwayml/stable-diffusion-v1-5.tgz)\t\r\n2个链接中仅有3个模型，实际的测试程序需要4个模型，\r\n\r\n",
        "state": "closed",
        "user": "caofx0418",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-02T10:34:34+00:00",
        "updated_at": "2025-02-11T06:43:34+00:00",
        "closed_at": "2025-02-11T06:43:34+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2148,
        "title": "Unet RKNN 3588 推理结果错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 瑞芯微rk3588\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/54393886/d386f0d3-7385-4e2d-8e35-9afd001b66c3)\r\n对于RK3588上的unet进行推理的时候，使用官方提供的预训练的unet模型，导出onnx并转为rknn之后推理发现结果不正确，请问光啊官方有推荐的rknn分割模型导出文档教程吗",
        "state": "closed",
        "user": "jasper-cell",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-05T08:30:53+00:00",
        "updated_at": "2025-02-25T06:45:10+00:00",
        "closed_at": "2025-02-25T06:45:10+00:00",
        "comments_count": [
            "rainyfly",
            "lswgh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2149,
        "title": "infer_ppyoloe_r.cc 推理结果不符合预期",
        "body": "\r\n编译运行infer_ppyoloe_r.cc没问题，就是推理的结果不符合预期。可视化的结果如下所示：\r\n![vis_result](https://github.com/PaddlePaddle/FastDeploy/assets/26791156/2f96979c-1b79-486e-92cf-1a83cd5b27d4)\r\n\r\n我使用的是下图中箭头所指的模型：\r\n![pp_yoloe_r](https://github.com/PaddlePaddle/FastDeploy/assets/26791156/6c5e728e-8b13-4c79-98cd-0f2ee76253f5)\r\n\r\n模型地址：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.6/configs/rotate/ppyoloe_r",
        "state": "closed",
        "user": "bo-scnu",
        "closed_by": "bo-scnu",
        "created_at": "2023-08-05T11:07:10+00:00",
        "updated_at": "2023-10-08T13:45:26+00:00",
        "closed_at": "2023-10-08T13:45:25+00:00",
        "comments_count": [
            "bo-scnu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2150,
        "title": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md",
        "body": "请问在华为鲲鹏中部署paddleocr的文档是没有呢，还是跳转出错呢https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md\r\n\r\n页面404\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/48897529/29924ec8-c580-41eb-b005-9a24a1f33eff)\r\n",
        "state": "closed",
        "user": "ruijie123",
        "closed_by": "rainyfly",
        "created_at": "2023-08-07T02:03:48+00:00",
        "updated_at": "2024-02-06T11:06:32+00:00",
        "closed_at": "2024-02-06T11:06:32+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2151,
        "title": "如何启动一个模型下面多个版本的模型",
        "body": "采用fastdeploy起triton的模型服务的时候，指定了model-repo的路径，里面模型有两个版本的模型，但是模型启动的时候只启动了随机的一个版本\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/22832382/93e92fb5-d3d0-4702-9477-8c62a2d3553e)\r\n上图的两个红框都有两个版本的模型，但是只启动了其中一个",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-07T08:27:33+00:00",
        "updated_at": "2025-02-11T06:43:35+00:00",
        "closed_at": "2025-02-11T06:43:35+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2152,
        "title": "C# Fastdeploy部署PaddleOCR 结果无法显示",
        "body": "**版本：fastdeploy 1.0.7\r\n硬件 :CPU**\r\n1. 按照PaddleOCR c#部署教程链接：`https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/cpu-gpu/csharp`，但是C#使用FastDeploy推理PaddleOCR的ch_PP-OCRv3模型不出结果.\r\n\r\n",
        "state": "closed",
        "user": "chccc1994",
        "closed_by": "chccc1994",
        "created_at": "2023-08-08T06:45:46+00:00",
        "updated_at": "2023-12-26T08:38:42+00:00",
        "closed_at": "2023-08-16T11:27:23+00:00",
        "comments_count": [
            "BigerPatax"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2155,
        "title": "请问uiex有什么好的方法可以进行部署？",
        "body": null,
        "state": "closed",
        "user": "aixuedegege",
        "closed_by": "aixuedegege",
        "created_at": "2023-08-09T03:25:10+00:00",
        "updated_at": "2024-02-29T09:19:47+00:00",
        "closed_at": "2024-02-29T09:19:47+00:00",
        "comments_count": [
            "aixuedegege",
            "neo502721",
            "aixuedegege",
            "neo502721",
            "aixuedegege",
            "neo502721"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2153,
        "title": "请问下，现在多卡流式推理的功能可以使用了吗",
        "body": "使用2个显卡流失推理chatglm6b模型，提供web服务。",
        "state": "closed",
        "user": "liuzhipengchd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-08T06:58:05+00:00",
        "updated_at": "2025-02-11T06:43:36+00:00",
        "closed_at": "2025-02-11T06:43:36+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2154,
        "title": "RKNPU2 rga支持",
        "body": "rkyolo方案中前端图片resize处理，是否考虑使用瑞芯微rga库进行加速。",
        "state": "closed",
        "user": "alexw914",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-08T07:09:44+00:00",
        "updated_at": "2025-02-11T06:43:37+00:00",
        "closed_at": "2025-02-11T06:43:37+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2157,
        "title": "如何进行模型加密,必须要有密钥才能加载模型",
        "body": "如何进行模型加密,\r\n必须要有密钥才能加载模型\r\n\r\n比如训练好的模型\r\n进行加密\r\n\r\n加密完成\r\n给 模型 和 密钥\r\n\r\nC++  C# python 这些 才能正常调用\r\n\r\n希望官方能提供 加密模型  加盐  生成密钥\r\n\r\n给别人用的时候  只需要给密钥就行了",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-11T07:24:25+00:00",
        "updated_at": "2025-02-11T06:43:38+00:00",
        "closed_at": "2025-02-11T06:43:38+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2158,
        "title": "使用fastdeploy的推理流程出现报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 我的数据shape==（1，20，560）\r\n- tcmalloc: large alloc 1099511627776 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 989560438784 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 890604355584 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 801543880704 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 721389486080 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 649250537472 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 584325464064 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 525892911104 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 473303613440 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 425973252096 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 383375900672 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 345038290944 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 310534438912 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 279480991744 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 251532886016 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 226379595776 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 203741626368 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 183367467008 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 165030723584 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 148527644672 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 133674876928 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 120307384320 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 108276645888 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 97448976384 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 87704076288 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 78933663744 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 71040294912 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 63936266240 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 57542639616 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 51788374016 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 46609530880 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 41948577792 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 37753724928 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 33978351616 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 30580514816 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 27522465792 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 24770215936 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 22293192704 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 20063879168 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 18057486336 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 16251740160 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 14626562048 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 13163905024 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 11847516160 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 10662764544 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 9596485632 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\ntcmalloc: large alloc 8636841984 bytes == (nil) @  0x44c6ee 0x46a9f4 0x46ad4c 0x7f6b223669d5\r\n2023-08-11 15:03:33.055375155 [E:onnxruntime:, sequential_executor.cc:368 Execute] Non-zero status code returned while running Range node. Name:'/encoder/make_pad_mask/Range' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:342 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool) Failed to allocate memory for requested buffer of size 7270363648\r\n\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(333)::Infer      Failed to Infer: Non-zero status code returned while running Range node. Name:'/encoder/make_pad_mask/Range' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:342 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool) Failed to allocate memory for requested buffer of size 7270363648\r\n- 【系统平台】: Linux\r\n- 【编译语言】： C++\r\n",
        "state": "closed",
        "user": "David19970306",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-11T07:37:31+00:00",
        "updated_at": "2025-02-11T06:43:39+00:00",
        "closed_at": "2025-02-11T06:43:38+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2159,
        "title": "鲲鹏ascend c++部署paddleocr make-j报错",
        "body": "- 【FastDeploy版本】：![image](https://github.com/PaddlePaddle/FastDeploy/assets/48897529/37eb04c6-6884-46d6-801d-f023dcee0b79)\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 鲲鹏920，\r\n![image](https://github.com/PaddlePaddle/PaddleOCR/assets/48897529/a1acbf23-5952-4153-898b-9df7ca6d4d81)\r\n- 【编译语言】： C++ \r\n报错：\r\n![image](https://github.com/PaddlePaddle/PaddleOCR/assets/48897529/5450de83-2fe4-454c-8509-9fafa019a026)\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer.cc.o\r\n/root/workspace/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/infer.cc: In function 'void AscendInfer(const string&, const string&, const string&, const string&, const string&, const fastdeploy::RuntimeOption&)':\r\n/root/workspace/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/infer.cc:37:29: error: declaration of 'fastdeploy::RuntimeOption option' shadows a parameter\r\n   fastdeploy::RuntimeOption option;\r\n                             ^~~~~~\r\n/root/workspace/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/infer.cc: In function 'int main(int, char**)':\r\n/root/workspace/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/infer.cc:106:25: error: too few arguments to function 'void AscendInfer(const string&, const string&, const string&, const string&, const string&, const fastdeploy::RuntimeOption&)'\r\n               test_image);\r\n                         ^\r\n/root/workspace/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/infer.cc:22:6: note: declared here\r\n void AscendInfer(const std::string &det_model_dir,\r\n      ^~~~~~~~~~~\r\nCMakeFiles/infer_demo.dir/build.make:65: recipe for target 'CMakeFiles/infer_demo.dir/infer.cc.o' failed\r\nmake[2]: *** [CMakeFiles/infer_demo.dir/infer.cc.o] Error 1\r\nCMakeFiles/Makefile2:106: recipe for target 'CMakeFiles/infer_demo.dir/all' failed\r\nmake[1]: *** [CMakeFiles/infer_demo.dir/all] Error 2\r\nMakefile:86: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n",
        "state": "closed",
        "user": "ruijie123",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-11T08:04:19+00:00",
        "updated_at": "2025-03-10T09:37:12+00:00",
        "closed_at": "2024-08-20T06:40:38+00:00",
        "comments_count": [
            "zaicyxu",
            "mytk2012"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2160,
        "title": "C# Fastdeploy部署人像抠图功能",
        "body": "C# Fastdeploy部署好像人像抠图功能没有提供接口，或者是提供接口了但是没有找到。希望有哪位大能能告知如何配置C#可以调用的人像抠图的接口",
        "state": "closed",
        "user": "yangyunyi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-11T08:19:14+00:00",
        "updated_at": "2025-02-11T06:43:41+00:00",
        "closed_at": "2025-02-11T06:43:41+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2162,
        "title": "ascend 310 鲲鹏920部署报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python  0.0.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，\r\n- 【系统平台】: 银河麒麟sp3\r\n- 【硬件】： ascend 310p\r\n- 【编译语言】： Python(3.7）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题  \r\n\r\n[W  8/14  7:57:37.535 .../src/driver/huawei_ascend_npu/utility.cc:57 InitializeAscendCL] CANN version mismatch. The build version is 0.0.0, but the current environment version is 5.1.2.\r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:41 Context] properties: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:66 Context] selected device ids: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:68 Context] 0\r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:78 Context] profiling path: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:88 Context] dump model path: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:98 Context] precision mode: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:120 Context] op select impl mode: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:130 Context] op type list for impl mode: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:140 Context] enable compressw weight: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:150 Context] auto tune mode: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:160 Context] enable dynamic shape range: \r\n[I  8/14  7:57:37.578 ...r/src/driver/huawei_ascend_npu/engine.cc:176 Context] initial buffer length of dynamic shape range: -1\r\n[W  8/14  7:57:37.578 ...ter/nnadapter/src/runtime/compilation.cc:334 Finish] Warning: Failed to create a program, No model and cache is provided.\r\n[W  8/14  7:57:37.578 ...le-Lite/lite/kernels/nnadapter/engine.cc:149 LoadFromCache] Warning: Build model failed(3) !\r\n[W  8/14  7:57:37.598 ...nnadapter/nnadapter/src/runtime/model.cc:86 GetSupportedOperations] Warning: Failed to get the supported operations for device 'huawei_ascend_npu', because the HAL interface 'validate_program' is not implemented!\r\n[W  8/14  7:57:37.598 ...kernels/nnadapter/converter/converter.cc:171 Apply] Warning: Failed to get the supported operations for the selected devices, one or more of the selected devices are not supported!\r\n[I  8/14  7:57:37.598 ...r/src/driver/huawei_ascend_npu/driver.cc:70 CreateProgram] Create program for huawei_ascend_npu.\r\n^C/usr/local/python3.7.5/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 91 leaked semaphores to clean up at shutdown\r\n",
        "state": "closed",
        "user": "yywangfei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-14T08:02:07+00:00",
        "updated_at": "2024-10-15T06:42:04+00:00",
        "closed_at": "2024-10-15T06:42:04+00:00",
        "comments_count": [
            "leiqing1",
            "jiangjiajun",
            "yywangfei",
            "yywangfei",
            "deshuai666",
            "louwenjie123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2163,
        "title": "请问openvino怎么进行int8量化",
        "body": "yolov5使用OpenVINO没有找到可以进行int8量化的配置\r\n参考文档https://github.com/PaddlePaddle/FastDeploy/blob/4e9bcc37183a8c4874551517c840d8078e31cfc3/serving/docs/zh_CN/model_configuration.md",
        "state": "closed",
        "user": "safehumeng",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-15T06:51:22+00:00",
        "updated_at": "2025-02-11T06:43:42+00:00",
        "closed_at": "2025-02-11T06:43:42+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2166,
        "title": "编译失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-gpu-1.0.7\r\n- 【编译命令】camke 编译\r\n- 【系统平台】 Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 3050， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n这是我的camke gui编译选项\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/50402380/1147da4b-6fbd-44e6-bde4-e7d0b8bc7b99)\r\n\r\n编译失败信息\r\nnvcc fatal   : Unsupported gpu architecture 'compute_OFF'\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/50402380/2fe891c6-1e87-42f4-b410-021e6f673db6)\r\n\r\n",
        "state": "closed",
        "user": "Shrinco",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-17T03:38:18+00:00",
        "updated_at": "2025-02-11T06:43:45+00:00",
        "closed_at": "2025-02-11T06:43:45+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2164,
        "title": "FastDeploy Streamer 容器内编译时报错",
        "body": "## 环境\r\n\r\n- 【硬件】： \r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/32097973/bcf5da9f-cde0-428c-9385-d335f73edc0b)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/32097973/650460bb-a233-4499-8190-57fb96969e28)\r\n- 【容器的宿主机环境】: Windows x64(Windows10) 21H2 19044.1586, WSL-Ubuntu-20.04, Docker version 24.0.2, build cb74dfc\r\n- 【编译库】: fastdeploy-linux-x64-1.0.7\r\n\r\n\r\n\r\n## 按[文档](https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.7/streamer/README_CN.md)进行编译报错\r\n\r\n```shell\r\n©°©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©´\r\n    ©¦                 ? MobaXterm Personal Edition v22.1 ?                 ©¦\r\n    ©¦               (SSH client, X server and network tools)               ©¦\r\n    ©¦                                                                      ©¦\r\n    ©¦ ? Linux distribution: ¬³Ubuntu-20.04                                 ©¦\r\n    ©¦ ? Windows drives are mounted into /mnt path (by default)             ©¦\r\n    ©¦ ? WSL DISPLAY is automatically redirected to Windows desktop         ©¦\r\n    ©¦ ? WSL filesystem is accessible in the sidebar browser                ©¦\r\n    ©¦ ? For more info, ctrl+click on help or visit our website.            ©¦\r\n    ©¸©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¤©¼\r\n\r\nroot@DESKTOP-Q6I8OIT:~# docker pull nvcr.io/nvidia/deepstream:6.1.1-devel\r\n6.1.1-devel: Pulling from nvidia/deepstream\r\n3b65ec22a9e9: Pull complete\r\n9bfa49b064c8: Pull complete\r\ncde16ef91ac2: Pull complete\r\n978ea3dcd5fb: Pull complete\r\n6c10428f3e1b: Pull complete\r\n25e1b86ea3b6: Pull complete\r\nab995ea0d0d0: Pull complete\r\n25c7edfc13eb: Pull complete\r\ndaf4a0e65bea: Pull complete\r\nde971f9d549d: Pull complete\r\n50dab9e483e1: Pull complete\r\nde899c7862b6: Pull complete\r\n8fe477dfad89: Pull complete\r\na261481bfaab: Pull complete\r\n7b029a04870b: Pull complete\r\n4b0e8b21c6bd: Pull complete\r\n9e45c75de592: Pull complete\r\ncb09c3fa7d9b: Pull complete\r\nf7aa1cebe3e3: Pull complete\r\ne1241e0526e4: Pull complete\r\nf7eb22717653: Pull complete\r\n2fe53a7e46c3: Pull complete\r\n6413c7c07022: Pull complete\r\n576f2c7da8dc: Pull complete\r\n85dfa125f287: Pull complete\r\n2fcbf0777988: Pull complete\r\nb0528175e25b: Pull complete\r\n8da62e68245e: Pull complete\r\nff3153ab1387: Pull complete\r\n5af267027073: Pull complete\r\n213a2dc2d850: Pull complete\r\n287570f14f24: Pull complete\r\n9a420b8e7945: Pull complete\r\n1c6a323a9e2f: Pull complete\r\n3c007dfa45bb: Pull complete\r\n1de3849df1aa: Pull complete\r\nf71cb0b15ba2: Pull complete\r\nd35a829e8cbf: Pull complete\r\nb46163547405: Pull complete\r\n818b7d5c4db7: Pull complete\r\nfdda6666feb6: Pull complete\r\n60bf999d8bfb: Pull complete\r\nDigest: sha256:6aac2e6cd547192b6e16202cef72c3625e5cde0feddb285566433d649081caaa\r\nStatus: Downloaded newer image for nvcr.io/nvidia/deepstream:6.1.1-devel\r\nnvcr.io/nvidia/deepstream:6.1.1-devel\r\nroot@DESKTOP-Q6I8OIT:~# xhost +\r\n\r\nCommand 'xhost' not found, but can be installed with:\r\n\r\napt install x11-xserver-utils\r\n\r\nroot@DESKTOP-Q6I8OIT:~# apt install ^C\r\nroot@DESKTOP-Q6I8OIT:~# apt install x11-xserver-utils\r\nReading package lists... Done\r\nBuilding dependency tree\r\nReading state information... Done\r\nThe following additional packages will be installed:\r\n  cpp cpp-9 gcc-9-base libisl22 libmpc3 libxcursor1\r\nSuggested packages:\r\n  cpp-doc gcc-9-locales nickle cairo-5c xorg-docs-core\r\nThe following NEW packages will be installed:\r\n  cpp cpp-9 gcc-9-base libisl22 libmpc3 libxcursor1 x11-xserver-utils\r\n0 upgraded, 7 newly installed, 0 to remove and 58 not upgraded.\r\nNeed to get 8362 kB of archives.\r\nAfter this operation, 29.9 MB of additional disk space will be used.\r\nDo you want to continue? [Y/n] y\r\nGet:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gcc-9-base amd64 9.4.0-1ubuntu1~20.04.1 [19.4 kB]\r\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libisl22 amd64 0.22.1-1 [592 kB]\r\nGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libmpc3 amd64 1.1.0-1 [40.8 kB]\r\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 cpp-9 amd64 9.4.0-1ubuntu1~20.04.1 [7500 kB]\r\nGet:5 http://archive.ubuntu.com/ubuntu focal/main amd64 cpp amd64 4:9.3.0-1ubuntu2 [27.6 kB]\r\nGet:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcursor1 amd64 1:1.2.0-2 [20.1 kB]\r\nGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xserver-utils amd64 7.7+8 [162 kB]\r\nFetched 8362 kB in 4s (1993 kB/s)\r\nSelecting previously unselected package gcc-9-base:amd64.\r\n(Reading database ... 33089 files and directories currently installed.)\r\nPreparing to unpack .../0-gcc-9-base_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\r\nUnpacking gcc-9-base:amd64 (9.4.0-1ubuntu1~20.04.1) ...\r\nSelecting previously unselected package libisl22:amd64.\r\nPreparing to unpack .../1-libisl22_0.22.1-1_amd64.deb ...\r\nUnpacking libisl22:amd64 (0.22.1-1) ...\r\nSelecting previously unselected package libmpc3:amd64.\r\nPreparing to unpack .../2-libmpc3_1.1.0-1_amd64.deb ...\r\nUnpacking libmpc3:amd64 (1.1.0-1) ...\r\nSelecting previously unselected package cpp-9.\r\nPreparing to unpack .../3-cpp-9_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\r\nUnpacking cpp-9 (9.4.0-1ubuntu1~20.04.1) ...\r\nSelecting previously unselected package cpp.\r\nPreparing to unpack .../4-cpp_4%3a9.3.0-1ubuntu2_amd64.deb ...\r\nUnpacking cpp (4:9.3.0-1ubuntu2) ...\r\nSelecting previously unselected package libxcursor1:amd64.\r\nPreparing to unpack .../5-libxcursor1_1%3a1.2.0-2_amd64.deb ...\r\nUnpacking libxcursor1:amd64 (1:1.2.0-2) ...\r\nSelecting previously unselected package x11-xserver-utils.\r\nPreparing to unpack .../6-x11-xserver-utils_7.7+8_amd64.deb ...\r\nUnpacking x11-xserver-utils (7.7+8) ...\r\nSetting up libxcursor1:amd64 (1:1.2.0-2) ...\r\nSetting up libmpc3:amd64 (1.1.0-1) ...\r\nSetting up libisl22:amd64 (0.22.1-1) ...\r\nSetting up gcc-9-base:amd64 (9.4.0-1ubuntu1~20.04.1) ...\r\nSetting up cpp-9 (9.4.0-1ubuntu1~20.04.1) ...\r\nSetting up cpp (4:9.3.0-1ubuntu2) ...\r\nSetting up x11-xserver-utils (7.7+8) ...\r\nProcessing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\r\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\r\n\r\nProcessing triggers for man-db (2.9.1-1) ...\r\nroot@DESKTOP-Q6I8OIT:~# xhost +\r\naccess control disabled, clients can connect from any host\r\nroot@DESKTOP-Q6I8OIT:~# docker run --gpus all -it --rm --net=host --privileged -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -w /opt/nvidia/deepstream/deepstream-6.1.1-devel nvcr.io/nvidia/deepstream:6.1.1-devel\r\nroot@docker-desktop:/opt/nvidia/deepstream/deepstream-6.1.1-devel# cd ~\r\nroot@docker-desktop:~# pwd\r\n/root\r\nroot@docker-desktop:~# ls\r\ntop\r\nroot@docker-desktop:~# git clone -b release/1.0.7 https://github.com/PaddlePaddle/FastDeploy\r\nCloning into 'FastDeploy'...\r\nremote: Enumerating objects: 40787, done.\r\nremote: Counting objects: 100% (7030/7030), done.\r\nremote: Compressing objects: 100% (1732/1732), done.\r\nremote: Total 40787 (delta 5571), reused 5771 (delta 5278), pack-reused 33757\r\nReceiving objects: 100% (40787/40787), 37.75 MiB | 537.00 KiB/s, done.\r\nResolving deltas: 100% (21353/21353), done.\r\nroot@docker-desktop:~# cd FastDeploy/streamer/\r\nroot@docker-desktop:~/FastDeploy/streamer# mkdir build && cd build/\r\nroot@docker-desktop:~/FastDeploy/streamer/build# python -V\r\nPython 2.7.18\r\nroot@docker-desktop:~/FastDeploy/streamer/build# python3 -V\r\nPython 3.8.10\r\nroot@docker-desktop:~/FastDeploy/streamer/build# pip list\r\nPackage             Version\r\n------------------- --------------------\r\ncertifi             2019.11.28\r\nchardet             3.0.4\r\ndbus-python         1.2.16\r\ndistro-info         0.23ubuntu1\r\ngraphsurgeon        0.4.6\r\nidna                2.8\r\nmeson               0.53.2\r\npip                 20.0.2\r\nPyGObject           3.36.0\r\npython-apt          2.0.0+ubuntu0.20.4.7\r\npython-dbusmock     0.19\r\nPyYAML              6.0\r\nrequests            2.22.0\r\nrequests-unixsocket 0.2.0\r\nsetuptools          45.2.0\r\nsix                 1.14.0\r\ntensorrt            8.4.1.5\r\nuff                 0.6.9\r\nunattended-upgrades 0.1\r\nurllib3             1.25.8\r\nwheel               0.34.2\r\nroot@docker-desktop:~/FastDeploy/streamer/build# wget https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-linux-x64-1.0.7.tgz\r\n--2023-08-16 05:46:37--  https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-linux-x64-1.0.7.tgz\r\nResolving bj.bcebos.com (bj.bcebos.com)... 36.110.192.178, 36.110.192.178, 2409:8c04:1001:1002:0:ff:b001:368a, ...\r\nConnecting to bj.bcebos.com (bj.bcebos.com)|36.110.192.178|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 188450981 (180M) [application/octet-stream]\r\nSaving to: 'fastdeploy-linux-x64-1.0.7.tgz'\r\n\r\nfastdeploy-linux-x64-1.0.7.tgz                    100%[==========================================================================================================>] 179.72M  40.6MB/s    in 4.6s\r\n\r\n2023-08-16 05:46:42 (38.9 MB/s) - 'fastdeploy-linux-x64-1.0.7.tgz' saved [188450981/188450981]\r\nroot@docker-desktop:~/FastDeploy/streamer/build# tar xvf fastdeploy-linux-x64-1.0.7.tgz\r\n...\r\nroot@docker-desktop:~/FastDeploy/streamer/build# pwd\r\n/root/FastDeploy/streamer/build\r\nroot@docker-desktop:~/FastDeploy/streamer/build# cmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-x64-1.0.7\r\n-- The C compiler identification is GNU 9.4.0\r\n-- The CXX compiler identification is GNU 9.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found PkgConfig: /usr/bin/pkg-config (found version \"0.29.1\")\r\n-- Checking for modules 'gstreamer-app-1.0;gstreamer-video-1.0'\r\n--   Found gstreamer-app-1.0, version 1.16.3\r\n--   Found gstreamer-video-1.0, version 1.16.3\r\n-- The path of ONNXRuntime is /root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/onnxruntime/lib.\r\n-- OPENVINO_LIBS = /root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/openvino/runtime/lib/libopenvino.so;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy\r\n-- The path of OpenCV is /root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/opencv.\r\n-- Found OpenCV: /root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/opencv (found version \"3.4.16\")\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.15.2\r\n--   CMake command             : /usr/local/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format\r\n--   EXE linker flags          :\r\n--   Shared linker flags       :\r\n--   Build type                :\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 1.0.7\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : ON\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       :\r\n--   Paddle Inference version  : 0.0.0.660f781b77\r\n--   OpenVINO version          : dev.2023.03.2\r\n--   DEPENDENCY_LIBS           : /root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/lib/libfastdeploy.so;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/onnxruntime/lib/libonnxruntime.so;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/paddle_inference/paddle/lib/libpaddle_inference.so;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/paddle_inference/third_party/install/mkldnn/lib/libmkldnn.so.0;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/paddle_inference/third_party/install/mklml/lib/libiomp5.so;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/openvino/runtime/lib/libopenvino.so;TBB::tbb;TBB::tbbmalloc;TBB::tbbmalloc_proxy;opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/fast_tokenizer/lib/libcore_tokenizers.so;/root/FastDeploy/streamer/build/fastdeploy-linux-x64-1.0.7/third_libs/install/paddle2onnx/lib/libpaddle2onnx.so\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /root/FastDeploy/streamer/build\r\nroot@docker-desktop:~/FastDeploy/streamer/build# make -j\r\nScanning dependencies of target fd_streamer\r\nScanning dependencies of target gstfdmeta\r\nScanning dependencies of target gstfdtracker\r\n[  4%] Building CXX object src/gstreamer/meta/CMakeFiles/gstfdmeta.dir/meta.cc.o\r\n[  9%] Building CXX object src/gstreamer/plugin/fdtracker/CMakeFiles/gstfdtracker.dir/gstfdtracker.cc.o\r\n[ 14%] Building CXX object src/gstreamer/plugin/fdtracker/CMakeFiles/gstfdtracker.dir/src/kalmantracker.cpp.o\r\n[ 19%] Building CXX object src/gstreamer/plugin/fdtracker/CMakeFiles/gstfdtracker.dir/src/ocsort.cpp.o\r\n[ 23%] Building CXX object src/gstreamer/plugin/fdtracker/CMakeFiles/gstfdtracker.dir/src/lapjv.cpp.o\r\n[ 28%] Building CXX object src/gstreamer/plugin/fdtracker/CMakeFiles/gstfdtracker.dir/src/trajectory.cpp.o\r\n[ 33%] Building CXX object CMakeFiles/fd_streamer.dir/src/app/base_app.cc.o\r\n[ 38%] Building CXX object CMakeFiles/fd_streamer.dir/src/app/video_analytics.cc.o\r\n[ 42%] Building CXX object CMakeFiles/fd_streamer.dir/src/app/video_decoder.cc.o\r\n[ 52%] Building CXX object CMakeFiles/fd_streamer.dir/src/app/yaml_parser.cc.o\r\n[ 52%] Building CXX object CMakeFiles/fd_streamer.dir/src/fd_streamer.cc.o\r\n[ 57%] Building CXX object CMakeFiles/fd_streamer.dir/src/gstreamer/perf.cc.o\r\n[ 61%] Building CXX object CMakeFiles/fd_streamer.dir/src/gstreamer/meta/meta.cc.o\r\n[ 66%] Building CXX object CMakeFiles/fd_streamer.dir/src/gstreamer/utils.cc.o\r\n[ 71%] Building CXX object CMakeFiles/fd_streamer.dir/src/deepstream/bbox_parser.cc.o\r\nIn file included from /usr/include/x86_64-linux-gnu/NvInferLegacyDims.h:16,\r\n                 from /usr/include/x86_64-linux-gnu/NvInfer.h:16,\r\n                 from /usr/include/x86_64-linux-gnu/NvCaffeParser.h:16,\r\n                 from /opt/nvidia/deepstream/deepstream/sources/includes/nvdsinfer_custom_impl.h:126,\r\n                 from /root/FastDeploy/streamer/src/deepstream/bbox_parser.cc:15:\r\n/usr/include/x86_64-linux-gnu/NvInferRuntimeCommon.h:19:10: fatal error: cuda_runtime_api.h: No such file or directory\r\n   19 | #include <cuda_runtime_api.h>\r\n      |          ^~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/fd_streamer.dir/build.make:167: CMakeFiles/fd_streamer.dir/src/deepstream/bbox_parser.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n[ 76%] Linking CXX shared library libgstfdmeta.so\r\n[ 76%] Built target gstfdmeta\r\nScanning dependencies of target gstfdinfer\r\n[ 80%] Building CXX object src/gstreamer/plugin/fdinfer/CMakeFiles/gstfdinfer.dir/fdinfer.cc.o\r\n[ 85%] Building CXX object src/gstreamer/plugin/fdinfer/CMakeFiles/gstfdinfer.dir/fdmodel.cc.o\r\nmake[1]: *** [CMakeFiles/Makefile2:112: CMakeFiles/fd_streamer.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 90%] Linking CXX shared library libgstfdtracker.so\r\n[ 90%] Built target gstfdtracker\r\n[ 95%] Linking CXX shared library libgstfdinfer.so\r\n[ 95%] Built target gstfdinfer\r\nmake: *** [Makefile:84: all] Error 2\r\nroot@docker-desktop:~/FastDeploy/streamer/build#\r\n``` ",
        "state": "closed",
        "user": "BingerOne",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-16T07:16:44+00:00",
        "updated_at": "2025-02-11T06:43:43+00:00",
        "closed_at": "2025-02-11T06:43:43+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2165,
        "title": "paddle2onnx/onnx_reader.cc doesn't support dtype of bool",
        "body": "## Environment\r\n\r\nFastDeploy version: the latest code in develop branch\r\nOS Platform: e.g. Linux x64\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.1 CUDNN 8.3\r\nProgram Language: c++\r\n\r\n## Problem description\r\n\r\nI have an onnx model that have inputs/outputs of boolean type, but it can't been read by fastdeploy, how can i do to use fastdeploy to handle it? \r\nThanks.",
        "state": "closed",
        "user": "easerene",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-16T08:24:01+00:00",
        "updated_at": "2025-02-11T06:43:44+00:00",
        "closed_at": "2025-02-11T06:43:44+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2167,
        "title": "硬件是CUDA12.0，目前是不是没有能支持fastdeploy_serving运行的镜像",
        "body": "按照这个[教程](https://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/deploy/fastdeploy/serving/fastdeploy_serving)想要体验GPU并发推理，进入容器启动服务端，关键报错如下：\r\n\r\nError: Failed to initialize NVML\r\nW0817 06:10:17.581837 173 metrics.cc:221] DCGM unable to start: DCGM initialization error\r\nW0817 06:10:17.581971 173 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0817 06:10:17.581993 173 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nE0817 06:10:17.582811 173 model_repository_manager.cc:1890] Poll failed for model directory 'cls_runtime': instance group cls_runtime_0 of model cls_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0817 06:10:17.583026 173 model_repository_manager.cc:1890] Poll failed for model directory 'det_runtime': instance group det_runtime_0 of model det_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0817 06:10:17.583324 173 model_repository_manager.cc:1890] Poll failed for model directory 'rec_runtime': instance group rec_runtime_0 of model rec_runtime specifies invalid or unsupported gpu id 0. GPUs with at least the minimum required CUDA compute compatibility of 6.000000 are: \r\nE0817 06:10:17.583358 173 model_repository_manager.cc:1375] Invalid argument: ensemble pp_ocr contains models that are not available: det_runtime\r\nE0817 06:10:17.583364 173 model_repository_manager.cc:1375] Invalid argument: ensemble rec_pp contains models that are not available: rec_runtime\r\nE0817 06:10:17.583367 173 model_repository_manager.cc:1375] Invalid argument: ensemble cls_pp contains models that are not available: cls_runtime\r\n\r\n查阅这个[issue](https://github.com/PaddlePaddle/FastDeploy/issues/1774)和这个[issue](https://github.com/PaddlePaddle/FastDeploy/issues/1745)后，发现是一样的问题。也就是说目前没有能支持硬件是CUDA12.0的版本吗。请问有计划何时支持吗",
        "state": "closed",
        "user": "JeremyGe07",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-17T06:29:08+00:00",
        "updated_at": "2025-02-11T06:43:45+00:00",
        "closed_at": "2025-02-11T06:43:45+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2168,
        "title": "3090机器上两张GPU卡并发出现core dump",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如 1.0.3-gpu-cuda11.4-trt8.4-21.10  \r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 说明具体硬件型号，如 NVIDIA GeForce TRX 3090， CUDA 11.4 \r\n- 【编译语言】： C++ / Python(3.6）\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n【流程】\r\n1. gpu服务有两张卡，-gpus=all,2. \r\n例子：\r\ndocker run -it -d --name test_im_gpu --gpus=all -p9800:8000 -p9801:8001 -p9802:8002 -v /workspace/triton/customer_im/gpu:/test_models harbor.prod.yxit.cc/sdc-ai/arges:argesdeploy-1.0.1 /bin/bash\r\n2. 单个请求，正常\r\n【日志】\r\n1. onnxruntime方式：\r\n2023-08-15 16:11:52.762890665 [E:onnxruntime:, cuda_call.cc:118 CudaCall] CUBLAS failure 14: CUBLAS_STATUS_INTERNAL_ERROR ; GPU=0 ; hostname=1d4d4ff6436c ; expr=cublasGemmHelper( Base::CublasHandle(), transB, transA, static_cast<int>(helper.N()), static_cast<int>(helper.M()), static_cast<int>(helper.K()), &alpha, reinterpret_cast<const CudaT*>(right_X->template Data<T>()), ldb, reinterpret_cast<const CudaT*>(left_X->template Data<T>()), lda, &zero, reinterpret_cast<CudaT*>(Y->template MutableData<T>()), ldc, device_prop);\r\n\r\n2023-08-15 16:11:52.762954041 [E:onnxruntime:, sequential_executor.cc:368 Execute] Non-zero status code returned while running MatMul node. Name:'p2o.MatMul.14' Status Message: CUBLAS error executing cublasGemmHelper( Base::CublasHandle(), transB, transA, static_cast<int>(helper.N()), static_cast<int>(helper.M()), static_cast<int>(helper.K()), &alpha, reinterpret_cast<const CudaT*>(right_X->template Data<T>()), ldb, reinterpret_cast<const CudaT*>(left_X->template Data<T>()), lda, &zero, reinterpret_cast<CudaT*>(Y->template MutableData<T>()), ldc, device_prop)\r\n\r\n2023-08-15 16:11:52.763008176 [E:onnxruntime:, cuda_call.cc:118 CudaCall] CUDA failure 700: an illegal memory access was encountered ; GPU=0 ; hostname=1d4d4ff6436c ; expr=cudaEventRecord(current_deferred_release_event, static_cast<cudaStream_t>(GetComputeStream()));\r\n\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(365)::Infer      Failed to Infer: Non-zero status code returned while running MatMul node. Name:'p2o.MatMul.14' Status Message: CUBLAS error executing cublasGemmHelper( Base::CublasHandle(), transB, transA, static_cast<int>(helper.N()), static_cast<int>(helper.M()), static_cast<int>(helper.K()), &alpha, reinterpret_cast<const CudaT*>(right_X->template Data<T>()), ldb, reinterpret_cast<const CudaT*>(left_X->template Data<T>()), lda, &zero, reinterpret_cast<CudaT*>(Y->template MutableData<T>()), ldc, device_prop)\r\n\r\n[WARNING] fastdeploy/runtime/runtime.cc(243)::GetOutputTensor   The output name [tmp_59] don't exist.\r\n2023-08-15 16:11:52.773263270 [E:onnxruntime:, cuda_call.cc:118 CudaCall] CUDA failure 700: an illegal memory access was encountered ; GPU=0 ; hostname=1d4d4ff6436c ; expr=cudaEventCreate(&current_deferred_release_event, 0x02);\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(365)::Infer      Failed to Infer: CUDA error executing cudaEventCreate(&current_deferred_release_event, cudaEventDisableTiming)\r\n\r\n\r\n4. tensorrt\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log   1: [runner.cpp::execute::718] Error Code 1: Myelin (Final synchronize failed (700))\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(348)::Infer Failed to Infer with TensorRT.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(479)::SetInputs     Error occurs while copy memory from CPU to GPU.\r\nSignal (6) received.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(479)::SetInputs     Error occurs while copy memory from CPU to GPU.\r\nSignal (6) received.\r\n\r\n",
        "state": "open",
        "user": "WeiboXu",
        "closed_by": null,
        "created_at": "2023-08-17T08:05:31+00:00",
        "updated_at": "2023-08-17T08:08:58+00:00",
        "closed_at": null,
        "comments_count": [
            "WeiboXu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2169,
        "title": "RK3588 支持部署 PaddleSpeech吗 ",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "lym169",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-17T09:34:01+00:00",
        "updated_at": "2024-08-20T06:40:39+00:00",
        "closed_at": "2024-08-20T06:40:39+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2170,
        "title": "PPOCRV4 初始化后的 predictor 不能重复使用",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-0.0.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia GPU P40， CUDA 11.7 CUDNN 8.3\r\n- 【编译语言】： Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用PP-Chat-OCR给出的 demo，使用如下方式是可以完成推理的\r\n<img width=\"435\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/19160255/dc835dac-86fe-4dc7-b1b1-0e50ba6a1ded\">\r\n但是，一旦将图中的ppocr_v4 作为函数传递出来（图中ppocr_pred返回ppocr_v4），再次调用 predict，就会出现 Segmentation Fault，调用方式如下图\r\n<img width=\"560\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/19160255/12cd6a9f-d7ea-4b43-852f-c7b00788f0a5\">\r\n\r\n",
        "state": "closed",
        "user": "ViewWholeWorld",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-17T10:26:04+00:00",
        "updated_at": "2024-12-17T06:42:05+00:00",
        "closed_at": "2024-12-17T06:42:05+00:00",
        "comments_count": [
            "jiangjiajun",
            "ViewWholeWorld",
            "Frosty-GTRIIP"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2173,
        "title": "加密失败 AttributeError: module 'fastdeploy.c_lib_wrap' has no attribute 'encryption'",
        "body": "*********************************************\r\n\r\n*********************************************\r\n\r\n## 环境\r\nfastdeploy-python-1.0.7\r\n\r\nmodel_buffer = open(model_file, 'rb')\r\nparams_buffer = open(params_file, 'rb')\r\nfd.encryption.encrypt(model_buffer.read(),)\r\n",
        "state": "closed",
        "user": "zoe531",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-20T06:33:20+00:00",
        "updated_at": "2025-02-11T06:43:46+00:00",
        "closed_at": "2025-02-11T06:43:46+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2172,
        "title": "跪求添加YOLOv8seg部署接口",
        "body": "跪求添加YOLOv8seg部署接口 \r\n![1](https://github.com/PaddlePaddle/FastDeploy/assets/40445051/3bfa67b2-b70e-4145-9288-ee67ced5ea78)\r\n",
        "state": "closed",
        "user": "panp4n",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-18T09:36:24+00:00",
        "updated_at": "2024-08-27T06:42:04+00:00",
        "closed_at": "2024-08-27T06:42:04+00:00",
        "comments_count": [
            "universea",
            "jiangjiajun",
            "universea",
            "universea",
            "jiangjiajun",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2174,
        "title": "visualdl启动后，打开页面报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： docker paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12\r\n- 【编译命令】docker paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12\r\n- 【系统平台】: docker paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.6-trt8.5-22.12\r\n- 【硬件】： Nvidia GPU 3070\r\n- 【编译语言】：python3.8\r\n- visualdl 2.5.3\r\n- \r\n## 问题日志及出现问题的操作流程\r\nroot@localhost:/FastDeploy/examples# visualdl --host 0.0.0.0 --port 8080\r\n/usr/lib/python3/dist-packages/requests/init.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\r\nwarnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nVisualDL 2.5.3\r\nRunning VisualDL at http://0.0.0.0:8080/ (Press CTRL+C to quit)\r\n[ERROR 2023-08-20 02:55:12,738 lib.py:637] Unexpected error: <class 'AttributeError'>\r\n'LogReader' object has no attribute '_model'\r\n<traceback object at 0x7fd9ca415340>\r\n[ERROR 2023-08-20 02:55:14,740 lib.py:637] Unexpected error: <class 'AttributeError'>\r\n'LogReader' object has no attribute '_model'\r\n<traceback object at 0x7fd9ca4150c0>\r\nTraceback (most recent call last):\r\nFile \"/usr/local/lib/python3.8/dist-packages/visualdl/server/lib.py\", line 633, in retry\r\nreturn function(*args, **kwargs)\r\nFile \"/usr/local/lib/python3.8/dist-packages/visualdl/server/lib.py\", line 585, in get_static_graph\r\nif log_reader.model:\r\nFile \"/usr/local/lib/python3.8/dist-packages/visualdl/reader/reader.py\", line 100, in model\r\nreturn self._model\r\nAttributeError: 'LogReader' object has no attribute '_model'\r\n[ERROR 2023-08-20 02:55:16,743 api.py:75] Internal server error. Retry later.\r\n界面也不正常\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/7421901/f4160ae0-b677-450c-89bb-305615bad3b6)\r\n",
        "state": "open",
        "user": "bltcn",
        "closed_by": null,
        "created_at": "2023-08-21T01:58:29+00:00",
        "updated_at": "2023-08-21T06:26:03+00:00",
        "closed_at": null,
        "comments_count": [
            "bltcn"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2175,
        "title": "yolo系列paddle格式在tensorrt后端推理不成功，输入不匹配",
        "body": "\r\n\r\n## 环境\r\n- 【FastDeploy版本】： fastdeploy develop版本 \r\n- 【编译命令】\r\n- cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DTRT_DIRECTORY=/usr/local/TensorRT-8.5.1.7 \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda-11.3 \\\r\n         -DENABLE_VISION=ON \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n- 【系统平台】: Linux x64(Ubuntu 20.04）\r\n- 【硬件】：  Nvidia GPU 3080TI， CUDA 11.3 CUDNN 8.6.0 TensorRT8.5.1.7 gcc/g++ 9.4.0\r\n- 【编译语言】： C++ \r\n##具体问题\r\n- 【模型跑不通】\r\n--example下的yolov7示例，前端是paddle格式模型，后端选择tensorrt，在跑示例时，以及自己实现的一版示例时都出现：\r\n![yolov7](https://github.com/PaddlePaddle/FastDeploy/assets/64945280/b5eb3c9c-2c83-456c-b05b-ca3c28a31211)\r\n后面自己实现了yolov8的也一样的错：\r\n![yolov8](https://github.com/PaddlePaddle/FastDeploy/assets/64945280/d818dc2f-b8c2-4245-87d3-edfb971b6457)\r\n看结果好像是输入不匹配，但我记得输入应该只需要设置图片，而不需要scale_factor。\r\n看结果是在底层trt_backend中的paddle转onnx，onnx序列化为trt文件这个转化时，输入的转化问题。",
        "state": "closed",
        "user": "chenhao-stick-to",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-21T03:55:31+00:00",
        "updated_at": "2024-08-27T06:42:05+00:00",
        "closed_at": "2024-08-27T06:42:05+00:00",
        "comments_count": [
            "jiangjiajun",
            "kkpssr"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2176,
        "title": "jetson AGX Xavier怎么安装fastdeploy?",
        "body": "执行pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n然后import fastdeploy报错如下：\r\nWARNING:root:Due to the package size limitation, we have not provide fastdeploy-gpu-python on pypi yet, please execute the following commands to reinstall fastdeploy-gpu-python.\r\nWARNING:root:   =================== Reinstall fastdeploy-gpu-python commands ==================\r\nWARNING:root:   python -m pip uninstall fastdeploy-gpu-python\r\nWARNING:root:   python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\nTraceback (most recent call last):\r\n  File \"infer_arcface_me.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"/home/deepnature/anaconda3/envs/paddle2.4/lib/python3.7/site-packages/fastdeploy/__init__.py\", line 22, in <module>\r\n    raise Exception(\"Failed to import fastdeploy module.\")\r\nException: Failed to import fastdeploy module.\r\n\r\n另外，我对安装fastdeploy的说明文档感到迷惑\r\n有的地方说安装python sdk即可，有的地方说要编译安装，麻烦说明一下，谢谢",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-21T09:15:49+00:00",
        "updated_at": "2024-08-27T06:42:06+00:00",
        "closed_at": "2024-08-27T06:42:06+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2177,
        "title": "jetson AGX Xavier安装fastdeploy后，推理报错，找不到libopencv_flann.so.3.4",
        "body": "自己编译坑太多了，官方能不能给一个编译好的库？\r\n我现在编译好了后，执行推理onnx模型的时候，报如下错误\r\n  File \"/home/deepnature/anaconda3/envs/paddle2.4/lib/python3.7/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n请问是咋回事？",
        "state": "closed",
        "user": "truthsun22",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-22T07:14:57+00:00",
        "updated_at": "2024-11-05T06:41:29+00:00",
        "closed_at": "2024-11-05T06:41:29+00:00",
        "comments_count": [
            "truthsun22",
            "jiangjiajun",
            "truthsun22",
            "Wilson-lamfung",
            "Wilson-lamfung",
            "truthsun22",
            "Wilson-lamfung",
            "jichengyuan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2178,
        "title": "求助，有没有大神可以告知如何将C++中Matting.MODNet模型增加C#的API方法",
        "body": "求助，有没有大神可以告知如何将C++中Matting.MODNet模型增加C#的API方法",
        "state": "closed",
        "user": "yangyunyi",
        "closed_by": "rainyfly",
        "created_at": "2023-08-23T06:02:26+00:00",
        "updated_at": "2024-02-06T11:08:40+00:00",
        "closed_at": "2024-02-06T11:08:40+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2179,
        "title": "自己修改的UTC推理代码使用paddle-lite后端出现问题",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 按照官方编译步骤编译的昇腾版本\r\n- 【编译命令】官网给出的命令\r\n- 【系统平台】: Linux aarch64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，Ascend910\r\n- 【编译语言】： Python3.9\r\n- 【补充】所需要的paddle npu环境以及paddlelite环境以及FastDeploy环境已经参照官方的代码编译安装成功\r\n\r\n- - `examples`下的代码可以运行，但自己的代码不能运行\r\n- -自己修改的是paddlenlp中无样本分类中的deploy中infer.py代码\r\n- -命令行命令 ：python deploy/python/infer.py --model_dir ./checkpoint/model_best --device ascend --backend paddle_lite\r\n- -自己修改的infer.py代码如下：\r\n# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nimport distutils.util\r\nimport os\r\nfrom typing import Any, Dict, List, Union\r\n\r\nimport fastdeploy as fd\r\nimport numpy as np\r\n\r\nfrom paddlenlp.prompt import PromptDataCollatorWithPadding, UTCTemplate\r\nfrom paddlenlp.transformers import AutoTokenizer\r\n\r\n\r\ndef parse_arguments():\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", required=True, help=\"The directory of model.\")\r\n    parser.add_argument(\"--vocab_path\", type=str, default=\"\", help=\"The path of tokenizer vocab.\")\r\n    parser.add_argument(\"--model_prefix\", type=str, default=\"model\", help=\"The model and params file prefix.\")\r\n    parser.add_argument(\r\n        \"--device\",\r\n        type=str,\r\n        default=\"cpu\",\r\n        choices=[\"gpu\", \"cpu\",\"ascend\"],\r\n        help=\"Type of inference device, support 'cpu' or 'gpu', 'ascend'.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--backend\",\r\n        type=str,\r\n        default=\"paddle\",\r\n        choices=[\"onnx_runtime\", \"paddle\", \"tensorrt\", \"paddle_tensorrt\", \"paddle_lite\"],\r\n        help=\"The inference runtime backend.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--pred_threshold\",\r\n        default=0.5,\r\n        type=float,\r\n        help=\"Probability threshold for start/end index probabiliry.\",\r\n    )\r\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"The batch size of data.\")\r\n    parser.add_argument(\"--max_length\", type=int, default=128, help=\"The max length of sequence.\")\r\n    parser.add_argument(\"--num_omask_tokens\", type=int, default=64, help=\"The max length of sequence.\")\r\n    parser.add_argument(\"--log_interval\", type=int, default=10, help=\"The interval of logging.\")\r\n    parser.add_argument(\"--use_fp16\", type=distutils.util.strtobool, default=False, help=\"Wheter to use FP16 mode\")\r\n    parser.add_argument(\"--cpu_threads\", type=int, default=1, help=\"Number of threads to predict when using cpu.\")\r\n    parser.add_argument(\"--device_id\", type=int, default=0, help=\"Select which gpu device to train model.\")\r\n    parser.add_argument(\"--nnadapter_context_properties\", type=str, default=\"\", help=\"以 Key-value 字串的形式表示设备参数，例如：如果希望使用 Atlas 300 I 3000/3010 加速卡（由四颗昇腾 310 芯片组成）的第 0 个昇腾 310 芯片，可以设置 “HUAWEI_ASCEND_NPU_SELECTED_DEVICE_IDS=0;” \")\r\n    parser.add_argument(\"--nnadapter_dynamic_shape_info\", default=None, help=\"Set nnadapter dynamic shape info for Paddle Lite backend.\")\r\n    parser.add_argument(\"--lite_power_mode\", default={}, help=\"Set POWER mode while using Paddle Lite backend on ARM CPU.\")\r\n    parser.add_argument(\"--nnadapter_model_cache_dir\", type=str, default=\"\", help=\"启用模型编译缓存功能，设置编译后的设备程序的缓存文件（以 .nnc 为扩展名）的存储目录，它能够跳过每次进程启动且模型首次推理时的编译步骤，减少首次推理耗时\")\r\n    parser.add_argument(\"--nnadapter_subgraph_partition_config_path\", type=str, default=\"\", help=\"设置自定义子图分割配置文件路径，用于将某些算子强制异构到 CPU ，防止因切分成过多子图而导致的性能下降，内存增加\")\r\n    parser.add_argument(\"--nnadapter_mixed_precision_quantization_config_path\", type=str, default=\"\", help=\"Set nnadapter mixed precision quantization config path for Paddle Lite backend..\")\r\n    return parser.parse_args()\r\n\r\n\r\nclass Predictor(object):\r\n    def __init__(self, args, schema: list = None):\r\n        self.set_schema(schema)\r\n        self.tokenizer = AutoTokenizer.from_pretrained(args.model_dir)\r\n        self.runtime = self.create_fd_runtime(args)\r\n        self.batch_size = args.batch_size\r\n        self.max_length = args.max_length\r\n        self.template = UTCTemplate(self.tokenizer, self.max_length)\r\n        self.collator = PromptDataCollatorWithPadding(self.tokenizer, return_tensors=\"np\")\r\n        self.pred_threshold = args.pred_threshold\r\n\r\n    def set_schema(self, schema):\r\n        if schema is None:\r\n            self._question = None\r\n            self._choices = None\r\n        elif isinstance(schema, list):\r\n            self._question = \"\"\r\n            self._choices = schema\r\n        elif isinstance(schema, dict) and len(schema) == 1:\r\n            for key, value in schema.items():\r\n                self._question = key\r\n                self._choices = value\r\n        else:\r\n            raise ValueError(f\"Invalid schema: {schema}.\")\r\n\r\n    def _check_input_text(self, inputs):\r\n        if isinstance(inputs, str) or isinstance(inputs, dict):\r\n            inputs = [inputs]\r\n\r\n        if isinstance(inputs, list):\r\n            input_list = []\r\n            for example in inputs:\r\n                data = {\"text_a\": \"\", \"text_b\": \"\", \"choices\": self._choices, \"question\": self._question}\r\n                if isinstance(example, dict):\r\n                    for k, v in example.items():\r\n                        if k in data:\r\n                            data[k] = example[k]\r\n                elif isinstance(example, str):\r\n                    data[\"text_a\"] = example\r\n                    data[\"text_b\"] = \"\"\r\n                elif isinstance(example, list):\r\n                    for x in example:\r\n                        if not isinstance(x, str):\r\n                            raise ValueError(\"Invalid inputs, input text should be strings.\")\r\n                    data[\"text_a\"] = example[0]\r\n                    data[\"text_b\"] = \"\".join(example[1:]) if len(example) > 1 else \"\"\r\n                else:\r\n                    raise ValueError(\r\n                        \"Invalid inputs, the input should be {'text_a': a, 'text_b': b}, a text or a list of text.\"\r\n                    )\r\n\r\n                if len(data[\"text_a\"]) < 1 and len(data[\"text_b\"]) < 1:\r\n                    raise ValueError(\"Invalid inputs, input `text_a` and `text_b` are both missing or empty.\")\r\n                if not isinstance(data[\"choices\"], list) or len(data[\"choices\"]) < 2:\r\n                    raise ValueError(\"Invalid inputs, label candidates should be a list with length >= 2.\")\r\n                if not isinstance(data[\"question\"], str):\r\n                    raise ValueError(\"Invalid inputs, prompt question should be a string.\")\r\n                input_list.append(data)\r\n        else:\r\n            raise TypeError(\"Invalid input format!\")\r\n        return input_list\r\n\r\n    def create_fd_runtime(self, args):\r\n        option = fd.RuntimeOption()\r\n        model_path = os.path.join(args.model_dir, args.model_prefix + \".pdmodel\")\r\n        params_path = os.path.join(args.model_dir, args.model_prefix + \".pdiparams\")\r\n        option.set_model_path(model_path, params_path)\r\n        if args.device == \"cpu\":\r\n            option.use_cpu()\r\n            option.set_cpu_thread_num(args.cpu_threads)\r\n        elif args.device == \"ascend\":\r\n            option.use_ascend()\r\n        else:\r\n            option.use_gpu(args.device_id)\r\n        if args.backend == \"paddle\":\r\n            option.use_paddle_infer_backend()\r\n        elif args.backend == \"onnx_runtime\":\r\n            option.use_ort_backend()\r\n        elif args.backend == \"openvino\":\r\n            option.use_openvino_backend()\r\n        elif args.backend == \"paddle_lite\":\r\n            option.use_paddle_lite_backend()\r\n            #option.paddle_lite_option.nnadapter_context_properties = args.nnadapter_context_properties\r\n            #option.paddle_lite_option.nnadapter_dynamic_shape_info = args.nnadapter_dynamic_shape_info\r\n            #option.paddle_lite_option.nnadapter_subgraph_partition_config_path = args.nnadapter_subgraph_partition_config_path\r\n            #option.paddle_lite_option.nnadapter_mixed_precision_quantization_config_path = args.nnadapter_mixed_precision_quantization_config_path\r\n            option.paddle_lite_option.nnadapter_model_cache_dir = args.nnadapter_model_cache_dir\r\n            option.set_model_path(model_path=args.model_dir,params_path=args.model_dir)\r\n            #option.paddle_lite_option.power_mode = args.lite_power_mode\r\n        else:\r\n            option.use_trt_backend()\r\n            if args.backend == \"paddle_tensorrt\":\r\n                option.use_paddle_infer_backend()\r\n                option.paddle_infer_option.collect_trt_shape = True\r\n                option.paddle_infer_option.enable_trt = True\r\n            trt_file = os.path.join(args.model_dir, \"model.trt\")\r\n            option.trt_option.set_shape(\r\n                \"input_ids\", [1, 1], [args.batch_size, args.max_length], [args.batch_size, args.max_length]\r\n            )\r\n            option.trt_option.set_shape(\r\n                \"token_type_ids\", [1, 1], [args.batch_size, args.max_length], [args.batch_size, args.max_length]\r\n            )\r\n            option.trt_option.set_shape(\r\n                \"position_ids\", [1, 1], [args.batch_size, args.max_length], [args.batch_size, args.max_length]\r\n            )\r\n            option.trt_option.set_shape(\r\n                \"attention_mask\",\r\n                [1, 1, 1, 1],\r\n                [args.batch_size, 1, args.max_length, args.max_length],\r\n                [args.batch_size, 1, args.max_length, args.max_length],\r\n            )\r\n            option.trt_option.set_shape(\r\n                \"omask_positions\",\r\n                [1, 1],\r\n                [args.batch_size, args.num_omask_tokens],\r\n                [args.batch_size, args.num_omask_tokens],\r\n            )\r\n            option.trt_option.set_shape(\"cls_positions\", [1], [args.batch_size], [args.batch_size])\r\n            if args.use_fp16:\r\n                option.trt_option.enable_fp16 = True\r\n                trt_file = trt_file + \".fp16\"\r\n            option.trt_option.serialize_file = trt_file\r\n        return fd.Runtime(option)\r\n\r\n    @staticmethod\r\n    def sigmoid(z):\r\n        return 1 / (1 + np.exp(-z))\r\n\r\n    def preprocess(self, inputs: Union[str, List[str]]) -> Dict[str, Any]:\r\n        \"\"\"\r\n        Transform the raw text to the model inputs, two steps involved:\r\n           1) Transform the raw text to token ids.\r\n           2) Generate the other model inputs from the raw text and token ids.\r\n        \"\"\"\r\n        inputs = self._check_input_text(inputs)\r\n        # Get the config from the kwargs\r\n        tokenized_inputs = [self.template(i) for i in inputs]\r\n        batches = [\r\n            tokenized_inputs[idx : idx + self.batch_size] for idx in range(0, len(tokenized_inputs), self.batch_size)\r\n        ]\r\n        outputs = {}\r\n        outputs[\"text\"] = inputs\r\n        outputs[\"batches\"] = [self.collator(batch) for batch in batches]\r\n\r\n        return outputs\r\n\r\n    def infer(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\r\n        outputs = {}\r\n        outputs[\"text\"] = inputs[\"text\"]\r\n        outputs[\"batch_logits\"] = []\r\n        dtype_list = [\"int64\", \"int64\", \"int64\", \"int64\", \"int64\", \"int64\"]\r\n        for batch in inputs[\"batches\"]:\r\n            batch = dict(batch)\r\n            for i in range(self.runtime.num_inputs()):\r\n                input_name = self.runtime.get_input_info(i).name\r\n                batch[input_name] = batch[input_name].astype(dtype_list[i])\r\n            del batch[\"soft_token_ids\"]\r\n            logits = self.runtime.infer(batch)[0]\r\n            outputs[\"batch_logits\"].append(logits)\r\n        return outputs\r\n\r\n    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\r\n        outputs = []\r\n        for logits in inputs[\"batch_logits\"]:\r\n            scores = self.sigmoid(np.array(logits))\r\n            output = {}\r\n            output[\"predictions\"] = []\r\n            for i, class_score in enumerate(scores[0]):\r\n                if class_score > self.pred_threshold:\r\n                    output[\"predictions\"].append({\"label\": i, \"score\": class_score})\r\n            outputs.append(output)\r\n\r\n        for i, output in enumerate(outputs):\r\n            if len(inputs[\"text\"][i][\"text_a\"]) > 0:\r\n                output[\"text_a\"] = inputs[\"text\"][i][\"text_a\"]\r\n            if len(inputs[\"text\"][i][\"text_b\"]) > 0:\r\n                output[\"text_b\"] = inputs[\"text\"][i][\"text_b\"]\r\n            for j, pred in enumerate(output[\"predictions\"]):\r\n                output[\"predictions\"][j] = {\r\n                    \"label\": inputs[\"text\"][i][\"choices\"][pred[\"label\"]],\r\n                    \"score\": pred[\"score\"],\r\n                }\r\n\r\n        return outputs\r\n\r\n    def predict(self, texts):\r\n        inputs = self.preprocess(texts)\r\n        outputs = self.infer(inputs)\r\n        results = self.postprocess(outputs)\r\n        return results\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = parse_arguments()\r\n    predictor = Predictor(args, schema=[\"这是一条差评\", \"这是一条好评\"])\r\n    results = predictor.predict(\"房间干净明亮，非常不错\")\r\n    print(results)\r\n=====================================================================================\r\n--命令行输出的错误：\r\n(paddle-npu) [ma-user zero_shot_text_classification]$python deploy/python/infer.py --model_dir ./checkpoint/model_best --device ascend --backend paddle_lite\r\nI0823 15:27:01.683387  6783 init.cc:239] ENV [CUSTOM_DEVICE_ROOT]=/home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/paddle_custom_device\r\nI0823 15:27:01.683435  6783 init.cc:145] Try loading custom device libs from: [/home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/paddle_custom_device]\r\nI0823 15:27:02.237061  6783 custom_device.cc:1112] Successed in loading custom runtime in lib: /home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/paddle_custom_device/libpaddle-custom-npu.so\r\nI0823 15:27:02.243542  6783 custom_kernel.cc:76] Successed in loading 325 custom kernel(s) from loaded lib(s), will be used like native ones.\r\nI0823 15:27:02.243674  6783 init.cc:157] Finished in LoadCustomDevice with libs_path: [/home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/paddle_custom_device]\r\nI0823 15:27:02.243705  6783 init.cc:245] CustomDevice: npu, visible devices count: 1\r\n/home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n[2023-08-23 15:27:05,892] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load './checkpoint/model_best'.\r\nLoading topology data from ./checkpoint/model_best\r\nTraceback (most recent call last):\r\n  File \"/home/ma-user/work/PaddleNLP/applications/zero_shot_text_classification/deploy/python/infer.py\", line 263, in <module>\r\n    predictor = Predictor(args, schema=[\"这是一条差评\", \"这是一条好评\"])\r\n  File \"/home/ma-user/work/PaddleNLP/applications/zero_shot_text_classification/deploy/python/infer.py\", line 73, in __init__\r\n    self.runtime = self.create_fd_runtime(args)\r\n  File \"/home/ma-user/work/PaddleNLP/applications/zero_shot_text_classification/deploy/python/infer.py\", line 191, in create_fd_runtime\r\n    return fd.Runtime(option)\r\n  File \"/home/ma-user/.conda/envs/paddle-npu/lib/python3.9/site-packages/fastdeploy/runtime.py\", line 33, in __init__\r\n    assert self._runtime.init(\r\nMemoryError: std::bad_alloc\r\n\r\n",
        "state": "closed",
        "user": "Wall-cn",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-23T07:34:03+00:00",
        "updated_at": "2025-05-27T06:44:32+00:00",
        "closed_at": "2025-05-27T06:44:32+00:00",
        "comments_count": [
            "rainyfly",
            "watertianyi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2180,
        "title": "Yolov5seg预处理固定为640问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-gpu-python=1.0.7\r\n- 【编译命令】\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060， CUDA 11.2 CUDNN 8.2\r\n- 【编译语言】： python\r\n\r\n\r\n- 【性能问题】\r\n因项目需求需要识别细长的小目标，训练了1280*1280的Yolov5seg模型才能保证分割效果良好，但使用fastdeploy yolov5seg预测接口报错提示输入图像尺寸和模型input不一致。\r\n查看fastdeploy yolov5seg 预处理部分发现resize尺寸是写死的640。\r\n因为使用paddle套件训练的模型都会读取infer_cfg.yml来确认预处理的参数，所以yolov5seg能否改成根据模型input尺寸来决定预处理resize尺寸。\r\n@wjj19950828 感谢大佬",
        "state": "closed",
        "user": "panp4n",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-23T07:34:37+00:00",
        "updated_at": "2025-04-29T06:46:01+00:00",
        "closed_at": "2025-04-29T06:46:01+00:00",
        "comments_count": [
            "jiangjiajun",
            "panp4n",
            "panp4n",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2181,
        "title": "ppyoloe 无法使用tensorrt",
        "body": "我通过如下命令运行\r\n  1 import cv2\r\n  2 import fastdeploy as fd\r\n  3 import os\r\n  4 option=fd.RuntimeOption()\r\n  5 option.use_gpu(0)\r\n  6 option.use_trt_backend()\r\n  7 option.enable_paddle_to_trt()\r\n  8 option.enable_paddle_trt_collect_shape()\r\n 10 option.set_trt_input_shape(\"image\", [1, 3, 256, 256], [1,3,512,512],[1, 3, 1024, 1024])\r\n 11 option.set_trt_input_shape(\"scale_factor\", [1,1], [1,1],[2,2])\r\n 12 option.enable_trt_fp16()\r\n 13 model_path=os.path.join(\"weights/car_detect\",\"model.pdmodel\")\r\n 14 params_file = os.path.join(\"weights/car_detect\", \"model.pdiparams\")\r\n 15 config_file = os.path.join(\"weights/car_detect\", \"infer_cfg.yml\")\r\n 16 model=fd.vision.detection.PPYOLOE(model_path,params_file,config_file,runtime_option=option)\r\n不知道是不是由于scale_factor指定格式不对，会报如下错误\r\nE0824 14:48:57.136653 11031 helper.h:127] Could not register plugin creator -  ::InstanceNormalization_TRT version 2\r\nE0824 14:49:01.417342 11031 helper.h:127] 4: [shapeCompiler.cpp::evaluateShapeChecks::1180] Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: IShuffleLayer reshape (Output: reshape2_6.tmp_01713): reshaping failed for tensor: conv2d_322.tmp_11711 Reshape dimension of -1 has no solution.)\r\nE0824 14:49:01.426880 11031 helper.h:127] 2: [builder.cpp::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )",
        "state": "closed",
        "user": "kkpssr",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-24T06:17:22+00:00",
        "updated_at": "2025-02-11T06:43:47+00:00",
        "closed_at": "2025-02-11T06:43:47+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2182,
        "title": "C++环境下执行fastdeploy库中的  paddle  Segmentation来预测分割模型，执行到model.Predict(&im, &res)报错，求问",
        "body": "- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： Nvidia GPU 4080， CUDA 11.8 CUDNN 8.9\r\n- 【编译语言】： C++\r\n\r\n- 【模型跑不通】\r\n- - 执行`examples`下的部署示例，在执行到model.Predict(&im, &res)时报错。\r\n- - - \r\n- 执行代码：\r\n--------------------------------------\r\n \tauto model_file = model_dir + sep + \"model.pdmodel\";\r\n \tauto params_file = model_dir + sep + \"model.pdiparams\";\r\n \tauto config_file = model_dir + sep + \"deploy.yaml\";\r\n \r\n \tauto option = fastdeploy::RuntimeOption();\r\n  \toption.UseGpu();\r\n \tauto model = fastdeploy::vision::segmentation::PaddleSegModel(\r\n \tmodel_file, params_file, config_file, option);\r\n \r\n  \r\n \tif (!model.Initialized()) {\r\n\t        std::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n \r\n \t//计时\r\n\tauto start = std::chrono::system_clock::now();\r\n\tauto im = cv::imread(image_file);\r\n\r\n \tfastdeploy::vision::SegmentationResult res;\r\n\tif (!model.Predict(&im, &res)) {\r\n  \t\tstd::cerr << \"Failed to predict.\" << std::endl;\r\n \t\treturn;\r\n\t}\r\n--------------------------------------\r\n（gpu环境激活成功，模型初始化成功，但执行预测的时候报错）\r\n--------------------------------------\r\n- [INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(273)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::GPU.\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\nNot support stack backtrace yet.\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED.\r\n  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnStatus_t) to get Nvidia's official solution and advice about CUDNN Error.] (at ../paddle/phi/kernels/fusion/gpu/conv_fusion_kernel.cu:609)\r\n\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/29684893/784560b4-3faa-4f4e-b5d6-45d1c253a2bb)\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/29684893/fbb076a2-92ed-411f-a720-6ebcc1d6a451)\r\n",
        "state": "closed",
        "user": "zhigangjun",
        "closed_by": "zhigangjun",
        "created_at": "2023-08-24T07:31:51+00:00",
        "updated_at": "2024-03-10T12:56:48+00:00",
        "closed_at": "2023-08-29T02:02:52+00:00",
        "comments_count": [
            "zhigangjun",
            "pcycccccc",
            "WangShengFeng1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2184,
        "title": "多个推理实例如何在一个GPU上推理",
        "body": "请问多个推理实在一个GPU上推理应该如何设置呢？",
        "state": "closed",
        "user": "Dandelion111",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-08-24T12:49:55+00:00",
        "updated_at": "2025-02-11T06:43:48+00:00",
        "closed_at": "2025-02-11T06:43:48+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2185,
        "title": "模型服务化部署了2个版本，调用历史版本报错",
        "body": "*********************************************\r\n## 环境\r\n\r\n- 【FastDeploy版本】：[ 说明具体的版本，如fastdeploy-linux-gpu-0.8.0](fastdeploy:1.0.1-gpu-cuda11.4-trt8.4-21.10)\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Centos7.9\r\n- 【硬件】： Nvidia GPU A16， CUDA 11.7\r\n- 【编译语言】： Python3.8\r\n\r\n-【服务化部署后调用报错】\r\n模型部署文件目录\r\nmodel_name  -  modele  -  frcnn - 1\r\n                                                     - 2\r\n                                                     - config.pbtxt\r\n                                         - preprocess - 1 - infer_cfg.yml\r\n                                                                   - model.py\r\n                                                              - 2 - infet_cfg.yml\r\n                                                                   - model.py\r\n                                         - postprocess - 1 - model.py\r\n                                                                - 2 - model.py\r\n                                                                - config.pbtxt\r\n                                         - runtime - 1 - model.pdiparams\r\n                                                               - model.pdmodel\r\n                                                         - 2 - model.pdiparams\r\n                                                               - model.pdmodel\r\n                                                         - config.pbtxt\r\n 用grpc的客户端进行调用，如果frcnn中的config.pbtxt中的modelversion写-1的时候会直接调用最新的模型版本，如果我同时部署了1,2两个版本的模型，按照triton中说的版本控制进行的修改modelversion，改为1或者2则无法调用，docker会直接闪退。\r\n报错如下\r\n08/25/2023 17:27:37 - ERROR - uvicorn.error -   Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 376, in run_asgi\r\n    result = await app(self.scope, self.receive, self.send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py\", line 75, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/fastapi/applications.py\", line 276, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/applications.py\", line 122, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n    raise exc\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n    raise e\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/routing.py\", line 718, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/routing.py\", line 66, in app\r\n    response = await func(request)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/fastapi/routing.py\", line 237, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/fastapi/routing.py\", line 165, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/starlette/concurrency.py\", line 41, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/home/liu/code/serving-online/mypaddledet_grpc_client_url.py\", line 114, in infer_image\r\n    infer_request = client_request(image_url, mission_name, version, draw_threshold)\r\n  File \"/home/liu/code/serving-online/mypaddledet_grpc_client_url.py\", line 238, in client_request\r\n    runner = SyncGRPCTritonRunner(model_url, model_type, model_version)\r\n  File \"/home/liu/code/serving-online/mypaddledet_grpc_client_url.py\", line 46, in __init__\r\n    error = self._verify_triton_state(self._client)\r\n  File \"/home/liu/code/serving-online/mypaddledet_grpc_client_url.py\", line 96, in _verify_triton_state\r\n    if not triton_client.is_server_live():\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 231, in is_server_live\r\n    raise_error_grpc(rpc_error)\r\n  File \"/home/j/.virtualenvs/rt/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 61, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Broken pipe\r\n",
        "state": "closed",
        "user": "bbmeat",
        "closed_by": "bbmeat",
        "created_at": "2023-08-25T09:28:38+00:00",
        "updated_at": "2023-08-28T06:06:39+00:00",
        "closed_at": "2023-08-28T06:06:39+00:00",
        "comments_count": [
            "bbmeat"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2186,
        "title": "在window的anconda环境下出现 no module named ‘resource’",
        "body": "\r\n在window的anconda环境下出现 no module named ‘resource’，请问如何解决？",
        "state": "closed",
        "user": "intjun",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-01T02:31:26+00:00",
        "updated_at": "2025-02-11T06:43:49+00:00",
        "closed_at": "2025-02-11T06:43:49+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2187,
        "title": "Failed to parse ONNX model by TensorRT.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy-release-1.0.7\r\n- 【编译命令】\r\n            cd FastDeploy-release-1.0.7\r\n            mkdir build && cd build\r\n           cmake .. -DBUILD_ON_JETSON=ON \\\r\n            DENABLE_VISION=ON \\\r\n            DENABLE_PADDLE_BACKEND=ON \\\r\n            DPADDLEINFERENCE_DIRECTORY=/home/racobit/Downloads/paddle_inference_install_dir \\\r\n            DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\n            make install\r\n\r\n           \r\n- 【系统平台】: NX Linux aarch64(Ubuntu 18.04)\r\n- 【硬件】：  jetpack4.6.3,  CUDA 10.2.300,  CUDNN 8.2.1.32,  tensorrt 8.2,   opencv4.5.3(cuda yes),   python 3.6.9\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型开启GPU-TRT后端跑不通】\r\n- - 执行`FastDeploy-release-1.0.7/examples/vision/detection/paddledetection/cpp/`下的yolov5部署示例，采用官方模型与代码,  (./examples/vision/detection/paddledetection/cpp/infer_yolov5.cc),  其他后端均可以正确执行,  只有gpu-trt后端报错,  \r\n\r\n- - 命令如下\r\ncd /Downloads/FastDeploy-release-1.0.7/examples/vision/detection/paddledetection/cpp/\r\nmkdir build && cd build\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=/home/racobit/Downloads/FastDeploy-release-1.0.7/build/installed_fastdeploy\r\nmake -j6\r\nwget https://gitee.com/paddlepaddle/PaddleDetection/raw/release/2.4/demo/000000014439.jpg\r\nwget https://bj.bcebos.com/paddlehub/fastdeploy/yolov5_s_300e_coco.tgz\r\ntar xvf yolov5_s_300e_coco.tgz\r\n./infer_yolov5_demo ./yolov5_s_300e_coco 000000014439.jpg 2\r\n\r\n\r\n- - 报错内容如下\r\nddledetection/cpp/build$ ./infer_yolov5_demo ./yolov5_s_300e_coco 000000014439.jpg 2\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(696)::CreateTrtEngineFromOnnx\tFailed to parse ONNX model by TensorRT.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(298)::InitFromOnnx\tFailed to create tensorrt engine.\r\n[ERROR] fastdeploy/runtime/runtime.cc(307)::CreateTrtBackend\tFailed to initialize TensorRT backend.\r\nAborted (core dumped)\r\n\r\n\r\n- - 【其他位置的模型开启GPU-TRT后端,也报相同的错误】\r\n位置/home/racobit/Downloads/FastDeploy-release-1.0.7/examples/vision/detection/yolov5/cpp/infer_paddle_model.cc\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "catofyuanyuan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-01T02:41:21+00:00",
        "updated_at": "2024-09-03T06:41:54+00:00",
        "closed_at": "2024-09-03T06:41:54+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2189,
        "title": "现版本是否还未支持rt-detr模型部署？",
        "body": "现版本是否还未支持rt-detr模型部署？在使用新版本的paddlex，训练出来的rt-detr模型，无法使用fastdeploy部署，报错如图\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/44393339/fc99e9df-97ea-43ef-9b41-0809a674d472)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/44393339/91016b10-39c9-4704-a125-dc7c4162a741)\r\n\r\n",
        "state": "closed",
        "user": "wzy123456wzy",
        "closed_by": "rainyfly",
        "created_at": "2023-09-04T09:33:55+00:00",
        "updated_at": "2024-02-06T08:53:27+00:00",
        "closed_at": "2024-02-06T08:53:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2188,
        "title": "安装官方文档使用FastDeploy部署OCR识别V3的版本，不稳定，在用不同尺寸的图片的时候就经常出问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 镜像registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【启动命令】fastdeployserver --model-repository=/ocr_serving/models\r\n- 【进入容器】 docker exec -it -u root fastdeploy bash\r\n- 【系统平台】: Linux x64(Ubuntu 16.04)\r\n- 【硬件】： Nvidia GPU 2070 super， CUDA 12，NVIDA driver 525.89.02\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【bug】\r\n- - FastDeploy启动成功之后使用client.py测试OCR的识别结果发现，如果一直是同一尺寸的图片貌似没问题，但是如果调用的过程中换其他的图片就会报错，有时候报显存不足，有时候识别的时候box报index out of range，输出box看的时候发现是空的\r\nI0901 03:58:34.384452 147 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0901 03:58:34.385091 147 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0901 03:58:34.426893 147 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\nW0901 03:58:45.404812 147 memory.cc:135] Failed to allocate CUDA memory with byte size 81885000 on GPU 0: CNMEM_STATUS_OUT_OF_MEMORY, falling back to pinned system memory\r\n0901 03:58:45.406278 226 pb_stub.cc:402] Failed to process the request(s) for model 'det_postprocess_0', message: TritonModelException: in ensemble 'rec_pp', softmax_5.tmp_0: failed to perform CUDA copy: invalid argument\r\n\r\nAt:\r\n  /ocr_serving/models/det_postprocess/1/model.py(206): execute\r\n\r\n",
        "state": "open",
        "user": "xiulianzw",
        "closed_by": null,
        "created_at": "2023-09-01T05:38:07+00:00",
        "updated_at": "2024-12-13T08:26:49+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "zznnxx"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2191,
        "title": "fastdeploy中example编译报错",
        "body": "\r\n### 问题:编译fastdeploy-kunlunxin包正常，但依赖编译好的库编译fastdeploy中的example示例报错，切换了环境在NVIDIA Jetson NVIDIA上编译一样报错，但在x86上编译不报错\r\n\r\n环境：飞腾CPU（D2000）+银河麒麟V10+昆仑芯r100  \r\n### \r\n****#第一步:编译fastdeploy包****\r\n\r\ncmake -DWITH_KUNLUNXIN=ON  \\\r\n      -DWITH_GPU=OFF  \\ # 不编译 GPU\r\n      -DCMAKE_INSTALL_PREFIX=fastdeploy-kunlunxin \\\r\n      -DENABLE_VISION=ON \\ # 是否编译集成视觉模型的部署模块，可选择开启\r\n      -DOPENCV_DIRECTORY=/usr/lib/aarch-linux-gnu/cmake/opencv4 \\（指定系统自带的opencv和不指定默认下载后面都会出错）  ..\r\nmake -j8\r\nmake install\r\n\r\n正常编译通过，生成fastdeploy-kunlunxin包，将其复制到yolov5 cpp例程中的build\r\n\r\n### **#第二步:编译example-vision-detection-yolov5下的cpp例程**\r\ncmake .. -DFASTDEPLOY_INSTALL_DIR=${PWD}/fastdeploy-linux-kunlunxin\r\nmake -j\r\n报错\r\n\r\n问题：编译fastdeploy-kunlunxin包没问题，但是在编译example中yolov5例子的时候报错，错误如下：\r\nd: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::softsign<float>(baidu::xpu::api::Context*, float const*, float*, long, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast<float16>(baidu::xpu::api::Context*, float16 const*, float16*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<float, float16, float, float>(baidu::xpu::api::Context*, float const*, float16 const*, float*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `xpu_host_unregister'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_greater_equal<long>(baidu::xpu::api::Context*, long const*, long const*, bool*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::interpolate2d<float16>(baidu::xpu::api::Context*, float16 const*, float16*, long, long, long, long, long, long, bool, long, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::box_coder_encoder<float>(baidu::xpu::api::Context*, float const*, float const*, float const*, float const*, float*, long, long, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::bicubic_resize2d<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, long, long, long, bool, float, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<float16, short, float16, short>(baidu::xpu::api::Context*, float16 const*, short const*, float16*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sorted_nms<float, int>(baidu::xpu::api::Context*, float const*, int*, long&, long, float, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::flip<signed char>(baidu::xpu::api::Context*, signed char const*, signed char*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sin<float>(baidu::xpu::api::Context*, float const*, float*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::reduce_sum<float>(baidu::xpu::api::Context*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion_norm<float, short, float16, int>(baidu::xpu::api::Context*, float const*, short const*, float16*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float, long, std::vector<int, std::allocator<int> >&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<float, signed char, signed char, signed char>(baidu::xpu::api::Context*, float const*, signed char const*, signed char*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::conv3d<float16, short, float, short>(baidu::xpu::api::Context*, float16 const*, short const*, float*, long, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, long, float const*, float const*, float*, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<float, float, float, short>(baidu::xpu::api::Context*, float const*, float const*, float*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_max<float>(baidu::xpu::api::Context*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<float, long>(baidu::xpu::api::Context*, float const*, long*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<int, int>(baidu::xpu::api::Context*, int const*, int*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::strided_slice<float>(baidu::xpu::api::Context*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sub<float>(baidu::xpu::api::Context*, float const*, float const*, float*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::range<long>(baidu::xpu::api::Context*, long*, long, long, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cos<float16>(baidu::xpu::api::Context*, float16 const*, float16*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::adaptive_max_pool2d<signed char, int>(baidu::xpu::api::Context*, signed char const*, signed char*, int*, long, long, long, long, long, long, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `baidu::xpu::api::Context::_nsdnn_list'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion_norm<float, signed char, signed char, int>(baidu::xpu::api::Context*, float const*, signed char const*, signed char*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float, long, std::vector<int, std::allocator<int> >&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<long, int>(baidu::xpu::api::Context*, long const*, int*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::quantization<float, signed char>(baidu::xpu::api::Context*, float const*, signed char*, long, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::box_decoder<float>(baidu::xpu::api::Context*, float const*, float const*, float const*, float*, long, bool, bool, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `baidu::xpu::api::Context::~Context()'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<signed char, bool>(baidu::xpu::api::Context*, signed char const*, bool*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::meshgrid<float16>(baidu::xpu::api::Context*, std::vector<float16 const*, std::allocator<float16 const*> > const&, std::vector<float16*, std::allocator<float16*> > const&, std::vector<std::vector<long, std::allocator<long> >, std::allocator<std::vector<long, std::allocator<long> > > > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::swish<float>(baidu::xpu::api::Context*, float const*, float*, long, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<float, bool>(baidu::xpu::api::Context*, float const*, bool*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::squeeze_excitation_block<float16, short, short>(baidu::xpu::api::Context*, float16 const*, short const*, short const*, float16*, long, long, long, long, long, float const*, float const*, float const*, float const*, float16 const*, baidu::xpu::api::Activation_t const&, baidu::xpu::api::Activation_t const&, baidu::xpu::api::Activation_t const&, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::reduce_mean<float>(baidu::xpu::api::Context*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::instance_norm<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, float, float const*, float const*, float*, float*, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<long, long>(baidu::xpu::api::Context*, long const*, long*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<long, bool>(baidu::xpu::api::Context*, long const*, bool*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_max<int>(baidu::xpu::api::Context*, int const*, int const*, int*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<unsigned char, int>(baidu::xpu::api::Context*, unsigned char const*, int*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `baidu::xpu::api::create_context()'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::leaky_relu<float>(baidu::xpu::api::Context*, float const*, float*, long, float, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::conv2d_fusion<signed char, signed char, float16, signed char>(baidu::xpu::api::Context*, signed char const*, signed char const*, float16*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, long, float const*, float const*, float*, bool, float const*, float16 const*, baidu::xpu::api::Activation_t const&, float const*, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_pow<long>(baidu::xpu::api::Context*, long const*, long const*, long*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `baidu::xpu::api::BufferMgr::set(void*, unsigned long, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fast_sigmoid<float>(baidu::xpu::api::Context*, float const*, float*, long, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::softplus<float>(baidu::xpu::api::Context*, float const*, float*, long, float, float, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::concat<float>(baidu::xpu::api::Context*, std::vector<float const*, std::allocator<float const*> > const&, float*, std::vector<std::vector<long, std::allocator<long> >, std::allocator<std::vector<long, std::allocator<long> > > > const&, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_mul<long>(baidu::xpu::api::Context*, long const*, long const*, long*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sequence_sum_pool<float, int>(baidu::xpu::api::Context*, float const*, float*, baidu::xpu::api::VectorParam<int> const&, long, long, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_floordiv<float>(baidu::xpu::api::Context*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gather<int, long>(baidu::xpu::api::Context*, int const*, long const*, int*, std::vector<long, std::allocator<long> > const&, long, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `xpu_stream_create'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc<float, float, float, short>(baidu::xpu::api::Context*, float const*, float const*, float*, long, long, long, bool, bool, float const*, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_min<float>(baidu::xpu::api::Context*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sequence_topk_avg_pooling<float, int>(baidu::xpu::api::Context*, float const*, float*, int*, long, baidu::xpu::api::VectorParam<int> const&, baidu::xpu::api::VectorParam<int> const&, baidu::xpu::api::VectorParam<int> const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::reduce_sum<long>(baidu::xpu::api::Context*, long const*, long*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::constant<int>(baidu::xpu::api::Context*, int*, long, int)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<long, float>(baidu::xpu::api::Context*, long const*, float*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::layer_norm<float>(baidu::xpu::api::Context*, float const*, float*, long, long, float, float const*, float const*, float*, float*, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::mul_activation_fusion<signed char, signed char, signed char, float>(baidu::xpu::api::Context*, signed char const*, signed char const*, signed char*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, float const*, float const*, float*, baidu::xpu::api::Activation_t const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<int, float16>(baidu::xpu::api::Context*, int const*, float16*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sorted_topk<float, int>(baidu::xpu::api::Context*, float const*, float*, int*, long, long, long, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::constant_pad3d<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, float, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<signed char, signed char, float, signed char>(baidu::xpu::api::Context*, signed char const*, signed char const*, float*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::adaptive_avg_pool3d<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, long, long, long, long, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_sub<float16>(baidu::xpu::api::Context*, float16 const*, float16 const*, float16*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::transformer_encoder<float, float16, float, int>(baidu::xpu::api::Context*, float const*, std::vector<float16 const*, std::allocator<float16 const*> > const&, float*, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, baidu::xpu::api::NewQKVAttnParam<int> const&, float const*, std::vector<float const*, std::allocator<float const*> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::constant<bool>(baidu::xpu::api::Context*, bool*, long, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::conv2d_transpose_fusion<float, float, float, short>(baidu::xpu::api::Context*, float const*, float const*, float*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, long, float const*, float const*, float*, float const*, baidu::xpu::api::Activation_t const&, bool, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gather<float, long>(baidu::xpu::api::Context*, float const*, long const*, float*, std::vector<long, std::allocator<long> > const&, long, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::bicubic_resize2d<float16>(baidu::xpu::api::Context*, float16 const*, float16*, long, long, long, long, long, long, long, bool, float, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_min<int>(baidu::xpu::api::Context*, int const*, int const*, int*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `baidu::xpu::api::NewCrossAttnParam<int>::selfcheck(baidu::xpu::api::Context*) const'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::select<float>(baidu::xpu::api::Context*, bool const*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_sub<int>(baidu::xpu::api::Context*, int const*, int const*, int*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `xpu_current_device'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::transpose<int>(baidu::xpu::api::Context*, int const*, int*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion<signed char, signed char, signed char, signed char>(baidu::xpu::api::Context*, signed char const*, signed char const*, signed char*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float const*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gather<float, int>(baidu::xpu::api::Context*, float const*, int const*, float*, std::vector<long, std::allocator<long> > const&, long, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::embedding<float, int>(baidu::xpu::api::Context*, float const*, int const*, float*, long, long, long, long, int)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gelu<float>(baidu::xpu::api::Context*, float const*, float*, long, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::transformer_encoder<float, float, float, int>(baidu::xpu::api::Context*, float const*, std::vector<float const*, std::allocator<float const*> > const&, float*, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, baidu::xpu::api::NewQKVAttnParam<int> const&, float const*, std::vector<float const*, std::allocator<float const*> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::strided_slice<float16>(baidu::xpu::api::Context*, float16 const*, float16*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::pad2d<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, std::vector<long, std::allocator<long> > const&, char const*, float, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::sequence_expand<float, int>(baidu::xpu::api::Context*, float const*, float*, baidu::xpu::api::VectorParam<int> const&, baidu::xpu::api::VectorParam<int> const&, baidu::xpu::api::VectorParam<int> const&, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_add<float16>(baidu::xpu::api::Context*, float16 const*, float16 const*, float16*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::avg_pool2d<signed char>(baidu::xpu::api::Context*, signed char const*, signed char*, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, bool, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::cast_v2<float16, long>(baidu::xpu::api::Context*, float16 const*, long*, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::grnn_cell<float, short, int, short>(baidu::xpu::api::Context*, float const*, float const*, std::vector<short const*, std::allocator<short const*> > const&, std::vector<short const*, std::allocator<short const*> > const&, float*, long, long, baidu::xpu::api::VectorParam<int> const&, float const*, float const*, std::vector<float const*, std::allocator<float const*> > const&, std::vector<float const*, std::allocator<float const*> > const&, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `xpu_wait'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::max_pool2d<float, int>(baidu::xpu::api::Context*, float const*, float*, int*, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, bool, float const*, float*, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::quick_gelu<float>(baidu::xpu::api::Context*, float const*, float*, long, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::grid_sample<float>(baidu::xpu::api::Context*, float const*, float const*, float*, long, long, long, long, long, long, bool, bool, long, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_add<float>(baidu::xpu::api::Context*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::max_pool3d<float16, int>(baidu::xpu::api::Context*, float16 const*, float16*, int*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::bidirection_embedding_add<float, long>(baidu::xpu::api::Context*, float const*, float*, float*, baidu::xpu::api::VectorParam<long> const&, baidu::xpu::api::VectorParam<long> const&, baidu::xpu::api::VectorParam<long> const&, long, long, long)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::max_pool3d<float, int>(baidu::xpu::api::Context*, float const*, float*, int*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::slice<signed char>(baidu::xpu::api::Context*, signed char const*, signed char*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::match_matrix_tensor<float, short, int>(baidu::xpu::api::Context*, float const*, float const*, short const*, float*, long, long, bool, baidu::xpu::api::VectorParam<int>, baidu::xpu::api::VectorParam<int>, float const*, float const*, float const*, baidu::xpu::api::Activation_t const&, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::softplus<float16>(baidu::xpu::api::Context*, float16 const*, float16*, long, float, float, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::fc_fusion_norm<float, float, float, int>(baidu::xpu::api::Context*, float const*, float const*, float*, long, long, long, bool, bool, float const*, float const*, float*, long, long, long, float, float, float const*, baidu::xpu::api::Activation_t const&, float, long, std::vector<int, std::allocator<int> >&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::constant<float>(baidu::xpu::api::Context*, float*, long, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::reflection_pad3d<float>(baidu::xpu::api::Context*, float const*, float*, long, long, long, long, long, std::vector<long, std::allocator<long> > const&, bool)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::flip<short>(baidu::xpu::api::Context*, short const*, short*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::broadcast_mod<float>(baidu::xpu::api::Context*, float const*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::adaptive_avg_pool3d<float16>(baidu::xpu::api::Context*, float16 const*, float16*, long, long, long, long, long, long, long, long, bool, float const*, float*)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gather_nd<long, int>(baidu::xpu::api::Context*, long const*, int const*, long*, baidu::xpu::api::VectorParam<long> const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::slice<long>(baidu::xpu::api::Context*, long const*, long*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::gather_nd<int, int>(baidu::xpu::api::Context*, int const*, int const*, int*, baidu::xpu::api::VectorParam<long> const&, std::vector<long, std::allocator<long> > const&)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::l2_norm<float16>(baidu::xpu::api::Context*, float16 const*, float16*, float16*, std::vector<long, std::allocator<long> > const&, long, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::pad<float>(baidu::xpu::api::Context*, float const*, float*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, float)'\r\n/usr/bin/ld: fastdeploy-kunlunxin/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: undefined reference to `int baidu::xpu::api::reduce_max<long>(baidu::xpu::api::Context*, long const*, long*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&)'\r\ncollect2: 错误： ld 返回 1\r\nmake[2]: *** [CMakeFiles/infer_paddle_demo.dir/build.make:104：infer_paddle_demo] 错误 1\r\nmake[1]: *** [CMakeFiles/Makefile2:77：CMakeFiles/infer_paddle_demo.dir/all] 错误 2\r\nmake: *** [Makefile:84：all] 错误 2\r\n@jiangjiajun ",
        "state": "closed",
        "user": "12w2",
        "closed_by": "rainyfly",
        "created_at": "2023-09-05T09:24:01+00:00",
        "updated_at": "2024-02-06T08:52:39+00:00",
        "closed_at": "2024-02-06T08:52:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2192,
        "title": "Is the picture automatically corrected, such as the picture rotated 90 degrees to return to the front?",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n",
        "state": "closed",
        "user": "kitzza",
        "closed_by": "rainyfly",
        "created_at": "2023-09-06T02:19:53+00:00",
        "updated_at": "2024-02-06T08:52:12+00:00",
        "closed_at": "2024-02-06T08:52:12+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2190,
        "title": "fastdeploy服务化部署，部分图片传到服务后会报错module 'numpy' has no attribute 'astype'",
        "body": "- 【FastDeploy版本】：fastdeploy:1.0.4-cpu-only-21.10\r\n- 【系统平台】: 官方docker\r\n\r\n报错如下\r\n```\r\n0904 12:09:52.059001 1627 pb_stub.cc:402] Failed to process the request(s) for model 'det_postprocess_0', message: AttributeError: module 'numpy' has no attribute 'astype'\r\n\r\nAt:\r\n  /usr/local/lib/python3.8/dist-packages/numpy/__init__.py(284): __getattr__\r\n  /opt/tritonserver/FastDeploy/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models/det_postprocess/1/model.py(197): execute\r\n```\r\n我根据报错尝试查看了一下这个det_postprocess/1/model.py的代码，execute方法里就两处用到了astype。其中有一行代码我看着很奇怪，尝试改了下（倒数第二行改为了倒数第一行）。改了之后就能正常识别之前会报错的那批图片了。这应该算是一个BUG吧，但是我不懂python不知道我改得对不对，希望大佬确认一下\r\n```\r\n                for index in range(len(image_list)):\r\n                    if cls_labels[index] == 1 and cls_scores[\r\n                            index] > self.cls_threshold:\r\n                        image_list[index] = cv2.rotate(\r\n                            image_list[index].astype(np.float32), 1)\r\n                        #image_list[index] = np.astype(np.uint8)\r\n\t\t\timage_list[index] = image_list[index].astype(np.uint8)\r\n```",
        "state": "open",
        "user": "zhouyiminga",
        "closed_by": null,
        "created_at": "2023-09-04T12:31:34+00:00",
        "updated_at": "2023-10-07T07:04:29+00:00",
        "closed_at": null,
        "comments_count": [
            "dengmingD",
            "zhouyiminga"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2195,
        "title": "python推理部署怎么处理fd.vision.ocr.PPStructureV2Table.predict(im)的result？",
        "body": "使用python example代码可以正常推理\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26252750/fd338880-34db-4ae0-a4cb-92fef27f9c79)\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26252750/b851f7fe-1222-423b-9e18-ca502a7db735)\r\n但是结果result我将如何处理成可以操作的数据结构呢？\r\npython下，result是<class 'fastdeploy.libs.fastdeploy_main.vision.OCRResult'>对象，属性只有ocr的一些结果\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26252750/7608c270-3b2f-40aa-9031-8beefc6c1f51)\r\n但是我查看c++源码，OCRResult结构体是有table_html, table_structure属性的\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26252750/4e2b0272-b19c-4d5a-b872-3face42fc90d)\r\n但是在python下无法访问这些属性\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/26252750/c44e5946-5a93-4deb-8ecc-1bd3fdff95c8)\r\n所以我应该如何处理fd.vision.ocr.PPStructureV2Table.predict(im)返回的结果？拿到表格结构？\r\n\r\n",
        "state": "open",
        "user": "gl94",
        "closed_by": null,
        "created_at": "2023-09-07T03:33:28+00:00",
        "updated_at": "2023-12-27T06:00:07+00:00",
        "closed_at": null,
        "comments_count": [
            "gl94",
            "siddhawan",
            "gl94",
            "zxx9407",
            "gl94"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2193,
        "title": "服务器docker部署运行部署示例yoloe error: creating server: Internal - failed to load all models",
        "body": "root@ubuntu:/serving/examples/vision/detection/paddledetection/serving# fastdeployserver --model-repository=./models/\r\nI0906 03:13:30.949881 244 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3070\r\nI0906 03:13:31.064709 244 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f4f02000000' with size 268435456\r\nI0906 03:13:31.064851 244 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0906 03:13:31.066167 244 model_repository_manager.cc:1022] loading: postprocess:1\r\nI0906 03:13:31.166556 244 model_repository_manager.cc:1022] loading: preprocess:1\r\nI0906 03:13:31.172910 244 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: postprocess_0 (CPU device 0)\r\nI0906 03:13:31.266781 244 model_repository_manager.cc:1022] loading: runtime:1\r\nmodel_config: {'name': 'postprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 0, 'input': [{'name': 'post_input1', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NONE', 'dims': [-1, 6], 'is_shape_tensor': False, 'allow_ragged_batch': False}, {'name': 'post_input2', 'data_type': 'TYPE_INT32', 'format': 'FORMAT_NONE', 'dims': [-1], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'post_output', 'data_type': 'TYPE_STRING', 'dims': [-1], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'postprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npostprocess input names: ['post_input1', 'post_input2']\r\npostprocess output names: ['post_output']\r\nI0906 03:13:32.080263 244 model_repository_manager.cc:1183] successfully loaded 'postprocess' version 1\r\nI0906 03:13:32.080383 244 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: preprocess_0 (CPU device 0)\r\nmodel_config: {'name': 'preprocess', 'platform': '', 'backend': 'python', 'version_policy': {'latest': {'num_versions': 1}}, 'max_batch_size': 0, 'input': [{'name': 'preprocess_input', 'data_type': 'TYPE_UINT8', 'format': 'FORMAT_NONE', 'dims': [-1, -1, -1, 3], 'is_shape_tensor': False, 'allow_ragged_batch': False}], 'output': [{'name': 'preprocess_output1', 'data_type': 'TYPE_FP32', 'dims': [-1, 3, -1, -1], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'preprocess_output2', 'data_type': 'TYPE_FP32', 'dims': [-1, 2], 'label_filename': '', 'is_shape_tensor': False}, {'name': 'preprocess_output3', 'data_type': 'TYPE_FP32', 'dims': [-1, 2], 'label_filename': '', 'is_shape_tensor': False}], 'batch_input': [], 'batch_output': [], 'optimization': {'priority': 'PRIORITY_DEFAULT', 'input_pinned_memory': {'enable': True}, 'output_pinned_memory': {'enable': True}, 'gather_kernel_buffer_threshold': 0, 'eager_batching': False}, 'instance_group': [{'name': 'preprocess_0', 'kind': 'KIND_CPU', 'count': 1, 'gpus': [], 'secondary_devices': [], 'profile': [], 'passive': False, 'host_policy': ''}], 'default_model_filename': '', 'cc_model_filenames': {}, 'metric_tags': {}, 'parameters': {}, 'model_warmup': []}\r\npreprocess input names: ['preprocess_input']\r\npreprocess output names: ['preprocess_output1', 'preprocess_output2', 'preprocess_output3']\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\nI0906 03:13:33.008112 244 model_repository_manager.cc:1183] successfully loaded 'preprocess' version 1\r\nE0906 03:13:33.008500 244 model_repository_manager.cc:1186] failed to load 'runtime' version 1: Not found: unable to load backend library: libopencv_video.so.3.4: cannot open shared object file: No such file or directory\r\nE0906 03:13:33.008573 244 model_repository_manager.cc:1375] Invalid argument: ensemble 'ppdet' depends on 'runtime' which has no loaded version\r\nI0906 03:13:33.008606 244 server.cc:522]\r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0906 03:13:33.008621 244 server.cc:549]\r\n+---------+-------------------------------------------------------+--------+\r\n| Backend | Path                                                  | Config |\r\n+---------+-------------------------------------------------------+--------+\r\n| python  | /opt/tritonserver/backends/python/libtriton_python.so | {}     |\r\n+---------+-------------------------------------------------------+--------+\r\n\r\nI0906 03:13:33.008653 244 server.cc:592]\r\n+-------------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n| Model       | Version | Status                                                                                                                            |\r\n+-------------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n| postprocess | 1       | READY                                                                                                                             |\r\n| preprocess  | 1       | READY                                                                                                                             |\r\n| runtime     | 1       | UNAVAILABLE: Not found: unable to load backend library: libopencv_video.so.3.4: cannot open shared object file: No such file or d |\r\n|             |         | irectory                                                                                                                          |\r\n+-------------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0906 03:13:33.008719 244 tritonserver.cc:1920]\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                    |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                   |\r\n| server_version                   | 2.15.0                                                                                                                   |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_ |\r\n|                                  | shared_memory cuda_shared_memory binary_tensor_data statistics                                                           |\r\n| model_repository_path[0]         | ./models/                                                                                                                |\r\n| model_control_mode               | MODE_NONE                                                                                                                |\r\n| strict_model_config              | 1                                                                                                                        |\r\n| rate_limit                       | OFF                                                                                                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                 |\r\n| response_cache_byte_size         | 0                                                                                                                        |\r\n| min_supported_compute_capability | 6.0                                                                                                                      |\r\n| strict_readiness                 | 1                                                                                                                        |\r\n| exit_timeout                     | 30                                                                                                                       |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0906 03:13:33.008740 244 server.cc:252] Waiting for in-flight requests to complete.\r\nI0906 03:13:33.008746 244 model_repository_manager.cc:1055] unloading: preprocess:1\r\nI0906 03:13:33.008765 244 model_repository_manager.cc:1055] unloading: postprocess:1\r\nI0906 03:13:33.008782 244 server.cc:267] Timeout 30: Found 2 live models and 0 in-flight non-inference requests\r\nI0906 03:13:34.008874 244 server.cc:267] Timeout 29: Found 2 live models and 0 in-flight non-inference requests\r\nCleaning up...\r\nCleaning up...\r\nI0906 03:13:34.132314 244 model_repository_manager.cc:1166] successfully unloaded 'preprocess' version 1\r\nI0906 03:13:34.132739 244 model_repository_manager.cc:1166] successfully unloaded 'postprocess' version 1\r\nI0906 03:13:35.009028 244 server.cc:267] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n",
        "state": "closed",
        "user": "liyunfei0411",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-06T03:26:59+00:00",
        "updated_at": "2025-01-28T06:40:14+00:00",
        "closed_at": "2025-01-28T06:40:14+00:00",
        "comments_count": [
            "liyunfei0411",
            "liyunfei0411",
            "NiZerin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2194,
        "title": "docker部署长时间运行内存泄露，导致容器卡死了",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：\r\n-  fastdeploy-linux-cpu-1.0.7,\r\n- 自行编译升级tritonserver版本22.12，\r\n- docker部署\r\n- 使用ppocr-v3模型\r\n- openvino推理\r\n- 【编译命令】\r\n-  cmake .. \\\r\n  -DCMAKE_BUILD_TYPE=Release \\\r\n  -DENABLE_TRT_BACKEND=OFF \\\r\n  -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy_install \\\r\n  -DWITH_GPU=OFF \\\r\n  -DENABLE_PADDLE_BACKEND=ON \\\r\n  -DENABLE_ORT_BACKEND=ON \\\r\n  -DENABLE_OPENVINO_BACKEND=ON \\\r\n  -DENABLE_VISION=ON \\\r\n  -DBUILD_FASTDEPLOY_PYTHON=OFF \\\r\n  -DENABLE_PADDLE2ONNX=ON \\\r\n  -DENABLE_TEXT=OFF \\\r\n  -DLIBRARY_NAME=fastdeploy_runtime\r\nmake -j8\r\nmake install\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： intel I7-9700\r\n- 【编译语言】： C++ / Python(3.10）\r\n\r\n## 问题日志及出现问题的操作流程\r\n    官方例子可以跑通、多张图片也没问题，持续调用GRPC接口，内存不释放，导致容器卡死\r\n- 【性能问题】\r\n<img width=\"1247\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/16932704/dfd609f9-f00c-4105-ac11-e1e5f645cfb6\">\r\n\r\n2023-09-05T01:55:36Z I 1 fastdeploy_runtime.cc:1489] model rec_runtime, instance rec_runtime_0, executing 1 requests\r\n2023-09-05T01:55:36Z I 1 python_be.cc:1092] model rec_postprocess, instance rec_postprocess_0_0, executing 1 requests\r\n2023-09-05T01:55:36Z I 1 fastdeploy_runtime.cc:968] TRITONBACKEND_ModelExecute: Running rec_runtime_0 with 1 requests\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: softmax_5.tmp_0, type: FP32, shape: [13,67,6625]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 23081500, addr 0x557460bcb180\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: softmax_5.tmp_0, size 23081500, addr 0x557460bcb180, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: POST_OUTPUT_0, type: BYTES, shape: [17]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 643, addr 0x7f7828047bf0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: POST_OUTPUT_0, size 643, addr 0x7f7828047bf0, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: POST_OUTPUT_1, type: FP64, shape: [17]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 136, addr 0x7f7828046ef0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: POST_OUTPUT_1, size 136, addr 0x7f7828046ef0, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 643, addr 0x7f7828047bf0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 136, addr 0x7f7828046ef0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:141] add response output: output: rec_scores, type: FP64, shape: [17]\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:141] add response output: output: rec_texts, type: BYTES, shape: [17]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f7828046ef0\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f7828047bf0\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f73fb9f4010\r\n2023-09-05T01:55:37Z I 1 python_be.cc:1978] TRITONBACKEND_ModelInstanceExecute: model instance name rec_postprocess_0_0 released 1 requests\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: POST_OUTPUT_0, type: BYTES, shape: [1,17]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 643, addr 0x7f77e0031040\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: POST_OUTPUT_0, size 643, addr 0x7f77e0031040, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: POST_OUTPUT_1, type: FP64, shape: [1,17]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 136, addr 0x7f771c134560\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: POST_OUTPUT_1, size 136, addr 0x7f771c134560, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:167] add response output: output: POST_OUTPUT_2, type: INT64, shape: [1,17,8]\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 1088, addr 0x7f7764032bc0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:540] Internal response allocation: POST_OUTPUT_2, size 1088, addr 0x7f7764032bc0, memory type 0, type id 0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 643, addr 0x7f77e0031040\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 136, addr 0x7f771c134560\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 1088, addr 0x7f7764032bc0\r\n2023-09-05T01:55:37Z I 1 infer_response.cc:141] add response output: output: rec_texts, type: BYTES, shape: [1,17]\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:2800] GRPC: using buffer for 'rec_texts', size: 643, addr: 0x7f776402a8b0\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f77e0031040\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:3964] ModelInferHandler::InferResponseComplete, 0 step ISSUED\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:2920] GRPC free: size 643, addr 0x7f776402a8b0\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f771c134560\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f7764032bc0\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f77d45a6840\r\n2023-09-05T01:55:37Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f7462f4a670\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:3518] ModelInferHandler::InferRequestComplete\r\n2023-09-05T01:55:37Z I 1 python_be.cc:1978] TRITONBACKEND_ModelInstanceExecute: model instance name det_postprocess_0_0 released 1 requests\r\n2023-09-05T01:55:37Z I 1 python_be.cc:1092] model det_postprocess, instance det_postprocess_0_0, executing 1 requests\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:3800] Process for ModelInferHandler, rpc_ok=1, 0 step COMPLETE\r\n2023-09-05T01:55:37Z I 1 grpc_server.cc:2710] Done for ModelInferHandler, 0\r\n2023-09-05T01:55:37Z I 1 ensemble_scheduler.cc:555] Internal response release: size 23081500, addr 0x557460bcb180\r\n2023-09-05T01:55:37Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x557459cfbd00] request id: , model: rec_postprocess, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 13, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n\"fastdeploy.log\" 16528084L, 1310757181C\r\ninputs:\r\n[0x0x7f4c70007a78] input: x, type: FP32, original shape: [98,3,48,192], batch + shape: [98,3,48,192], shape: [3,48,192]\r\noriginal requested outputs:\r\nsoftmax_0.tmp_0\r\nrequested outputs:\r\nsoftmax_0.tmp_0\r\n\r\n2023-09-06T03:53:24Z I 1 fastdeploy_runtime.cc:1489] model cls_runtime, instance cls_runtime_0, executing 1 requests\r\n2023-09-06T03:53:24Z I 1 fastdeploy_runtime.cc:968] TRITONBACKEND_ModelExecute: Running cls_runtime_0 with 1 requests\r\n2023-09-06T03:53:24Z I 1 grpc_server.cc:3800] Process for ModelInferHandler, rpc_ok=1, 0 step START\r\n2023-09-06T03:53:24Z I 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4ce8496bf0] request id: , model: pp_ocr, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f4ce80024b8] input: INPUT, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f4ce80024b8] input: INPUT, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\noriginal requested outputs:\r\nrec_texts\r\nrequested outputs:\r\nrec_texts\r\n\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4ce800c4f0] request id: , model: det_preprocess, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f4ce80096f8] input: INPUT_0, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f4ce80096f8] input: INPUT_0, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\noriginal requested outputs:\r\nOUTPUT_0\r\nOUTPUT_1\r\nrequested outputs:\r\nOUTPUT_0\r\nOUTPUT_1\r\n\r\n2023-09-06T03:53:24Z I 1 python_be.cc:1092] model det_preprocess, instance det_preprocess_0_0, executing 1 requests\r\n2023-09-06T03:53:24Z I 1 infer_response.cc:167] add response output: output: OUTPUT_0, type: FP32, shape: [1,3,736,960]\r\n2023-09-06T03:53:24Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 8478720, addr 0x7f493520a980\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:540] Internal response allocation: OUTPUT_0, size 8478720, addr 0x7f493520a980, memory type 0, type id 0\r\n2023-09-06T03:53:24Z I 1 infer_response.cc:167] add response output: output: OUTPUT_1, type: INT32, shape: [1,4]\r\n2023-09-06T03:53:24Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 16, addr 0x7f4b1b526ff0\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:540] Internal response allocation: OUTPUT_1, size 16, addr 0x7f4b1b526ff0, memory type 0, type id 0\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:555] Internal response release: size 8478720, addr 0x7f493520a980\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:555] Internal response release: size 16, addr 0x7f4b1b526ff0\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4cec01d190] request id: , model: det_runtime, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f4cef1ac878] input: x, type: FP32, original shape: [1,3,736,960], batch + shape: [1,3,736,960], shape: [3,736,960]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f4cef1ac878] input: x, type: FP32, original shape: [1,3,736,960], batch + shape: [1,3,736,960], shape: [3,736,960]\r\noriginal requested outputs:\r\nsigmoid_0.tmp_0\r\nrequested outputs:\r\nsigmoid_0.tmp_0\r\n\r\n2023-09-06T03:53:24Z I 1 python_be.cc:1978] TRITONBACKEND_ModelInstanceExecute: model instance name det_preprocess_0_0 released 1 requests\r\n2023-09-06T03:53:24Z I 1 fastdeploy_runtime.cc:1489] model det_runtime, instance det_runtime_0, executing 1 requests\r\n2023-09-06T03:53:24Z I 1 fastdeploy_runtime.cc:968] TRITONBACKEND_ModelExecute: Running det_runtime_0 with 1 requests\r\n2023-09-06T03:53:24Z I 1 infer_response.cc:167] add response output: output: softmax_0.tmp_0, type: FP32, shape: [98,2]\r\n2023-09-06T03:53:24Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 784, addr 0x7f4c200415c0\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:540] Internal response allocation: softmax_0.tmp_0, size 784, addr 0x7f4c200415c0, memory type 0, type id 0\r\n2023-09-06T03:53:24Z I 1 ensemble_scheduler.cc:555] Internal response release: size 784, addr 0x7f4c200415c0\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4c2c0043a0] request id: , model: cls_postprocess, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 98, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4a3a8e1970] request id: , model: det_postprocess, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f4c5c00be78] input: ORI_IMG, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\n[0x0x7f4c5c008538] input: POST_INPUT_1, type: INT32, original shape: [1,4], batch + shape: [1,4], shape: [4]\r\n[0x0x7f4cec064b08] input: POST_INPUT_0, type: FP32, original shape: [1,1,736,960], batch + shape: [1,1,736,960], shape: [1,736,960]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f4cec064b08] input: POST_INPUT_0, type: FP32, original shape: [1,1,736,960], batch + shape: [1,1,736,960], shape: [1,736,960]\r\n[0x0x7f4c5c008538] input: POST_INPUT_1, type: INT32, original shape: [1,4], batch + shape: [1,4], shape: [4]\r\n[0x0x7f4c5c00be78] input: ORI_IMG, type: UINT8, original shape: [1,823,1058,3], batch + shape: [1,823,1058,3], shape: [823,1058,3]\r\noriginal requested outputs:\r\nPOST_OUTPUT_0\r\nPOST_OUTPUT_1\r\nPOST_OUTPUT_2\r\nrequested outputs:\r\nPOST_OUTPUT_0\r\nPOST_OUTPUT_1\r\nPOST_OUTPUT_2\r\n\r\n2023-09-06T03:53:24Z I 1 pinned_memory_manager.cc:190] non-pinned memory deallocation: addr 0x7f493520a980\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4adc0c2f80] request id: , model: rec_pp, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 98, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f496f023178] input: x, type: FP32, original shape: [98,3,48,2799], batch + shape: [98,3,48,2799], shape: [3,48,2799]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f496f023178] input: x, type: FP32, original shape: [98,3,48,2799], batch + shape: [98,3,48,2799], shape: [3,48,2799]\r\noriginal requested outputs:\r\nrec_scores\r\nrec_texts\r\nrequested outputs:\r\nrec_scores\r\nrec_texts\r\n\r\n2023-09-06T03:53:24Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4986e3f6c0] request id: , model: rec_runtime, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 98, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f46d9b0d028] input: x, type: FP32, original shape: [98,3,48,2799], batch + shape: [98,3,48,2799], shape: [3,48,2799]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f46d9b0d028] input: x, type: FP32, original shape: [98,3,48,2799], batch + shape: [98,3,48,2799], shape: [3,48,2799]\r\noriginal requested outputs:\r\nsoftmax_5.tmp_0\r\nrequested outputs:\r\nsoftmax_5.tmp_0\r\n\r\n2023-09-06T03:53:28Z I 1 infer_response.cc:167] add response output: output: softmax_5.tmp_0, type: FP32, shape: [98,350,6625]\r\n2023-09-06T03:53:28Z I 1 pinned_memory_manager.cc:161] non-pinned memory allocation: size 908950000, addr 0x7f464d212010\r\n2023-09-06T03:53:28Z I 1 ensemble_scheduler.cc:540] Internal response allocation: softmax_5.tmp_0, size 908950000, addr 0x7f464d212010, memory type 0, type id 0\r\n2023-09-06T03:53:28Z I 1 ensemble_scheduler.cc:555] Internal response release: size 908950000, addr 0x7f464d212010\r\n2023-09-06T03:53:28Z I 1 infer_request.cc:729] [request id: <id_unknown>] prepared: [0x0x7f4c20041320] request id: , model: rec_postprocess, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 98, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f4c20031468] input: POST_INPUT_0, type: FP32, original shape: [98,350,6625], batch + shape: [98,350,6625], shape: [350,6625]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f4c20031468] input: POST_INPUT_0, type: FP32, original shape: [98,350,6625], batch + shape: [98,350,6625], shape: [350,6625]\r\noriginal requested outputs:\r\nPOST_OUTPUT_0\r\nPOST_OUTPUT_1\r\nrequested outputs:\r\nPOST_OUTPUT_0\r\nPOST_OUTPUT_1\r\n\r\n2023-09-06T03:53:28Z I 1 fastdeploy_runtime.cc:1489] model rec_runtime, instance rec_runtime_0, executing 1 requests\r\n2023-09-06T03:53:28Z I 1 fastdeploy_runtime.cc:968] TRITONBACKEND_ModelExecute: Running rec_runtime_0 with 1 requests\r\n2023-09-06T03:53:28Z I 1 python_be.cc:1092] model rec_postprocess, instance rec_postprocess_0_0, executing 1 requests\r\n",
        "state": "closed",
        "user": "polarisunny",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-06T03:36:45+00:00",
        "updated_at": "2024-09-24T06:41:12+00:00",
        "closed_at": "2024-09-24T06:41:11+00:00",
        "comments_count": [
            "fastislow"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2199,
        "title": "请问从哪里可以找到关于tracking的c++ api文档",
        "body": "请问，我在以下地址中：https://baidu-paddle.github.io/fastdeploy-api/cpp/html/ 找不到关于tracking的任何api文档，请问对应的文档地址是哪里？谢谢",
        "state": "closed",
        "user": "HGD-ai",
        "closed_by": "HGD-ai",
        "created_at": "2023-09-13T01:44:05+00:00",
        "updated_at": "2023-09-13T01:55:20+00:00",
        "closed_at": "2023-09-13T01:55:20+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2197,
        "title": "使用ppyoloe进行多线程推理时使用gpu报错",
        "body": "仿照multi_thread使用ppyoloe进行多线程推理时，使用trtbackend正常，但是使用gpu时出现断错误，定位发现是\r\n option.UseGpu();和 option.UsePaddleBackend();不能同时使用，请问这是什么原因呢？非常感谢你的回复。\r\n部分代码如下：\r\n  auto option =  fastdeploy::RuntimeOption();\r\n  option.UseGpu();\r\n  option.UsePaddleBackend();\r\n",
        "state": "closed",
        "user": "HGD-ai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-12T07:27:13+00:00",
        "updated_at": "2025-02-11T06:43:49+00:00",
        "closed_at": "2025-02-11T06:43:49+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2196,
        "title": "C API什么时候增加模型尺寸设置啊",
        "body": "现在没有找到关于C API来设置模型尺寸的接口，官方有计划吗？这是刚需吧？",
        "state": "closed",
        "user": "imfe888",
        "closed_by": "imfe888",
        "created_at": "2023-09-08T04:15:44+00:00",
        "updated_at": "2023-09-11T08:23:00+00:00",
        "closed_at": "2023-09-11T08:22:24+00:00",
        "comments_count": [
            "imfe888"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2198,
        "title": "请问从哪里可以找到关于tracking的c++ api文档",
        "body": "请问，我在以下地址中：https://baidu-paddle.github.io/fastdeploy-api/cpp/html/ 找不到关于tracking的任何api文档，请问对应的文档地址是哪里？谢谢",
        "state": "closed",
        "user": "HGD-ai",
        "closed_by": "HGD-ai",
        "created_at": "2023-09-13T01:43:14+00:00",
        "updated_at": "2023-09-14T01:02:02+00:00",
        "closed_at": "2023-09-14T01:02:02+00:00",
        "comments_count": [
            "jiangjiajun",
            "HGD-ai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2200,
        "title": "使用GPU推理内存显存泄露",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-gpu-1.0.7 官方C++ SDK\r\n- 【编译命令】/\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： Nvidia GPU 1660s， CUDA 11.2 cuDNN8.4  Tensorrt8.4.1.5\r\n- 【编译语言】： C++\r\n- 【内存泄露】： 使用CPU推理端内存可以释放，但是GPU的无法完全释放内存和显存。\r\n- 【使用模型】：为官方指定：\r\n- 下载PPYOLOE模型文件和测试图片\r\nwget https://bj.bcebos.com/paddlehub/fastdeploy/ppyoloe_crn_l_300e_coco.tgz\r\nwget https://gitee.com/paddlepaddle/PaddleDetection/raw/release/2.4/demo/000000014439.jpg\r\n\r\n- 【代码如下】：_\r\n\r\n```\r\n#include \"fastdeploy/vision.h\"\r\n#include <iostream>\r\n#include <chrono>\r\n#include <thread>\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\nvoid CpuInfer(const std::string& model_dir, const std::string& image_file) {\r\n\tstd::cout << \"Waiting for 5 seconds...\" << std::endl;\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(5)); \r\n\tstd::cout << \"Done!\" << std::endl;\r\n\tstd::cout << model_dir << typeid(model_dir).name() << std::endl;\r\n\tauto model_file = model_dir + sep + \"model.pdmodel\";\r\n\tauto params_file = model_dir + sep + \"model.pdiparams\";\r\n\tauto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n\tauto option = fastdeploy::RuntimeOption();\r\n\toption.UseCpu();\r\n\t//option.UseGpu();\r\n\toption.UseOpenVINOBackend();\r\n\t//option.UseOrtBackend();\r\n\t//option.UseTrtBackend();\r\n\tstd::shared_ptr<fastdeploy::vision::detection::PPYOLOE> model = std::make_shared < fastdeploy::vision::detection::PPYOLOE>(model_file, params_file,\r\n\t\tconfig_file, option);\r\n\t/* auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file, config_file, option);*/\r\n\tif (!model->Initialized()) {\r\n\t\tstd::cerr << \"Failed to initialize.\" << std::endl;\r\n\t\treturn;\r\n\t}\r\n\r\n\tauto im = cv::imread(image_file);\r\n\tfastdeploy::vision::DetectionResult res;\r\n\r\n\tfor (int i = 0; i < 30; i++)\r\n\t\tif (!model->Predict(im, &res)) {\r\n\t\t\tstd::cerr << \"Failed to predict.\" << std::endl;\r\n\t\t\treturn;\r\n\t\t}\r\n\tstd::cout << \"delete!\" << std::endl;\r\n\tmodel.reset();\r\n\tmodel = nullptr;\r\n\tstd::cout << \"delete Done!\" << std::endl;\r\n\r\n\tstd::cout << \"Waiting for 5 seconds...\" << std::endl;\r\n        // 在此处cpu推理内存可以释放，gpu推理不能完全释放。\r\n\tstd::this_thread::sleep_for(std::chrono::seconds(5)); \r\n\tstd::cout << \"Done!\" << std::endl;\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n\tif (argc < 4) {\r\n\t\tstd::cout\r\n\t\t\t<< \"Usage: infer_demo path/to/model_dir path/to/image run_option, \"\r\n\t\t\t\"e.g ./infer_model ./ppyoloe_model_dir ./test.jpeg 0\"\r\n\t\t\t<< std::endl;\r\n\t\tstd::cout << \"The data type of run_option is int, 0: run with cpu; 1: run \"\r\n\t\t\t\"with gpu; 2: run with gpu and use tensorrt backend; 3: run with kunlunxin.\"\r\n\t\t\t<< std::endl;\r\n\t\treturn -1;\r\n\t}\r\n\tCpuInfer(argv[1], argv[2]);\r\n\treturn 0;\r\n}\r\n```",
        "state": "closed",
        "user": "YOU-007",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-13T13:39:02+00:00",
        "updated_at": "2024-09-17T06:42:45+00:00",
        "closed_at": "2024-09-17T06:42:45+00:00",
        "comments_count": [
            "jiangjiajun",
            "SchrodingerLLX"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2201,
        "title": "在jetson中编译c++版本，下载third_libs出错",
        "body": "\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/26866665/1b49d701-7445-4882-8dfa-42bf9cbefba1)\r\n\r\n这俩文件有离线的么",
        "state": "closed",
        "user": "kankanjiuzou123",
        "closed_by": "jiangjiajun",
        "created_at": "2023-09-14T02:36:07+00:00",
        "updated_at": "2024-08-29T08:00:48+00:00",
        "closed_at": "2023-09-15T06:48:07+00:00",
        "comments_count": [
            "jiangjiajun",
            "kankanjiuzou123",
            "xiaohongri",
            "xiaohongri"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2202,
        "title": "基于gpu的分类推理结果错误，基于CPU正确",
        "body": "./infer_demo  /data/PaddleClas/deploy/models/inference /data/infer_demo/build/test_det.jpg 0\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nClassifyResult(\r\nlabel_ids: 0, \r\nscores: 0.881795, \r\n)\r\n\r\n./infer_demo  /data/PaddleClas/deploy/models/inference /data/infer_demo/build/test_det.jpg 4\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\nClassifyResult(\r\nlabel_ids: 1, \r\nscores: 0.505666, \r\n)\r\n\r\n",
        "state": "closed",
        "user": "YasinFu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-14T09:38:24+00:00",
        "updated_at": "2025-02-11T06:43:50+00:00",
        "closed_at": "2025-02-11T06:43:50+00:00",
        "comments_count": [
            "jiangjiajun",
            "ChenjieXu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2203,
        "title": "自行编译的FastDeploy，部署NVIDIA Gpu PPOCR时只有cuda11.2版本能运行，但仍然有报错",
        "body": "## 环境\r\n- **【编译命令】**：自行编译的FastDeploy，编译方式使用vs2019(x64 Native Tools Command Prompt for VS 2019)\r\n**GPU版**\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64  -DENABLE_PADDLE_BACKEND=ON \r\n                                                                      -DENABLE_VISION=ON \r\n                                                                      -DENABLE_TEXT=ON \r\n                                                                      -DWITH_GPU=ON \r\n                                                                      -DCUDA_DIRECTORY=\"D:\\cuda\" \r\n                                                                      -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy_cuda11.2\"\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\nmsbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64\r\n**CPU版**\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64  -DENABLE_OPENVINO_BACKEND=ON \r\n                                                                      -DENABLE_VISION=ON \r\n                                                                      -DENABLE_TEXT=ON                                                             \r\n                                                                      -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy_cpu\"\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\nmsbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64\r\n- **【系统平台】**：Windows10 x64 Intel(R) Xeon(R) Silver 4110 CPU\r\n- **【硬件】**：Nvidia GPU RTX 2070， **CUDA 11.2.0， CUDNN 8.2.0**  ，**最高cuda版本支持12.2**\r\n- **【编译语言】**： C++ \r\n## 推理\r\n- **【推理代码】**\r\n[https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/cpu-gpu/cpp/infer.cc](url)\r\n**推理模型使用的是ch_PP-OCRv3系列**\r\n- **【CPU版推理】**：\r\n单张图片推理结果没有问题，使用opencvVINO推理框架，输入尺寸640*480，循环跑速度200ms左右\r\n- **【GPU版推理】**：\r\n**单张图片推理结果没有问题，但是有个CUDA释放错误(乱码是cmd编码问题，不影响CUDA报错)，使用飞浆推理框架，输入尺寸640*480，循环跑速度140ms左右**\r\n![out](https://github.com/PaddlePaddle/FastDeploy/assets/102496227/41ea1013-fa05-4765-9970-b113bc364443)\r\n- **【尝试解决】**\r\n用的是官方的代码，代码肯定没有问题，尝试其他cuda版本(cuda11.5.2，cudnn8.3.3)(cuda11.6.0,cudnn8.4.0),在运行demo时都出现另一个错误，程序跑不起来，忘了截图\r\n**Could not load library cudnn_cnn_infer64_8.dll.**\r\n我把cudnn_cnn_infer64_8.dll放到当前目录了不行，下载了zlibwapi.dll放在exe当前目录和配置环境变量不行，保持cuda版本不动使用了几个其他版本的cudnn_cnn_infer64_8.dll不行\r\n## 问题\r\n- **【问题请教】**\r\n三个问题请教大佬，折腾好几天没有解决\r\n1.(cuda11.2.0,cudnn8.2.0)的报错CUDA error(4), driver shutting down有什么思路吗\r\n2.(cuda11.5.2，cudnn8.3.3)(cuda11.6.0,cudnn8.4.0)的报错Could not load library cudnn_cnn_infer64_8.dll.有什么思路吗\r\n3.GPU版demo发送到另外一台机子跑，速度变成了500ms，他的显卡是1070，是显卡的原因吗\r\n",
        "state": "open",
        "user": "Y-huange",
        "closed_by": null,
        "created_at": "2023-09-15T06:34:59+00:00",
        "updated_at": "2024-02-06T08:51:39+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "Y-huange"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2207,
        "title": "如何在golang中解析FD_C_OCRResult类型？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-x64-gpu-1.0.7\r\n- 【系统平台】: Linux x64(Ubuntu 10.04)\r\n- 【硬件】： Nvidia GPU 1060TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Golang(1.19)\r\n\r\n## 问题日志及出现问题的操作流程\r\n```golang\r\nvar image C.FD_C_Mat = C.FD_C_Imread(imageFile)\r\n\r\nvar result *C.FD_C_OCRResult = C.FD_C_CreateOCRResult()\r\n\r\n// 开始预测，判断返回值是否正常\r\nif !FDBooleanToGo(C.FD_C_PPOCRv3WrapperPredict(ppocrV3, image, result)) {\r\n\tfmt.Printf(\"Failed to predict.\\n\")\r\n}\r\n```\r\n我怎么才能在golang中解析result拿到Texts, boxes, cls_score, rec_score等信息呢？\r\n",
        "state": "closed",
        "user": "zhepoch",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-22T04:30:52+00:00",
        "updated_at": "2025-02-11T06:43:54+00:00",
        "closed_at": "2025-02-11T06:43:53+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2204,
        "title": "fastdeploy server不支持paddleocr v4版本",
        "body": "直接将paddleocr v4版本的模型文件替换进去，无法正常启动，大概什么什么时候能支持该版本的使用？",
        "state": "closed",
        "user": "bltcn",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-18T06:52:57+00:00",
        "updated_at": "2025-02-11T06:43:51+00:00",
        "closed_at": "2025-02-11T06:43:51+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2205,
        "title": "请问有没有测试用例可以测试算子在CPU和TPU上的表现？",
        "body": "如题。",
        "state": "closed",
        "user": "ijiami-01",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-19T06:32:47+00:00",
        "updated_at": "2025-02-11T06:43:52+00:00",
        "closed_at": "2025-02-11T06:43:52+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2206,
        "title": "fastdeploy不支持量化模型吗",
        "body": "使用V3量化模型报错，推理使用paddle、openvino、onxx都不行\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/16932704/5ed258ca-3599-4d34-a3fc-ff3973ca43ec)\r\n",
        "state": "closed",
        "user": "polarisunny",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-21T02:48:54+00:00",
        "updated_at": "2025-02-11T06:43:53+00:00",
        "closed_at": "2025-02-11T06:43:53+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2208,
        "title": " No module named paddle_serving_client.convert",
        "body": "inference model转serving model的过程中，报错： No module named paddle_serving_client.convert\r\n",
        "state": "closed",
        "user": "lx1054331851",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-23T07:22:41+00:00",
        "updated_at": "2024-10-29T06:43:37+00:00",
        "closed_at": "2024-10-29T06:43:37+00:00",
        "comments_count": [
            "zhangjin2233"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2210,
        "title": "fastdeploy make error : SyntaxError: invalid syntax",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy-release-1.0.6    (paddle_inference_install_dir--4.6 or paddle_inference_install_dir--5.0.2)\r\n- 【编译命令】\r\ncd FastDeploy-release-1.0.6\r\nmkdir build && cd build\r\ncmake .. -DBUILD_ON_JETSON=ON\r\nDENABLE_VISION=ON\r\nDENABLE_PADDLE_BACKEND=ON\r\nDPADDLEINFERENCE_DIRECTORY=../paddle_inference_install_dir\r\nDCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\nmake -j6\r\n\r\n\r\n- 【系统平台】: NX Linux aarch64(Ubuntu 20.04)\r\n- 【硬件】：  jetpack5.1.2, CUDA 11.4.315, CUDNN 8.6.0.166, tensorrt 5.1.2, opencv4.5.4(cuda no), python 3.8.10\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【\"make -j6\"跑不通】\r\n/报错内容如下\r\n\r\ntest@tegra-ubuntu:~/Downloads/FastDeploy-release-1.0.6/build$ make -j8\r\nScanning dependencies of target extern_onnxruntime\r\n[  0%] Running cpp protocol buffer compiler on p2o_paddle.proto\r\nScanning dependencies of target yaml-cpp\r\n[  0%] Creating directories for 'extern_onnxruntime'\r\nScanning dependencies of target gen_onnx_proto\r\nScanning dependencies of target onnxifi_dummy\r\nScanning dependencies of target onnxifi_loader\r\n[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o\r\n[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_loader.dir/onnx/onnxifi_loader.c.o\r\n[  1%] Running gen_proto.py on onnx/onnx.in.proto\r\n  File \"/home/test/Downloads/FastDeploy-release-1.0.6/third_party/onnx/onnx/gen_proto.py\", line 36\r\n    def process_ifs(lines: Iterable[Text], onnx_ml: bool) -> Iterable[Text]:\r\n                         ^\r\nSyntaxError: invalid syntax\r\nmake[2]: *** [third_party/onnx/CMakeFiles/gen_onnx_proto.dir/build.make:69: third_party/onnx/onnx/onnx_paddle2onnx-ml.proto] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:610: third_party/onnx/CMakeFiles/gen_onnx_proto.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\nScanning dependencies of target p2o_paddle_proto\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  2%] Linking C static library libonnxifi_loader.a\r\n[  2%] Performing download step (download, verify and extract) for 'extern_onnxruntime'\r\n[  2%] Linking C shared library libonnxifi_dummy.so\r\n[  2%] Building CXX object paddle2onnx/proto/CMakeFiles/p2o_paddle_proto.dir/p2o_paddle.pb.cc.o\r\n-- Downloading...\r\n   dst='/home/test/Downloads/FastDeploy-release-1.0.6/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/paddle2onnx/libs/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n[  2%] Built target onnxifi_loader\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  3%] Built target onnxifi_dummy\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n-- Downloading... done\r\n-- extracting...\r\n     src='/home/test/Downloads/FastDeploy-release-1.0.6/build/third_libs/onnxruntime/src/onnxruntime-linux-aarch64-1.12.0.tgz'\r\n     dst='/home/test/Downloads/FastDeploy-release-1.0.6/build/third_libs/onnxruntime/src/extern_onnxruntime'\r\n-- extracting... [tar xfz]\r\n-- extracting... [analysis]\r\n-- extracting... [rename]\r\n-- extracting... [clean up]\r\n-- extracting... done\r\n[  3%] No patch step for 'extern_onnxruntime'\r\n[  4%] No update step for 'extern_onnxruntime'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  4%] No configure step for 'extern_onnxruntime'\r\n[  4%] No build step for 'extern_onnxruntime'\r\n[  4%] Performing install step for 'extern_onnxruntime'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  4%] Completed 'extern_onnxruntime'\r\n[  4%] Built target extern_onnxruntime\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 10%] Linking CXX static library libyaml-cpp.a\r\n[ 10%] Built target yaml-cpp\r\n[ 10%] Linking CXX static library libp2o_paddle_proto.a\r\n[ 10%] Built target p2o_paddle_proto\r\nmake: *** [Makefile:152: all] Error 2\r\ntest@tegra-ubuntu:~/Downloads/FastDeploy-release-1.0.6/build$ \r\n\r\n\r\n",
        "state": "closed",
        "user": "catofyuanyuan",
        "closed_by": "rainyfly",
        "created_at": "2023-09-26T06:52:57+00:00",
        "updated_at": "2024-02-06T08:03:38+00:00",
        "closed_at": "2024-02-06T08:03:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2209,
        "title": "什么时候出pp-tinypos rv1126版本",
        "body": "等...",
        "state": "open",
        "user": "erroot",
        "closed_by": null,
        "created_at": "2023-09-24T07:10:48+00:00",
        "updated_at": "2023-09-29T11:02:17+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "erroot"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2214,
        "title": "ocr多语言检测openvino和onnx两个引擎在cpu上报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.7\r\n- 【编译命令】官方docker\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： Nvidia GPU 3070， CUDA 12.2 CUDNN 8.3\r\n- 【编译语言】： python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\nocr多语言检测openvino和onnx两个引擎在cpu上报错\r\n模型文件：\r\nhttps://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/Multilingual_PP-OCRv3_det_infer.tar\r\n使用测试代码\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/cpu-gpu/python/infer.py\r\n调用参数：\r\n--det_model Multilingual_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu --backend openvino\r\n报错：\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 321, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/root/.vscode-server/extensions/ms-python.python-2023.16.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python/infer.py\", line 211, in <module>\r\n    result = ppocr_v3.predict(im)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/vision/ocr/ppocr/__init__.py\", line 966, in predict\r\n    return self.system_.predict(input_image)\r\nRuntimeError: Can't set input blob with name: x, because model input (shape=[?,3,960,960]) and blob (shape=(1.3.960.672)) are incompatible",
        "state": "closed",
        "user": "bltcn",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-28T07:35:04+00:00",
        "updated_at": "2025-02-11T06:43:54+00:00",
        "closed_at": "2025-02-11T06:43:54+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2212,
        "title": "Cannot compile with openvino while in linux-aarch64 platform",
        "body": "\r\nver:fastdeploy-release-1.0.7\r\ncpu:飞腾 ft2000\r\n统统：uos\r\npython: 3.9\r\n编译：\r\ncmake .. -DENABLE_ORT_BACKEND=OFF \\\r\n-DENABLE_OPENVINO_BACKEND=ON \\\r\n-DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n-DENABLE_VISION=OFF \\\r\n-DENABLE_TEXT=OFF \\\r\n\r\n问题，编译时如果加下面这个就会报错：Cannot compile with openvino while in linux-aarch64 platform\r\n-DENABLE_OPENVINO_BACKEND=ON\r\n\r\n当然如果换成\r\n-DENABLE_PADDLE_BACKEND=ON，会报如下错误：\r\n  Paddle Backend doesn't support linux aarch64 now.\r\n\r\n如果上面两个选项OFF是可以编译通过的，但是预测速度非常慢（cpp/python）\r\n如果没有cpu加速，一个完整的ocr大约会在20s左右，同样的图片，有加速的（win）只要500ms\r\n希望官方支持支持openvino\r\n谢谢，祝国庆快乐\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "dengmingD",
        "closed_by": "rainyfly",
        "created_at": "2023-09-26T09:26:57+00:00",
        "updated_at": "2024-02-06T11:09:44+00:00",
        "closed_at": "2024-02-06T11:09:44+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2211,
        "title": "rk3568,rk3588 跑自己转换的rknn例子模型PP_TinyPose, picodet  中的例子 Invalid RKNN model version 6",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】交叉编译FastDeploy\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： rk3568,rk3588\r\n- 【编译语言】： C++ / Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n--demo 没有跑通 \r\n使用 转换好的开箱模型没有问题：wget https://bj.bcebos.com/paddlehub/fastdeploy/rknpu2/picodet_s_416_coco_lcnet.zip\r\n根据 https://gitee.com/paddlepaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rknpu2#/paddlepaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/rknpu2.md 文档\r\n自己转换模型报错  Invalid RKNN model version 6\r\n\r\ncpu测试中间onnx 模型文件是正常加载推理结果正常，因此推测是“ONNX模型转RKNN模型”出错。\r\npython tools/rknpu2/export.py --config_path tools/rknpu2/config/picodet_s_416_coco_lcnet_unquantized.yaml \\\r\n                              --target_platform rk3588\r\n 但是转换调试输出没有报错\r\nD RKNN: [12:36:04.936] --------------------------------------------------------------------------------------+---------------------------------\r\nD RKNN: [12:36:04.938] ----------------------------------------\r\nD RKNN: [12:36:04.938] Total Weight Memory Size: 2641920\r\nD RKNN: [12:36:04.938] Total Internal Memory Size: 6230016\r\nD RKNN: [12:36:04.938] Predict Internal Memory RW Amount: 175689408\r\nD RKNN: [12:36:04.938] Predict Weight Memory RW Amount: 2755696\r\nD RKNN: [12:36:04.938] ----------------------------------------\r\nD RKNN: [12:36:04.938] <<<<<<<< end: N4rknn21RKNNMemStatisticsPassE\r\nI rknn buiding done.\r\nW init_runtime: Target is None, use simulator!\r\nExport OK!\r\n\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/38774713/d960579a-6c7f-42a2-9dd9-08a9c65821e5)\r\n\r\n\r\nRK3568 ：\r\nroot@RK356X:/userdata/app_data/pp/TinyPos# ./infer_tinypose_demo PP_TinyPose_256x192_infer hrnet_demo.jpg \r\nPP_TinyPose_256x192_infer/PP_TinyPose_256x192_infer_rk3588_unquantized.rknn\r\nE RKNN: [15:17:54.643] 6, 1\r\nE RKNN: [15:17:54.643] Invalid RKNN model version 6\r\nE RKNN: [15:17:54.643] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(180)::LoadModel\tThe function(rknn_init) failed! ret=-6\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(123)::Init\tLoad model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(361)::CreateRKNPU2Backend\tFailed to initialize RKNPU2 backend.\r\nAborted\r\n\r\nRK3588:\r\nroot@firefly:/opt/pp/FastDeploy-develop/examples/vision/keypointdetection/tiny_pose/rknpu2/cpp/build# ./infer_tinypose_demo ./PP_TinyPose_256x192_infer ./hrnet_demo.jpg\r\n./PP_TinyPose_256x192_infer/PP_TinyPose_256x192_infer_rk3588_unquantized.rknn\r\nE RKNN: [10:30:34.599] 6, 1\r\nE RKNN: [10:30:34.600] Invalid RKNN model version 6\r\nE RKNN: [10:30:34.600] rknn_init, load model failed!\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(180)::LoadModel\tThe function(rknn_init) failed! ret=-6\r\n[ERROR] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(123)::Init\tLoad model failed\r\n[ERROR] fastdeploy/runtime/runtime.cc(361)::CreateRKNPU2Backend\tFailed to initialize RKNPU2 backend.\r\n",
        "state": "closed",
        "user": "erroot",
        "closed_by": "erroot",
        "created_at": "2023-09-26T07:40:23+00:00",
        "updated_at": "2024-12-10T10:24:32+00:00",
        "closed_at": "2023-10-25T09:50:27+00:00",
        "comments_count": [
            "erroot",
            "erroot",
            "Luo73",
            "ltj19900609",
            "133673",
            "APeiZou",
            "x53151231"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2213,
        "title": "MaskRCNN模型推理结果缺少Mask掩码图",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-1.0.7\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 3090TI， CUDA 11.4 CUDNN 8.3\r\n- 【编译语言】：Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【推理结果缺少mask掩码图】\r\n运行examples/vision/detection/paddledetection/python中的infer_mask_rcnn.py文件，最后的可视化输出没有掩码图，只有矩形框。我查找推理结果中的result.masks，发现所有masks的值都全部是0。\r\n",
        "state": "open",
        "user": "qianbin1989228",
        "closed_by": null,
        "created_at": "2023-09-26T13:58:28+00:00",
        "updated_at": "2024-01-29T06:45:46+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "David-dotcom666",
            "qianbin1989228"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2215,
        "title": "FastDeploy在Atlas 200I DK A2上交叉编译出现的问题？",
        "body": "## 环境\r\n- 【FastDeploy版本】： FastDeploy-1.0.7和FastDeploy-develop\r\n- 【硬件】： HUAWEI Atlas 200I DK A2开发板\r\n- 【编译语言】： C++\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 板端编译时由于板端CPU有限无法完成编译，因此专用交叉编译方式进行编译，但是1.0.7和develop版本下的交叉编译会存在下面错误，具体运行命令和日志如下所示：\r\n分析日志报错位置“error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed”，感觉错误出在新版本的FastDeploy源码无法提供包的下载链接，请问这个问题有什么解决方法？\r\n详细日志如下：\r\n\r\n(base) amax@amax:~/HUAWEI/FastDeploy/build$ git branch -a\r\n  develop\r\n  release/1.0.6\r\n* release/1.0.7\r\n  remotes/origin/HEAD -> origin/develop\r\n  remotes/origin/develop\r\n  remotes/origin/jiangjiajun-patch-1\r\n  remotes/origin/llm\r\n  remotes/origin/release/0.1\r\n  remotes/origin/release/0.2\r\n  remotes/origin/release/0.3\r\n  remotes/origin/release/0.4\r\n  remotes/origin/release/0.5\r\n  remotes/origin/release/0.6\r\n  remotes/origin/release/0.7\r\n  remotes/origin/release/0.8\r\n  remotes/origin/release/1.0\r\n  remotes/origin/release/1.0.1\r\n  remotes/origin/release/1.0.2\r\n  remotes/origin/release/1.0.3\r\n  remotes/origin/release/1.0.4\r\n  remotes/origin/release/1.0.5\r\n  remotes/origin/release/1.0.6\r\n  remotes/origin/release/1.0.7\r\n(base) amax@amax:~/HUAWEI/FastDeploy/build$ su\r\nPassword: \r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# cmake .. -DCMAKE_C_COMPILER=/opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=/opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DTARGET_ABI=arm64 -DWITH_ASCEND=ON -DENABLE_VISION=ON -DENABLE_LITE_BACKEND=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-ascend\r\n-- The C compiler identification is GNU 6.3.1\r\n-- The CXX compiler identification is GNU 6.3.1\r\n-- Check for working C compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc\r\n-- Check for working C compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n-- Check for working CXX compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-x86_64.tar.gz to /home/amax/HUAWEI/FastDeploy/build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 17% complete]\r\n-- [download 22% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 43% complete]\r\n-- [download 48% complete]\r\n-- [download 53% complete]\r\n-- [download 58% complete]\r\n-- [download 64% complete]\r\n-- [download 69% complete]\r\n-- [download 74% complete]\r\n-- [download 79% complete]\r\n-- [download 84% complete]\r\n-- [download 90% complete]\r\n-- [download 95% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/amax/HUAWEI/FastDeploy/build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- Build FastDeploy Ascend C++ library on X86 platform.\r\n-- Use the default OpenCV lib from: https://bj.bcebos.com/fastdeploy/third_libs/opencv-linux-aarch64-4.6.0.tgz\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/opencv-linux-aarch64-4.6.0.tgz to /home/amax/HUAWEI/FastDeploy/build/opencv-linux-aarch64-4.6.0.tgz ...\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n-- [download 2% complete]\r\n-- [download 3% complete]\r\n-- [download 4% complete]\r\n-- [download 5% complete]\r\n-- [download 6% complete]\r\n-- [download 7% complete]\r\n-- [download 8% complete]\r\n-- [download 9% complete]\r\n-- [download 10% complete]\r\n-- [download 11% complete]\r\n-- [download 12% complete]\r\n-- [download 13% complete]\r\n-- [download 14% complete]\r\n-- [download 15% complete]\r\n-- [download 16% complete]\r\n-- [download 17% complete]\r\n-- [download 18% complete]\r\n-- [download 19% complete]\r\n-- [download 20% complete]\r\n-- [download 21% complete]\r\n-- [download 22% complete]\r\n-- [download 23% complete]\r\n-- [download 24% complete]\r\n-- [download 25% complete]\r\n-- [download 26% complete]\r\n-- [download 27% complete]\r\n-- [download 28% complete]\r\n-- [download 29% complete]\r\n-- [download 30% complete]\r\n-- [download 31% complete]\r\n-- [download 32% complete]\r\n-- [download 33% complete]\r\n-- [download 34% complete]\r\n-- [download 35% complete]\r\n-- [download 36% complete]\r\n-- [download 37% complete]\r\n-- [download 38% complete]\r\n-- [download 39% complete]\r\n-- [download 40% complete]\r\n-- [download 41% complete]\r\n-- [download 42% complete]\r\n-- [download 43% complete]\r\n-- [download 44% complete]\r\n-- [download 45% complete]\r\n-- [download 46% complete]\r\n-- [download 47% complete]\r\n-- [download 48% complete]\r\n-- [download 49% complete]\r\n-- [download 50% complete]\r\n-- [download 51% complete]\r\n-- [download 52% complete]\r\n-- [download 53% complete]\r\n-- [download 54% complete]\r\n-- [download 55% complete]\r\n-- [download 56% complete]\r\n-- [download 57% complete]\r\n-- [download 58% complete]\r\n-- [download 59% complete]\r\n-- [download 60% complete]\r\n-- [download 61% complete]\r\n-- [download 62% complete]\r\n-- [download 63% complete]\r\n-- [download 64% complete]\r\n-- [download 65% complete]\r\n-- [download 66% complete]\r\n-- [download 67% complete]\r\n-- [download 68% complete]\r\n-- [download 69% complete]\r\n-- [download 70% complete]\r\n-- [download 71% complete]\r\n-- [download 72% complete]\r\n-- [download 73% complete]\r\n-- [download 74% complete]\r\n-- [download 75% complete]\r\n-- [download 76% complete]\r\n-- [download 77% complete]\r\n-- [download 78% complete]\r\n-- [download 79% complete]\r\n-- [download 80% complete]\r\n-- [download 81% complete]\r\n-- [download 82% complete]\r\n-- [download 83% complete]\r\n-- [download 84% complete]\r\n-- [download 85% complete]\r\n-- [download 86% complete]\r\n-- [download 87% complete]\r\n-- [download 88% complete]\r\n-- [download 89% complete]\r\n-- [download 90% complete]\r\n-- [download 91% complete]\r\n-- [download 92% complete]\r\n-- [download 93% complete]\r\n-- [download 94% complete]\r\n-- [download 95% complete]\r\n-- [download 96% complete]\r\n-- [download 97% complete]\r\n-- [download 98% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/amax/HUAWEI/FastDeploy/build/opencv-linux-aarch64-4.6.0.tgz ...\r\n-- Found OpenCV: /home/amax/HUAWEI/FastDeploy/build/third_libs/install/opencv (found version \"4.6.0\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n--   C++ compiler version      : 6.3.1\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : MinSizeRel\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_LITE_BACKEND;ENABLE_VISION\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 1.0.7\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : ON\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : ON\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   Paddle Lite version       : \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/amax/HUAWEI/FastDeploy/build\r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# make -j8\r\nScanning dependencies of target extern_paddlelite\r\nScanning dependencies of target yaml-cpp\r\n[  1%] Creating directories for 'extern_paddlelite'\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n[  4%] Performing download step (download, verify and extract) for 'extern_paddlelite'\r\n-- Downloading...\r\n   dst='/home/amax/HUAWEI/FastDeploy/build/third_libs/paddlelite/src/lite-linux-arm64-20230316.tgz'\r\n   timeout='none'\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\n-- Retrying...\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitfromevents.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitter.cpp.o\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterstate.cpp.o\r\n-- Retry after 5 seconds (attempt #2) ...\r\n[  5%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emitterutils.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exceptions.cpp.o\r\n[  6%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/exp.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/memory.cpp.o\r\n[  7%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/node_data.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodebuilder.cpp.o\r\n[  8%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/nodeevents.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/null.cpp.o\r\n[  9%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/ostream_wrapper.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parse.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/parser.cpp.o\r\n[ 10%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/regex_yaml.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanner.cpp.o\r\n[ 11%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scanscalar.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantag.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/scantoken.cpp.o\r\n[ 12%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/simplekey.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/singledocparser.cpp.o\r\n[ 13%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/stream.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/tag.cpp.o\r\n[ 14%] Linking CXX static library libyaml-cpp.a\r\n[ 14%] Built target yaml-cpp\r\nScanning dependencies of target yaml-cpp-read\r\nScanning dependencies of target yaml-cpp-parse\r\nScanning dependencies of target yaml-cpp-sandbox\r\n[ 14%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-parse.dir/parse.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-read.dir/read.cpp.o\r\n[ 14%] Building CXX object third_party/yaml-cpp/util/CMakeFiles/yaml-cpp-sandbox.dir/sandbox.cpp.o\r\n[ 15%] Linking CXX executable parse\r\n[ 16%] Linking CXX executable read\r\n[ 17%] Linking CXX executable sandbox\r\n[ 17%] Built target yaml-cpp-parse\r\n[ 17%] Built target yaml-cpp-read\r\n[ 17%] Built target yaml-cpp-sandbox\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\n-- Retry after 5 seconds (attempt #3) ...\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\n-- Retry after 15 seconds (attempt #4) ...\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\n-- Retry after 60 seconds (attempt #5) ...\r\n-- Using src='https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz'\r\nCMake Error at extern_paddlelite-stamp/download-extern_paddlelite.cmake:159 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (2409:8c04:1001:1002:0:ff:b001:368a) port 443\r\n  (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         error: downloading 'https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz' failed\r\n         status_code: 22\r\n         status_string: \"HTTP response code said error\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n           Trying 2409:8c04:1001:1002:0:ff:b001:368a:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n    Trying 36.110.192.178:443...\r\n\r\n  TCP_NODELAY set\r\n\r\n  Connected to bj.bcebos.com (36.110.192.178) port 443 (#0)\r\n\r\n  ALPN, offering h2\r\n\r\n  ALPN, offering http/1.1\r\n\r\n  successfully set certificate verify locations:\r\n\r\n    CAfile: /etc/ssl/certs/ca-certificates.crt\r\n    CApath: /etc/ssl/certs\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [88 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\r\n\r\n  [1 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Client hello (1):\r\n\r\n  [512 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Server hello (2):\r\n\r\n  [155 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\r\n\r\n  [25 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Certificate (11):\r\n\r\n  [2974 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, CERT verify (15):\r\n\r\n  [264 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS handshake, Finished (20):\r\n\r\n  [52 bytes data]\r\n\r\n  SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\r\n\r\n  ALPN, server accepted to use http/1.1\r\n\r\n  Server certificate:\r\n\r\n   subject: C=CN; ST=Beijing; O=BeiJing Baidu Netcom Science Technology Co., Ltd; CN=*.bcebos.com\r\n   start date: Sep 23 00:00:00 2022 GMT\r\n   expire date: Oct 18 23:59:59 2023 GMT\r\n   subjectAltName: host \"bj.bcebos.com\" matched cert's \"*.bcebos.com\"\r\n   issuer: C=US; O=DigiCert Inc; CN=DigiCert Secure Site Pro CN CA G3\r\n   SSL certificate verify ok.\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  GET /fastdeploy/third_libs/lite-linux-arm64-20230316.tgz HTTP/1.1\r\n\r\n  Host: bj.bcebos.com\r\n\r\n  User-Agent: curl/7.66.0\r\n\r\n  Accept: */*\r\n\r\n  \r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n\r\n  [265 bytes data]\r\n\r\n  old SSL session ID is stale, removing\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  Mark bundle as not supporting multiuse\r\n\r\n  The requested URL returned error: 404 Not Found\r\n\r\n  Closing connection 0\r\n\r\n  [5 bytes data]\r\n\r\n  [1 bytes data]\r\n\r\n  TLSv1.3 (OUT), TLS alert, close notify (256):\r\n\r\n  [2 bytes data]\r\n\r\n  \r\n\r\n         --- LOG END ---\r\n         \r\n    \r\n\r\n\r\nmake[2]: *** [CMakeFiles/extern_paddlelite.dir/build.make:91: third_libs/paddlelite/src/extern_paddlelite-stamp/extern_paddlelite-download] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:172: CMakeFiles/extern_paddlelite.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# \r\n\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-28T09:06:27+00:00",
        "updated_at": "2025-02-11T06:43:55+00:00",
        "closed_at": "2025-02-11T06:43:55+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2216,
        "title": "PPOCR和yolov5模型在Atlas 200I DK A2上推理部署遇到的问题？",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy-1.0.6\r\n- 【编译命令】编译命令见详细日志\r\n- 【硬件】： HUAWEI Atlas 200I DK A2\r\n- 【硬件环境】：OS版本：Ubuntu22.04 LTS Arm64；固件与驱动版本：23.0.RC2；CANN版本：6.2.RC2\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 用FastDeploy-1.0.6版本的源码进行交叉编译后可以正常编译通过生成相应的库，移植到板端进行模型部署推理时会报错。\r\n- 【模型跑不通】\r\n- - 执行`examples`下的PPOCR和yolov5部署示例，无法运行，会出现“Check failed: !nodes_.count(id): duplicate Node 'elementwise_mul_0'”报错，具体运行日志如下所示：\r\n- - fastdeploy-1.0.6交叉编译日志：\r\n(base) amax@amax:~/HUAWEI/FastDeploy$ git branch -a\r\n  develop\r\n* release/1.0.6\r\n  remotes/origin/HEAD -> origin/develop\r\n  remotes/origin/develop\r\n  remotes/origin/jiangjiajun-patch-1\r\n  remotes/origin/llm\r\n  remotes/origin/release/0.1\r\n  remotes/origin/release/0.2\r\n  remotes/origin/release/0.3\r\n  remotes/origin/release/0.4\r\n  remotes/origin/release/0.5\r\n  remotes/origin/release/0.6\r\n  remotes/origin/release/0.7\r\n  remotes/origin/release/0.8\r\n  remotes/origin/release/1.0\r\n  remotes/origin/release/1.0.1\r\n  remotes/origin/release/1.0.2\r\n  remotes/origin/release/1.0.3\r\n  remotes/origin/release/1.0.4\r\n  remotes/origin/release/1.0.5\r\n  remotes/origin/release/1.0.6\r\n  remotes/origin/release/1.0.7\r\n  remotes/origin/third_engine_test\r\n\r\n(base) amax@amax:~/HUAWEI/FastDeploy$ mkdir build && cd build\r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# cmake .. -DCMAKE_C_COMPILER=/opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=/opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DTARGET_ABI=arm64 -DWITH_ASCEND=ON -DENABLE_VISION=ON -DENABLE_LITE_BACKEND=ON -DCMAKE_INSTALL_PREFIX=fastdeploy-ascend\r\n-- The C compiler identification is GNU 6.3.1\r\n-- The CXX compiler identification is GNU 6.3.1\r\n-- Check for working C compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc\r\n-- Check for working C compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n-- Check for working CXX compiler: /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-x86_64.tar.gz to /home/amax/HUAWEI/FastDeploy/build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 17% complete]\r\n-- [download 22% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 43% complete]\r\n-- [download 48% complete]\r\n-- [download 53% complete]\r\n-- [download 58% complete]\r\n-- [download 64% complete]\r\n-- [download 69% complete]\r\n-- [download 74% complete]\r\n......\r\n......\r\n......\r\nDecompress file /home/amax/HUAWEI/FastDeploy/build/opencv-linux-aarch64-4.6.0.tgz ...\r\n-- Found OpenCV: /home/amax/HUAWEI/FastDeploy/build/third_libs/install/opencv (found version \"4.6.0\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /opt/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n--   C++ compiler version      : 6.3.1\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : MinSizeRel\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_LITE_BACKEND;ENABLE_VISION\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 1.0.6\r\n--   ENABLE_ORT_BACKEND        : OFF\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : OFF\r\n--   ENABLE_LITE_BACKEND       : ON\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : OFF\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : OFF\r\n--   WITH_IPU                  : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : ON\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n-- Configuring done\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    CMAKE_TOOLCHAIN_FILE\r\n\r\n\r\n-- Build files have been written to: /home/amax/HUAWEI/FastDeploy/build\r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# make -j8\r\nScanning dependencies of target extern_paddlelite\r\nScanning dependencies of target yaml-cpp\r\n[  1%] Creating directories for 'extern_paddlelite'\r\n[  1%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilder.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/contrib/graphbuilderadapter.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/binary.cpp.o\r\n[  2%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/convert.cpp.o\r\n[  3%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/depthguard.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/emit.cpp.o\r\n[  4%] Building CXX object third_party/yaml-cpp/CMakeFiles/yaml-cpp.dir/src/directives.cpp.o\r\n......\r\n......\r\n......\r\n......\r\n[100%] Linking CXX shared library libfastdeploy.so\r\n[100%] Built target fastdeploy\r\n(base) root@amax:/home/amax/HUAWEI/FastDeploy/build# make install\r\n[ 14%] Built target yaml-cpp\r\n[ 17%] Built target extern_paddlelite\r\n[ 97%] Built target fastdeploy\r\n[ 98%] Built target yaml-cpp-read\r\n[ 99%] Built target yaml-cpp-parse\r\n[100%] Built target yaml-cpp-sandbox\r\nInstall the project...\r\n-- Install configuration: \"MinSizeRel\"\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/lib/libfastdeploy.so.1.0.6\r\n-- Set runtime path of \"/home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/lib/libfastdeploy.so.1.0.6\" to \"\"\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/lib/libfastdeploy.so\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/include/fastdeploy\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/include/fastdeploy/runtime\r\n............\r\n\r\n.......\r\n......\r\n......\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/paddle_image_preprocess.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/._paddle_place.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/._paddle_use_passes.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/._paddle_use_ops.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/._paddle_use_kernels.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/._paddle_lite_factory_helper.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/paddle_lite_factory_helper.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/paddle_place.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/include/paddle_api.h\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/lib\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/lib/libpaddle_full_api_shared.so\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/third_libs/install/paddlelite/lib/._libpaddle_full_api_shared.so\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/LICENSE\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/ThirdPartyNotices.txt\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/VERSION_NUMBER\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/FastDeploy.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/FastDeployCSharp.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/FastDeployConfig.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/utils.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/summary.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/openmp.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/utils/gflags.cmake\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/fastdeploy_init.sh\r\n-- Installing: /home/amax/HUAWEI/FastDeploy/build/fastdeploy-ascend/ascend_init.sh\r\n\r\n- - Yolov5模型推理部署日志：\r\n(base) HwHiAiUser@davinci-mini:~/HUAWEI/FastDeploy/examples/vision/detection/yolov5/cpp/build$ ./infer_paddle_demo yolov5s_infer 000000014439.jpg 4\r\n[I  9/29  0:52:42.734 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:52:42.734 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:52:42.734 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:52:42.734 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: \r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 4\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 3598228KB\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  9/29  0:52:42.735 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  9/29  0:52:42.736 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  9/29  0:52:42.736 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from yolov5s_infer/model.pdmodel\r\n[I  9/29  0:52:42.762 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from yolov5s_infer/model.pdiparams\r\n[I  9/29  0:52:43.131 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!\r\n[I  9/29  0:52:43.198 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: lite_quant_dequant_fuse_pass\r\n[I  9/29  0:52:43.249 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: lite_quant_dequant_fuse_pass\r\n[I  9/29  0:52:43.249 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: weight_quantization_preprocess_pass\r\n[I  9/29  0:52:43.252 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: weight_quantization_preprocess_pass\r\n[I  9/29  0:52:43.252 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: op_transformation_pass\r\n[I  9/29  0:52:43.255 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: op_transformation_pass\r\n[I  9/29  0:52:43.255 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: assign_value_calc_offline_pass\r\n[I  9/29  0:52:43.258 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: assign_value_calc_offline_pass\r\n[I  9/29  0:52:43.258 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: p_norm_fill_constant_max_div_fuse_pass\r\n[I  9/29  0:52:43.259 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: p_norm_fill_constant_max_div_fuse_pass\r\n[I  9/29  0:52:43.259 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: fill_constant_calc_offline_pass\r\n[I  9/29  0:52:43.297 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: fill_constant_calc_offline_pass\r\n[I  9/29  0:52:43.297 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: range_calc_offline_pass\r\n[I  9/29  0:52:43.300 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: range_calc_offline_pass\r\n[I  9/29  0:52:43.300 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: scale_calc_offline_pass\r\n[I  9/29  0:52:43.303 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: scale_calc_offline_pass\r\n[I  9/29  0:52:43.303 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: unsqueeze_calc_offline_pass\r\n[I  9/29  0:52:43.305 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: unsqueeze_calc_offline_pass\r\n[I  9/29  0:52:43.305 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: reshape_calc_offline_pass\r\n[I  9/29  0:52:43.341 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: reshape_calc_offline_pass\r\n[I  9/29  0:52:43.341 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: ssd_boxes_calc_offline_pass\r\n[I  9/29  0:52:43.351 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: ssd_boxes_calc_offline_pass\r\n[I  9/29  0:52:43.351 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: op_fusion_minimal_set_pass\r\n[I  9/29  0:52:43.353 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: op_fusion_minimal_set_pass\r\n[I  9/29  0:52:43.353 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: quantization_parameters_propagation_pass\r\n[I  9/29  0:52:43.440 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: quantization_parameters_propagation_pass\r\n[I  9/29  0:52:43.440 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: quantization_parameters_removal_pass\r\n[I  9/29  0:52:43.443 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: quantization_parameters_removal_pass\r\n[I  9/29  0:52:43.443 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: nnadapter_subgraph_pass\r\n[F  9/29  0:52:43.457 ...addle-Lite/lite/core/optimizer/mir/dot.h:122 AddNode] Check failed: !nodes_.count(id): duplicate Node 'elementwise_mul_0'\r\nAborted (core dumped)\r\n\r\n\r\n\r\n- - PPOCR模型推理部署日志：\r\n(base) root@davinci-mini:/home/HwHiAiUser/HUAWEI/FastDeploy/examples/vision/ocr/PP-OCR/ascend/cpp/build# ./infer_demo ./ch_PP-OCRv3_det_infer ./ch_ppocr_mobile_v2.0_cls_infer ./ch_PP-OCRv3_rec_infer ./ppocr_keys_v1.txt ./12.jpg \r\n[I  9/29  0:14:34.937 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:14:34.937 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:14:34.937 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:14:34.937 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 3330\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: \r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 4\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 0, min freq: 0, cluster ID: 0, CPU ARCH: A-1\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 512 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 3598228KB\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  9/29  0:14:34.938 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  9/29  0:14:34.938 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  9/29  0:14:34.938 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from ./ch_PP-OCRv3_det_infer/inference.pdmodel\r\n[I  9/29  0:14:34.986 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from ./ch_PP-OCRv3_det_infer/inference.pdiparams\r\n[I  9/29  0:14:34.989 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!\r\n[I  9/29  0:14:35. 66 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: lite_quant_dequant_fuse_pass\r\n[I  9/29  0:14:35.119 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: lite_quant_dequant_fuse_pass\r\n[I  9/29  0:14:35.119 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: weight_quantization_preprocess_pass\r\n[I  9/29  0:14:35.123 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: weight_quantization_preprocess_pass\r\n[I  9/29  0:14:35.123 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: op_transformation_pass\r\n[I  9/29  0:14:35.126 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: op_transformation_pass\r\n[I  9/29  0:14:35.126 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: assign_value_calc_offline_pass\r\n[I  9/29  0:14:35.130 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: assign_value_calc_offline_pass\r\n[I  9/29  0:14:35.130 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: p_norm_fill_constant_max_div_fuse_pass\r\n[I  9/29  0:14:35.131 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: p_norm_fill_constant_max_div_fuse_pass\r\n[I  9/29  0:14:35.131 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: fill_constant_calc_offline_pass\r\n[I  9/29  0:14:35.134 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: fill_constant_calc_offline_pass\r\n[I  9/29  0:14:35.134 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: range_calc_offline_pass\r\n[I  9/29  0:14:35.138 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: range_calc_offline_pass\r\n[I  9/29  0:14:35.138 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: scale_calc_offline_pass\r\n[I  9/29  0:14:35.141 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: scale_calc_offline_pass\r\n[I  9/29  0:14:35.141 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: unsqueeze_calc_offline_pass\r\n[I  9/29  0:14:35.144 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: unsqueeze_calc_offline_pass\r\n[I  9/29  0:14:35.144 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: reshape_calc_offline_pass\r\n[I  9/29  0:14:35.147 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: reshape_calc_offline_pass\r\n[I  9/29  0:14:35.147 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: ssd_boxes_calc_offline_pass\r\n[I  9/29  0:14:35.160 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: ssd_boxes_calc_offline_pass\r\n[I  9/29  0:14:35.160 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: op_fusion_minimal_set_pass\r\n[I  9/29  0:14:35.163 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: op_fusion_minimal_set_pass\r\n[I  9/29  0:14:35.163 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: quantization_parameters_propagation_pass\r\n[I  9/29  0:14:35.260 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: quantization_parameters_propagation_pass\r\n[I  9/29  0:14:35.260 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: quantization_parameters_removal_pass\r\n[I  9/29  0:14:35.263 ...le-Lite/lite/core/optimizer/optimizer.cc:119 ApplyPasses] == Finished running: quantization_parameters_removal_pass\r\n[I  9/29  0:14:35.263 ...le-Lite/lite/core/optimizer/optimizer.cc:99 ApplyPasses] == Running pass: nnadapter_subgraph_pass\r\n[F  9/29  0:14:35.281 ...addle-Lite/lite/core/optimizer/mir/dot.h:122 AddNode] Check failed: !nodes_.count(id): duplicate Node 'elementwise_add_0'\r\nAborted (core dumped)\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-28T09:19:56+00:00",
        "updated_at": "2025-02-11T06:43:56+00:00",
        "closed_at": "2025-02-11T06:43:56+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2217,
        "title": "部署Ascend310P推理paddledetecetion模型 Failed to create a program, No model and cache is provided.",
        "body": "[W  9/29  1:47:17.718 ...ter/nnadapter/src/runtime/compilation.cc:334 Finish] Warning: Failed to create a program, No model and cache is provided.\r\n[W  9/29  1:47:17.718 ...le-Lite/lite/kernels/nnadapter/engine.cc:149 LoadFromCache] Warning: Build model failed(3) !\r\n[W  9/29  1:47:17.739 ...nnadapter/nnadapter/src/runtime/model.cc:86 GetSupportedOperations] Warning: Failed to get the supported operations for device 'huawei_ascend_npu', because the HAL interface 'validate_program' is not implemented!\r\n[W  9/29  1:47:17.739 ...kernels/nnadapter/converter/converter.cc:171 Apply] Warning: Failed to get the supported operations for the selected devices, one or more of the selected devices are not supported!\r\n[I  9/29  1:47:17.739 ...r/src/driver/huawei_ascend_npu/driver.cc:70 CreateProgram] Create program for huawei_ascend_npu.\r\n\r\n设备Itlas 300V ",
        "state": "closed",
        "user": "deshuai666",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-09-29T01:50:23+00:00",
        "updated_at": "2025-04-29T06:46:02+00:00",
        "closed_at": "2025-04-29T06:46:02+00:00",
        "comments_count": [
            "xiaomujiang",
            "shengzhe8688",
            "xfbxag",
            "shengzhe8688",
            "bb0928"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2218,
        "title": "RK3588跑示例ocr模型，输出乱码，但是图片标注正确。",
        "body": "`blueberry@poodle:~/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ ./infer_demo ch_PP-OCRv3_det_infer_rk3588_unquantized.rknn  ch_ppocr_mobile_v20_cls_infer_rk3588_unquantized.rknn ch_PP-OCRv3_rec_infer_rk3588_unquantized.rknn ppocr_keys_v1.txt 12.jpg 1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 960, 960, 3], n_elems=2764800, size=5529600, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=sigmoid_0.tmp_0, n_dims=4, dims=[1, 1, 960, 960], n_elems=921600, size=1843200, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 48, 192, 3], n_elems=27648, size=55296, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=softmax_0.tmp_0, n_dims=2, dims=[1, 2, 0, 0], n_elems=2, size=4, fmt=UNDEFINED, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 48, 320, 3], n_elems=46080, size=92160, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=softmax_5.tmp_0, n_dims=4, dims=[1, 40, 6625, 1], n_elems=265000, size=530000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\ndet boxes: [[276,174],[285,173],[285,178],[276,179]]rec text: 洗酸兼滋染斜洗免 rec score:0.996582 cls label: 1 cls score: 0.766602\r\ndet boxes: [[43,408],[483,390],[483,431],[44,449]]rec text: 洗洗武斜武酸兼滋龈斜洗免 rec score:0.953939 cls label: 0 cls score: 1.000000\r\ndet boxes: [[186,456],[399,448],[399,480],[186,488]]rec text: 冏洗泰权地斜武泰滋龈斜洗 rec score:0.994914 cls label: 0 cls score: 1.000000\r\ndet boxes: [[18,501],[513,485],[514,537],[18,554]]rec text: 洗久泰酸兼滋龈斜洗免 rec score:0.992969 cls label: 0 cls score: 1.000000\r\ndet boxes: [[78,553],[404,541],[404,573],[78,585]]rec text: 洗鼠▏开泰才钧泰滋龈斜洗免 rec score:0.992751 cls label: 0 cls score: 1.000000\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n`\r\n请问这个可能会是什么问题导致的。\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/71710433/306dea18-c630-42fe-abed-d7d81b8eb218)\r\n换了图片后情况一样，标注框准确，但是识别内容错误：\r\n`blueberry@poodle:~/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ ./infer_demo ch_PP-OCRv3_det_infer_rk3588_unquantized.rknn  ch_ppocr_mobile_v20_cls_infer_rk3588_unquantized.rknn ch_PP-OCRv3_rec_infer_rk3588_unquantized.rknn ppocr_keys_v1.txt 14.png 1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 960, 960, 3], n_elems=2764800, size=5529600, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=sigmoid_0.tmp_0, n_dims=4, dims=[1, 1, 960, 960], n_elems=921600, size=1843200, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 48, 192, 3], n_elems=27648, size=55296, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=softmax_0.tmp_0, n_dims=2, dims=[1, 2, 0, 0], n_elems=2, size=4, fmt=UNDEFINED, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.2 (c6b7b351a@2023-08-23T15:28:22)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 48, 320, 3], n_elems=46080, size=92160, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=softmax_5.tmp_0, n_dims=4, dims=[1, 40, 6625, 1], n_elems=265000, size=530000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is UINT8\r\ndet boxes: [[609,19],[648,19],[648,36],[609,36]]rec text: 男-现才钧吃冈滋染斜洗免 rec score:0.997559 cls label: 0 cls score: 0.998047\r\ndet boxes: [[641,66],[682,66],[682,82],[641,82]]rec text: 贪洗窝奶兼盖 rec score:1.000000 cls label: 0 cls score: 1.000000\r\ndet boxes: [[262,87],[305,87],[305,103],[262,103]]rec text: 纱盲绽男洗授现才钧吃冈滋染斜洗免 rec score:0.997681 cls label: 0 cls score: 1.000000\r\ndet boxes: [[767,84],[811,84],[811,105],[767,105]]rec text: ?酸兼滋染斜洗免 rec score:0.994751 cls label: 0 cls score: 1.000000\r\ndet boxes: [[648,99],[693,99],[693,116],[648,116]]rec text: 男洗津地才钧吃冈滋染斜洗免 rec score:0.995718 cls label: 0 cls score: 1.000000\r\ndet boxes: [[648,123],[694,123],[694,140],[648,140]]rec text: 盲滋染斜洗 rec score:0.989551 cls label: 0 cls score: 1.000000\r\ndet boxes: [[272,163],[291,163],[291,179],[272,179]]rec text: 最显欧成才钧巾滋染斜洗免 rec score:0.998006 cls label: 0 cls score: 0.886230\r\ndet boxes: [[642,164],[701,164],[701,181],[642,181]]rec text: 飘窝魏兼盖 rec score:0.974805 cls label: 0 cls score: 1.000000\r\ndet boxes: [[42,177],[107,179],[106,195],[41,194]]rec text: 纱盲绽男洗津洗才钧泰滋染斜洗免 rec score:0.988021 cls label: 0 cls score: 1.000000\r\ndet boxes: [[266,186],[296,186],[296,204],[266,204]]rec text: 纱盲绽权欧成泰才钧泰滋染斜洗免 rec score:0.998665 cls label: 0 cls score: 0.999512\r\ndet boxes: [[466,190],[503,190],[503,207],[466,207]]rec text: 怕酸兼滋染洗洗免 rec score:0.995666 cls label: 0 cls score: 1.000000\r\ndet boxes: [[45,201],[103,204],[100,219],[43,216]]rec text: 授授才钧吃冈滋染斜洗免 rec score:0.986461 cls label: 0 cls score: 0.992676\r\ndet boxes: [[264,210],[300,210],[300,228],[264,228]]rec text: 壹鼠化洗窝团斜武酸兼滋染斜洗免 rec score:0.997786 cls label: 0 cls score: 1.000000\r\ndet boxes: [[265,233],[299,233],[299,251],[265,251]]rec text: 最洗才钧泰滋染斜洗免 rec score:0.997754 cls label: 0 cls score: 1.000000\r\ndet boxes: [[630,232],[691,231],[692,248],[631,249]]rec text: 冏个东才钧泰滋染斜洗免 rec score:0.984508 cls label: 0 cls score: 1.000000\r\ndet boxes: [[768,239],[867,239],[867,258],[768,258]]rec text: 洗久泰固酸兼滋染斜洗免 rec score:0.976163 cls label: 0 cls score: 1.000000\r\ndet boxes: [[631,257],[704,258],[703,275],[630,274]]rec text: 下积滋染斜洗 rec score:0.968343 cls label: 0 cls score: 1.000000\r\ndet boxes: [[641,282],[694,282],[694,299],[641,299]]rec text: 授洗伞览洗地酸兼滋染 rec score:0.993066 cls label: 0 cls score: 1.000000\r\ndet boxes: [[468,296],[511,296],[511,312],[468,312]]rec text: 纱盲之°叉-显申成泰才钧武喻义金示命栽授叶 rec score:1.000000 cls label: 0 cls score: 1.000000\r\ndet boxes: [[473,337],[500,337],[500,353],[473,353]]rec text: 授-洗窝团斜武斜洗免 rec score:1.000000 cls label: 0 cls score: 0.983887\r\ndet boxes: [[260,367],[304,367],[304,385],[260,385]]rec text: 冏授洗武斜武酸兼盖滋染斜洗 rec score:0.996657 cls label: 0 cls score: 1.000000\r\ndet boxes: [[271,408],[290,411],[286,426],[266,424]]rec text: 魏兼 rec score:1.000000 cls label: 0 cls score: 0.997070\r\ndet boxes: [[463,410],[510,410],[510,425],[463,425]]rec text: 壹鼠洗才钧泰滋染斜洗免 rec score:0.993874 cls label: 0 cls score: 1.000000\r\ndet boxes: [[636,407],[679,407],[679,428],[636,428]]rec text: ↓祖现地洗盲透酸兼盖滋染洗洗免 rec score:0.997135 cls label: 0 cls score: 1.000000\r\ndet boxes: [[559,414],[563,417],[559,420],[554,417]]rec text: 硚冶革中皮魏兼盖 rec score:0.997742 cls label: 0 cls score: 0.615723\r\ndet boxes: [[130,418],[151,418],[151,424],[130,424]]rec text: 欧冰才钧泰滋染斜洗免 rec score:0.997558 cls label: 1 cls score: 0.741699\r\ndet boxes: [[136,414],[145,414],[145,420],[136,420]]rec text: 洗久泰授东聘地斜武泰滋染斜洗免 rec score:0.997721 cls label: 0 cls score: 0.620117\r\ndet boxes: [[268,480],[295,480],[295,497],[268,497]]rec text: 纱盲之°叉-显申成泰才地吃洗'广冶洗寄武 rec score:1.000000 cls label: 0 cls score: 1.000000\r\ndet boxes: [[114,420],[161,420],[161,437],[114,437]]rec text: 累•响罩地武洗赵洗5剑 rec score:1.000000 cls label: 0 cls score: 0.553223\r\ndet boxes: [[463,481],[511,481],[511,499],[463,499]]rec text: 织窝奶兼盖 rec score:0.995898 cls label: 0 cls score: 1.000000\r\ndet boxes: [[647,481],[691,481],[691,503],[647,503]]rec text: 洗地感鼠酸兼滋染斜洗免 rec score:0.996182 cls label: 0 cls score: 1.000000\r\ndet boxes: [[688,521],[877,521],[877,533],[688,533]]rec text: 最授东才钧泰滋龈斜洗免 rec score:0.988681 cls label: 0 cls score: 1.000000\r\n\r\nVisualized result saved in ./vis_result.jpg\r\n`\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/71710433/594b9725-77e1-4bed-849d-f20ac9877ae0)\r\n\r\n",
        "state": "closed",
        "user": "7288Fzq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-02T21:52:37+00:00",
        "updated_at": "2025-02-04T06:41:28+00:00",
        "closed_at": "2025-02-04T06:41:28+00:00",
        "comments_count": [
            "ibiz4j",
            "7288Fzq",
            "190948804",
            "equalman"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2219,
        "title": "在jetsonnano上部属fastdeploy的python案例时，报错！！！！",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，fastdeploy_gpu_python-0.0.0-cp36-cp36m-linux_aarch64.whl，在jetsonnano中编译的，下载最新的fastdeploy版本。\r\n- 【编译命令】git clone https://github.com/PaddlePaddle/FastDeploy.git\r\n                              cd FastDeploy/python\r\n                               export BUILD_ON_JETSON=ON\r\n                               export ENABLE_VISION=ON\r\n                               python setup.py build\r\n                               python setup.py bdist_wheel\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： 说明具体硬件型号，jetsonnano\r\n- 【编译语言】：Python(3.6.9）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- -在案例使用ppyooe模型在cpu、GPU时可以跑通，但使用trt时报错（python infer_ppyoloe.py --model_dir ppyoloe_crn_l_300e_coco --image 000000014439.jpg --device gpu --use_trt True），错误信息如下：\r\n- [WARN][Paddle2ONNX] [multiclass_nms3: multiclass_nms3_0.tmp_1] Paramter nms_top_k:10000 is exceed limit in TensorRT BatchedNMS plugin, will force to 4096.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(572)::BuildTrtEngine\tStart to building TensorRT Engine...\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(659)::BuildTrtEngine\tTensorRT Engine is built successfully.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(661)::BuildTrtEngine\tSerialize TensorRTEngine to local file ./tensorrt_cache/model.trt.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(666)::BuildTrtEngine\tFailed to open ./tensorrt_cache/model.trt to write.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(752)::CreateTrtEngineFromOnnx\tFailed to build tensorrt engine.\r\n[INFO] fastdeploy/runtime/runtime.cc(339)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log\t2: [pluginV2DynamicExtRunner.cpp::execute::115] Error Code 2: Internal Error (Assertion status == kSTATUS_SUCCESS failed. )\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(348)::InferFailed to Infer with TensorRT.\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(73)::BatchPredict\tFailed to inference by runtime\r\n\r\n- 第二个问题是运行案例中的ppyolo模型报错（python infer_ppyolo.py），报错信息如下：\r\n- dlinano@jetson-nano:~/FastDeploy/examples/vision/detection/paddledetection/python$ python infer_ppyolo.py\r\n100%|█████████████████████████| 171084/171084 [00:20<00:00, 8417.70KB/s]\r\nSuccessfully download model at path: /home/dlinano/.fastdeploy/models/ppyolo_r50vd_dcn_1x_coco\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend\tFound no valid backend for model: PaddleDetection/PP-YOLO\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(32)::Initialize\tFailed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"infer_ppyolo.py\", line 62, in <module>\r\n    model_file, params_file, config_file, runtime_option=runtime_option)\r\n  File \"/usr/local/lib/python3.6/dist-packages/fastdeploy/vision/detection/ppdet/__init__.py\", line 188, in __init__\r\n    assert self.initialized, \"PPYOLO model initialize failed.\"\r\nAssertionError: PPYOLO model initialize failed.\r\n\r\n",
        "state": "open",
        "user": "jiangming7301",
        "closed_by": null,
        "created_at": "2023-10-05T04:27:33+00:00",
        "updated_at": "2023-11-15T13:44:11+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "jiangming7301",
            "jiangming7301",
            "jiangming7301",
            "jiangjiajun",
            "wf2000cn",
            "wf2000cn",
            "jiangjiajun",
            "wf2000cn"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2224,
        "title": "pp-Tracking 推理报错     出现异常。 Segmentation fault",
        "body": "std::shared_ptr<fastdeploy::vision::tracking::PPTracking> model;\r\nmodel = std::make_shared<fastdeploy::vision::tracking::PPTracking>(model_file, params_file, config_file, option);\r\n初始化正常\r\nif (!model->Predict(&img &result))\r\n{\r\n  std::cerr << \"Failed to predict.\" << std::endl;\r\n  return;\r\n}\r\n执行predict时候 报错  出现异常。 Segmentation fault\r\n\r\n不使用指针实例化不报错   检查了输入图片和指针都正常\r\n定义在类内\r\nstd::shared_ptr<fastdeploy::vision::tracking::PPTracking> model;\r\n调用构造部分在我类内的init函数\r\nmodel = std::make_shared<fastdeploy::vision::tracking::PPTracking>(model_file, params_file, config_file, option);\r\npredict调用在我类内的predict函数  \r\n当初始化和推理写在一个函数内是正常不报错",
        "state": "closed",
        "user": "294978174",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-08T07:23:27+00:00",
        "updated_at": "2024-10-15T06:42:05+00:00",
        "closed_at": "2024-10-15T06:42:05+00:00",
        "comments_count": [
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2222,
        "title": "在Ubuntu上交叉编译Rknpu2的FastDeploy库无法使用",
        "body": "在自己的CmakeLists.txt文件中的target_link_libraries下\r\n使用在Ubuntu上交叉编译好的FastDeploy/build/fastdeploy-0.0.0/lib/libfastdeploy.so.0.0.0库\r\n遇到如下问题：\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/71710433/c8693075-b30f-4f83-9b61-384b71b105d7)\r\n\r\n小白开发恭请大佬们看看这个是怎么回事！\r\n谢谢啦",
        "state": "open",
        "user": "7288Fzq",
        "closed_by": null,
        "created_at": "2023-10-07T21:55:49+00:00",
        "updated_at": "2023-10-08T01:51:06+00:00",
        "closed_at": null,
        "comments_count": [
            "7288Fzq",
            "7288Fzq"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2220,
        "title": "FastDeploy with_capi 调用GpuInfer报错",
        "body": "在openeular20.03上将CAPI example编译成so，调用后报错\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-1.0.6\r\n- 【编译命令】自行编译 使用CAPI\r\ncmake .. -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_OPENVINO_BACKEND=ON -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk -DENABLE_VISION=ON -DENABLE_TEXT=ON -DWITH_CAPI=ON -DWITH_GPU=ON -DCUDA_DIRECTORY=/usr/local/cuda\r\n- 【系统平台】:\r\n Linux x64(openEuler 20.03 (LTS-SP3))\r\n- 【硬件】： \r\n NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     \r\ncudnn-linux-x86_64-8.9.0.131\r\nNVIDIA Corporation TU104GL [Tesla T4] (rev a1)\r\ng++ (GCC) 7.3.0\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n1、/root/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/c  先执行`examples`下的部署示例，包括使用examples提供的模型，都可以正常运行\r\n\r\n2、把examples下的infer_demo编译成so共享库，另写一个main测试，也可正常运行\r\nCMakeLists.txt\r\nPROJECT(infer_demo C)\r\nCMAKE_MINIMUM_REQUIRED (VERSION 3.10)\r\n\r\n# 指定下载解压后的fastdeploy库路径\r\noption(FASTDEPLOY_INSTALL_DIR \"Path of downloaded fastdeploy sdk.\")\r\n\r\ninclude(${FASTDEPLOY_INSTALL_DIR}/FastDeploy.cmake)\r\n\r\n# 添加FastDeploy依赖头文件\r\ninclude_directories(${FASTDEPLOY_INCS})\r\ninclude_directories(${PROJECT_SOURCE_DIR})\r\nadd_library(infer_demo SHARED ${PROJECT_SOURCE_DIR}/infer.c)\r\n\r\ntarget_link_libraries(infer_demo ${FASTDEPLOY_LIBS})\r\n\r\nMain.c\r\n#include \"/root/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/c/infer.h\"\r\nint main()\r\n{\r\n    const char *det_model_dir1 = \"/root/packet-box/config/models/ocr/det\";\r\n    const char *cls_model_dir1 = \"/root/packet-box/config/models/ocr/cls\";\r\n    const char *rec_model_dir1 = \"/root/packet-box/config/models/ocr/rec\";\r\n    const char *rec_label_file1 = \"/root/packet-box/config/models/ocr/ppocr_keys_v1.txt\";\r\n    const char *image1 = \"/root/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/c/build/12.jpg\";\r\n    GpuInfer(det_model_dir1, cls_model_dir1, rec_model_dir1, rec_label_file1, image1);\r\n    return 0;\r\n}\r\n结果正常\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(266)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\ndet boxes: [[42,413],[483,391],[484,428],[43,450]]rec text: 上海斯格威铂尔大酒店 rec score:0.980085 cls label: 0 cls score: 1.000000\r\ndet boxes: [[187,456],[399,448],[400,480],[188,488]]rec text: 打浦路15号 rec score:0.964993 cls label: 0 cls score: 1.000000\r\ndet boxes: [[23,507],[513,488],[515,529],[24,548]]rec text: 绿洲仕格维花园公寓args)0 rec score:0.993726 cls label: 0 cls score: 1.000000\r\ndet boxes: [[74,553],[427,542],[428,571],[75,582]]rec text: 打浦路252935号 rec score:0.947723 cls label: 0 cls score: 1.000000\r\nVisualized result saved in ./vis_result.jpg\r\n\r\n3、在自己工程中调用上面的so共享库，报错\r\n部分代码：\r\n        int reuslt = is_image_file(img_dir);\r\n        if (reuslt)\r\n        {\r\n            const char *det_model_dir1 = \"/root/packet-box/config/models/ocr/det\";\r\n            const char *cls_model_dir1 = \"/root/packet-box/config/models/ocr/cls\";\r\n            const char *rec_model_dir1 = \"/root/packet-box/config/models/ocr/rec\";\r\n            const char *rec_label_file1 = \"/root/packet-box/config/models/ocr/ppocr_keys_v1.txt\";\r\n            const char *image1 = \"/root/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/c/build/12.jpg\";\r\n            GpuInfer(det_model_dir1, cls_model_dir1, rec_model_dir1, rec_label_file1, image1);\r\n        }\r\n\r\n相关报错日志\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  \r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   cm_dpdk_actor_exec\r\n1   GpuInfer\r\n2   paddle_infer::CreatePredictor(paddle::AnalysisConfig const&)\r\n3   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n4   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n5   paddle::AnalysisConfig::fraction_of_gpu_memory_for_pool() const\r\n6   phi::backends::gpu::SetDeviceId(int)\r\n7   phi::backends::gpu::GetGPUDeviceCount()\r\n8   phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n9   phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDA error(2), out of memory. \r\n  [Hint: Please search for the error code(2) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /workspace/qiuyanjun/fastdeploy/Paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:65)\r\n\r\n另外，使用CpuInfer也会报错\r\n1   CpuInfer\r\n2   paddle_infer::CreatePredictor(paddle::AnalysisConfig const&)\r\n3   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n4   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n5   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n6   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n7   paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n8   paddle::inference::analysis::IrGraphBuildPass::RunImpl(paddle::inference::analysis::Argument*)\r\n9   paddle::inference::analysis::IrGraphBuildPass::LoadModel(std::string const&, std::string const&, paddle::framework::Scope*, phi::Place const&, bool, bool)\r\n10  paddle::inference::Load(paddle::framework::Executor*, paddle::framework::Scope*, std::string const&, std::string const&, bool)\r\n11  paddle::inference::LoadPersistables(paddle::framework::Executor*, paddle::framework::Scope*, paddle::framework::ProgramDesc const&, std::string const&, std::string const&, bool)\r\n12  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string > > const&, bool, bool)\r\n13  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)\r\n14  paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)\r\n15  paddle::framework::CPUGarbageCollector::CPUGarbageCollector(phi::CPUPlace const&, unsigned long)\r\n16  paddle::framework::GarbageCollector::GarbageCollector(phi::Place const&, unsigned long)\r\n17  phi::DeviceContextPool::Get(phi::Place const&)\r\n18  std::__future_base::_Deferred_state<std::thread::_Invoker<std::tuple<std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > (*)(phi::Place const&, bool, int), phi::Place, bool, int> >, std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > >::_M_complete_async()\r\n19  std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > >, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > (*)(phi::Place const&, bool, int), phi::Place, bool, int> >, std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > > >::_M_invoke(std::_Any_data const&)\r\n20  std::unique_ptr<phi::DeviceContext, std::default_delete<phi::DeviceContext> > paddle::platform::CreateDeviceContext<phi::OneDNNContext>(phi::Place const&, bool, int)\r\n21  paddle::memory::allocation::AllocatorFacade::Instance()\r\n22  paddle::memory::allocation::AllocatorFacade::AllocatorFacade()\r\n23  paddle::memory::allocation::AllocatorFacadePrivate::AllocatorFacadePrivate(bool)\r\n24  phi::backends::gpu::GetGPUDeviceCount()\r\n25  phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n26  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDA error(2), out of memory. \r\n  [Hint: Please search for the error code(2) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CUDA Error.] (at /workspace/qiuyanjun/fastdeploy/Paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:65)\r\n4、不带GPU编译时，CpuInfer是好使的\r\n",
        "state": "closed",
        "user": "zhengxiaoqing",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-07T01:20:39+00:00",
        "updated_at": "2025-02-11T06:43:57+00:00",
        "closed_at": "2025-02-11T06:43:57+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2225,
        "title": "PaddlePaddle/FastDeploy/tree/develop/examples/application/js/web_demo   GPU加速",
        "body": "(https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/application/js)\r\n/web_demo/\r\n\r\n这个项目的ocr可以利用 nodejs webgl 进行 gpu加速吗？如果可以，怎么用呢",
        "state": "closed",
        "user": "lymgithub",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-09T00:33:03+00:00",
        "updated_at": "2024-10-15T06:42:06+00:00",
        "closed_at": "2024-10-15T06:42:06+00:00",
        "comments_count": [
            "chenqianhe"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2227,
        "title": "web_demo 集成问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nnode 12.20.0\r\n\r\nvue 2.5.3\r\n\r\n使用 npm install @paddle-js-models/ocr  安装完成后 在页面调用\r\n```js\r\n<template>\r\n  <div id=\"app\">\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nimport * as ocr from '@paddle-js-models/ocr'\r\n\r\nexport default {\r\n  name: 'App',\r\n  mounted() {\r\n    ocr.init()\r\n  }\r\n}\r\n</script>\r\n\r\n<style>\r\n</style>\r\n```\r\n\r\n报错 \r\nindex.esm.js:404 Uncaught ReferenceError: Module is not defined\r\n    at eval (index.esm.js:404:1)\r\n    at eval (index.esm.js:401:1)\r\n    at eval (index.esm.js:404:1)\r\n    at createCommonjsModule (index.esm.js:395:1)\r\n    at Object.eval (index.esm.js:398:1)\r\n    at eval (index.esm.js:1095:30)\r\n    at ./node_modules/@paddle-js-models/ocr/lib/index.esm.js (app.js:734:1)\r\n    at __webpack_require__ (app.js:679:30)\r\n    at fn (app.js:89:20)\r\n    at eval (selector.js?type=script&index=0!./src/App.vue:2:80)\r\n\r\n",
        "state": "closed",
        "user": "effortcheep",
        "closed_by": "effortcheep",
        "created_at": "2023-10-09T07:37:35+00:00",
        "updated_at": "2023-12-04T03:32:11+00:00",
        "closed_at": "2023-11-22T06:39:31+00:00",
        "comments_count": [
            "FuriousSnail",
            "effortcheep",
            "muleoo",
            "mzzya"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2226,
        "title": "yolov5得分socre远超过1",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： Nvidia GPU 2080TI， CUDA 11.8 CUDNN 8.8\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型精度问题】\r\n跑yolov5-face-l模型，直接调用yolov5face的对象，输出score远大于1。\r\n模型是[yolov5](https://github.com/deepcam-cn/yolov5-face)官方模型yolov5l转onnx得到\r\n测试用例\r\n链接：https://pan.baidu.com/s/1e5dCIvRG0su3YkCYdmLitw?pwd=4ulv \r\n提取码：4ulv \r\n\r\n```\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(362)::InitFromOnnx    number of streams:1.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(372)::InitFromOnnx    affinity:YES.\r\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(384)::InitFromOnnx    Compile OpenVINO model on device_name:CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(299)::CreateOpenVINOBackend        Runtime initialized with Backend::OPENVINO in Device::CPU.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n173.943359,204.884766, 429.113281, 553.580078, 240.251007, 2\r\n173.945312,204.882812, 429.109375, 553.578125, 237.010391, 8\r\n173.945312,204.882812, 429.113281, 553.578125, 208.403168, 4\r\n173.945312,204.882812, 429.109375, 553.578125, 205.119736, 6\r\n173.943573,204.884018, 429.113281, 553.579773, 200.753296, 0\r\n173.945312,204.882812, 429.109375, 553.578125, 190.783569, 7\r\n173.937500,204.890625, 429.109375, 553.578125, 189.610611, 9\r\n173.945312,204.882812, 429.109375, 553.578125, 167.099899, 5\r\n173.943359,204.883789, 429.113281, 553.580078, 141.013428, 1\r\n173.945312,204.882812, 429.113281, 553.578125, 138.959610, 3\r\n304.843750,957.109375, 325.203125, 984.781250, 2.805375, 9\r\n304.843750,957.101562, 325.203125, 984.773438, 2.800912, 7\r\n304.843750,957.101562, 325.203125, 984.773438, 2.789726, 5\r\n304.843750,957.105469, 325.203125, 984.777344, 2.778498, 3\r\n304.842773,957.105469, 325.202148, 984.775391, 2.773043, 1\r\n304.841797,957.105469, 325.201172, 984.775391, 1.716787, 2\r\n304.843750,957.101562, 325.203125, 984.773438, 1.714982, 8\r\n304.843750,957.105469, 325.203125, 984.777344, 1.705539, 4\r\n304.842651,957.104980, 325.201782, 984.775757, 1.695807, 0\r\n304.843750,957.101562, 325.203125, 984.773438, 1.694840, 6\r\n173.937500,204.890625, 429.109375, 553.578125, 0.808734, 10\r\n298.859375,951.382812, 336.976562, 994.359375, 0.321541, 7\r\n298.859375,951.375000, 336.984375, 994.359375, 0.321474, 9\r\n298.859375,951.382812, 336.976562, 994.359375, 0.317842, 5\r\n298.861328,951.382812, 336.977539, 994.358398, 0.315172, 1\r\n298.863281,951.382812, 336.976562, 994.359375, 0.315045, 3\r\n\r\n```\r\n",
        "state": "closed",
        "user": "luameows",
        "closed_by": "luameows",
        "created_at": "2023-10-09T03:14:48+00:00",
        "updated_at": "2023-11-27T09:15:04+00:00",
        "closed_at": "2023-11-27T09:15:04+00:00",
        "comments_count": [
            "luameows"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2228,
        "title": "infer_torchscript_poros示例报错",
        "body": "## 环境\r\n\r\n- 已安装fastdeploy, 跑其他推理后端都没问题\r\n- 跑https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/runtime/python/infer_torchscript_poros.py报错\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/86715812/150bbc34-5517-40a2-9ef9-9f6068932619)\r\n",
        "state": "closed",
        "user": "GDbbq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-09T08:53:00+00:00",
        "updated_at": "2025-02-11T06:43:58+00:00",
        "closed_at": "2025-02-11T06:43:58+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2229,
        "title": "fastdeply配合gradio部署报错",
        "body": "大佬 帮忙看一下啥问题？\r\n\r\n```python\r\n2023-10-09T11:36:37.611521996Z [ERROR] fastdeploy/vision/detection/ppdet/preprocessor.cc(38)::BuildPreprocessPipelineFromConfig\tFailed to load yaml file /home/aistudio/launch/infer_output/yolov3_darknet53_original_270e_coco/infer_cfg.yml, maybe you should check this file.\r\n2023-10-09T11:36:37.611558825Z [ERROR] fastdeploy/vision/detection/ppdet/preprocessor.cc(28)::PaddleDetPreprocessor\tFailed to create PaddleDetPreprocessor.\r\n2023-10-09T11:36:37.766238182Z /opt/deploy.sh: line 142:    76 Aborted                 (core dumped) python $APP_MAIN_FILE\r\n``` ",
        "state": "closed",
        "user": "Jin-Shuai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-09T11:47:21+00:00",
        "updated_at": "2024-10-15T06:42:06+00:00",
        "closed_at": "2024-10-15T06:42:06+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2238,
        "title": "TensorRT and GPU performance is much slower than CPU on C++",
        "body": "## Environment\r\n\r\nFastDeploy version: 0.0.0 compiled locally from repo (latest) (compile flags below)\r\nOS Platform: Linux x64 debian 11 OS\r\nHardware: tested on NVIDIA RTX A4000 & NVIDIA RTX 3050 with same environment CUDA 11.6 CUDNN 8.7.0.84 tensorRT 8.5.3.1\r\nProgram Language: C++\r\n\r\n## Problem description\r\n\r\nI ran the example [here](https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/cpu-gpu/cpp) and when I use a CPU option the performance in much faster around 0.5 seconds while using GPU option (either TRT or paddle GPU) the infer time is about 3-4 seconds after reading from a serialized cached file.\r\nTensorRT or the GPU option should be much faster than the CPU however my results differ.\r\n\r\n## Things I have tried\r\n\r\n - use the precompiled fastdeploy library instead of building one locally \r\n - use different GPU\r\n - use paddle GPU & paddle TRT  & TRT same results which is slower than CPU\r\n\r\nI wanted to try cuda 11.2 but it is not available on my OS debian 11\r\nany help is appreciated.",
        "state": "closed",
        "user": "mohblnk",
        "closed_by": "mohblnk",
        "created_at": "2023-10-12T12:55:05+00:00",
        "updated_at": "2023-11-16T16:02:04+00:00",
        "closed_at": "2023-11-16T16:02:04+00:00",
        "comments_count": [
            "mohblnk",
            "jiangjiajun",
            "mohblnk"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2242,
        "title": "classification模型部署在RKNPU2上遇到问题。跑通了但是结果不对。",
        "body": "提示信息如下：\r\n`[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion\trknpu2 runtime version: 1.5.1b19 (32afb0e92@2023-07-14T12:46:17)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion\trknpu2 driver version: 0.8.2\r\nindex=0, name=inputs, n_dims=4, dims=[1, 224, 224, 3], n_elems=150528, size=301056, fmt=NHWC, type=FP16, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\nindex=0, name=save_infer_model/scale_0.tmp_1, n_dims=2, dims=[1, 1000, 0, 0], n_elems=1000, size=2000, fmt=UNDEFINED, type=FP32, qnt_type=AFFINE, zp=0, scale=1.000000, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend\tRuntime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemory\tThe input tensor type != model's inputs type.The input_type need FP16,but inputs[0].type is FP32\r\nClassifyResult(\r\nlabel_ids: 644, \r\nscores: 0.073181, \r\n)\r\n[FastDeploy] PPClas in RKNPU2 duration = 0.028111s.\r\n`\r\n模型以及图片均为官网下载使用。严格按照步骤执行。\r\n盼望各位大佬回复！感谢",
        "state": "closed",
        "user": "7288Fzq",
        "closed_by": "rainyfly",
        "created_at": "2023-10-14T10:00:07+00:00",
        "updated_at": "2024-02-06T07:56:16+00:00",
        "closed_at": "2024-02-06T07:56:16+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2239,
        "title": "registry.baidubce.com/paddlepaddle 查看所有版本的镜像",
        "body": "registry.baidubce.com/paddlepaddle \r\n请问下如何查看这个docker仓库里还有没有别的版本的镜像\r\n使用docker search和docker login命令都失败",
        "state": "closed",
        "user": "TherChenYang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-13T02:21:30+00:00",
        "updated_at": "2024-12-17T06:42:06+00:00",
        "closed_at": "2024-12-17T06:42:06+00:00",
        "comments_count": [
            "huangjun11"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2243,
        "title": "请问有基于cuda12的pythonSDK么",
        "body": "我根据快速开始安装了SDK发现调用的是cuda11的so文件，是不是这个sdk是基于cuda11编译的?请问有cuda12的版本么",
        "state": "closed",
        "user": "kkpssr",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-16T01:41:21+00:00",
        "updated_at": "2024-10-29T06:43:38+00:00",
        "closed_at": "2024-10-29T06:43:38+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2253,
        "title": "FastDeploy模型多线程或多进程预测的使用 \"vision.detection.YOLOv8   \"AttributeError: 'YOLOv8' object has no attribute 'clone'\"",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-windows-gpu-develop\r\n- 【编译命令】无\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： Nvidia GPU 2060super， CUDA 11.8 CUDNN 8.6.1\r\n- 【编译语言】： Python 3.10.11\r\n\r\n## 问题日志及出现问题的操作流程\r\nFastDeploy模型多线程或多进程预测的使用 \"vision.detection.YOLOv8\"\r\n\r\n\"AttributeError: 'YOLOv8' object has no attribute 'clone'\"\r\n希望能尽快支持，如果不能支持请回复，。我再想其它办法！ 或者有没有其它能实现 YOLOv8 detection 的多线程方法 谢谢。",
        "state": "closed",
        "user": "lootwealth",
        "closed_by": "rainyfly",
        "created_at": "2023-10-19T07:15:52+00:00",
        "updated_at": "2024-02-06T07:53:50+00:00",
        "closed_at": "2024-02-06T07:53:50+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2252,
        "title": "Sophon-TPU(bm1684, bm1684x)上怎么对视频流做目标检测(例如使用yolov5)?",
        "body": "如题。打开infer.py文件只能看到输入image的路径，但是没有视频流的路径。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/129148313/4de3e5f2-4bff-446d-9fbd-ef3eef6dcda6)\r\n",
        "state": "closed",
        "user": "ijiami-01",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-19T06:36:28+00:00",
        "updated_at": "2025-02-11T06:44:01+00:00",
        "closed_at": "2025-02-11T06:44:01+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2249,
        "title": "运行昇腾demo报异常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\ndemo地址：\r\nhttps://gitee.com/jia0510/FastDeploy/blob/develop/examples/vision/detection/yolov5/cpp/README_CN.md\r\n驱动型号：\r\nascend-6.3 RC1 device\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/24822418/7d2a6d38-0bda-4251-93db-8115faee8516)\r\n\r\n",
        "state": "closed",
        "user": "jia0511",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-18T14:06:41+00:00",
        "updated_at": "2025-02-11T06:44:00+00:00",
        "closed_at": "2025-02-11T06:44:00+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2244,
        "title": "Quadro K620 使用1.0.0-gpu-cuda11.4-trt8.4-21.10  异常",
        "body": "# 配置\r\nQuadro K620\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/86715812/54c7e5ae-11ca-4d4d-a306-a73e46b25cef)\r\n\r\n\r\n使用镜像异常\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/86715812/0ae92dc0-4371-4c5f-b75b-99a439b09540)\r\n",
        "state": "closed",
        "user": "GDbbq",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-16T10:31:12+00:00",
        "updated_at": "2025-02-11T06:43:59+00:00",
        "closed_at": "2025-02-11T06:43:59+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2256,
        "title": "FastDeploy initalized failed! Error: libiomp5.so: cannot open shared object file: No such file or directory",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本 自己变异的\r\n- https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md\r\n- 使用官方的docker 进行源码编译\r\n- 【编译命令】\r\n- 【系统平台】:  Linux 192.168.31.5 5.10.0-153.29.0.106.oe2203sp2.x86_64 #1 SMP Thu Oct 12 10:04:22 CST 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n- 【硬件】： ascend 310p\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n# Download the latest source code\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport WITH_ASCEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\n\r\n## 问题日志及出现问题的操作流程,  fastdeploy导入失败\r\nroot@192:/Work/FastDeploy/python/dist# python -c \"import fastdeploy\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy_python-0.0.0-py3.7-linux-x86_64.egg/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy_python-0.0.0-py3.7-linux-x86_64.egg/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy_python-0.0.0-py3.7-linux-x86_64.egg/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: libiomp5.so: cannot open shared object file: No such file or directory\r\n\r\n搜索libiomp5.so\r\n/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy_python-0.0.0-py3.7-linux-x86_64.egg/fastdeploy/libs/third_libs/paddlelite/lib/lib/libiomp5.so\r\n/Work/python/fastdeploy/libs/third_libs/paddlelite/lib/libbak/libiomp5.so\r\n/Work/python/fastdeploy/libs/third_libs/paddlelite/lib/lib/libiomp5.so\r\n/Work/python/.setuptools-cmake-build/third_libs/install/paddlelite/lib/lib/libiomp5.so\r\n/Work/FastDeploy/python/fastdeploy/libs/third_libs/paddlelite/lib/lib/libiomp5.so\r\n/Work/FastDeploy/python/build/lib.linux-x86_64-cpython-37/fastdeploy/libs/third_libs/paddlelite/lib/lib/libiomp5.so\r\n/Work/FastDeploy/python/build/lib.linux-x86_64-3.7/fastdeploy/libs/third_libs/paddlelite/lib/lib/libiomp5.so\r\n/Work/FastDeploy/python/.setuptools-cmake-build/third_libs/install/paddlelite/lib/lib/libiomp5.so\r\n\r\n\r\n",
        "state": "open",
        "user": "xiaomujiang",
        "closed_by": null,
        "created_at": "2023-10-20T07:38:13+00:00",
        "updated_at": "2024-10-24T09:46:58+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "Hakstar"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2258,
        "title": "封装目标检测dll失败",
        "body": "我想将目标检测模型封装成dll进行调用，但是失败了，请问是哪里有问题？\r\n### CMakeLists.txt\r\nproject(infer_ppyoloe_demo C CXX)\r\ncmake_minimum_required(VERSION 3.12)\r\n\r\n#Only support \"Release\" mode now  \r\nset(CMAKE_BUILD_TYPE \"Release\")\r\n\r\n#Set FastDeploy install dir\r\nset(FASTDEPLOY_INSTALL_DIR \"F:/fastdeploy/fastdeploy-win-x64-1.0.2\"\r\n    CACHE PATH \"Path to downloaded or built fastdeploy sdk.\")\r\n\r\n#载入并运行来自于文件或模块的 CMake 代码。\r\ninclude(${FASTDEPLOY_INSTALL_DIR}/FastDeploy.cmake)\r\n\r\ninclude_directories(${FASTDEPLOY_INCS})\r\n\r\n#add_executable(infer_ppyoloe_demo ${PROJECT_SOURCE_DIR}/fastdeploy_infer.cpp)\r\n#target_link_libraries(infer_ppyoloe_demo ${FASTDEPLOY_LIBS})  \r\n\r\nadd_library(infer_ppyoloe_dll SHARED ${PROJECT_SOURCE_DIR}/fastdeploy_infer.cpp \"fastdeploy_infer.h\")\r\ntarget_link_libraries(infer_ppyoloe_dll ${FASTDEPLOY_LIBS})\r\n\r\n\r\n#Optional: install all DLLs to binary dir.\r\ninstall_fastdeploy_libraries(${CMAKE_CURRENT_BINARY_DIR}/Release)\r\n\r\n### fastdeploy_infer.h\r\n#pragma once\r\n\r\nextern \"C\" __declspec(dllexport) void CpuInfer(const std::string& model_dir, const std::string& image_file); \r\n\r\n### fastdeploy_infer.cpp\r\n#include \"fastdeploy/vision.h\"\r\n#include \"fastdeploy_infer.h\"\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\nvoid CpuInfer(const std::string& model_dir, const std::string& image_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseCpu();\r\n    auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n    auto im = cv::imread(image_file);\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n    std::cout << res.Str() << std::endl;\r\n    auto vis_im = fastdeploy::vision::VisDetection(im, res, 0.5);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\n### main.cpp\r\n#include <Windows.h>\r\n#include <iostream>\r\n#include <string>\r\n\r\nint main()\r\n{\r\n    // 动态加载 DLL 文件\r\n    // HMODULE hModule = LoadLibrary(L\"F:\\\\c++\\\\workspace\\\\fastdeploy_test\\\\fastdeploy_infer\\\\out\\\\build\\\\x64-Debug\\\\Release\\\\infer_ppyoloe_dll.dll\");\r\n    HMODULE hModule = LoadLibraryEx(L\"infer_ppyoloe_dll.dll\", NULL, LOAD_WITH_ALTERED_SEARCH_PATH);\r\n    if (hModule)\r\n    {\r\n        typedef void (*CpuInferFunction)(const std::string& model_dir, const std::string& image_file);\r\n        CpuInferFunction cpuInfer = (CpuInferFunction)GetProcAddress(hModule, \"CpuInfer\");\r\n        if (cpuInfer) { \r\n            // 调用加载的函数\r\n            std::string modelDir = \"F:\\\\c++\\\\workspace\\\\fastdeploy_test\\\\ppyoloe_crn_l_300e_coco\"; // 传递模型目录\r\n            std::string imageFile = \"F:\\\\c++\\\\workspace\\\\fastdeploy_test\\\\1.jpg\"; // 传递图像文件路径\r\n            cpuInfer(modelDir, imageFile);\r\n        }\r\n        else {\r\n            std::cerr << \"无法获取函数指针\" << std::endl;\r\n        }\r\n        // 卸载 DLL\r\n        FreeLibrary(hModule);\r\n    }\r\n    else\r\n    {\r\n        std::cerr << \"无法加载 DLL\" << std::endl;\r\n    }\r\n    return 0;\r\n}\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/44918749/f596cfa9-b062-4588-b55f-3a2ba5aa283a)\r\n\r\n加载函数失败，通过dependency Walker 发现CpuInfer函数已经导出，请问应该怎么修改代码，谢谢",
        "state": "closed",
        "user": "KyleWang-Hunter",
        "closed_by": "KyleWang-Hunter",
        "created_at": "2023-10-23T02:04:11+00:00",
        "updated_at": "2023-10-23T02:32:25+00:00",
        "closed_at": "2023-10-23T02:32:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2257,
        "title": "fastdeploy和taskflow不能共存吗？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\nfastdeploy-gpu-python          1.0.7\r\nfastdeploy-tools               0.0.5\r\n\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\nlinux\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\ncuda 11.8\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\npython 3.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n当同时使用fastdeploy和taskflow部署的时候，会报错Segmentation fault。\r\n但是我线上环境有一些模型已经用taskflow在管理了。需要怎么解决这个问题呢？\r\n\r\n`    \r\n    from paddlenlp import Taskflow\r\n    import fastdeploy as fd\r\n\r\n    option = fd.RuntimeOption()\r\n    model_path ='/mnt/storage/utc/plm_v2/static/inference.pdmodel'\r\n    params_path = '/mnt/storage/utc/plm_v2/static/inference.pdiparams'\r\n    option.set_model_path(model_path, params_path)\r\n    option.use_gpu(1)\r\n    model=fd.Runtime(option)\r\n    utcClassification = Taskflow(\"zero_shot_text_classification\",\r\n                                 model=\"utc-xbase\",\r\n                                 schema=utc_labels,\r\n                                 task_path='/mnt/storage/utc/plm_v2',\r\n                                 pred_threshold=0,\r\n                                 max_seq_len=2048,\r\n                                 device_id=0,\r\n                                 num_workers=1)\r\n`",
        "state": "closed",
        "user": "magicleo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-20T09:03:39+00:00",
        "updated_at": "2025-05-27T06:44:32+00:00",
        "closed_at": "2025-05-27T06:44:32+00:00",
        "comments_count": [
            "jiangjiajun",
            "magicleo",
            "magicleo",
            "watertianyi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2259,
        "title": "请问升腾npu编译的时候，paddlelite怎么和fastdeploy进行结合？",
        "body": "## 环境\r\nascend 310p  On X86_64 platform\r\n\r\n## 方法\r\n（1）https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/huawei_ascend.md 按照这个官方文档讲到了自己编译的方法，成功后会在 Paddle-Lite/build.lite.linux.x86.gcc/inference_lite_lib/ 生成 Paddle Lite 编译包\r\n（2）C++ FastDeploy library compilation based on Paddle Lite\r\n         这一节中按照命令进行编译，但是没有看到如何关联上一步自己编译的Paddle Lite 编译包，而cmake中默认用的是https://paddle-qa.bj.bcebos.com/Paddle-Lite/DevelopDailyBuild/FastDeploy.CPP.inference_lite_lib.ubuntu.x86.huawei_ascend_npu.CANN5.1.RC2.alpha001.tar.gz\r\n## 问题\r\n怎么编译fastdelpoy的时候，用上自己在ascend上编译出来的paddlelite编译包，而不是用默认的\r\n",
        "state": "closed",
        "user": "xiaomujiang",
        "closed_by": "rainyfly",
        "created_at": "2023-10-24T05:14:15+00:00",
        "updated_at": "2024-02-06T11:14:10+00:00",
        "closed_at": "2024-02-06T11:14:09+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2260,
        "title": "使用paddleDetection的部署模型 如何设置跳帧处理",
        "body": "使用paddleDetection的部署模型 如何设置跳帧处理",
        "state": "closed",
        "user": "jack00000",
        "closed_by": "rainyfly",
        "created_at": "2023-10-24T09:21:09+00:00",
        "updated_at": "2024-02-06T07:52:09+00:00",
        "closed_at": "2024-02-06T07:52:08+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2261,
        "title": "ERROR: No matching distribution found for fast-tokenizer-python",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： master\r\n- 【编译命令】FastDeploy/examples/text/ernie-3.0/python \r\n- 【系统平台】:  mac M2 OSX 13.4 (22F66)\r\n- 【编译语言】：Python 3.11.4\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\npip install -r requirements.txt\r\n",
        "state": "closed",
        "user": "Lowpower",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-24T12:05:53+00:00",
        "updated_at": "2024-12-31T06:41:00+00:00",
        "closed_at": "2024-12-31T06:41:00+00:00",
        "comments_count": [
            "sususnow"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2263,
        "title": "GPU版本 paddlebackend下matting出来的图片不对，有很多横纹，CPU版本正常。跑的官方demo、模型",
        "body": "另：对于个别图片，https://bj.bcebos.com/paddlehub/fastdeploy/PP-Matting-512.tgz CPU版正常抠图，https://paddleseg.bj.bcebos.com/matting/models/deploy/ppmatting-hrnet_w18-human_512.zip CPU和GPU这个模型都会报错",
        "state": "closed",
        "user": "faxiangui",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-25T09:03:47+00:00",
        "updated_at": "2025-05-13T06:50:19+00:00",
        "closed_at": "2025-05-13T06:50:19+00:00",
        "comments_count": [
            "rainyfly",
            "yunwuhen"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2262,
        "title": "facedet 检测结果少一半，输入图片右边没有结果",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-cpu/rknn \r\n- 【编译命令】cmake -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake \\\r\n      -DWITH_TIMVX=ON  \\\r\n      -DTARGET_ABI=armhf \\\r\n      -DENABLE_FLYCV=ON \\\r\n      -DCMAKE_INSTALL_PREFIX=fastdeploy-timvx \\\r\n      -DENABLE_VISION=ON -Wno-dev ..\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： rv1126\r\n- 【编译语言】： C++ \r\n\r\n- 【模型精度问题】\r\n开源项目scrfd  examples 没有提供paddleLite 模型，根据提供的onnx 模型自己下载转换为paddleLite \r\n转换命令：\r\nx2paddle --framework=onnx  --model=/home/aistudio/X2Paddle/face/scrfd/scrfd_10g_bnkps_shape640x640.onnx  --save_dir=./out --lite_valid_places=rockchip_npu,arm --lite_model_type=naive_buffer\r\n开源项目scrfd  examples 没有提供rknn demo 与arm cpu demo 自己修改代码添加Backend::LITE\r\nSCRFD::SCRFD(const std::string& model_file, const std::string& params_file,\r\n                                       const RuntimeOption& custom_option,\r\n                                       const ModelFormat& model_format) {\r\n           if (model_format == ModelFormat::ONNX) {\r\n                                  valid_cpu_backends = {Backend::ORT};\r\n                                  valid_gpu_backends = {Backend::ORT, Backend::TRT};  \r\n            } else {\r\n                              valid_cpu_backends = {Backend::PDINFER, Backend::ORT, Backend::LITE};\r\n                              valid_gpu_backends = {Backend::PDINFER, Backend::ORT, Backend::TRT};\r\n                              valid_rknpu_backends = {Backend::RKNPU2};\r\n                              valid_sophgonpu_backends = {Backend::SOPHGOTPU};\r\n                              valid_timvx_backends = {Backend::LITE};\r\n               }\r\n }\r\n编译下载到板子推理执行，程序正常跑完，但是画面中人脸的结果少了一半，输入图片右边没有结果：啥情况请指导一下！！！\r\n测试多张图片一样效果，相同的现象在yolov5sface，ultr demo 中复现，因此猜测是fastdepoy facedet 中的代码问题\r\n![vis_result](https://github.com/PaddlePaddle/FastDeploy/assets/38774713/38f9e384-6699-465d-ace6-042d793fcfe0)\r\n![vis_result (4)](https://github.com/PaddlePaddle/FastDeploy/assets/38774713/62ee85c8-3e03-4efd-8156-c18cb35a3aa1)\r\n\r\n\r\n- 【性能问题】描述清楚对比的方式\r\n模型： scrfd_10g_bnkps_shape640x640\r\ncpu 推理时间：7.9725s\r\n量化后cpu 推理时间： 0.081284s\r\nnpu 加速效果不明显： 0.08341s\r\n\r\nsubgraph.txt  中间层是否异构到arm cpu 推理时间差不多，啥原因？？\r\nfeed:feed:x2paddle_input_1\r\nXXXX\r\nfetch:sigmoid_3.tmp_0:fetch\r\nfetch:sigmoid_4.tmp_0:fetch\r\nfetch:sigmoid_5.tmp_0:fetch\r\nfetch:reshape2_2.tmp_0:fetch\r\nfetch:reshape2_5.tmp_0:fetch\r\nfetch:reshape2_8.tmp_0:fetch\r\nfetch:reshape2_1.tmp_0:fetch\r\nfetch:reshape2_4.tmp_0:fetch\r\nfetch:reshape2_7.tmp_0:fetch\r\n\r\n\r\n",
        "state": "closed",
        "user": "erroot",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-24T13:45:39+00:00",
        "updated_at": "2025-02-11T06:44:02+00:00",
        "closed_at": "2025-02-11T06:44:02+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2264,
        "title": "此页面多目标追踪模型  fastdeploy 不可 fd.vision.tracking.PPTracking使用",
        "body": "model = fd.vision.detection.PPYOLO(model_file, params_file, infer_cfg_file, option)  这样不报错 但预测的result里面没有",
        "state": "closed",
        "user": "jack00000",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-25T12:17:59+00:00",
        "updated_at": "2025-02-11T06:44:02+00:00",
        "closed_at": "2025-02-11T06:44:02+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2265,
        "title": "关于Windows下paddleseg C++部署，遇到“没有为 infer_demo.exe 加载的符号文件”的问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7（下载官方提供的）\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：NVIDIA GeForce RTX 3080， CUDA 11.8  CUDNN 8.7.0\r\n- 【编译语言】： C++ （vs2017）\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【操作流程】我是按照在命令行上来cmake编译环境和项目的，也是通过bat文件将dll文件放在了release下面。\r\n-  【文件目录如下】\r\n- \r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108455796/9675b5b4-c3e7-4c29-a5a4-4600727c8e56)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108455796/34065a24-e4c7-4649-aabd-60550de91196)\r\n![1698305663790](https://github.com/PaddlePaddle/FastDeploy/assets/108455796/ab61410d-baff-4d26-9c3b-71d7978aa80a)\r\n- 【推理代码修改处】\r\n- 我是用官方提供的infer.cc只修改了主函数main的内容，引入了图片和模型的路径，备注了没有用到的东西。\r\n-\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108455796/783b85cd-d111-49dd-8aab-fce69999bfe4)\r\n- 【报错内容】“没有为 infer_demo.exe 加载的符号文件”\r\n-  前面操作一直都是很顺利的，release下编译也是没有问题的，但是在运行时报错，dll文件也是通过bat来复制到release下的，实在是不知道哪里有问题，有没有大佬可以指点一下，在这个问题上已经困惑一天了。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108455796/580410ce-9067-410a-bf48-a55b6bbaf422)\r\n\r\n-【关于测试模型和图像】我使用的是官方提供好的模型，PP_LiteSeg_B_STDC2_cityscapes_with_argmax_infer和cityscapes_demo.jpg\r\n\r\n\r\n",
        "state": "closed",
        "user": "pcycccccc",
        "closed_by": "pcycccccc",
        "created_at": "2023-10-26T07:41:03+00:00",
        "updated_at": "2023-10-27T05:33:58+00:00",
        "closed_at": "2023-10-27T05:33:58+00:00",
        "comments_count": [
            "pcycccccc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2266,
        "title": "InvalidArgumentError: some trt inputs dynamic shape info not set, check the INFO log above for more details.",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 \r\nOS Platform: Windows x64 \r\nHardware: Nvidia GPU 3050  CUDA 11.7 CUDNN 8.4\r\nProgram Language: Python 3.8\r\n\r\n## Problem description\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/107621428/1bc09f10-fb7d-48d6-ae8f-bb9394b08181)\r\n\r\ni have set the inputs for tensort its showing still missing, how to fix this error? i am using paddle fasterrcnn model.",
        "state": "closed",
        "user": "mahesh11T",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-10-27T10:27:41+00:00",
        "updated_at": "2025-02-11T06:44:03+00:00",
        "closed_at": "2025-02-11T06:44:03+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2270,
        "title": "Yolov5seg RK3588 启用 NPU 报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：develop\r\n- 【编译命令】\r\n\r\n```shell\r\ncmake ..  -DENABLE_ORT_BACKEND=OFF \\                                                 ─╯\r\n              -DENABLE_RKNPU2_BACKEND=ON \\\r\n              -DENABLE_VISION=ON \\\r\n              -DRKNN2_TARGET_SOC=RK3588 \\\r\n          -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.0\r\nmake -j8\r\nmake install\r\n```\r\n\r\n- 【系统平台】: Linux arm64(Ubuntu 22.04)\r\n- 【硬件】： RK3588\r\n- 【编译语言】： C++ / Python(3.10）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【yolov5seg 开启 npu 报错】\r\n```python\r\nimport os\r\n\r\nimport cv2\r\nimport fastdeploy as fd\r\n\r\nruntime_option = fd.RuntimeOption()\r\nruntime_option.use_rknpu2()\r\n\r\nmodel = fd.vision.detection.YOLOv5Seg('best-slim.rknn',\r\n                                      runtime_option=runtime_option,\r\n                                      model_format=fd.ModelFormat.RKNN\r\n                                      )\r\n\r\nim = cv2.imread('frame_0025.jpg')\r\nresult = model.predict(im)\r\n\r\n# Visualization\r\nvis_im = fd.vision.vis_detection(im, result)\r\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\r\nprint(\"Visualized result save in ./visualized_result.jpg\")\r\n\r\n```\r\n\r\n```\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model: yolov5seg\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov5seg/yolov5seg.cc(43)::Initialize      Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"/opt/FastDeploy/examples/vision/segmentation/paddleseg/semantic_segmentation/rockchip/rknpu2/python/main.py\", line 9, in <module>\r\n    model = fd.vision.detection.YOLOv5Seg('best-slim.onnx',\r\n  File \"/root/miniconda3/envs/fastdeploy/lib/python3.10/site-packages/fastdeploy/vision/detection/contrib/yolov5seg.py\", line 181, in __init__\r\n    assert self.initialized, \"YOLOv5Seg initialize failed.\"\r\nAssertionError: YOLOv5Seg initialize failed.\r\n```\r\n\r\nCPU 模式下正确，注释掉 runtime_option.use_rknpu2() 和使用 ONNX模型\r\n",
        "state": "closed",
        "user": "chenglong-do",
        "closed_by": "rainyfly",
        "created_at": "2023-11-04T07:28:05+00:00",
        "updated_at": "2024-02-06T07:44:35+00:00",
        "closed_at": "2024-02-06T07:44:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2267,
        "title": "使用ocr运行模板执行onnx加速报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】系统提供\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： AI加速卡Tesla-V100-PCIE-32GB， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： C++\r\n\r\n- 【模型跑不通】\r\n- - 执行`examples`下的ONNX runtime部署示例会报错，其余示例均可正常执行,模型也为官方提供模型\r\n- 【执行命令】\r\n- - ./infer_demo ./ch_PP-OCRv3_det_infer ./ch_ppocr_mobile_v2.0_cls_infer ./ch_PP-OCRv3_rec_infer ./ppocr_keys_v1.txt ./12.jpg 6\r\n- 【报错日志】\r\n- - [WARNING] fastdeploy/runtime/runtime_option.cc(395)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(395)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\n[WARNING] fastdeploy/runtime/runtime_option.cc(395)::SetTrtInputShape   `RuntimeOption::SetTrtInputShape` will be removed in v1.2.0, please use `RuntimeOption.trt_option.SetShape()` instead.\r\nterminate called after throwing an instance of 'Ort::Exception'\r\n  what():  /onnxruntime_src/onnxruntime/core/platform/posix/env.cc:183 onnxruntime::{anonymous}::PosixThread::PosixThread(const char*, int, unsigned int (*)(int, Eigen::ThreadPoolInterface*), Eigen::ThreadPoolInterface*, const onnxruntime::ThreadOptions&) pthread_setaffinity_np failed, error code: 17 error msg: File exists\r\n\r\nAborted\r\n",
        "state": "open",
        "user": "Jiangyangya",
        "closed_by": null,
        "created_at": "2023-10-30T06:48:22+00:00",
        "updated_at": "2023-11-02T01:22:37+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiangyangya"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2277,
        "title": "arm64  -DENABLE_ENCRYPTION=ON 编译问题及解决",
        "body": "*********************************************\r\n温馨提示：arm64 环境，-DENABLE_ENCRYPTION=ON 编译问题及解决方案\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-1.0.7\r\n- 【编译命令】git clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DBUILD_ON_JETSON=ON \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_ENCRYPTION=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DPADDLEINFERENCE_DIRECTORY=/home/paddle_inference_install_dir \\\r\n         -DPADDLEINFERENCE_VERSION=2.4.2 \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\nmake -j8\r\nmake install\r\n- 【系统平台】: Linux aarch64 (Ubuntu 20.04)\r\n- 【硬件】： Jetson， Nvidia GPU， JetPack 5.0.2\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【加上 -DENABLE_ENCRYPTION=ON 无法编译】\r\n- 【问题1】执行 make -j8 命令时，报错 openssl 库问题\r\n- 【解决1】若开启加密，需要依赖openssl库，FastDeploy编译时会自动下载（见FastDeploy/cmake/openssl.cmake），但是下载的库是 linux-x86-64架构的，jetson 是 arm64 架构，所以需要在官网主动下载相应版本的 openssl 源码在 jetson 编译，把编译的包放入\"build/third_libs\" 文件夹（目录结构：openssl-1.1.0k/install-aarch64/include,openssl-1.1.0k/install-aarch64/lib），并打开 \"FastDeploy/cmake/openssl.cmake\" 文件，注释掉下载并解压那行代码， 再重新执行 make -j8 命令。\r\n-  建议官方更改 \"FastDeploy/cmake/openssl.cmake\" 文件，使其支持 arm64 openssl.\r\n",
        "state": "closed",
        "user": "qx1216",
        "closed_by": "rainyfly",
        "created_at": "2023-11-08T03:26:49+00:00",
        "updated_at": "2024-02-06T07:45:31+00:00",
        "closed_at": "2024-02-06T07:45:31+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2271,
        "title": "ocrv4 支持了吗？",
        "body": null,
        "state": "open",
        "user": "xinsuinizhuan",
        "closed_by": null,
        "created_at": "2023-11-06T09:31:01+00:00",
        "updated_at": "2024-07-30T03:12:26+00:00",
        "closed_at": null,
        "comments_count": [
            "schild",
            "thomasbergersen"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2274,
        "title": "FastDeploy支持在windows 32位系统上编译布署吗？",
        "body": "FastDeploy支持在windows 32位系统上编译布署吗？",
        "state": "closed",
        "user": "JackLee1",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-07T05:46:23+00:00",
        "updated_at": "2024-11-12T06:42:29+00:00",
        "closed_at": "2024-11-12T06:42:29+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2273,
        "title": "ENABLE_ENCRYPTION=ON 在windows x64 平台编译报错及解决",
        "body": "*********************************************\r\n温馨提示：开启 ENABLE_ENCRYPTION 时在windows x64 平台编译报错及解决方式，本人参试可行，仅作参考\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7\r\n- 【编译命令】: mkdir build && cd build\r\n   cmake .. -G \"Visual Studio 16 2019\" -A x64   ^\r\n                 -DENABLE_ORT_BACKEND=ON   ^\r\n                 -DENABLE_VISION=ON   ^\r\n                 -DENABLE_TEXT=ON   ^\r\n                 -DWITH_GPU=ON   ^\r\n                 -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\"   ^\r\n                 -DENABLE_ENCRYPTION=ON  ^\r\n                 -DCMAKE_INSTALL_PREFIX=\"fastdeploy-win-x64-ort-gpu-1.0.7\"\r\n   msbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\n   msbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 3050， CUDA 11.6 CUDNN 8.9\r\n- 【编译语言】： C++\r\n\r\n## 问题及解决方法\r\n- - 【加上 -DENABLE_ENCRYPTION=ON 后编译报错】\r\n- - 【问题-1】C 不支持 std::vector<std::string> 返回\r\n- - 【解决-1】更改 \"fastdeploy/encryption/include/encrypt.h\":\r\n- - 原函数 FASTDEPLOY_DECL std::vector<std::string> Encrypt(const std::string& input, const std::string& key);\r\n- - 更改为 FASTDEPLOY_DECL std::string Encrypt(const std::string& input, const std::string& key);\r\n- - 对应的 encrypt.cc 文件相应函数更改为：\r\n- -  std::string Encrypt(const std::string& input,  const std::string& key) {\r\n    std::istringstream isst(input);\r\n    std::ostringstream osst;\r\n    int ret =  EncryptStream(isst, osst, key);\r\n    if (ret != 0) {\r\n       FDERROR << ret << \", Failed encrypt \" << std::endl;\r\n       return {\"\", \"\"};\r\n    }\r\n  \r\n   return baidu::base::base64::base64_encode(osst.str());\r\n  }\r\n- - 【问题-2】libcrypto_static.lib(b_sock.obj) : error LNK2019: 无法解析的外部符号 \r\n- - 【解决-2】 \"cmake/openssl.cmake\" 设置 OPENSSL_LIBRARIES 时加入 ws2_32，具体为：\r\n- -  IF(CMAKE_SYSTEM_NAME MATCHES \"Windows\")\r\n        set(OPENSSL_LIBRARIES\r\n                        \"${OPENSSL_ROOT_DIR}/lib/libssl_static.lib\"\r\n                        \"${OPENSSL_ROOT_DIR}/lib/libcrypto_static.lib\"\r\n                        ${GFLAGS_LIBRARIES}\r\n                        shlwapi\r\n                        ws2_32\r\n                        CACHE FILEPATH \"OPENSSL_LIBRARIES\" FORCE)\r\n",
        "state": "closed",
        "user": "qx1216",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-07T04:25:45+00:00",
        "updated_at": "2024-11-12T06:42:28+00:00",
        "closed_at": "2024-11-12T06:42:28+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2278,
        "title": "不支持 tensorrt 模型加密问题及解决",
        "body": "*********************************************\r\n温馨提示：不支持 tensorrt 模型加密问题及解决\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-1.0.7\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n-  【问题1】支持 onnx 或 paddle 模型的加密，但不支持对 tensorrt 模型的加密\r\n-  【解决1】按以下进行更改，并在调用时加上密钥设置：fd::RuntimeOption runtime_option; runtime_option.trt_option.encryption_key_ =\"pwd\";\r\n-  \"./fastdeploy/runtime/backends/tensorrt/option.h\" 在 struct TrtBackendOption 结构体内，增加：\r\nstd::string encryption_key_ = \"\";\r\n-  \"./fastdeploy/runtime/backends/tensorrt/trt_backend.cc\" 在第26行加入：\r\n#ifdef ENABLE_ENCRYPTION\r\n#include \"fastdeploy/encryption/include/decrypt.h\"\r\n#endif\r\n-  \"./fastdeploy/runtime/backends/tensorrt/trt_backend.cc\" 在第70行（bool TrtBackend::LoadTrtCache(const std::string& trt_engine_file)函数内）\r\n增加以下代码：\r\n  // 解密\r\n  if (option_.encryption_key_ != \"\")\r\n  {\r\n#ifdef ENABLE_ENCRYPTION\r\n    engine_buffer = Decrypt(engine_buffer, option_.encryption_key_);\r\n#endif\r\n  }\r\n",
        "state": "closed",
        "user": "qx1216",
        "closed_by": "rainyfly",
        "created_at": "2023-11-08T03:29:17+00:00",
        "updated_at": "2024-02-06T07:45:04+00:00",
        "closed_at": "2024-02-06T07:45:03+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2280,
        "title": "./infer_demo: symbol lookup error",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： CPU\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\n./infer_demo: symbol lookup error : /usr/local/lib/libfastdeploy.so.1.0.7: undefined symbol _ZN2cv3MatC1EV",
        "state": "open",
        "user": "LeungWaiHo",
        "closed_by": null,
        "created_at": "2023-11-09T05:23:59+00:00",
        "updated_at": "2023-11-13T00:47:29+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "LeungWaiHo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2279,
        "title": "serving部署，openvino报错找不到模型仅支持onnx和pdmodel",
        "body": "我看了文档，说支持openvino后端\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108776297/d318c49c-8929-405c-adb6-8532106cc83f)\r\n这是我的模型和命名\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108776297/855a7972-b580-4d6c-82d6-f0be6a322321)\r\n报错，找不到模型，仅支持onnx和pdmodel\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/108776297/237545fc-cf7b-42ee-ad87-f0eb08940c37)\r\n这是为什么呢？",
        "state": "closed",
        "user": "lxiaoxiaoxing",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-08T03:33:40+00:00",
        "updated_at": "2025-02-11T06:44:04+00:00",
        "closed_at": "2025-02-11T06:44:04+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2281,
        "title": "error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: 在两个平台上测试均有该问题，平台1：AGX Ubuntu 20.04;  平台2：RG3588（LINUX）\r\n- 【硬件】： 平台1： CUDA 11.4 CUDNN 8.6 TRT5.1.1\r\n- 【编译语言】： C++ \r\n\r\n\r\n## 运行报错内容\r\n- - 先执行`examples`下的部署示例，出现报错“error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory”\r\n\r\n##  尝试以下三种解决方式\r\n- -1. export LD_LIBRARY_PATH= /path/to/libonnxruntime.so.1.12.0，同一窗口下然后执行二进制\r\n- -2.直接将文件粘贴至build文件夹下，然后执行二进制\r\n- -3.执行 source /path/to/fastdeploy-linux-xxx/fastdeploy_init.sh，然后执行二进制\r\n- -以上三种方式均没有解决此类问题，可能不是路径的问题，但是想不到还有什么点没有考虑到\r\n",
        "state": "open",
        "user": "catofyuanyuan",
        "closed_by": null,
        "created_at": "2023-11-09T08:50:44+00:00",
        "updated_at": "2025-01-08T03:22:03+00:00",
        "closed_at": null,
        "comments_count": [
            "catofyuanyuan",
            "wwfcnu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2282,
        "title": " rk3588交叉编译问题：collect2: error: ld returned 1 exit status",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【FastDeploy版本】： fastdeploy-1.0.7\r\n- 【编译命令】\r\n        cd FastDeploy-release-1.0.7/build\r\n        sudo cmake .. \\\r\n        -DCMAKE_C_COMPILER=/opt/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc \\\r\n        -DCMAKE_CXX_COMPILER=/opt/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ \\\r\n        -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake -DTARGET_ABI=arm64 \\\r\n        -DENABLE_ORT_BACKEND=ON \\\r\n        -DENABLE_RKNPU2_BACKEND=ON \\\r\n        -DENABLE_VISION=ON \\\r\n        -DRKNN2_TARGET_SOC=RK3588 \\\r\n        -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.0\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 无GPU， CUDA 11.1\r\n- 【编译语言】： C++ \r\n\r\n## rk3588编译问题\r\nsudo make -j8 编译运行至100%，出现问题如下：\r\n...\r\n[ 99%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/vision/visualize/visualize.cc.o\r\n[ 99%] Building CXX object CMakeFiles/fastdeploy.dir/fastdeploy/pipeline/pptinypose/pipeline.cc.o\r\n[100%] Linking CXX shared library libfastdeploy.so\r\nthird_libs/install/onnxruntime/lib/libonnxruntime.so: error adding symbols: File in wrong format\r\n@collect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:3098: libfastdeploy.so.1.0.7] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:148: CMakeFiles/fastdeploy.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\n\r\n",
        "state": "closed",
        "user": "catofyuanyuan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-09T09:15:07+00:00",
        "updated_at": "2024-12-31T06:41:01+00:00",
        "closed_at": "2024-12-31T06:41:00+00:00",
        "comments_count": [
            "catofyuanyuan",
            "Luo73"
        ],
        "labels": [
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2283,
        "title": "Is TensorRT model supported by ModelFormat?",
        "body": "Why doesn’t ModelFormat support TensorRT? It’s inconvenient to recompile from ONNX to TRT model every time when using TRT. Is there a way to directly load TensorRT models?\r\n",
        "state": "closed",
        "user": "bo-scnu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-09T12:28:25+00:00",
        "updated_at": "2024-11-12T06:42:30+00:00",
        "closed_at": "2024-11-12T06:42:29+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2284,
        "title": "RK3588 FastDeploy 部署 pp_liteseg 结果异常",
        "body": "### 问题确认 Search before asking\r\n\r\n- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleSeg/issues)(包括open与closed)，没有发现相似的bug。I have searched the [open and closed issues](https://github.com/PaddlePaddle/PaddleSeg/issues) and found no similar bug report.\r\n\r\n\r\n### Bug描述 Describe the Bug\r\n\r\n按照 [PaddleSeg RKNPU2 C++部署示例] 导出模型、转换模型、使用C++和Python示例代码均无法识别出结果。在X86环境下使用fastdeploy部署结果正常。(https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/deploy/fastdeploy/semantic_segmentation/rockchip/rknpu2/cpp#paddleseg-rknpu2-c%E9%83%A8%E7%BD%B2%E7%A4%BA%E4%BE%8B)\r\n\r\npp_liteseg_stdc2.yml\r\n\r\n```xml\r\n_base_: './pp_liteseg_stdc1_camvid_960x720_10k.yml'\r\n\r\nbatch_size: 6  # total: 4*6\r\niters: 10000\r\n\r\ntrain_dataset:\r\n  type: Dataset\r\n  dataset_root: /opt/drone/datasets-seg-paddle\r\n  num_classes: 2\r\n  mode: train\r\n  train_path: /opt/drone/datasets-seg-paddle/train.txt\r\n  transforms:\r\n    - type: ResizeStepScaling\r\n      min_scale_factor: 0.5\r\n      max_scale_factor: 2.5\r\n      scale_step_size: 0.25\r\n    - type: RandomPaddingCrop\r\n      crop_size: [960, 720]\r\n    - type: RandomHorizontalFlip\r\n    - type: RandomDistort\r\n      brightness_range: 0.5\r\n      contrast_range: 0.5\r\n      saturation_range: 0.5\r\n    - type: Normalize\r\n\r\nval_dataset:\r\n  type: Dataset\r\n  dataset_root: /opt/drone/datasets-seg-paddle\r\n  num_classes: 2\r\n  mode: val\r\n  val_path: /opt/drone/datasets-seg-paddle/val.txt\r\n  transforms:\r\n    - type: Normalize\r\n\r\noptimizer:\r\n  type: SGD\r\n  momentum: 0.9\r\n  weight_decay: 5.0e-4\r\n\r\nlr_scheduler:\r\n  type: PolynomialDecay\r\n  learning_rate: 0.01\r\n  end_lr: 0\r\n  power: 0.9\r\n  warmup_iters: 200\r\n  warmup_start_lr: 1.0e-5\r\n\r\nloss:\r\n  types:\r\n    - type: OhemCrossEntropyLoss\r\n      min_kept: 250000   # batch_size * 960 * 720 // 16\r\n    - type: OhemCrossEntropyLoss\r\n      min_kept: 250000\r\n    - type: OhemCrossEntropyLoss\r\n      min_kept: 250000\r\n  coef: [1, 1, 1]\r\n\r\nmodel:\r\n  _inherited_: False  # not inherit the model params from the base yaml\r\n  type: PPLiteSeg\r\n  backbone:\r\n    type: STDC2\r\n    pretrained: https://bj.bcebos.com/paddleseg/dygraph/PP_STDCNet2.tar.gz\r\n  arm_out_chs: [32, 64, 128]\r\n  seg_head_inter_chs: [32, 64, 64]\r\n```\r\n\r\n导出模型命令\r\n\r\n```shell\r\npython tools/export.py \\\r\n        --config configs/pp_liteseg/pp_liteseg_stdc2.yml \\\r\n        --input_shape 1 3 960 720 \\\r\n        --output_op none \\\r\n       --model_path output/best_model/model.pdparams \\\r\n        --save_dir output/inference_model\r\n```\r\n\r\n转换 ONNX\r\n\r\n```shell\r\npaddle2onnx --model_dir PaddleSeg/output/inference_model/ \\\r\n            --model_filename model.pdmodel \\\r\n            --params_filename model.pdiparams \\\r\n            --save_file PaddleSeg/output/inference_model/infer.onnx \\\r\n            --enable_dev_version True --opset_version 11\r\n# 日志\r\n[Paddle2ONNX] Start to parse PaddlePaddle model...\r\n[Paddle2ONNX] Model file path: /opt/drone/PaddleSeg/output/inference_model/model.pdmodel\r\n[Paddle2ONNX] Paramters file path: /opt/drone/PaddleSeg/output/inference_model/model.pdiparams\r\n[Paddle2ONNX] Start to parsing Paddle model...\r\n[Paddle2ONNX] Use opset_version = 11 for ONNX export.\r\n[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\r\n```\r\n\r\n转换 rknn\r\n\r\n```xml\r\nmean:\r\n  -\r\n    - 123.675\r\n    - 116.28\r\n    - 103.53\r\nstd:\r\n  -\r\n    - 58.395\r\n    - 57.12\r\n    - 57.375\r\nmodel_path: PaddleSeg/output/inference_model/infer.onnx\r\noutputs_nodes:\r\ndo_quantization: True\r\ndataset: \"FastDeploy/tools/rknpu2/dataset.txt\"\r\noutput_folder: \"./output\"\r\n```\r\n\r\n```shell\r\npython tools/rknpu2/export.py --config_path config/pp_liteseg.yaml  --target_platform rk3588\r\n# 日志\r\n{'mean': [[123.675, 116.28, 103.53]], 'std': [[58.395, 57.12, 57.375]], 'model_path': '/opt/drone/PaddleSeg/output/inference_model/infer.onnx', 'outputs_nodes': None, 'do_quantization': True, 'dataset': '/opt/drone/FastDeploy/tools/rknpu2/dataset.txt', 'output_folder': './output'}\r\nW __init__: rknn-toolkit2 version: 1.5.2+b642f30c\r\nW load_onnx: It is recommended onnx opset 12, but your onnx model opset is 11!\r\n\r\nI base_optimize ...\r\nI base_optimize done.\r\nI \r\nI fold_constant ...\r\nI fold_constant done.\r\nI fold_constant remove nodes = ['p2o.Concat.36', 'p2o.Slice.11', 'p2o.Shape.22', 'p2o.Cast.11', 'p2o.Concat.33', 'p2o.Slice.10', 'p2o.Shape.20', 'p2o.Cast.10', 'p2o.Slice.9', 'p2o.Cast.9', 'p2o.Shape.18', 'p2o.Concat.30', 'p2o.Slice.8', 'p2o.Shape.16', 'p2o.Cast.8', 'p2o.Slice.7', 'p2o.Cast.7', 'p2o.Shape.14', 'p2o.Concat.27', 'p2o.Slice.6', 'p2o.Shape.12', 'p2o.Cast.6', 'p2o.Slice.5', 'p2o.Cast.5', 'p2o.Shape.10', 'p2o.Concat.26', 'p2o.Slice.4', 'p2o.Shape.8', 'p2o.Cast.4', 'p2o.Concat.25', 'p2o.Slice.3', 'p2o.Shape.6', 'p2o.Cast.3', 'p2o.Concat.24', 'p2o.Slice.2', 'p2o.Shape.4', 'p2o.Cast.2', 'p2o.Slice.1', 'p2o.Cast.1', 'p2o.Shape.2', 'p2o.Slice.0', 'p2o.Cast.0', 'p2o.Shape.0']\r\nI \r\nI Output[bilinear_interp_v2_6.tmp_0] shape with str value may cause error, replace [1, 2, 'unk__36', 'unk__37'] with [1, 2, 960, 720].\r\nI correct_ops ...\r\nI correct_ops done.\r\nI \r\nI fuse_ops ...\r\nI fuse_ops results:\r\nI     fuse_bn_into_conv: remove node = ['p2o.BatchNormalization.0', 'p2o.BatchNormalization.1', 'p2o.BatchNormalization.2', 'p2o.BatchNormalization.3', 'p2o.BatchNormalization.4', 'p2o.BatchNormalization.5', 'p2o.BatchNormalization.6', 'p2o.BatchNormalization.7', 'p2o.BatchNormalization.8', 'p2o.BatchNormalization.9', 'p2o.BatchNormalization.10', 'p2o.BatchNormalization.11', 'p2o.BatchNormalization.12', 'p2o.BatchNormalization.13', 'p2o.BatchNormalization.14', 'p2o.BatchNormalization.15', 'p2o.BatchNormalization.16', 'p2o.BatchNormalization.17', 'p2o.BatchNormalization.18', 'p2o.BatchNormalization.19', 'p2o.BatchNormalization.20', 'p2o.BatchNormalization.21', 'p2o.BatchNormalization.22', 'p2o.BatchNormalization.23', 'p2o.BatchNormalization.24', 'p2o.BatchNormalization.25', 'p2o.BatchNormalization.26', 'p2o.BatchNormalization.27', 'p2o.BatchNormalization.28', 'p2o.BatchNormalization.29', 'p2o.BatchNormalization.30', 'p2o.BatchNormalization.31', 'p2o.BatchNormalization.32', 'p2o.BatchNormalization.33', 'p2o.BatchNormalization.34', 'p2o.BatchNormalization.35', 'p2o.BatchNormalization.36', 'p2o.BatchNormalization.37', 'p2o.BatchNormalization.38', 'p2o.BatchNormalization.39', 'p2o.BatchNormalization.40', 'p2o.BatchNormalization.41', 'p2o.BatchNormalization.42', 'p2o.BatchNormalization.43', 'p2o.BatchNormalization.44', 'p2o.BatchNormalization.45', 'p2o.BatchNormalization.46', 'p2o.BatchNormalization.47', 'p2o.BatchNormalization.48', 'p2o.BatchNormalization.49', 'p2o.BatchNormalization.50', 'p2o.BatchNormalization.51', 'p2o.BatchNormalization.52', 'p2o.BatchNormalization.53', 'p2o.BatchNormalization.54', 'p2o.BatchNormalization.55', 'p2o.BatchNormalization.56']\r\nI     remove_invalid_resize: remove node = ['p2o.Resize.3']\r\nI     fuse_bn_into_conv: remove node = ['p2o.BatchNormalization.57', 'p2o.BatchNormalization.58', 'p2o.BatchNormalization.59', 'p2o.BatchNormalization.60', 'p2o.BatchNormalization.61', 'p2o.BatchNormalization.62', 'p2o.BatchNormalization.63', 'p2o.BatchNormalization.64', 'p2o.BatchNormalization.65', 'p2o.BatchNormalization.66', 'p2o.BatchNormalization.67', 'p2o.BatchNormalization.68', 'p2o.BatchNormalization.69']\r\nI     convert_global_avgpool_to_conv: remove node = ['p2o.GlobalAveragePool.0'], add node = ['p2o.GlobalAveragePool.0_2conv_0', 'p2o.GlobalAveragePool.1']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.2'], add node = ['p2o.ReduceMean.2_2avgpool']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.0'], add node = ['p2o.ReduceMean.0_2avgpool']\r\nI     convert_concat_to_conv_concat: remove node = [], add node = ['p2o.ReduceMean.1_conv_p2o.Concat.28', 'p2o.ReduceMax.1_conv_p2o.Concat.28', 'p2o.ReduceMean.3_conv_p2o.Concat.28', 'p2o.Concat.29_conv']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.6'], add node = ['p2o.ReduceMean.6_2avgpool']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.4'], add node = ['p2o.ReduceMean.4_2avgpool']\r\nI     convert_concat_to_conv_concat: remove node = [], add node = ['p2o.ReduceMean.5_conv_p2o.Concat.31', 'p2o.ReduceMax.5_conv_p2o.Concat.31', 'p2o.ReduceMean.7_conv_p2o.Concat.31', 'p2o.Concat.32_conv']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.10'], add node = ['p2o.ReduceMean.10_2avgpool']\r\nI     convert_reduce_mean_to_avgpool: remove node = ['p2o.ReduceMean.8'], add node = ['p2o.ReduceMean.8_2avgpool']\r\nI     convert_concat_to_conv_concat: remove node = [], add node = ['p2o.ReduceMean.9_conv_p2o.Concat.34', 'p2o.ReduceMax.9_conv_p2o.Concat.34', 'p2o.ReduceMean.11_conv_p2o.Concat.34', 'p2o.Concat.35_conv']\r\nI     fold_constant ...\r\nI     fold_constant done.\r\nI fuse_ops done.\r\n\r\nI rknn building ...\r\nI RKNN: [15:46:28.270] compress = 0, conv_eltwise_activation_fuse = 1, global_fuse = 1, multi-core-model-mode = 7, output_optimize = 1,enable_argb_group=0\r\nI RKNN: librknnc version: 1.5.2 (c6b7b351a@2023-08-23T07:30:34)\r\nD RKNN: [15:46:28.308] RKNN is invoked\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.9\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.10\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.15\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.16\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.21\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.22\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.40\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.41\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.49\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.50\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.54\r\nW RKNN: [15:46:28.408] Model initializer tensor data is empty, name: p2o.helper.constant.55\r\nD RKNN: [15:46:28.413] >>>>>> start: N4rknn19RKNNSetOpTargetPassE\r\nD RKNN: [15:46:28.413] <<<<<<<< end: N4rknn19RKNNSetOpTargetPassE\r\nD RKNN: [15:46:28.413] >>>>>> start: N4rknn16RKNNAddFirstConvE\r\nD RKNN: [15:46:28.413] <<<<<<<< end: N4rknn16RKNNAddFirstConvE\r\nD RKNN: [15:46:28.413] >>>>>> start: N4rknn27RKNNEliminateQATDataConvertE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn27RKNNEliminateQATDataConvertE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn17RKNNTileGroupConvE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn17RKNNTileGroupConvE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn15RKNNAddConvBiasE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn15RKNNAddConvBiasE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn15RKNNTileChannelE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn15RKNNTileChannelE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn18RKNNPerChannelPrepE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn18RKNNPerChannelPrepE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn11RKNNBnQuantE\r\nD RKNN: [15:46:28.414] <<<<<<<< end: N4rknn11RKNNBnQuantE\r\nD RKNN: [15:46:28.414] >>>>>> start: N4rknn21RKNNFuseOptimizerPassE\r\nD RKNN: [15:46:28.415] <<<<<<<< end: N4rknn21RKNNFuseOptimizerPassE\r\nD RKNN: [15:46:28.415] >>>>>> start: N4rknn15RKNNTurnAutoPadE\r\nD RKNN: [15:46:28.415] <<<<<<<< end: N4rknn15RKNNTurnAutoPadE\r\nD RKNN: [15:46:28.415] >>>>>> start: N4rknn16RKNNInitRNNConstE\r\nD RKNN: [15:46:28.415] <<<<<<<< end: N4rknn16RKNNInitRNNConstE\r\nD RKNN: [15:46:28.415] >>>>>> start: N4rknn17RKNNInitCastConstE\r\nD RKNN: [15:46:28.415] <<<<<<<< end: N4rknn17RKNNInitCastConstE\r\nD RKNN: [15:46:28.415] >>>>>> start: N4rknn20RKNNMultiSurfacePassE\r\nD RKNN: [15:46:28.415] <<<<<<<< end: N4rknn20RKNNMultiSurfacePassE\r\nD RKNN: [15:46:28.415] >>>>>> start: N4rknn14RKNNTilingPassE\r\nW RKNN: [15:46:28.417] Failed to config layer: 'Conv:p2o.GlobalAveragePool.0_2conv_0' using 2Core fallback to single core mode,\r\nW RKNN: [15:46:28.417] core_num 2 ori_Ih 30 ori_Iw 23 ori_Ic 1024 ori_Ib 1 \r\nW RKNN: [15:46:28.417] ori_Kh 7 ori_Kw 7 ori_Kk 1024 ori_Kc 1 ori_Ksx 7 ori_Ksy 7 \r\nW RKNN: [15:46:28.417] ori_Oh 5 oriOw 4 oriOc 1024 pad_t 2 pad_b 3 pad_l 2 pad_r 3,\r\nW RKNN: [15:46:28.417] Please help report this bug!\r\nW RKNN: [15:46:28.417] Failed to config layer: 'Conv:p2o.GlobalAveragePool.0_2conv_0' using 3Core fallback to single core mode,\r\nW RKNN: [15:46:28.417] core_num 3 ori_Ih 30 ori_Iw 23 ori_Ic 1024 ori_Ib 1 \r\nW RKNN: [15:46:28.417] ori_Kh 7 ori_Kw 7 ori_Kk 1024 ori_Kc 1 ori_Ksx 7 ori_Ksy 7 \r\nW RKNN: [15:46:28.417] ori_Oh 5 oriOw 4 oriOc 1024 pad_t 2 pad_b 3 pad_l 2 pad_r 3,\r\nW RKNN: [15:46:28.417] Please help report this bug!\r\nD RKNN: [15:46:28.417] <<<<<<<< end: N4rknn14RKNNTilingPassE\r\nD RKNN: [15:46:28.417] >>>>>> start: OpEmit\r\nD RKNN: [15:46:28.420] <<<<<<<< end: OpEmit\r\nD RKNN: [15:46:28.420] >>>>>> start: N4rknn19RKNNLayoutMatchPassE\r\nD RKNN: [15:46:28.420] <<<<<<<< end: N4rknn19RKNNLayoutMatchPassE\r\nD RKNN: [15:46:28.420] >>>>>> start: N4rknn20RKNNAddSecondaryNodeE\r\nD RKNN: [15:46:28.420] <<<<<<<< end: N4rknn20RKNNAddSecondaryNodeE\r\nD RKNN: [15:46:28.420] >>>>>> start: OpEmit\r\nW RKNN: [15:46:28.423] AveragePool count_include_pad=0, fallback to cpu\r\nW RKNN: [15:46:28.426] AveragePool count_include_pad=0, fallback to cpu\r\nW RKNN: [15:46:28.430] AveragePool count_include_pad=0, fallback to cpu\r\nD RKNN: [15:46:28.440] <<<<<<<< end: OpEmit\r\nD RKNN: [15:46:28.440] >>>>>> start: N4rknn23RKNNProfileAnalysisPassE\r\nD RKNN: [15:46:28.440] <<<<<<<< end: N4rknn23RKNNProfileAnalysisPassE\r\nD RKNN: [15:46:28.442] >>>>>> start: N4rknn21RKNNOperatorIdGenPassE\r\nD RKNN: [15:46:28.442] <<<<<<<< end: N4rknn21RKNNOperatorIdGenPassE\r\nD RKNN: [15:46:28.442] >>>>>> start: N4rknn23RKNNWeightTransposePassE\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.concat.0 need paramter qtype, type is set to float16 by default!\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.constant.9 need paramter qtype, type is set to float16 by default!\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.concat.5 need paramter qtype, type is set to float16 by default!\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.constant.49 need paramter qtype, type is set to float16 by default!\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.concat.6 need paramter qtype, type is set to float16 by default!\r\nW RKNN: [15:46:28.558] Warning: Tensor p2o.helper.constant.54 need paramter qtype, type is set to float16 by default!\r\nD RKNN: [15:46:28.565] <<<<<<<< end: N4rknn23RKNNWeightTransposePassE\r\nD RKNN: [15:46:28.565] >>>>>> start: N4rknn26RKNNCPUWeightTransposePassE\r\nD RKNN: [15:46:28.565] <<<<<<<< end: N4rknn26RKNNCPUWeightTransposePassE\r\nD RKNN: [15:46:28.565] >>>>>> start: N4rknn18RKNNModelBuildPassE\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.GlobalAveragePool.0_2conv_0\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.GlobalAveragePool.1\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.GlobalAveragePool.1\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.59\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.59\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.59\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.59\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.59\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.63\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.63\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.63\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.63\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.63\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.67\r\nD RKNN: [15:46:28.568] remove core consumption 2 regtasks for op Conv:p2o.Conv.67\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.67\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.67\r\nD RKNN: [15:46:28.568] remove core consumption 3 regtasks for op Conv:p2o.Conv.67\r\nD RKNN: [15:46:28.814] RKNNModelBuildPass: [Statistics]\r\nD RKNN: [15:46:28.814] total_regcfg_size     :    495328\r\nD RKNN: [15:46:28.814] total_diff_regcfg_size:    237416\r\n\r\nD RKNN: [15:46:28.848] Total Weight Memory Size: 12214784\r\nD RKNN: [15:46:28.848] Total Internal Memory Size: 44761984\r\nD RKNN: [15:46:28.848] Predict Internal Memory RW Amount: 231446657\r\nD RKNN: [15:46:28.848] Predict Weight Memory RW Amount: 12212352\r\nD RKNN: [15:46:28.848] ----------------------------------------\r\nD RKNN: [15:46:28.848] <<<<<<<< end: N4rknn21RKNNMemStatisticsPassE\r\nI rknn buiding done.\r\nW init_runtime: Target is None, use simulator!\r\nExport OK!\r\n```\r\n\r\n使用 PaddleSeg Python 示例代码预测 \r\n\r\n```shell\r\npython3 infer.py --model_file infer_rk3588_quantized.rknn --config_file deploy.yaml --image /opt/images/frame_0000.jpg\r\n# 日志\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.5.1b19 (32afb0e92@2023-07-14T12:46:17)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.8.2\r\nindex=0, name=x, n_dims=4, dims=[1, 960, 720, 3], n_elems=2073600, size=2073600, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-14, scale=0.018658, pass_through=0\r\nindex=0, name=bilinear_interp_v2_6.tmp_0, n_dims=4, dims=[1, 2, 960, 720], n_elems=1382400, size=1382400, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=40, scale=0.086458, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[WARNING] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(420)::InitRKNNTensorMemoryThe input tensor type != model's inputs type.The input_type need INT8,but inputs[0].type is UINT8\r\nSegmentationResult Image masks 10 rows x 10 cols: \r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .....]\r\n...........\r\nresult shape is: [1080 1920]\r\n```\r\n\r\n图片 vis_img.png 无渲染结果\r\n\r\n模型等相关文件链接: https://pan.baidu.com/s/1KRXdXKjLg7Ehygftd5SAEQ?pwd=3hb5 提取码: 3hb5 \r\n\r\n\r\n\r\n### 复现环境 Environment\r\n\r\n- OS: Linux(Ubuntu 20.04)\r\n- PaddlePaddle: 2.4.2.post112\r\n- PaddleSeg: release/2.8\r\n- Python: 3.10.12\r\n- CUDA: 11.2\r\n- RKNN: rknn_toolkit_lite2-1.5.2-cp310-cp310-linux_aarch64.whl(RK3588 运行时)\r\n- RKNN: rknn-toolkit2 1.5.2+b642f30c(X86 转换 RKNN时)\r\n\r\n### Bug描述确认 Bug description confirmation\r\n\r\n- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.\r\n\r\n\r\n### 是否愿意提交PR？ Are you willing to submit a PR?\r\n\r\n- [ ] 我愿意提交PR！I'd like to help by submitting a PR!",
        "state": "closed",
        "user": "chenglong-do",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-10T07:20:14+00:00",
        "updated_at": "2025-04-15T06:43:36+00:00",
        "closed_at": "2025-04-15T06:43:36+00:00",
        "comments_count": [
            "Zheng-Bicheng",
            "1314520gu"
        ],
        "labels": [
            "RK3588"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2290,
        "title": "UIE C++部署报invalid unordered_map<K, T> key",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win64-cpu-1.0.4\r\n- 【编译命令】C++ SDK\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： \r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n运行UIE C++部署示例报以下错误\r\ncore_tokenizers.dll\r\ninvalid unordered_map<K, T> key\r\n模型在python版本下是可以跑通的，请问是什么问题\r\n\r\n\r\n",
        "state": "open",
        "user": "lumin2",
        "closed_by": null,
        "created_at": "2023-11-16T10:28:32+00:00",
        "updated_at": "2025-02-10T03:26:26+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "zhaoyii"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2287,
        "title": "能否支持一下vcpkg安装？",
        "body": "如题",
        "state": "closed",
        "user": "hhxdestiny",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-15T09:39:43+00:00",
        "updated_at": "2025-02-11T06:44:05+00:00",
        "closed_at": "2025-02-11T06:44:05+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2294,
        "title": "请",
        "body": null,
        "state": "closed",
        "user": "Xiaopu17",
        "closed_by": "jiangjiajun",
        "created_at": "2023-11-19T08:09:31+00:00",
        "updated_at": "2023-11-22T06:02:34+00:00",
        "closed_at": "2023-11-22T06:02:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2292,
        "title": "rk3566 python 导入 fastdeploy 报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_RKNPU2_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport RKNN2_TARGET_SOC=RK356X\r\npython3 setup.py build -j3\r\n\r\npython3 setup.py bdist_wheel\r\ncd dist\r\npip3 install fastdeploy_python-0.0.0-cp38-cp38-linux_aarch64.whl\r\n- 【系统平台】: Ubuntu 20.04\r\n- 【硬件】：rk3566\r\n- 【编译语言】： Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n导入 fastdeploy 报错\r\n import fastdeploy\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /usr/local/lib/python3.8/dist-packages/fastdeploy/libs/libfastdeploy.so.0.0.0: undefined symbol: _ZN10fastdeploy8FDLogger14enable_warningE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: /usr/local/lib/python3.8/dist-packages/fastdeploy/libs/libfastdeploy.so.0.0.0: undefined symbol: _ZN10fastdeploy8FDLogger14enable_warningE\r\n\r\n",
        "state": "closed",
        "user": "gerhardt-1024",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-17T08:42:23+00:00",
        "updated_at": "2025-03-04T06:48:52+00:00",
        "closed_at": "2025-03-04T06:48:52+00:00",
        "comments_count": [
            "zhangjunhst",
            "lsx66"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2293,
        "title": "[ERROR] fastdeploy/utils/utils.cc(54)::fastdeploy::ReadBinaryFromFile",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：python cpu部署 Develop版本（Nightly build）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： \r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/runtime/python/infer_onnx_onnxruntime.py   方式推理本地onnx模型\r\n**调用代码**：\r\n```\r\nmodel_path = \"salary_onnx.onnx\"\r\n\r\noption = fd.RuntimeOption()\r\noption.set_model_path(model_path,model_format=ModelFormat.ONNX)\r\n\r\noption.use_cpu()\r\noption.use_ort_backend()\r\noption.set_cpu_thread_num(12)\r\n\r\nrt = fd.Runtime(option)\r\ninput_name = rt.get_input_info(0).name\r\n\r\n```\r\n调试运行至rt = fd.Runtime(option)\r\n**出现错误报告：**\r\n[ERROR] fastdeploy/utils/utils.cc(54)::fastdeploy::ReadBinaryFromFile\tFailed to open file: salary_onnx.onnx to read.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(155)::fastdeploy::OrtBackend::Init\tFailed to read model file.",
        "state": "closed",
        "user": "Xiaopu17",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-19T07:58:58+00:00",
        "updated_at": "2025-02-11T06:44:06+00:00",
        "closed_at": "2025-02-11T06:44:06+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2295,
        "title": "PP-LiteSeg分割模型在核显跑不通",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： [fastdeploy-win-x64-1.0.7.zip](https://bj.bcebos.com/fastdeploy/release/cpp/fastdeploy-win-x64-1.0.7.zip)\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： i9-9900k  核显Intel(R) UHD Graphics 630\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【分割模型在核显跑不通】\r\n- - 跑PP-LiteSeg模型  只设置CPU  模型可以正常推理\r\n- - 在增加了核显参数\r\n- --option.openvino_option.SetDevice(\"HETERO:GPU,CPU\")；\r\n- --shape_info_seg[\"x\"] = { 1, 3, 320, 320 };\r\n- --option.openvino_option.SetShapeInfo(shape_info_seg);\r\n- --option.openvino_option.SetCpuOperators({ \"MulticlassNms\" })\r\n- -在auto model = fastdeploy::vision::segmentation::PaddleSegModel(model_filename, params_filename, cfg_file, option);报错\r\n- -报错截图如下\r\n- -\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/88479008/4a3a4436-7b54-40f4-a00a-4088fba5d3dd)\r\n\r\n\r\n- 【ppyoloe模型在核显能正常跑通】\r\n- - ![image](https://github.com/PaddlePaddle/FastDeploy/assets/88479008/6185bba8-ffdd-4578-9124-e928c29bed00)\r\n- - 调用库及环境应该没问题\r\n",
        "state": "open",
        "user": "534114658",
        "closed_by": null,
        "created_at": "2023-11-20T04:26:02+00:00",
        "updated_at": "2024-03-07T05:39:24+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "zjx424",
            "534114658"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2296,
        "title": "UIE模型部署时遇到错误，E1120 07:33:13.977535 1004 model_repository_manager.cc:1186] failed to load 'uie' version 1: Invalid argument: unable to load model 'uie', configuration expects 2 inputs, model provides 4",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "huangjun11",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-20T07:34:58+00:00",
        "updated_at": "2024-12-03T06:42:50+00:00",
        "closed_at": "2024-12-03T06:42:50+00:00",
        "comments_count": [
            "huangjun11",
            "Xiaopu17"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2297,
        "title": "Failed to load yaml file weights/cls_mv3.yml ",
        "body": "我用C++部署paddleocr里的官网给的推理模型时，也就是下面这个，\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/58733665/d52d95e1-d675-44e5-95ef-e0951d8144dc)\r\n对应的yml配置文件也是官网给的推理模型对应的cls_mv3\r\n\r\n部署代码也使用的是官方例子，如下：\r\noption.UseGpu();\r\nmodel = new fastdeploy::vision::classification::MobileNetv2(model_file, params_file, infer_cfg_file, option)\r\n给我报错了：\r\n[ERROR] fastdeploy/vision/classification/ppcls/preprocessor.cc(36)::fastdeploy::vision::classification::PaddleClasPreprocessor::BuildPreprocessPipelineFromConfig       Failed to load yaml file weights/cdn_cls.yml, maybe you should check this file.\r\n[ERROR] fastdeploy/vision/classification/ppcls/preprocessor.cc(26)::fastdeploy::vision::classification::PaddleClasPreprocessor::PaddleClasPreprocessor  Failed to create PaddleClasPreprocessor.\r\n\r\n请问这是怎么回事",
        "state": "closed",
        "user": "xpzwzwz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-20T12:50:05+00:00",
        "updated_at": "2025-02-11T06:44:07+00:00",
        "closed_at": "2025-02-11T06:44:07+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2299,
        "title": "服务端部署PPYoloe模型，在执行客户端请求时，报错 tritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Socket closed",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： paddlepaddle/fastdeploy:1.0.4-cpu-only-21.10\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) /\r\n- 【硬件】： Nvidia GPU 2080TI， CUDA 11.4 CUDNN 8.3\r\n- 【编译语言】：  Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- 执行examples中的paddledetection，能够成功启动服务\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/86581237/227ca192-f01b-4280-ae1d-5f672f315f1b)\r\n但是在执行客户端请求时python3 paddledet_grpc_client.py，出现如下错误。\r\ntm: name: \"INPUT\"\r\ndatatype: \"UINT8\"\r\nshape: -1\r\nshape: -1\r\nshape: -1\r\nshape: 3\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jsjx/junhuang/FastDeploy/examples/vision/detection/paddledetection/serving/paddledet_grpc_client.py\", line 103, in <module>\r\n    result = runner.Run([im, ])\r\n  File \"/home/jsjx/junhuang/FastDeploy/examples/vision/detection/paddledetection/serving/paddledet_grpc_client.py\", line 73, in Run\r\n    results = self._client.infer(\r\n  File \"/home/jsjx/anaconda3/envs/tuobao/lib/python3.9/site-packages/tritonclient/grpc/_client.py\", line 1380, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/home/jsjx/anaconda3/envs/tuobao/lib/python3.9/site-packages/tritonclient/grpc/_utils.py\", line 77, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\n此时，服务断也被终止\r\n![图片](https://github.com/PaddlePaddle/FastDeploy/assets/86581237/d2de22f9-f447-4bb3-8dd8-12ebe4e5d3cb)\r\n\r\n\r\n",
        "state": "closed",
        "user": "huangjun11",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-21T11:25:22+00:00",
        "updated_at": "2025-02-11T06:44:07+00:00",
        "closed_at": "2025-02-11T06:44:07+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2300,
        "title": "fd.vision.tracking.PPTracking部署目标追踪模型时，报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python                 1.0.0\r\n- 【系统平台】:  Windows10\r\n- 【编译语言】： Python(3.7）\r\n\r\n## 错误信息\r\n[ERROR] fastdeploy/backends/paddle/paddle_backend.cc(179)::fastdeploy::PaddleBackend::GetInputInfo                                                                 The index: 2 should less than the number of inputs: 2.\r\n\r\ninfer_cfg.yml文件内容：\r\nmode: paddle\r\ndraw_threshold: 0.5\r\nmetric: COCO\r\nuse_dynamic_shape: false\r\narch: YOLO\r\nmin_subgraph_size: 3\r\nPreprocess:\r\n- interp: 2\r\n  keep_ratio: false\r\n  target_size:\r\n  - 640\r\n  - 640\r\n  type: Resize\r\n- is_scale: true\r\n  mean:\r\n  - 0.485\r\n  - 0.456\r\n  - 0.406\r\n  std:\r\n  - 0.229\r\n  - 0.224\r\n  - 0.225\r\n  type: NormalizeImage\r\n- type: Permute\r\nlabel_list:\r\n- person\r\n",
        "state": "closed",
        "user": "Firestick-Xia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-22T06:25:48+00:00",
        "updated_at": "2025-02-11T06:44:08+00:00",
        "closed_at": "2025-02-11T06:44:08+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2301,
        "title": "RK3588系统选择的debian 11 和ubuntu20.04有区别吗",
        "body": "目前我们想将fastdeploy在rk3588上使用，想请问debian 11和ubuntu20.04这两个不同的系统版本会有影响吗？还是说只能在ubuntu上部署。感谢回复！！！\r\n",
        "state": "closed",
        "user": "HGD-ai",
        "closed_by": "HGD-ai",
        "created_at": "2023-11-23T05:42:37+00:00",
        "updated_at": "2023-11-23T08:17:10+00:00",
        "closed_at": "2023-11-23T08:17:10+00:00",
        "comments_count": [
            "HGD-ai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2303,
        "title": "fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 1.0.7 or the latest code in develop branch\r\nOS Platform:  Linux x64 \r\nHardware: CPU\r\nProgram Language:  Python 3.8.10\r\n\r\n## Problem description\r\nI first run: docker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-cpu-only-21.10, it's my environment.\r\nThen, I run it in python ,fastdeploy.vision.facedet.SCRFD('scrfd_500m_bnkps_shape640x640.onnx',  runtime_option=None, model_format=ModelFormat.ONNX)\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model: scrfd\r\n[ERROR] fastdeploy/vision/facedet/contrib/scrfd.cc(96)::Initialize      Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"test01.py\", line 29, in <module>\r\n    model = fastdeploy.vision.facedet.SCRFD('scrfd_500m_bnkps_shape640x640.onnx',  runtime_option=None, model_format=ModelFormat.ONNX)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/vision/facedet/contrib/scrfd.py\", line 41, in __init__\r\n    assert self.initialized, \"SCRFD initialize failed.\"\r\nAssertionError: SCRFD initialize failed.",
        "state": "closed",
        "user": "lxiaoxiaoxing",
        "closed_by": "lxiaoxiaoxing",
        "created_at": "2023-11-28T02:36:23+00:00",
        "updated_at": "2023-11-28T09:44:05+00:00",
        "closed_at": "2023-11-28T09:44:05+00:00",
        "comments_count": [
            "lxiaoxiaoxing"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2302,
        "title": "FDtensor 多次获取输出结果异常",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n在一个class里面初始化成员变量，通过SetData方式创建FDtensor，获取tensor的值正常。在该class的其他成员函数再次获取tensor值的时候，结果发生了变化。\r\n参考代码如下\r\n```\r\n    std::vector<float> yaw_(120);\r\n    std::iota(yaw_.begin(),yaw_.end(),0);\r\n    yaw_idx_.SetData({120}, FDDataType::FP32, yaw_.data());\r\n    yaw_idx_.PrintInfo();\r\n    std::vector<float> data;\r\n    data.resize(yaw_idx_.Numel());\r\n    std::memcpy(data.data(), yaw_idx_.Data(), yaw_idx_.Nbytes());\r\n\r\n    for(size_t i=0; i<data.size();i++){\r\n        std::cout<<data[i]<<\",\";\r\n    }\r\n\r\n```\r\n两次调用上述代码块，得到的结果不一致\r\n```\r\nDebug TensorInfo: : name=, shape=120 , buffer_=0, external_data_ptr=0x4bd78c0, dtype=FDDataType::FP32, mean=59.5, max=119, min=0\r\n0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,\r\n\r\nDebug TensorInfo: : name=, shape=120 , buffer_=0, external_data_ptr=0x4bd78c0, dtype=FDDataType::FP32, mean=6.41035e+29, max=7.68307e+31, min=-3576.39\r\n-3569.44,4.56837e-41,1.4013e-45,1.4013e-45,-3576.39,4.56837e-41,4.45452e-36,0,5.04467e-44,0,5.04467e-44,0,12,13,4.45448e-36,0,0,0,18,19,20,21,0,0,24,25,4.4545e-36,0,0,0,30,31,32,33,0,35,36,37,6.86636e-44,0,7.68307e+31,6.97683e+22,2.73716e+20,1.89362e+23,2.73758e+20,1.46644e-19,7.56562e+28,3.86892e-11,5.27204e-14,49,2.25609e-43,0,-3569.44,4.56837e-41,1.4013e-45,1.4013e-45,-3576.39,4.56837e-41,756.665,0,5.88545e-44,0,5.88545e-44,0,64,65,4.45456e-36,0,0,0,70,71,72,73,0,0,76,77,4.45458e-36,0,0,0,82,83,84,85,1.4013e-45,87,88,89,6.86636e-44,0,50512.3,6.40997e-10,1.73878e+25,2.06165e-19,5.2638e+25,3.78335e-39,98,99,100,101,6.86636e-44,0,1.7752e+28,1.78169e-19,1.06455e+24,1.73879e+25,2.06165e-19,5.14043e+22,3.80632e-39,111,112,113,4.62428e-44,0,1.4013e-45,0,1.68156e-43,0,\r\n\r\n```\r\n",
        "state": "open",
        "user": "luameows",
        "closed_by": null,
        "created_at": "2023-11-27T09:13:54+00:00",
        "updated_at": "2024-02-18T01:55:48+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "luameows"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2304,
        "title": "window10 运行崩溃",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】官方提供预编译sdk\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】：  Nvidia GPU 3050， CUDA 11.6  CUDNN 8.9\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- - 运行demo: infer_onnx_onnxruntime.cc，程序执行到这一步时崩溃：std::vector<fd::TensorInfo> inputs_info = runtime.GetInputInfos();\r\n- - 无任何报错，直接崩溃，用官方预编译或自行编译的fastdeploy都一样。\r\n",
        "state": "closed",
        "user": "qx1216",
        "closed_by": "rainyfly",
        "created_at": "2023-11-28T06:19:59+00:00",
        "updated_at": "2024-02-06T06:38:59+00:00",
        "closed_at": "2024-02-06T06:38:59+00:00",
        "comments_count": [
            "qx1216",
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2306,
        "title": "serving http post 如何发送base64编码",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： cpu\r\n- 【编译语言】： Python 3.8.10\r\n\r\n我看到官网的示例，http发送的数据是 {\r\n      \"name\" : \"INPUT\",\r\n      \"shape\" : image.shape,\r\n      \"datatype\" : \"UINT8\",\r\n      \"data\" : image.tolist()\r\n    } ，当我尝试把它改成把它改成base64 {\r\n      \"name\" : \"INPUT\",\r\n      \"shape\" : (1,1),\r\n      \"datatype\" : \"STRING\",\r\n      \"data\" : [img_base64]\r\n    }  \r\n报错 {'error': 'invalid datatype for input INPUT'}，我想知道，发送base64编码应该怎么做 ",
        "state": "closed",
        "user": "lxiaoxiaoxing",
        "closed_by": "lxiaoxiaoxing",
        "created_at": "2023-11-30T03:45:55+00:00",
        "updated_at": "2023-11-30T05:52:54+00:00",
        "closed_at": "2023-11-30T05:52:54+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2305,
        "title": "能否尽快支持华为智能边缘硬件Atlas200I A2 \\ Atlas500A2",
        "body": "各位大佬，现在华为智能边缘硬件的应用越来越多了，客户也逐渐在提及此类应用。paddle和fastdeploy都是非常好的框架和开发工具，性能很好，使用也方便，虽然也对华为昇腾系列有支持，但支持的版本还是有点低，这些智能边缘盒子都用不了。在PaddleLite和Fastdeploy的issue上有很多类似问题，看得出还是有很多人想用paddle和fastdeploy。我本人也尝试在Atlas200I A2上编译和运行paddlite和fasedeploy，但都报错，问华为技术人员，得到的答复是目前Paddlite还不支持310B芯片。希望各位大佬能尽快更新支持新版本的芯片，满足广大粉丝需求，同时也能让好的架构和技术能够快速应用和推广，不至于让我等小白再转战其他方法，道阻且长，非我等所愿。十分感谢。",
        "state": "closed",
        "user": "truemaomao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-11-29T09:51:40+00:00",
        "updated_at": "2025-01-07T06:40:29+00:00",
        "closed_at": "2025-01-07T06:40:29+00:00",
        "comments_count": [
            "MrMzl",
            "shengzhe8688"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2308,
        "title": "Nuitka打包: FastDeploy initalized failed! Error",
        "body": "`pip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n`\r\n\r\n```python\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nim = cv2.imread(\"000000014439.jpg\")\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\")\r\n\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n```\r\n`python -m pip install nuitka`\r\n\r\npython -m nuitka --standalone 1.py\r\n\r\nc_lib_wrap.py\r\n```python\r\ntry:\r\n    from .libs.fastdeploy_main import *\r\nexcept Exception as e:\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\n```\r\n\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"E:\\2023_C~1\\DB_202~1\\DB_FD~1.DIS\\fastdeploy\\c_lib_wrap.py\", line 164, in <module fastdeploy.c_lib_wrap>\r\nImportError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\2023_C~1\\DB_202~1\\DB_FD~1.DIS\\db_FD.py\", line 5, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"E:\\2023_C~1\\DB_202~1\\DB_FD~1.DIS\\fastdeploy\\__init__.py\", line 122, in <module fastdeploy>\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"E:\\2023_C~1\\DB_202~1\\DB_FD~1.DIS\\fastdeploy\\c_lib_wrap.py\", line 166, in <module fastdeploy.c_lib_wrap>\r\nRuntimeError: FastDeploy initalized failed! Error:\r\n```\r\n\r\n-----------------\r\n\r\nhttps://github.com/Nuitka/Nuitka/issues/2590\r\nNuitka 也去提交了",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "rainyfly",
        "created_at": "2023-12-03T03:52:22+00:00",
        "updated_at": "2024-02-06T06:36:21+00:00",
        "closed_at": "2024-02-06T06:36:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2307,
        "title": "Support tensort? ",
        "body": "When can we expect support for tensorrt models? \r\n\r\n![Screenshot_2023-12-03-07-46-43-157_com android chrome~2](https://github.com/PaddlePaddle/FastDeploy/assets/7268915/921d1bdb-4fdc-4e6f-a6ff-64895d32fcb1)\r\n\r\n",
        "state": "closed",
        "user": "maheshs11",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-03T02:51:50+00:00",
        "updated_at": "2025-02-11T06:44:09+00:00",
        "closed_at": "2025-02-11T06:44:09+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2310,
        "title": "利好Paddle......yolov5,yolov8表示，只要你用了模型,包含ONNX模型,都必须要开源",
        "body": "https://github.com/ultralytics/ultralytics/issues/6789\r\n\r\n利好  Paddle.....\r\n\r\nyolov5,yolov8表示，\r\n只要你训练模型,调用了pt模型或者转换为onnx\r\n\r\n整个项目必须要开源\r\n遵守AGPL-3.0 污染开源协议\r\n\r\n商业项目, 只能购买企业许可证 \r\n\r\n国内那么多用yolov5的小伙伴们，要小心了\r\n虽然国内环境大家都懂 ， 但是人家协议是这样...\r\n\r\n也就是说\r\n用了 FastDeploy  调用 yolov5 yolov8的onnx\r\n你的项目都必须要开源\r\n\r\n如果用他们的框架训练，他们会加上在线统计，记录你的IP ,\r\n\r\n虽然Google Analytics统计在国内用不了\r\n\r\n[Why does training have Google statistics ?? · Issue #2250 · ultralytics/ultralytics (github.com)](https://github.com/ultralytics/ultralytics/issues/2250)",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "rainyfly",
        "created_at": "2023-12-06T00:02:00+00:00",
        "updated_at": "2024-02-06T06:33:56+00:00",
        "closed_at": "2024-02-06T06:33:56+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2309,
        "title": "最新的fastdeploy镜像版本是多少，怎么下载？",
        "body": "readme里面都是不新的，麻烦给一个确定的版本",
        "state": "open",
        "user": "bltcn",
        "closed_by": null,
        "created_at": "2023-12-05T02:54:42+00:00",
        "updated_at": "2024-03-04T06:14:52+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "bltcn"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2311,
        "title": "PP-PicoDet 在爱芯npu上面部署",
        "body": "你好，请问下，PP-PicoDet，有全量化部署到爱心650的npu上面的参考吗？我有看到rk npu上面的部署教程。",
        "state": "closed",
        "user": "xiaotailang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-06T02:41:41+00:00",
        "updated_at": "2025-02-11T06:44:10+00:00",
        "closed_at": "2025-02-11T06:44:10+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2312,
        "title": "Android：Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 10161 (GLThread 77735), pid 10121 (oy.app.examples) A/Paddle-Lite: [F 12/ 7 11:38:37.898 .../lite/kernels/host/elementwise_op_func.h:340 Update] Wrong broadcast type",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： [fastdeploy-android-sdk-with-text-0.0.0.aar](https://bj.bcebos.com/fastdeploy/dev/android/fastdeploy-android-sdk-with-text-0.0.0.aar)\r\n\r\n- 【系统平台】: Anrdoid (小米8)\r\n- 【硬件】： cpu\r\n\r\n## 问题日志及出现问题的操作流程\r\n```\r\nI/[FastDeploy][WARNING]: fastdeploy/[fastdeploy_model.cc](http://fastdeploy_model.cc/)(482)::PrintStatisInfoOfRuntime\tPrintStatisInfoOfRuntime require the runtime ran 10 times at least, but now you only ran 9 times.\r\nD/[FastDeploy][JNI]: Avg runtime costs 462.126495 ms\r\nD/[FastDeploy][JNI]: Avg runtime costs 1.832125 ms\r\nI/[FastDeploy][WARNING]: fastdeploy/[fastdeploy_model.cc](http://fastdeploy_model.cc/)(482)::PrintStatisInfoOfRuntime\tPrintStatisInfoOfRuntime require the runtime ran 10 times at least, but now you only ran 9 times.\r\nD/[FastDeploy][JNI]: Avg runtime costs 152.234634 ms\r\nW/OpenGLRenderer: [SurfaceTexture-1-10121-1] bindTextureImage: clearing GL error: 0x500\r\nA/Paddle-Lite: [F 12/ 7 11:38:37.894 .../lite/kernels/host/elementwise_op_func.h:340 Update] Wrong broadcast type\r\nA/libc: Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 10161 (GLThread 77735), pid 10121 (oy.app.examples)\r\nA/Paddle-Lite: [F 12/ 7 11:38:37.898 .../lite/kernels/host/elementwise_op_func.h:340 Update] Wrong broadcast type\r\n```\r\n- 【替换模型后运行奔溃】\r\n- - 下载https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.7/deploy/fastdeploy/android\r\n- - 下载ppocr模型https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_det_infer.tar\r\n                            https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_rec_infer.tar\r\n\r\n- - 模型替换到android里面报如上错误\r\n",
        "state": "closed",
        "user": "is-sixfive",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-07T03:46:21+00:00",
        "updated_at": "2025-02-04T06:41:29+00:00",
        "closed_at": "2025-02-04T06:41:29+00:00",
        "comments_count": [
            "nkhlS141"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2313,
        "title": "飞桨啊飞桨请问我解决！PaddleLite  ConvertConv2D] Check failed: IsSameSymmQuantParams(input_scales, quan",
        "body": "使用的环境：Ascend huawei 300i  cann6.0RC1alph005\r\n使用paddleLite库：下载官方inference_lite_lib.ky10.armv8.huawei_ascend_npu.CANN6.0.0.alpha006.tar.gz 和对应opt\r\n我的服务器：华为鲲鹏服务器，查了一下就是armv8的\r\n模型转换：./opt_linux_aarch64 --model_file=./model/ch_ppocr_mobile_v2.0_det_slim_infer/inference.pdmodel --param_file=./model/ch_ppocr_mobile_v2.0_det_slim_infer/inference.pdiparams --optimize_out=./model/ch_ppocr_mobile_v2.0_det_slim_npu --valid_targets=huawei_ascend_npu,arm --optimize_out_type=naive_buffer\r\n./opt_linux_aarch64 --model_file=./model/ch_ppocr_mobile_v2.0_rec_slim_infer/inference.pdmodel --param_file=./model/ch_ppocr_mobile_v2.0_rec_slim_infer/inference.pdiparams --optimize_out=./model/ch_ppocr_mobile_v2.0_rec_slim_npu --valid_targets=huawei_ascend_npu,arm --optimize_out_type=naive_buffer\r\n./opt_linux_aarch64 --model_file=./model/ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdmodel --param_file=./model/ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdiparams --optimize_out=./model/ch_ppocr_mobile_v2.0_cls_slim_npu --valid_targets=huawei_ascend_npu,arm --optimize_out_type=naive_buffer\r\n\r\nc++代码：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/16428236/629e3c48-5113-4d1e-b18f-ff4e3a1674cf)\r\n错误代码：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/16428236/03dabf89-cfb1-46f5-9e96-0bd8d39f21a7)\r\n",
        "state": "closed",
        "user": "shengzhe8688",
        "closed_by": "rainyfly",
        "created_at": "2023-12-07T10:06:55+00:00",
        "updated_at": "2024-02-06T11:14:58+00:00",
        "closed_at": "2024-02-06T11:14:58+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2314,
        "title": "PaddleSeg推理出来的模型目标边缘不平滑，锯齿状很明显",
        "body": "用的这个模型Portrait_PP_HumanSegV2_Lite_256x144_infer，还测了其他小模型，就这个效果最好，但还是有锯齿\r\n![1702095819304](https://github.com/PaddlePaddle/FastDeploy/assets/14149284/595424f4-af73-442c-a152-f108a8fe5068)\r\n\r\n\r\n\r\n\r\n*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "holylong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-09T04:24:32+00:00",
        "updated_at": "2024-12-17T06:42:07+00:00",
        "closed_at": "2024-12-17T06:42:07+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2315,
        "title": "error : Illegal instruction",
        "body": "## Environment\r\nFastDeploy version: 1.0.2\r\nOS Platform: arm \r\nHardware: raspberry pi 4\r\nProgram Language: Python 3.9.2\r\n\r\n## Problem description\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/python\r\n\r\nfollowed example for faster-rcnn.\r\n```py\r\npi@raspberrypi:~/Desktop/FastDeploy/examples/vision/detection/paddledetection/python $ python infer_faster_rcnn.py --model_dir faster_rcnn_r50_vd_fpn_2x_coco --image 000000014439.jpg --device cpu\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: HARDWARE\t: BCM2835\r\n\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 4\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 1900, min freq: 1900, cluster ID: 0, CPU ARCH: A72\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 1900, min freq: 1900, cluster ID: 0, CPU ARCH: A72\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 1900, min freq: 1900, cluster ID: 0, CPU ARCH: A72\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 1900, min freq: 1900, cluster ID: 0, CPU ARCH: A72\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 48 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 1024 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 0 KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 1842500KB\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I 12/11  4:43:43.607 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I 12/11  4:43:43.608 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I 12/11  4:43:43.608 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from faster_rcnn_r50_vd_fpn_2x_coco/model.pdmodel\r\n[I 12/11  4:43:43.802 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from faster_rcnn_r50_vd_fpn_2x_coco/model.pdiparams\r\n[I 12/11  4:43:44.128 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!\r\nIllegal instruction\r\n\r\n```\r\n",
        "state": "closed",
        "user": "mahesh11T",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-11T04:52:14+00:00",
        "updated_at": "2025-02-11T06:44:11+00:00",
        "closed_at": "2025-02-11T06:44:11+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2316,
        "title": "SCRFD Android Demo相机画面闪屏(上个画面帧残留)",
        "body": "是使用人脸识别项目的时候，手机前后摄像头，在手机移动的时候，会有上个画面帧残留，使用了小米手机和vivo手机，必现。\r\nFastDeploy/blob/develop/examples/vision/facedet/scrfd/android",
        "state": "closed",
        "user": "bear123445",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-12T04:10:33+00:00",
        "updated_at": "2025-02-11T06:44:12+00:00",
        "closed_at": "2025-02-11T06:44:12+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2318,
        "title": "how to build python .whl for raspberry pi, cross compile?",
        "body": "is there any flag to set to build python wheel for raspberry pi,  while running setup.py to cross compile? ",
        "state": "closed",
        "user": "mahesh11T",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-12T12:48:13+00:00",
        "updated_at": "2025-02-11T06:44:13+00:00",
        "closed_at": "2025-02-11T06:44:13+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2319,
        "title": "推理ppdet中导出的MaskRCNN模型，结果结构体中mask无内容，可视化结果无掩膜。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "open",
        "user": "panp4n",
        "closed_by": null,
        "created_at": "2023-12-13T00:56:35+00:00",
        "updated_at": "2024-02-06T06:31:57+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "panp4n"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2320,
        "title": "用rknpu转换识别模型出错: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of type is required but missing.",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop分支，4月27日的2c5fd91a7f321f483140f1a1a7ff899de6f8845e\r\n- 【编译命令】x86标准编译\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： cpu\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n按照这个文档操作的https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/rockchip/cpp/README.md。\r\n使用上面文档里面链接下载的PP-OCRv3文字识别模型，可以顺利转onnx，转rknn，使用我自己训练过的识别模型可以转onnx，但转rknn就报错退出了：\r\n`python ../export.py --config_path ppocrv3_rec.yaml --target_platform rk3588\r\n{'mean': [[127.5, 127.5, 127.5]], 'std': [[127.5, 127.5, 127.5]], 'model_path': './ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer.onnx', 'outputs_nodes': None, 'do_quantization': False, 'dataset': None, 'output_folder': './ch_PP-OCRv3_rec_infer'}\r\nW __init__: rknn-toolkit2 version: 1.5.0+1fa95b5c\r\nE load_onnx: Catch exception when loading onnx model: /home/joey/samplecodes/2023/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/rknpu2_tools/config/ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1382, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 658, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 58, in rknn.api.ir_graph.IRGraph.__init__\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 503, in rknn.api.ir_graph.IRGraph.rebuild\r\nE load_onnx:   File \"/home/joey/anaconda3/envs/rknn/lib/python3.6/site-packages/onnx/checker.py\", line 106, in check_model\r\nE load_onnx:     C.check_model(protobuf_string)\r\nE load_onnx: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of type is required but missing.\r\nW If you can't handle this error, please try updating to the latest version of the toolkit2 and runtime from:\r\n  https://eyun.baidu.com/s/3eTDMk6Y (Pwd: rknn)  Path: RK_NPU_SDK / RK_NPU_SDK_1.X.0 / develop /\r\n  If the error still exists in the latest version, please collect the corresponding error logs and the model,\r\n  convert script, and input data that can reproduce the problem, and then submit an issue on:\r\n  https://redmine.rock-chips.com (Please consult our sales or FAE for the redmine account)\r\nTraceback (most recent call last):\r\n  File \"../export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n`\r\n\r\n训练代码是：PaddleOCR-release-2.6，训练命令是：\r\n````\r\npython3 -m paddle.distributed.launch --gpus '0'  tools/train.py -c configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=./pretrain_models/ch_PP-OCRv3_rec_train/best_accuracy\r\n````\r\n\r\n导出pdmodel命令是：\r\n````\r\npython3 tools/export_model.py -c  configs/rec/PP-OCRv3/ch_PP-OCRv3_rec_distillation.yml -o Global.pretrained_model=./output/rec_ppocr_v3_distillation/best_accuracy  Global.save_inference_dir=./inference/ch_PP-OCRv3_rec_rubber/\r\n````\r\n\r\n训练配置文件：\r\n`Global:\r\n  debug: false\r\n  use_gpu: true\r\n  epoch_num: 900\r\n  log_smooth_window: 20\r\n  print_batch_step: 10\r\n  save_model_dir: ./output/rec_ppocr_v3_distillation\r\n  save_epoch_step: 90\r\n  eval_batch_step: [0, 2000]\r\n  cal_metric_during_train: true\r\n  pretrained_model:\r\n  checkpoints:\r\n  save_inference_dir:\r\n  use_visualdl: false\r\n  infer_img: doc/imgs_words/ch/word_1.jpg\r\n  character_dict_path: ppocr/utils/ppocr_keys_v1.txt\r\n  max_text_length: &max_text_length 40\r\n  infer_mode: false\r\n  use_space_char: true\r\n  distributed: true\r\n  save_res_path: ./output/rec/predicts_ppocrv3_distillation.txt\r\n  use_amp: False\r\n  #  scale_loss: 1024.0\r\n  #use_dynamic_loss_scaling: True\r\n\r\nOptimizer:\r\n  name: Adam\r\n  beta1: 0.9\r\n  beta2: 0.999\r\n  lr:\r\n    name: Piecewise\r\n    decay_epochs : [700]\r\n    #values : [0.0005, 0.00005]\r\n    values: [0.00005, 0.00001]\r\n    warmup_epoch: 5\r\n  regularizer:\r\n    name: L2\r\n    factor: 3.0e-05\r\n\r\n\r\nArchitecture:\r\n  model_type: &model_type \"rec\"\r\n  name: DistillationModel\r\n  algorithm: Distillation\r\n  Models:\r\n    Teacher:\r\n      pretrained:\r\n      freeze_params: false\r\n      return_all_feats: true\r\n      model_type: *model_type\r\n      algorithm: SVTR\r\n      Transform:\r\n      Backbone:\r\n        name: MobileNetV1Enhance\r\n        scale: 0.5\r\n        last_conv_stride: [1, 2]\r\n        last_pool_type: avg\r\n      Head:\r\n        name: MultiHead\r\n        head_list:\r\n          - CTCHead:\r\n              Neck:\r\n                name: svtr\r\n                dims: 64\r\n                depth: 2\r\n                hidden_dims: 120\r\n                use_guide: True\r\n              Head:\r\n                fc_decay: 0.00001\r\n          - SARHead:\r\n              enc_dim: 512\r\n              max_text_length: *max_text_length\r\n    Student:\r\n      pretrained:\r\n      freeze_params: false\r\n      return_all_feats: true\r\n      model_type: *model_type\r\n      algorithm: SVTR\r\n      Transform:\r\n      Backbone:\r\n        name: MobileNetV1Enhance\r\n        scale: 0.5\r\n        last_conv_stride: [1, 2]\r\n        last_pool_type: avg\r\n      Head:\r\n        name: MultiHead\r\n        head_list:\r\n          - CTCHead:\r\n              Neck:\r\n                name: svtr\r\n                dims: 64\r\n                depth: 2\r\n                hidden_dims: 120\r\n                use_guide: True\r\n              Head:\r\n                fc_decay: 0.00001\r\n          - SARHead:\r\n              enc_dim: 512\r\n              max_text_length: *max_text_length\r\nLoss:\r\n  name: CombinedLoss\r\n  loss_config_list:\r\n  - DistillationDMLLoss:\r\n      weight: 1.0\r\n      act: \"softmax\"\r\n      use_log: true\r\n      model_name_pairs:\r\n      - [\"Student\", \"Teacher\"]\r\n      key: head_out\r\n      multi_head: True\r\n      dis_head: ctc\r\n      name: dml_ctc\r\n  - DistillationDMLLoss:\r\n      weight: 0.5\r\n      act: \"softmax\"\r\n      use_log: true\r\n      model_name_pairs:\r\n      - [\"Student\", \"Teacher\"]\r\n      key: head_out\r\n      multi_head: True\r\n      dis_head: sar\r\n      name: dml_sar\r\n  - DistillationDistanceLoss:\r\n      weight: 1.0\r\n      mode: \"l2\"\r\n      model_name_pairs:\r\n      - [\"Student\", \"Teacher\"]\r\n      key: backbone_out\r\n  - DistillationCTCLoss:\r\n      weight: 1.0\r\n      model_name_list: [\"Student\", \"Teacher\"]\r\n      key: head_out\r\n      multi_head: True\r\n  - DistillationSARLoss:\r\n      weight: 1.0\r\n      model_name_list: [\"Student\", \"Teacher\"]\r\n      key: head_out\r\n      multi_head: True\r\n\r\nPostProcess:\r\n  name: DistillationCTCLabelDecode\r\n  model_name: [\"Student\", \"Teacher\"]\r\n  key: head_out\r\n  multi_head: True\r\n\r\nMetric:\r\n  name: DistillationMetric\r\n  base_metric_name: RecMetric\r\n  main_indicator: acc\r\n  key: \"Student\"\r\n  ignore_space: False\r\n\r\nTrain:\r\n  dataset:\r\n    name: SimpleDataSet\r\n    data_dir: /dataset/redecode_txt_recognition/\r\n    ext_op_transform_idx: 1\r\n    label_file_list:\r\n    - /dataset/redecode_txt_recognition/train_label.txt\r\n    transforms:\r\n    - DecodeImage:\r\n        img_mode: BGR\r\n        channel_first: false\r\n    - RecConAug:\r\n        prob: 0.5\r\n        ext_data_num: 2\r\n        image_shape: [48, 320, 3]\r\n        max_text_length: *max_text_length\r\n    - RecAug:\r\n    - MultiLabelEncode:\r\n    - RecResizeImg:\r\n        image_shape: [3, 48, 320]\r\n    - KeepKeys:\r\n        keep_keys:\r\n        - image\r\n        - label_ctc\r\n        - label_sar\r\n        - length\r\n        - valid_ratio\r\n  loader:\r\n    shuffle: true\r\n    batch_size_per_card: 32\r\n    drop_last: true\r\n    num_workers: 4\r\nEval:\r\n  dataset:\r\n    name: SimpleDataSet\r\n    data_dir: /dataset/redecode_txt_recognition/\r\n    label_file_list:\r\n    - /dataset/redecode_txt_recognition/valid_label.txt\r\n    transforms:\r\n    - DecodeImage:\r\n        img_mode: BGR\r\n        channel_first: false\r\n    - MultiLabelEncode:\r\n    - RecResizeImg:\r\n        image_shape: [3, 48, 320]\r\n    - KeepKeys:\r\n        - image\r\n        - label_ctc\r\n        - label_sar\r\n        - length\r\n        - valid_ratio\r\n  loader:\r\n    shuffle: false\r\n    drop_last: false\r\n    batch_size_per_card: 4\r\n    num_workers: 4\r\n\r\n`\r\n\r\n````\r\n\r\n\r\n",
        "state": "open",
        "user": "JoeyZhu",
        "closed_by": null,
        "created_at": "2023-12-13T01:48:23+00:00",
        "updated_at": "2023-12-13T01:52:55+00:00",
        "closed_at": null,
        "comments_count": [
            "JoeyZhu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2322,
        "title": "docker部署内存增加不降？？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "Ahua-Tan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-14T03:24:18+00:00",
        "updated_at": "2025-02-11T06:44:14+00:00",
        "closed_at": "2025-02-11T06:44:14+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2323,
        "title": "使用C++进行端到端多线程推理时：如果把ppocr3模型对象 Clone() 2次，每个线程使用一个unique_ptr<fastdeploy::pipeline::PPOCRv3>预测，  程序必出现Mats shapes are not consisten",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Nvidia GPU 3080TI\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n[ERROR] fastdeploy/vision/common/processors/mat_batch.cc(33)::Tensor    Mats shapes are not consistent.\r\n\r\n- 【模型跑不通】\r\n    auto ppocr_v3 = fastdeploy::pipeline::PPOCRv3(&det_model, &cls_model, &rec_model);\r\n    auto ppocr1 =ppocr_v3.Clone();\r\n    auto ppocr2 =ppocr_v3.Clone();\r\n\r\ncv::Mat im = imageModel.GetIm();\r\nfastdeploy::vision::OCRResult result;\r\nppocr_v3.Predict(&im, &result);\r\n\r\ncv::Mat im1 = imageModel.GetIm();\r\nfastdeploy::vision::OCRResult result1;\r\nppocr1->Predict(&im1, &result1);\r\n\r\ncv::Mat im2 = imageModel.GetIm();\r\nfastdeploy::vision::OCRResult result2;\r\nppocr2->Predict(&im2, &result2);\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/15935949/3d4c1595-bb0c-41e5-afa1-93d7cf5278f9)\r\n\r\n",
        "state": "open",
        "user": "huangtao2999",
        "closed_by": null,
        "created_at": "2023-12-14T03:31:18+00:00",
        "updated_at": "2025-01-01T15:00:57+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "tim7-m",
            "KyleWang-Hunter",
            "tsing-luo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2324,
        "title": "交叉编译测试C++部署示例程序时，编译报错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：develop分支\r\n- 【编译命令】按照 [瑞芯微RK3588部署环境编译安装](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rknpu2.md)的交叉编译方式安装的fastdeploy\r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4090Ti， CUDA 11.7 CUDNN 8.9\r\n- 【编译语言】： gcc 11.4 / cmake 3.22.1 / Python 3.6.5\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 交叉编译方式安装好fastdeploy完成，然后按照文档[PP-YOLOE C++部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/quick_start/models/cpp.md)测试C++模型推理时，在make阶段报错\r\n\r\n[ 50%] Building CXX object CMakeFiles/infer_demo.dir/infer_demo.cc.o\r\n[100%] Linking CXX executable infer_demo\r\n/usr/bin/ld: /home/FastDeploy/build/fastdeploy-0.0.0/lib/libfastdeploy.so: error adding symbols: file in wrong format\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/infer_demo.dir/build.make:114：infer_demo] 错误 1\r\nmake[1]: *** [CMakeFiles/Makefile2:110：CMakeFiles/infer_demo.dir/all] 错误 2\r\n",
        "state": "closed",
        "user": "danny-zhu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-14T03:44:08+00:00",
        "updated_at": "2025-02-11T06:44:14+00:00",
        "closed_at": "2025-02-11T06:44:14+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2327,
        "title": "fastdeploy部署抠图（vis_matting），比直接使用PaddleSeg效果差很多",
        "body": "如题，\r\n参数都是按照教程中的参数来设置的\r\n是不是fastdeploy没有同步pp-matting的最新版本？\r\n我看[PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg)是2.0.9了\r\nfastdeply引用的版本是2.0.6\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/66047793/713c0540-280a-4b08-acfe-2330bb2bab8c)\r\n",
        "state": "closed",
        "user": "ubeytech",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-14T11:48:05+00:00",
        "updated_at": "2025-02-11T06:44:15+00:00",
        "closed_at": "2025-02-11T06:44:15+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2328,
        "title": "could not create a primitive descriptor for a reorder primitive",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-gpu-python 1.0.7    paddlepaddle-gpu      2.5.2\r\n- 【编译命令】pip install\r\n- 【系统平台】:  Windows x64(Windows11) \r\n- 【硬件】： CUDA 11.8 CUDNN 8.6\r\n- 【编译语言】： Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n```python\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.MaskRCNN(\"E:/PaddlePaddle/PaddleDetection/output_dir_inference_model/mask_rcnn_r50_vd_fpn_ssld_1x_coco/model.pdmodel\",\r\n                                 \"E:/PaddlePaddle/PaddleDetection/output_dir_inference_model/mask_rcnn_r50_vd_fpn_ssld_1x_coco/model.pdiparams\",\r\n                                 \"E:/PaddlePaddle/PaddleDetection/output_dir_inference_model/mask_rcnn_r50_vd_fpn_ssld_1x_coco/infer_cfg.yml\")\r\n\r\nim = cv2.imread(r\"E:/20231214_10010118.jpg\")\r\n\r\n\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\nvis_im = vision.vis_detection(im, result, score_threshold=0.5)\r\ncv2.imwrite(\"vis_image.jpg\", vis_im)\r\n\r\n```\r\n```python\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW1219 12:03:14.043656 17448 analysis_config.cc:971] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(273)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::CPU.\r\nTraceback (most recent call last):\r\n  File \"e:/2023_Code/py_2023/FD/cs.py\", line 12, in <module>\r\n    result = model.predict(im)\r\n  File \"E:\\anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\fastdeploy\\vision\\detection\\ppdet\\__init__.py\", line 126, in predict\r\n    return self._model.predict(im)\r\nRuntimeError: could not create a primitive descriptor for a reorder primitive\r\n\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-19T03:58:07+00:00",
        "updated_at": "2025-01-14T06:40:36+00:00",
        "closed_at": "2025-01-14T06:40:36+00:00",
        "comments_count": [
            "laishenghui"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2330,
        "title": "升级ppdiffusers 使用paddle_tensorrt后端报错 ",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： develop版本，安装命令 pip install fastdeploy-gpu-python==0.0.0 -f https://www.paddlepaddle.org.cn/whl/fastdeploy_nightly_build.html\r\n- 【编译命令】\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】：  Nvidia GPU V100 32G， CUDA 11.7 CUDNN 8.6\r\n- 【编译语言】：  Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n\r\nhttps://github.com/PaddlePaddle/PaddleMIX/pull/322#issuecomment-1862094402 升级PaddleMIX代码\r\n代码分支 https://github.com/co63oc/PaddleMIX/tree/attention\r\n\r\n使用[这里的脚本](https://github.com/PaddlePaddle/PaddleMIX/blob/develop/ppdiffusers/tests/deploy/stable_diffusion/test_export_and_inference.sh)，参考[这里的文档](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/deploy) 顺利跑出结果\r\n\r\n使用paddle后端可以正常运行\r\npython infer.py --model_dir static_model/stable-diffusion-v1-5 --scheduler \"preconfig-euler-ancestral\" --backend paddle --device gpu --task_name text2img\r\n使用paddle_tensorrt后端报错\r\npython infer.py --model_dir static_model/stable-diffusion-v1-5 --scheduler \"preconfig-euler-ancestral\" --backend paddle_tensorrt --device gpu --task_name text2img\r\n错误堆栈为C++错误\r\n创建 CustomSkipLayerNormPluginDynamic 失败\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/4617245/025fa52f-0a6d-43ff-9628-9ffeefb69265)\r\n\r\n",
        "state": "closed",
        "user": "co63oc",
        "closed_by": "co63oc",
        "created_at": "2023-12-20T11:36:48+00:00",
        "updated_at": "2024-12-12T01:59:30+00:00",
        "closed_at": "2024-12-12T01:59:30+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2331,
        "title": "编译FastDeploy/examples/vision/facedet/scrfd/cpp报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【编译命令】cmake .. -DENABLE_ORT_BACKEND=ON          -DENABLE_PADDLE_BACKEND=ON          -DENABLE_OPENVINO_BACKEND=ON          -DENABLE_TRT_BACKEND=ON          -DWITH_GPU=ON          -DTRT_DIRECTORY=/usr/local/TensorRT-8.6.1.6          -DCUDA_DIRECTORY=/usr/local/cuda-11.8          -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk          -DENABLE_VISION=ON          -DOPENCV_DIRECTORY=/usr/local/lib/cmake/opencv4 -DENABLE_TEXT=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-11.8/bin/nvcc\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： cuda11.8\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n\r\n错误：\r\n/usr/bin/ld: /home/wxb/computerprograms/c++/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.so: undefined reference to `nvjpegJpegStreamParseHeader'\r\n/usr/bin/ld: /home/wxb/computerprograms/c++/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.so: undefined reference to `nvjpegDecodeBatchedSupported'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/infer_with_face_align_demo.dir/build.make:128: infer_with_face_align_demo] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:112: CMakeFiles/infer_with_face_align_demo.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n/usr/bin/ld: /home/wxb/computerprograms/c++/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.so: undefined reference to `nvjpegJpegStreamParseHeader'\r\n/usr/bin/ld: /home/wxb/computerprograms/c++/FastDeploy/build/compiled_fastdeploy_sdk/lib/libfastdeploy.so: undefined reference to `nvjpegDecodeBatchedSupported'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/infer_without_face_align_demo.dir/build.make:128: infer_without_face_align_demo] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:138: CMakeFiles/infer_without_face_align_demo.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2",
        "state": "closed",
        "user": "XiaBing992",
        "closed_by": "XiaBing992",
        "created_at": "2023-12-22T07:17:38+00:00",
        "updated_at": "2024-02-06T03:58:38+00:00",
        "closed_at": "2024-02-06T03:58:38+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2332,
        "title": "FastDeploy yolov5 batch推理",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-develop\r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3060TI， CUDA 11.5 CUDNN 8.3\r\n- 【编译语言】： C++ 10.5\r\n\r\n## 问题日志及出现问题的操作流程\r\n问题：当使用fastdeploy中fastdeploy::vision::detection::YOLOv5 进行batch推理，举例：两个线程调用 每个线程传一个batch的数据\r\n现象：不加锁的话 会导致结果错乱  我在外部调用加锁 那和串行执行没什么区别,YOLOv5Postprocessor::Run在这内部局部加锁会不会更好些 \r\n\r\nstd::vector<cv::Mat> img_batch ;\r\nstd::vector<fastdeploy::vision::DetectionResult> results;\r\nBatchPredict(img_batch, &results)\r\n我查看源码BatchPredict内部\r\nbool YOLOv5::BatchPredict(const std::vector<cv::Mat>& images, std::vector<DetectionResult>* results) {\r\n  std::vector<std::map<std::string, std::array<float, 2>>> ims_info;\r\n  std::vector<FDMat> fd_images = WrapMat(images);\r\n\r\n  if (!preprocessor_.Run(&fd_images, &reused_input_tensors_, &ims_info)) {\r\n    FDERROR << \"Failed to preprocess the input image.\" << std::endl;\r\n    return false;\r\n  }\r\n\r\n  reused_input_tensors_[0].name = InputInfoOfRuntime(0).name;\r\n  if (!Infer(reused_input_tensors_, &reused_output_tensors_)) {\r\n    FDERROR << \"Failed to inference by runtime.\" << std::endl;\r\n    return false;\r\n  }\r\n\r\n  if (!postprocessor_.Run(reused_output_tensors_, results, ims_info)) {\r\n    FDERROR << \"Failed to postprocess the inference results by runtime.\" << std::endl;\r\n    return false;\r\n  }\r\n\r\n  return true;\r\n}\r\n",
        "state": "closed",
        "user": "294978174",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-25T07:26:46+00:00",
        "updated_at": "2025-04-15T06:43:37+00:00",
        "closed_at": "2025-04-15T06:43:37+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2334,
        "title": "Fastdeploy交叉编译过程中出现的libpaddle_full_api_shared.so: file not recognized: File format not recognized collect2: error: ld returned 1 exit status错误？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-1.0.6\r\n- 【编译命令】交叉编译方式，具体命令见下面log。\r\n- 【系统平台】: ：本地Ubuntu20.04（x86-64）\r\n- 【硬件】： 树莓派4B（aarch64）\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【需求描述】\r\n- - 由于树莓派板端性能的限制，考虑采用交叉编译的方式在本地PC上进行源码编译，完成后上传到树莓派上进行模型部署验证。通过Fastdeploy以前的I和树莓派相关的issue查询，发现树莓派是支持paddlelite的，因此在交叉编译中打开了paddlelite配置项。\r\n- 【编译方式】\r\n- - Step1：本地PC安装树莓派交叉编译工具，已完成。\r\n- - Step2：执行交叉编译命令如下：\r\n- - - ` cmake .. -DCMAKE_C_COMPILER=/opt/raspberrypi/tools/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/arm-linux-gnueabihf-gcc -DCMAKE_CXX_COMPILER=/opt/raspberrypi/tools/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/arm-linux-gnueabihf-g++ -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchain.cmake -DTARGET_ABI=arm64 -DENABLE_VISION=ON -DENABLE_LITE_BACKEND=ON -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-1.0.6`\r\n- - - cmake编译通过。\r\n- - Step3：执行make，‘make -j10’，**报错信息如下：**\r\n- - - `[100%] Linking CXX shared library libfastdeploy.so\r\nthird_libs/install/paddlelite/lib/libpaddle_full_api_shared.so: file not recognized: File format not recognized\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/fastdeploy.dir/build.make:2751: libfastdeploy.so.1.0.6] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:145: CMakeFiles/fastdeploy.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2`\r\n- - 详细编译日志：\r\n[log_2(1).txt](https://github.com/PaddlePaddle/FastDeploy/files/13768341/log_2.1.txt)\r\n- -修改-DCMAKE_TOOLCHAIN_FILE后重新编译\r\n- - Step2：执行交叉编译命令如下：\r\n- - - ` cmake .. -DCMAKE_C_COMPILER=/opt/raspberrypi/tools/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/arm-linux-gnueabihf-gcc -DCMAKE_CXX_COMPILER=/opt/raspberrypi/tools/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/arm-linux-gnueabihf-g++ -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchain.cmake -DTARGET_ABI=armhf -DENABLE_VISION=ON -DENABLE_LITE_BACKEND=ON -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-1.0.6` \r\n- - - cmake编译不通过\r\n- - Step3：执行make，‘make -j10’，**报错信息如下：**\r\n- - - `CMake Error at cmake/paddlelite.cmake:51 (message):\r\n  Only support Linux aarch64 now, x64 is not supported with backend Paddle\r\n  Lite.`\r\n- - 详细编译日志：\r\n[log_1.txt](https://github.com/PaddlePaddle/FastDeploy/files/13768378/log_1.txt)\r\n",
        "state": "closed",
        "user": "MrMzl",
        "closed_by": "rainyfly",
        "created_at": "2023-12-26T03:51:41+00:00",
        "updated_at": "2024-02-06T11:18:38+00:00",
        "closed_at": "2024-02-06T11:18:38+00:00",
        "comments_count": [
            "rainyfly",
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2336,
        "title": "Fastdeploy支持Debug模式了吗？",
        "body": "现在的Fastdeploy支持Debug模式了吗，每次写程序报错的时候想要debug，但是这不支持调试，就搞得很难受。",
        "state": "closed",
        "user": "li784",
        "closed_by": "li784",
        "created_at": "2023-12-28T07:11:58+00:00",
        "updated_at": "2024-02-20T00:40:03+00:00",
        "closed_at": "2024-02-20T00:40:03+00:00",
        "comments_count": [
            "rainyfly",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2337,
        "title": "fastdeploy OCR Atlas300i Pro 支持310p-npu-driver_23.0.rc3高版本不？",
        "body": "请问OCR在这个驱动版本上能不能支持\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/16428236/5a591d24-3358-4431-9b63-558348040bb4)\r\n",
        "state": "closed",
        "user": "shengzhe8688",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-12-29T08:53:47+00:00",
        "updated_at": "2025-02-11T06:44:16+00:00",
        "closed_at": "2025-02-11T06:44:16+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2338,
        "title": "Ascend执行OCR推理失败，报环境不支持。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy master\r\n- 【编译命令】容器镜像编译FastDeploy 使用5.0rc2和7.0rc1均测试过都不行\r\n- 【系统平台】: Linux aarch64(Ubuntu 18.04)\r\n- 【硬件】： Atlas 300i Pro\r\n- 【Cann-Toolkit】: 7.0.rc1.beta1\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例vision/ocr/PP-OCR/infer_demo\r\n- 【报错如下】\r\n`[INFO] fastdeploy/runtime/runtime.cc(354)::CreateLiteBackend\tRuntime initialized with Backend::PDLITE in Device::ASCEND.\r\n[W  1/ 2  7:47:58.861 .../src/driver/huawei_ascend_npu/utility.cc:57 InitializeAscendCL] CANN version mismatch. The build version is 0.0.0, but the current environment version is 5.1.2.\r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:41 Context] properties: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:66 Context] selected device ids: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:68 Context] 0\r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:78 Context] profiling path: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:88 Context] dump model path: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:98 Context] precision mode: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:120 Context] op select impl mode: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:130 Context] op type list for impl mode: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:140 Context] enable compressw weight: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:150 Context] auto tune mode: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:160 Context] enable dynamic shape range: \r\n[I  1/ 2  7:47:59.223 ...r/src/driver/huawei_ascend_npu/engine.cc:176 Context] initial buffer length of dynamic shape range: -1\r\n[W  1/ 2  7:47:59.223 ...ter/nnadapter/src/runtime/compilation.cc:334 Finish] Warning: Failed to create a program, No model and cache is provided.\r\n[W  1/ 2  7:47:59.223 ...le-Lite/lite/kernels/nnadapter/engine.cc:149 LoadFromCache] Warning: Build model failed(3) !\r\n[W  1/ 2  7:47:59.234 ...nnadapter/nnadapter/src/runtime/model.cc:86 GetSupportedOperations] Warning: Failed to get the supported operations for device 'huawei_ascend_npu', because the HAL interface 'validate_program' is not implemented!\r\n[W  1/ 2  7:47:59.234 ...kernels/nnadapter/converter/converter.cc:171 Apply] Warning: Failed to get the supported operations for the selected devices, one or more of the selected devices are not supported!\r\n[I  1/ 2  7:47:59.234 ...r/src/driver/huawei_ascend_npu/driver.cc:70 CreateProgram] Create program for huawei_ascend_npu.\r\n[F  1/ 2  7:48:22.824 ...driver/huawei_ascend_npu/model_client.cc:54 InitAclClientEnv] Check failed: (reinterpret_cast<aclError>(aclrtSetDevice(device_id_)) == ACL_ERROR_NONE): 507033!==0 507033 Unknown ACL error code(507033)\r\nterminate called after throwing an instance of 'nnadapter::logging::Exception'\r\n  what():  NNAdapter C++ Exception: \r\n[F  1/ 2  7:48:22.824 ...driver/huawei_ascend_npu/model_client.cc:54 InitAclClientEnv] Check failed: (reinterpret_cast<aclError>(aclrtSetDevice(device_id_)) == ACL_ERROR_NONE): 507033!==0 507033 Unknown ACL error code(507033)\r\n\r\nAborted (core dumped)\r\n`\r\n",
        "state": "open",
        "user": "shengzhe8688",
        "closed_by": null,
        "created_at": "2024-01-02T07:48:52+00:00",
        "updated_at": "2024-11-16T07:17:51+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "xfbxag",
            "SongheGao",
            "SongheGao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2339,
        "title": "Support for RKYolov6 for RKNPU2 inference",
        "body": "Hi! Can someone add Rockchip (rknpu2) compatible yolov6? Airockchip recently updated their model zoo: https://github.com/airockchip/rknn_model_zoo/tree/main/examples/yolov6/cpp\r\n\r\nYolov6 is impressively performant on RKNPU...\r\n",
        "state": "closed",
        "user": "debugmenot",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-04T12:21:21+00:00",
        "updated_at": "2025-02-11T06:44:17+00:00",
        "closed_at": "2025-02-11T06:44:17+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2340,
        "title": "Fastdeploy在JetsonNano上，通过TensorRT调用picodet，内存占用过高问题。",
        "body": "## 环境\r\n- 【编译命令】cmake .. -DBUILD_ON_JETSON=ON \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF\\\r\n         -DPADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： JetsonNano， CUDA 10.2.300 CUDNN 8.2.1.32\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【性能问题】描述清楚对比的方式\r\n- 同时初始化picodet-s-320、pplcnet_x1_0模型，使用框架为TensorRT，在jetson_nano平台上，占用内存超过1.5G，这个问题有解决的办法嘛/\r\n\r\n",
        "state": "closed",
        "user": "czyczyczy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-05T06:12:47+00:00",
        "updated_at": "2025-02-11T06:44:18+00:00",
        "closed_at": "2025-02-11T06:44:18+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2343,
        "title": "ppyolo无法再arm中部署",
        "body": "pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[INFO] fastdeploy/runtime.cc(579)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\n[INFO] fastdeploy/runtime.cc(579)::Init Runtime initialized with Backend::ORT in Device::CPU.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.932 ...ild/Paddle-Lite/lite/core/device_info.cc:282 get_cpu_arch] Unknow cpu arch: 1635\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1275 Setup] ARM multiprocessors name: MODEL NAME      : PHYTIUM,D2000/8 E8C\r\n\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1276 Setup] ARM multiprocessors number: 8\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 0, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 1, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 2, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 3, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 4, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 5, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 6, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1278 Setup] ARM multiprocessors ID: 7, max freq: 2300, min freq: 2300, cluster ID: 0, CPU ARCH: A-1\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1284 Setup] L1 DataCache size is: \r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1286 Setup] 32 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1288 Setup] L2 Cache size is: \r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1290 Setup] 2048 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1292 Setup] L3 Cache size is: \r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1294 Setup] 4096 KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1296 Setup] Total memory: 7970868KB\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1297 Setup] SVE2 support: 0\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1298 Setup] SVE2 f32mm support: 0\r\n[I  1/ 9  8:32:48.934 ...ild/Paddle-Lite/lite/core/device_info.cc:1299 Setup] SVE2 i8mm support: 0\r\n[I  1/ 9  8:32:48.935 ...ly_build/Paddle-Lite/lite/api/cxx_api.cc:366 Build] Load model from file.\r\n[I  1/ 9  8:32:48.936 ...e-Lite/lite/model_parser/model_parser.cc:241 LoadModelPb] Loading topology data from models/det/seal/best_model.pdmodel\r\n[I  1/ 9  8:32:48.984 ...e-Lite/lite/model_parser/model_parser.cc:259 LoadModelPb] Loading params data from models/det/seal/best_model.pdiparams\r\n[I  1/ 9  8:32:48.995 ...e-Lite/lite/model_parser/model_parser.cc:269 LoadModelPb] 1. Model is successfully loaded!",
        "state": "closed",
        "user": "KyleWang-Hunter",
        "closed_by": "KyleWang-Hunter",
        "created_at": "2024-01-10T01:38:58+00:00",
        "updated_at": "2024-01-22T01:13:23+00:00",
        "closed_at": "2024-01-22T01:13:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2341,
        "title": "FastDeploy部署 examples/vision/detection/paddledetection/python/infer_mask_rcnn.py运行官方给出的模型可以推理，自己训练的模型推理报错",
        "body": "- 【FastDeploy版本】： [说明具体的版本，如fastdeploy-linux-gpu-0.8.0]\r\n-   fastdeploy-python         1.0.7\r\n-   由fastdeploy_python-1.0.7-cp39-cp39-win_amd64.whl文件安装\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： 使用的时CPU版本\r\n- 【编译语言】： Python == 3.9 \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- 使用examples运行官方提供的模型，可以正常推理\r\n- 使用自己训练的模型不能推理\r\n- 代码-examples/vision/detection/paddledetection/python/infer_mask_rcnn.py未改动该文件\r\n- 运行参数 \r\n- --model_dir\r\nD:\\Paddle\\3.PaddleDetection\\FastDeploy\\ModelTest\\mask_rcnn_r50_1x_coco_all\r\n--image\r\nD:\\Paddle\\3.PaddleDetection\\FastDeploy\\ModelTest\\Images\\000000014439.jpg\r\n--device\r\ncpu\r\n 使用的官方模型下载链接：https://bj.bcebos.com/paddlehub/fastdeploy/mask_rcnn_r50_1x_coco.tgz\r\n自己训练的模型：太大了，我传不上来，我是用coco数据集训练的，配置用的就是paddledetection里面的configs/mask_rcnn/mask_rcnn_r50_1x_coco.yml文件，我只改动了里面的训练batchsize\r\n\r\n\r\n错误log：\r\nD:\\Anaconda3\\envs\\fastdeploy\\python.exe D:\\Paddle\\3.PaddleDetection\\FastDeploy\\FastDeploy-develop\\examples\\vision\\detection\\paddledetection\\python\\infer_mask_rcnn.py --model_dir D:\\Paddle\\3.PaddleDetection\\FastDeploy\\ModelTest\\mask_rcnn_r50_1x_coco_all --image D:\\Paddle\\3.PaddleDetection\\FastDeploy\\ModelTest\\Images\\000000014439.jpg --device cpu \r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert\tBGR2RGB and Normalize are fused to Normalize with swap_rb=1\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0109 16:42:01.589668 10892 analysis_config.cc:971] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\r\n[INFO] fastdeploy/runtime/runtime.cc(273)::fastdeploy::Runtime::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\r\nTraceback (most recent call last):\r\n  File \"D:\\Paddle\\3.PaddleDetection\\FastDeploy\\FastDeploy-develop\\examples\\vision\\detection\\paddledetection\\python\\infer_mask_rcnn.py\", line 74, in <module>\r\n    result = model.predict(im)\r\n  File \"D:\\Anaconda3\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\vision\\detection\\ppdet\\__init__.py\", line 126, in predict\r\n    return self._model.predict(im)\r\nRuntimeError: could not create a primitive descriptor for a reorder primitive\r\n\r\nProcess finished with exit code 1\r\n",
        "state": "closed",
        "user": "laishenghui",
        "closed_by": "laishenghui",
        "created_at": "2024-01-09T09:01:19+00:00",
        "updated_at": "2024-11-09T15:50:44+00:00",
        "closed_at": "2024-02-18T01:11:13+00:00",
        "comments_count": [
            "rainyfly",
            "NyquistBodeTu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2342,
        "title": "PPYOLOE  TRT 批量推理，当输入尺寸小于224时结果异常",
        "body": "## 环境\r\n- 【FastDeploy版本】： 官方编译的fastdeploy-windows-gpu-1.0.7\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】：  Nvidia GPU 3080TI / 1660S， CUDA 11.2 CUDNN 8.2\r\n- 【编译语言】： C++\r\n\r\n## 操作流程\r\n- - 使用PPYOLOE-S模型\r\n- - export模型尺寸小于224，如[192，192] [160，96]\r\n- - 推理端使用Tensorrt，单/半精度\r\n- - 批量推理相同的图片（图片中目标越少出现概率越高）\r\n- - 结果中随机出现nan或者0\r\n\r\n## 运行结果（同一批推理结果截图）\r\n正常结果：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/57038101/62ff6c7f-2d28-40a1-a277-68ce6a97a304)\r\n\r\n异常结果：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/57038101/198089ca-4d57-47a0-b1e3-e5f48ca25c57)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/57038101/20cc3d10-43a5-445b-8e13-e8cfe547eccd)\r\n\r\n## 代码使用demo\r\n`\r\n#include \"fastdeploy/vision.h\"\r\n#include <iostream>\r\nusing namespace std;\r\n\r\nvoid Infer(){\r\n  int batch_num = 10;\r\n  cv::Mat im = cv::imread(\"./model/a.png\");\r\n  auto model_file = \"./model/model.pdmodel\";\r\n  auto params_file = \"./model/model.pdiparams\";\r\n  auto config_file = \"./model/model.yml\";\r\n  auto backend = \"trt\";\r\n  auto option = fastdeploy::RuntimeOption();\r\n  option.UseGpu();\r\n  option.UseTrtBackend();\r\n\r\n  auto model = fastdeploy::vision::detection::PPYOLOE(\r\n      model_file, params_file, config_file, option);\r\n\r\n  if (!model.Initialized()) {\r\n    std::cerr << \"Failed to initialize.\" << std::endl;\r\n    return;\r\n  }\r\n  \r\n  std::vector<cv::Mat> imgs;\r\n  for (int i = 0; i < batch_num; ++i)\r\n  {\r\n    imgs.push_back(im);\r\n  }\r\n  std::vector<fastdeploy::vision::DetectionResult> results;\r\n\r\n  for(int f=0;f<100;f++){\r\n    if (!model.BatchPredict(imgs, &results)) {\r\n    std::cerr << \"Failed to BatchPredict.\" << std::endl;\r\n    return;\r\n    }\r\n  }\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  Infer();\r\n  return 0;\r\n}\r\n`\r\n## 日志\r\n`\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::fastdeploy::vision::FuseNormalizeCast      Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert     BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(715)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx    Cannot build engine right now, because there's dynamic input shape exists, list as below,\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(719)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx    Input 0: TensorInfo(name: image, shape: [-1, 3, 96, 192], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(719)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx    Input 1: TensorInfo(name: scale_factor, shape: [-1, 2], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(721)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx    FastDeploy will build the engine while inference with input data, and will also collect the input shape range information. You should be noticed that FastDeploy will rebuild the engine while new input shape is out of the collected shape range, this may bring some time consuming problem, refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/runtime/runtime.cc(313)::fastdeploy::Runtime::CreateTrtBackend        Runtime initialized with Backend::TRT in Device::GPU.\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(40)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] input name: image, shape: [10, 3, 96, 192], The shape range before: min_shape=[-1, 3, 96, 192], max_shape=[-1, 3, 96, 192].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(52)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] The updated shape range now: min_shape=[10, 3, 96, 192], max_shape=[10, 3, 96, 192].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(40)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] input name: scale_factor, shape: [10, 2], The shape range before: min_shape=[-1, 2], max_shape=[-1, 2].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(52)::fastdeploy::ShapeRangeInfo::Update [New Shape Out of Range] The updated shape range now: min_shape=[10, 2], max_shape=[10, 2].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(330)::fastdeploy::TrtBackend::Infer       TensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(556)::fastdeploy::TrtBackend::BuildTrtEngine Start to building TensorRT Engine...\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(643)::fastdeploy::TrtBackend::BuildTrtEngine TensorRT Engine is built successfully.\r\n`\r\n\r\n",
        "state": "closed",
        "user": "YOU-007",
        "closed_by": "YOU-007",
        "created_at": "2024-01-09T13:07:36+00:00",
        "updated_at": "2024-01-12T05:32:46+00:00",
        "closed_at": "2024-01-12T05:32:45+00:00",
        "comments_count": [
            "YOU-007",
            "YOU-007"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2344,
        "title": "在rk3588上部署paddleOCR",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Rk3588\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n问题一：为什么非要在Windows系统下的Ubuntu系统中完成paddle转onnx、onnx转rknn呢？在3588板子上或者在服务器linux上进行不可以吗？\r\n问题二：模型转换完成后调用GPU进行测试无问题，调用CPU进行测试报错，命令和报错信息如下：\r\n(rknn) topeet@iTOP-RK3588:~/OCR/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/build$ \r\n![Uploading error.JPG…]()\r\n./infer_demo ./ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer.onnx ./ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.onnx ./ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer.onnx ./ppocr_keys_v1.txt ./12.jpg 0\r\nONNX Model\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model: ppocr/ocr_det\r\n[ERROR] fastdeploy/vision/ocr/ppocr/dbdetector.cc(52)::Initialize       Failed to initialize fastdeploy backend.\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model: ppocr/ocr_cls\r\n[ERROR] fastdeploy/vision/ocr/ppocr/classifier.cc(51)::Initialize       Failed to initialize fastdeploy backend.\r\n[ERROR] fastdeploy/fastdeploy_model.cc(239)::CreateCpuBackend   Found no valid backend for model: ppocr/ocr_rec\r\n[ERROR] fastdeploy/vision/ocr/ppocr/recognizer.cc(55)::Initialize       Failed to initialize fastdeploy backend.\r\ninfer_demo: /home/topeet/OCR/FastDeploy/examples/vision/ocr/PP-OCR/rockchip/cpp/infer.cc:56: void InitAndInfer(const string&, const string&, const string&, const string&, const string&, const fastdeploy::RuntimeOption&, const fastdeploy::ModelFormat&): Assertion `det_model.Initialized()' failed.\r\n已放弃 (核心已转储)\r\n\r\n",
        "state": "closed",
        "user": "bai-0829",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-11T07:29:35+00:00",
        "updated_at": "2025-02-11T06:44:19+00:00",
        "closed_at": "2025-02-11T06:44:19+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2345,
        "title": "多分类跟踪模型加载",
        "body": "目前这个模型https://bj.bcebos.com/v1/paddledet/models/mot/mcfairmot_hrnetv2_w18_dlafpn_30e_576x320_bdd100k_mcmot.tar 是不支持吗？\r\n错误信息：[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW   Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including conditional_block,lod_array_length,masked_select,select_input,tensor_array_to_tensor,write_to_array,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.\r\n",
        "state": "closed",
        "user": "kitterive",
        "closed_by": "kitterive",
        "created_at": "2024-01-12T03:18:57+00:00",
        "updated_at": "2024-01-16T08:29:31+00:00",
        "closed_at": "2024-01-16T08:29:31+00:00",
        "comments_count": [
            "kitterive",
            "kitterive",
            "ChaoII",
            "kitterive"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2346,
        "title": "Paddle2ONNX 不支持conv3d_transpose运算符导出",
        "body": "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/jit/api.py:606: UserWarning: The InputSpec(shape=(-1, 1, -1, -1, -1), dtype=paddle.float32, name=None, stop_gradient=False)'s name is None. When using jit.save, please set InputSepc's name in to_static(input_spec=[]) and jit.save(input_spec=[]) and make sure they are consistent.\r\n  warnings.warn(name_none_error % spec)\r\nI0112 14:04:08.392467  9519 program_interpreter.cc:212] New Executor is Running.\r\n2024-01-12 14:04:10 [INFO]\tStatic PaddlePaddle model saved in paddle_model_static_onnx_temp_dir.\r\n[Paddle2ONNX] Start to parse PaddlePaddle model...\r\n[Paddle2ONNX] Model file path: paddle_model_static_onnx_temp_dir/model.pdmodel\r\n[Paddle2ONNX] Paramters file path: paddle_model_static_onnx_temp_dir/model.pdiparams\r\n[Paddle2ONNX] Start to parsing Paddle model...\r\n[Paddle2ONNX] Oops, there are some operators not supported yet, including conv3d_transpose,\r\n[ERROR] Due to the unsupported operators, the conversion is aborted.\r\n\r\npaddle2onnx仓库似乎已经很久没有更新了，有支持这个算子的计划没有",
        "state": "closed",
        "user": "lantudou",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-12T06:11:42+00:00",
        "updated_at": "2025-01-21T06:40:46+00:00",
        "closed_at": "2025-01-21T06:40:46+00:00",
        "comments_count": [
            "jiangjiajun",
            "lantudou",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2348,
        "title": "ppyoloe sod模型支持切图拼图操作吗？",
        "body": "如果不支持的话，有计划吗？这个功能对小目标比较有帮助。",
        "state": "closed",
        "user": "kitterive",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-16T08:29:06+00:00",
        "updated_at": "2025-02-11T06:44:19+00:00",
        "closed_at": "2025-02-11T06:44:19+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2349,
        "title": "C_api Detcction batchPredict 问题",
        "body": "```\r\n#define DECLARE_AND_IMPLEMENT_BATCH_PREDICT_FUNCTION(model_type, wrapper_var_name) FD_C_Bool FD_C_##model_type##WrapperBatchPredict( \\\r\n    FD_C_##model_type##Wrapper* wrapper_var_name, FD_C_OneDimMat imgs, \\\r\n    FD_C_OneDimDetectionResult* results) { \\\r\n  std::vector<cv::Mat> imgs_vec; \\\r\n  std::vector<fastdeploy::vision::DetectionResult> results_out; \\\r\n  std::vector<FD_C_DetectionResultWrapper*> results_wrapper_out; \\\r\n  for (int i = 0; i < imgs.size; i++) { \\\r\n    imgs_vec.push_back(*(reinterpret_cast<cv::Mat*>(imgs.data[i]))); \\\r\n    FD_C_DetectionResultWrapper* fd_detection_result_wrapper = FD_C_CreateDetectionResultWrapper(); \\\r\n    results_wrapper_out.push_back(fd_detection_result_wrapper); \\\r\n  } \\\r\n  auto& model = \\\r\n      CHECK_AND_CONVERT_FD_TYPE(model_type##Wrapper, wrapper_var_name); \\\r\n  bool successful = model->BatchPredict(imgs_vec, &results_out); \\\r\n  if (successful) { \\\r\n    results->size = results_out.size(); \\\r\n    results->data = new FD_C_DetectionResult[results->size]; \\\r\n    for (int i = 0; i < results_out.size(); i++) { \\\r\n      (*CHECK_AND_CONVERT_FD_TYPE(DetectionResultWrapper, \\\r\n                                  results_wrapper_out[i])) = std::move(results_out[i]); \\\r\n      FD_C_DetectionResultWrapperToCResult(results_wrapper_out[i], &results->data[i]); \\\r\n    } \\\r\n  } \\\r\n  for (int i = 0; i < results_out.size(); i++) { \\\r\n    FD_C_DestroyDetectionResultWrapper(results_wrapper_out[i]); \\\r\n  }\\\r\n  return successful; \\\r\n}\r\n```\r\n请问一下在batch_predict 方法中，传入FD_C_OneDimDetectionResult* 在C内部进行了new(`    results->data = new FD_C_DetectionResult[results->size]; \r\n`)，需要再提供destroyFD_C_OneDImDetection方法么？",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-17T01:24:13+00:00",
        "updated_at": "2025-01-21T06:40:47+00:00",
        "closed_at": "2025-01-21T06:40:47+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2350,
        "title": "想知道是否支持FastDeploy 部署serving 的 表格识别模型，目前没看到有相关文档",
        "body": null,
        "state": "closed",
        "user": "Chenkeyi43",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-17T01:36:13+00:00",
        "updated_at": "2025-02-11T06:44:20+00:00",
        "closed_at": "2025-02-11T06:44:20+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2351,
        "title": "本地使用pytorch的yolov5模型跑出来的识别效果要远好于在fastdeploy部署后跑出来的效果",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10)\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080， CUDA 12.2\r\n- 【编译语言】： Python\r\n\r\n本地使用的yolov5模型，加上自己的训练集训练出来的pt文件，转为onnx后，按照官方文档在fastdeploy上部署，开启服务后，跑出来目标检测的识别精度远低于本地测试效果；在本地，转化后的onnx文件也已经测试过了，与转化前的识别效果进度一致\r\n",
        "state": "closed",
        "user": "magicendaver",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-17T08:56:17+00:00",
        "updated_at": "2025-01-21T06:40:47+00:00",
        "closed_at": "2025-01-21T06:40:47+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2352,
        "title": "希望官方能补全rkyolo的C_API",
        "body": "希望能补全rkyolo的CAPI接口\r\n现在的CAPI中不存在rkyolo各版本的接口，如rkyolov7，是否可以补全一下\r\n以及Csharp接口也希望能够补全",
        "state": "closed",
        "user": "chatop2020",
        "closed_by": "chatop2020",
        "created_at": "2024-01-23T05:38:16+00:00",
        "updated_at": "2024-02-08T13:54:26+00:00",
        "closed_at": "2024-02-08T13:54:10+00:00",
        "comments_count": [
            "leiqing1",
            "chatop2020",
            "chatop2020"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2353,
        "title": "test",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_OPENVINO_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TEXT=ON\r\nexport ENABLE_TRT_BACKEND=ON\r\nexport WITH_GPU=ON\r\nexport TRT_DIRECTORY=/Paddle/TensorRT-8.6.1.6\r\nexport CUDA_DIRECTORY=/usr/local/cuda\r\nexport OPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \\\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n##\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4090TI， CUDA 11.7 CUDNN 8.9\r\n- 【编译语言】：Python3.8\r\n##\r\n- 【性能问题】描述清楚对比的方式\r\n我对单张图片进行测试，每次换一张图片就会构建很长时间，我想知道如果我进行部署推理，如果每次目标（图像/视频）都不一样，那每次都会很长时间，不能只构建一次，后面换图片或者视频就迅速推理预测么？\r\n\r\n",
        "state": "open",
        "user": "txy00001",
        "closed_by": null,
        "created_at": "2024-01-24T07:47:23+00:00",
        "updated_at": "2024-02-06T05:58:27+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "txy00001",
            "jiangjiajun",
            "txy00001"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2354,
        "title": "module 'paddle' has no attribute 'io'",
        "body": "参考\r\nhttps://github.com/PaddlePaddle/PaddleNLP/blob/develop/model_zoo/ernie-tiny/deploy/python/README.md\r\n环境信息：\r\npython=3.10\r\npip list |grep paddle\r\npaddle2onnx               1.1.0\r\npaddlefsl                 1.1.0\r\npaddlenlp                 2.6.1\r\npaddlepaddle-gpu          2.6.0.post120\r\n\r\n pip list |grep fast\r\nfastapi                   0.109.0\r\nfastdeploy-gpu-python     1.0.7\r\nfastdeploy-tools          0.0.5\r\n系统：CentOS Linux release 7.5.1804 (Core) \r\n\r\n报错信息：\r\n```\r\n from paddlenlp.transformers import AutoTokenizer\r\n  File \"/root/miniconda3/envs/fastdeploy-gpu/lib/python3.10/site-packages/paddlenlp/__init__.py\", line 35, in <module>\r\n    from . import (\r\n  File \"/root/miniconda3/envs/fastdeploy-gpu/lib/python3.10/site-packages/paddlenlp/data/__init__.py\", line 15, in <module>\r\n    from .blendable_dataset import *\r\n  File \"/root/miniconda3/envs/fastdeploy-gpu/lib/python3.10/site-packages/paddlenlp/data/blendable_dataset.py\", line 30, in <module>\r\n    class BlendableDataset(paddle.io.Dataset):\r\nAttributeError: module 'paddle' has no attribute 'io'\r\n```",
        "state": "closed",
        "user": "JoyousPHPer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-01-24T12:42:54+00:00",
        "updated_at": "2025-02-11T06:44:21+00:00",
        "closed_at": "2025-02-11T06:44:21+00:00",
        "comments_count": [
            "JoyousPHPer",
            "JoyousPHPer",
            "JoyousPHPer",
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2355,
        "title": "运行出错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： \r\n- 【编译命令】\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_OPENVINO_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TEXT=ON\r\nexport ENABLE_TRT_BACKEND=ON\r\nexport WITH_GPU=ON\r\nexport TRT_DIRECTORY=/Paddle/TensorRT-8.6.1.6\r\nexport CUDA_DIRECTORY=/usr/local/cuda\r\n\r\nexport OPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \\\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n- 【系统平台】: Linux x64\r\n- 【硬件】：  Nvidia GPU 4090， CUDA 11.7 CUDNN 8.9\r\n- 【编译语言】： Python3.8等\r\n\r\n\r\n- 【模型跑不通】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/3b9f3759-af12-4299-9a3a-20590dc134bb)\r\n\r\n我的执行脚本：\r\n![Uploading image.png…]()\r\n\r\n",
        "state": "open",
        "user": "txy00001",
        "closed_by": null,
        "created_at": "2024-01-26T03:39:46+00:00",
        "updated_at": "2024-03-26T03:01:23+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "txy00001",
            "funny000",
            "txy00001"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2356,
        "title": "About using Nuitka to package fastdeploy",
        "body": "Add <code>--nofollow-import-to=fastdeploy</code> when packing, and copy the python package by yourself after the packaging is successful\r\n\r\n**After packaging, run main.exe**\r\n![RTWD7UV `3CZW5DQ6JIIKNS](https://github.com/PaddlePaddle/FastDeploy/assets/97466410/23324e51-b30d-42bb-8dda-910ee6ee49ea)\r\n\r\nCopy <code>fastdeploy</code> from <code>.../lib/site-packages</code> to the sibling directory of <code>main.exe</code>\r\n**Run again**\r\n![~NS7S6QYIT BN8Z82`QS6ZA](https://github.com/PaddlePaddle/FastDeploy/assets/97466410/e71cbd5b-c402-48cf-a24a-42153fca5870)\r\n\r\nThis time <code>tqdm</code> is missing, then copy <code>tqdm</code> to the sibling directory of <code>main.exe</code> as well\r\n\r\n**The run was successful**\r\n![{IF1)DH5 }R7CWL6QN812WW](https://github.com/PaddlePaddle/FastDeploy/assets/97466410/5b145386-e79f-4dcf-9398-451b2b256c59)\r\n\r\n**My packing command**\r\n```python\r\nimport os\r\nimport sys\r\n# 1.9.7\r\nimport nuitka\r\n\r\n# 检查是否安装了这个nuitka\r\nprint(nuitka.__path__)\r\n\r\n# python 程序入口文件路径\r\nmain_file_path = 'main.py'\r\n# 程序图标路径，png图片需要安装imageio库\r\nicon_file_path = 'icon.png'\r\n# 打包结果保存路径\r\noutput_dir = 'output'\r\n\r\n# C编译器的选项,msvc需要指定版本\r\n# mingw 自行下载配置\r\n# vs2022 14.3\r\n# vs2019 14.2\r\n# vs2017 14.1\r\n# vs2015 14.0\r\nc_compiler = ['mingw64', 'msvc=14.3']\r\n# 使用的C编译器，此处使用msvc，根据自己情况调整\r\nuse_c_compiler = c_compiler[1]\r\n# 有些步骤需要键盘输入，故另外启动一个命令行执行\r\nos.system(f'start  \"Nuitka打包\"  cmd /K \"'\r\n          f'{sys.executable} -m nuitka '\r\n          f'--{use_c_compiler} '\r\n          f'--standalone '\r\n          f'--show-progress '\r\n          f'--output-dir={output_dir}   '\r\n          # 启用Pyqt5支持\r\n          f'--enable-plugin=pyqt5 '\r\n          f'--nofollow-import-to=fastdeploy '\r\n          # 编译出来的程序不带黑框框\r\n          # 初次打包建议不带这个选项\r\n          # 打包完成运行时可以看报错信息，等到运行正常再勾上，重新打包\r\n          # f'--disable-console  '\r\n          f'--windows-icon-from-ico={icon_file_path} '\r\n          f'{main_file_path} '\r\n          f'& pause '\r\n          f'& exit\"')\r\n```",
        "state": "open",
        "user": "Liu-jian-kang",
        "closed_by": null,
        "created_at": "2024-01-26T08:31:03+00:00",
        "updated_at": "2025-04-08T02:02:17+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "Liu-jian-kang",
            "elky98",
            "formero009",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2357,
        "title": "当前版本示例代码中的MASK RCNN不会输出MASK，查看MASK data都是0.C++版和python版都存在这个问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：1.0.7\r\n- 【编译命令】C++编译的FastDeploy develop版，python用的release 1.0.7\r\n- 【系统平台】: Windows x64(Windows10) /  Win11\r\n- 【硬件】： Nvidia GPU 4060TI， CUDA 11.7 /Nvidia GPU 2080TI\r\n- 【编译语言】： C++ / Python3.7\r\n在https://github.com/PaddlePaddle/FastDeploy/pull/218这里我看到了说明中的那个图是有mask的，不知道是不是之后更新导致了bug\r\n![123](https://github.com/PaddlePaddle/FastDeploy/assets/84842493/48d5b97d-d02c-4e5e-9860-826a97ec872d)\r\n\r\n",
        "state": "open",
        "user": "David-dotcom666",
        "closed_by": null,
        "created_at": "2024-01-26T09:02:17+00:00",
        "updated_at": "2024-01-30T01:52:55+00:00",
        "closed_at": null,
        "comments_count": [
            "David-dotcom666",
            "David-dotcom666",
            "David-dotcom666"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2359,
        "title": "[bug] tinypose 使用 C++ 推理过程中 通过负数索引访问数组",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 最新的 develop 分支\r\n- 【编译命令】\r\n    ```\r\n    cmake .. \\\r\n    -D ENABLE_ORT_BACKEND=ON \\\r\n    -D ENABLE_VISION=ON \\\r\n    ```\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： ORT CPU 推理\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 1. 在 `fastdeploy/vision/utils/dark_parse.cc` 添加打印代码\r\n```\r\nvoid DarkParse(const std::vector<float>& heatmap, const std::vector<int>& dim,\r\n               std::vector<float>* coords, const int px, const int py,\r\n               const int index, const int ch) {\r\n  /*DARK postpocessing, Zhang et al. Distribution-Aware Coordinate\r\n  Representation for Human Pose Estimation (CVPR 2020).\r\n  1) offset = - hassian.inv() * derivative\r\n  2) dx = (heatmap[x+1] - heatmap[x-1])/2.\r\n  3) dxx = (dx[x+1] - dx[x-1])/2.\r\n  4) derivative = Mat([dx, dy])\r\n  5) hassian = Mat([[dxx, dxy], [dxy, dyy]])\r\n  */\r\n  std::vector<float>::const_iterator first1 = heatmap.begin() + index;\r\n  std::vector<float>::const_iterator last1 =\r\n      heatmap.begin() + index + dim[2] * dim[3];\r\n  std::vector<float> heatmap_ch(first1, last1);\r\n  cv::Mat heatmap_mat = cv::Mat(heatmap_ch).reshape(0, dim[2]);\r\n  heatmap_mat.convertTo(heatmap_mat, CV_32FC1);\r\n  cv::GaussianBlur(heatmap_mat, heatmap_mat, cv::Size(3, 3), 0, 0);\r\n  heatmap_mat = heatmap_mat.reshape(1, 1);\r\n  heatmap_ch = std::vector<float>(heatmap_mat.reshape(1, 1));\r\n\r\n  printf(\"<%s ## %d> heatmap_ch.size(): %d\\n\", __FILE__, __LINE__, heatmap_ch.size());\r\n  float epsilon = 1e-10;\r\n  // sample heatmap to get values in around target location\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, py * dim[3] + px);\r\n  float xy = log(fmax(heatmap_ch[py * dim[3] + px], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, py * dim[3] + px + 1);\r\n  float xr = log(fmax(heatmap_ch[py * dim[3] + px + 1], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, py * dim[3] + px - 1);\r\n  float xl = log(fmax(heatmap_ch[py * dim[3] + px - 1], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, py * dim[3] + px + 2);\r\n  float xr2 = log(fmax(heatmap_ch[py * dim[3] + px + 2], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, py * dim[3] + px - 2);\r\n  float xl2 = log(fmax(heatmap_ch[py * dim[3] + px - 2], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py + 1) * dim[3] + px);\r\n  float yu = log(fmax(heatmap_ch[(py + 1) * dim[3] + px], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py - 1) * dim[3] + px);\r\n  float yd = log(fmax(heatmap_ch[(py - 1) * dim[3] + px], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py + 2) * dim[3] + px);\r\n  float yu2 = log(fmax(heatmap_ch[(py + 2) * dim[3] + px], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py - 2) * dim[3] + px);\r\n  float yd2 = log(fmax(heatmap_ch[(py - 2) * dim[3] + px], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py + 1) * dim[3] + px + 1);\r\n  float xryu = log(fmax(heatmap_ch[(py + 1) * dim[3] + px + 1], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py - 1) * dim[3] + px + 1);\r\n  float xryd = log(fmax(heatmap_ch[(py - 1) * dim[3] + px + 1], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py + 1) * dim[3] + px - 1);\r\n  float xlyu = log(fmax(heatmap_ch[(py + 1) * dim[3] + px - 1], epsilon));\r\n\r\n  printf(\"< ## %d> index: %d\\n\", __LINE__, (py - 1) * dim[3] + px - 1);\r\n  float xlyd = log(fmax(heatmap_ch[(py - 1) * dim[3] + px - 1], epsilon));\r\n\r\n  printf(\"<%s ## %d>\\n\", __FILE__, __LINE__);\r\n\r\n  // compute dx/dy and dxx/dyy with sampled values\r\n  float dx = 0.5 * (xr - xl);\r\n  float dy = 0.5 * (yu - yd);\r\n  float dxx = 0.25 * (xr2 - 2 * xy + xl2);\r\n  float dxy = 0.25 * (xryu - xryd - xlyu + xlyd);\r\n  float dyy = 0.25 * (yu2 - 2 * xy + yd2);\r\n\r\n  // finally get offset by derivative and hassian, which combined by dx/dy and\r\n  // dxx/dyy\r\n  if (dxx * dyy - dxy * dxy != 0) {\r\n    float M[2][2] = {dxx, dxy, dxy, dyy};\r\n    float D[2] = {dx, dy};\r\n    cv::Mat hassian(2, 2, CV_32F, M);\r\n    cv::Mat derivative(2, 1, CV_32F, D);\r\n    cv::Mat offset = -hassian.inv() * derivative;\r\n    (*coords)[ch * 2] += offset.at<float>(0, 0);\r\n    (*coords)[ch * 2 + 1] += offset.at<float>(1, 0);\r\n  }\r\n}\r\n```\r\n- 2. 编译并执行 `examples/vision/keypointdetection/tiny_pose/cpp/pptinypose_infer.cc`\r\n- 3. 使用的模型文件和图片: 链接: https://pan.baidu.com/s/1GizYI2qduObOHw0ygKg7Jg?pwd=imes 提取码: imes \r\n- 4. 获得日志:  (**奇怪的是虽然这个 -23 每次都出现, 但只是小概率导致程序退出**)\r\n```\r\nD:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-release-1.0.7\\examples\\vision\\keypointdetection\\tiny_pose\\cpp\\build\\Release>infer_tinypose_demo.exe\r\n[INFO] fastdeploy/runtime/runtime.cc(326)::fastdeploy::Runtime::CreateOrtBackend        Runtime initialized with Backend::ORT in Device::CPU.\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 0\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 3000\r\n< ## 48> index: 3001\r\n< ## 51> index: 2999\r\n< ## 54> index: 3002\r\n< ## 57> index: 2998\r\n< ## 60> index: 3048\r\n< ## 63> index: 2952\r\n< ## 66> index: 3096\r\n< ## 69> index: 2904\r\n< ## 72> index: 3049\r\n< ## 75> index: 2953\r\n< ## 78> index: 3047\r\n< ## 81> index: 2951\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 0\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 1\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 3002\r\n< ## 48> index: 3003\r\n< ## 51> index: 3001\r\n< ## 54> index: 3004\r\n< ## 57> index: 3000\r\n< ## 60> index: 3050\r\n< ## 63> index: 2954\r\n< ## 66> index: 3098\r\n< ## 69> index: 2906\r\n< ## 72> index: 3051\r\n< ## 75> index: 2955\r\n< ## 78> index: 3049\r\n< ## 81> index: 2953\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 1\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 2\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 119\r\n< ## 48> index: 120\r\n< ## 51> index: 118\r\n< ## 54> index: 121\r\n< ## 57> index: 117\r\n< ## 60> index: 167\r\n< ## 63> index: 71\r\n< ## 66> index: 215\r\n< ## 69> index: 23\r\n< ## 72> index: 168\r\n< ## 75> index: 72\r\n< ## 78> index: 166\r\n< ## 81> index: 70\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 2\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 3\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 73\r\n< ## 48> index: 74\r\n< ## 51> index: 72\r\n< ## 54> index: 75\r\n< ## 57> index: 71\r\n< ## 60> index: 121\r\n< ## 63> index: 25\r\n< ## 66> index: 169\r\n< ## 69> index: -23\r\n< ## 72> index: 122\r\n< ## 75> index: 26\r\n< ## 78> index: 120\r\n< ## 81> index: 24\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 3\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 4\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 2809\r\n< ## 48> index: 2810\r\n< ## 51> index: 2808\r\n< ## 54> index: 2811\r\n< ## 57> index: 2807\r\n< ## 60> index: 2857\r\n< ## 63> index: 2761\r\n< ## 66> index: 2905\r\n< ## 69> index: 2713\r\n< ## 72> index: 2858\r\n< ## 75> index: 2762\r\n< ## 78> index: 2856\r\n< ## 81> index: 2760\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 4\r\nTinyPose Prediction Done!\r\nKeyPointDetectionResult: [x, y, conf]\r\n405.110565,1040.763550, 0.982505\r\n435.185120,1040.723511, 0.941282\r\n379.676727,25.008259, 0.943791\r\n413.712097,16.569639, 0.949045\r\n419.809235,976.065735, 0.979562\r\nnum_joints:5\r\n\r\nTinyPose visualized result saved in ./tinypose_vis_result.jpg\r\n\r\nD:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-release-1.0.7\\examples\\vision\\keypointdetection\\tiny_pose\\cpp\\build\\Release>infer_tinypose_demo.exe\r\n[INFO] fastdeploy/runtime/runtime.cc(326)::fastdeploy::Runtime::CreateOrtBackend        Runtime initialized with Backend::ORT in Device::CPU.\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 0\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 3000\r\n< ## 48> index: 3001\r\n< ## 51> index: 2999\r\n< ## 54> index: 3002\r\n< ## 57> index: 2998\r\n< ## 60> index: 3048\r\n< ## 63> index: 2952\r\n< ## 66> index: 3096\r\n< ## 69> index: 2904\r\n< ## 72> index: 3049\r\n< ## 75> index: 2953\r\n< ## 78> index: 3047\r\n< ## 81> index: 2951\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 0\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 1\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 3002\r\n< ## 48> index: 3003\r\n< ## 51> index: 3001\r\n< ## 54> index: 3004\r\n< ## 57> index: 3000\r\n< ## 60> index: 3050\r\n< ## 63> index: 2954\r\n< ## 66> index: 3098\r\n< ## 69> index: 2906\r\n< ## 72> index: 3051\r\n< ## 75> index: 2955\r\n< ## 78> index: 3049\r\n< ## 81> index: 2953\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 1\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 2\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 119\r\n< ## 48> index: 120\r\n< ## 51> index: 118\r\n< ## 54> index: 121\r\n< ## 57> index: 117\r\n< ## 60> index: 167\r\n< ## 63> index: 71\r\n< ## 66> index: 215\r\n< ## 69> index: 23\r\n< ## 72> index: 168\r\n< ## 75> index: 72\r\n< ## 78> index: 166\r\n< ## 81> index: 70\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 84>\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 107> j: 2\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\keypointdet\\pptinypose\\pptinypose_utils.cc ## 105> j: 3\r\n<D:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-develop\\fastdeploy\\vision\\utils\\dark_parse.cc ## 42> heatmap_ch.size(): 3072\r\n< ## 45> index: 73\r\n< ## 48> index: 74\r\n< ## 51> index: 72\r\n< ## 54> index: 75\r\n< ## 57> index: 71\r\n< ## 60> index: 121\r\n< ## 63> index: 25\r\n< ## 66> index: 169\r\n< ## 69> index: -23\r\n\r\nD:\\Work\\__DataSet\\__Deploy_bak\\fastdeploy\\FastDeploy-release-1.0.7\\examples\\vision\\keypointdetection\\tiny_pose\\cpp\\build\\Release>\r\n```",
        "state": "closed",
        "user": "ThinkWD",
        "closed_by": "ThinkWD",
        "created_at": "2024-01-26T10:12:48+00:00",
        "updated_at": "2024-01-29T14:08:35+00:00",
        "closed_at": "2024-01-29T14:08:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2358,
        "title": "WARNING:root:`RuntimeOption.enable_trt_fp16` will be deprecated in v1.2.0, please use `RuntimeOption.trt_option.enable_fp16 = True` instead.",
        "body": "\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/5920ec3b-18aa-40b5-a0f8-9c048d823a92)\r\n如上图，这个需要在脚本里更改么？我看源码脚本没有变化",
        "state": "closed",
        "user": "txy00001",
        "closed_by": "heliqi",
        "created_at": "2024-01-26T09:29:53+00:00",
        "updated_at": "2024-03-01T03:21:48+00:00",
        "closed_at": "2024-03-01T03:21:48+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2360,
        "title": "tensorrt_yolov8",
        "body": "我是按照官网最新的步骤来的,因为是新手不知道是否与要安装其它的东西求大佬们帮助\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(556)::fastdeploy::TrtBackend::BuildTrtEngine\tStart to building TensorRT Engine...\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::fastdeploy::FDTrtLogger::log\t4: [network.cpp::nvinfer1::Network::validate::2770] Error Code 4: Internal Error (Network must have at least one output)\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::fastdeploy::FDTrtLogger::log\t2: [builder.cpp::nvinfer1::builder::Builder::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(620)::fastdeploy::TrtBackend::BuildTrtEngine\tFailed to call buildSerializedNetwork().\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(736)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx\tFailed to build tensorrt engine.\r\n[INFO] fastdeploy/runtime/runtime.cc(313)::fastdeploy::Runtime::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.",
        "state": "closed",
        "user": "lllxxzzzzz",
        "closed_by": "heliqi",
        "created_at": "2024-01-26T14:03:56+00:00",
        "updated_at": "2024-03-01T03:21:16+00:00",
        "closed_at": "2024-03-01T03:21:16+00:00",
        "comments_count": [
            "rainyfly",
            "lllxxzzzzz",
            "lllxxzzzzz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2361,
        "title": "将rknpu2中的rkyolov5添加到capi，执行中无连接错误",
        "body": "[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.5.1b19 (32afb0e92@2023-07-14T12:46:17)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.9.2\r\nindex=0, name=images, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=output0, n_dims=4, dims=[1, 36, 80, 80], n_elems=230400, size=230400, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\nindex=1, name=283, n_dims=4, dims=[1, 36, 40, 40], n_elems=57600, size=57600, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\nindex=2, name=285, n_dims=4, dims=[1, 36, 20, 20], n_elems=14400, size=14400, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n15\r\ndotnet: symbol lookup error: /data1/YoloDetect/TestLibFastDepoly/bin/Debug/net6.0/libfastdeploy.so: undefined symbol: _ZN10fastdeploy35FD_C_CheckAndConvertRKYOLOV5WrapperEP20FD_C_RKYOLOV5Wrapper\r\n\r\n\r\n\r\n但编译过程中并没有任何报错\r\n\r\n\r\n我用ldd和nm分析了so文件，结果如下\r\n\r\n\r\n ldd -r libfastdeploy.so\r\n        linux-vdso.so.1 (0x0000007fa5fb8000)\r\n        librknnrt.so => /data1/FastDeploy/build/third_libs/install/rknpu2_runtime/lib/librknnrt.so (0x0000007fa5189000)\r\n        libopencv_video.so.3.4 => /data1/FastDeploy/build/third_libs/install/opencv/lib/libopencv_video.so.3.4 (0x0000007fa5136000)\r\n        libopencv_calib3d.so.3.4 => /data1/FastDeploy/build/third_libs/install/opencv/lib/libopencv_calib3d.so.3.4 (0x0000007fa502b000)\r\n        libopencv_imgcodecs.so.3.4 => /data1/FastDeploy/build/third_libs/install/opencv/lib/libopencv_imgcodecs.so.3.4 (0x0000007fa4e63000)\r\n        libopencv_imgproc.so.3.4 => /data1/FastDeploy/build/third_libs/install/opencv/lib/libopencv_imgproc.so.3.4 (0x0000007fa4a68000)\r\n        libopencv_core.so.3.4 => /data1/FastDeploy/build/third_libs/install/opencv/lib/libopencv_core.so.3.4 (0x0000007fa475b000)\r\n        libstdc++.so.6 => /lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000007fa4560000)\r\n        libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000007fa44b5000)\r\n        libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000007fa4491000)\r\n        libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000007fa431d000)\r\n        libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007fa42ec000)\r\n        libopencv_flann.so.3.4 => /opt/libs/fastDeploy/third_libs/install/opencv/lib/libopencv_flann.so.3.4 (0x0000007fa4288000)\r\n        libz.so.1 => /lib/aarch64-linux-gnu/libz.so.1 (0x0000007fa425e000)\r\n        libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007fa424a000)\r\n        /lib/ld-linux-aarch64.so.1 (0x0000007fa5f88000)\r\nundefined symbol: _ZN10fastdeploy35FD_C_CheckAndConvertRKYOLOV5WrapperEP20FD_C_RKYOLOV5Wrapper  (./libfastdeploy.so)\r\n\r\n\r\n\r\nnm libfastdeploy.so|grep RKYOLOV5\r\n00000000006aade0 T FD_C_CreateRKYOLOV5Wrapper\r\n00000000006ab310 T FD_C_DestroyRKYOLOV5Wrapper\r\n00000000006abe20 t FD_C_RKYOLOV5WrapperBatchPredict\r\n00000000006ab6f0 T FD_C_RKYOLOV5WrapperInitialized\r\n00000000006ab640 T FD_C_RKYOLOV5WrapperPredict\r\n                 U _ZN10fastdeploy35FD_C_CheckAndConvertRKYOLOV5WrapperEP20FD_C_RKYOLOV5Wrapper\r\n0000000000468520 T _ZN10fastdeploy6vision9detection8RKYOLOV510InitializeEv\r\n00000000004689e0 T _ZN10fastdeploy6vision9detection8RKYOLOV512BatchPredictERKSt6vectorIN2cv3MatESaIS5_EEPS3_INS0_15DetectionResultESaISA_EE\r\n000000000046ac60 W _ZN10fastdeploy6vision9detection8RKYOLOV515GetPreprocessorEv\r\n000000000046ac70 W _ZN10fastdeploy6vision9detection8RKYOLOV516GetPostprocessorEv\r\n0000000000463cf0 T _ZN10fastdeploy6vision9detection8RKYOLOV57PredictEPN2cv3MatEPNS0_15DetectionResultEff\r\n0000000000468300 T _ZN10fastdeploy6vision9detection8RKYOLOV57PredictERKN2cv3MatEPNS0_15DetectionResultE\r\n0000000000469ab0 T _ZN10fastdeploy6vision9detection8RKYOLOV5C1ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESA_RKNS_13RuntimeOptionERKNS_11ModelFormatE\r\n0000000000469ab0 T _ZN10fastdeploy6vision9detection8RKYOLOV5C2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESA_RKNS_13RuntimeOptionERKNS_11ModelFormatE\r\n000000000046ac80 W _ZNK10fastdeploy6vision9detection8RKYOLOV59ModelNameB5cxx11Ev\r\n000000000092dcc8 V _ZTIN10fastdeploy6vision9detection8RKYOLOV5E\r\n000000000083e4c8 V _ZTSN10fastdeploy6vision9detection8RKYOLOV5E\r\n000000000092dce0 V _ZTVN10fastdeploy6vision9detection8RKYOLOV5E\r\n\r\n\r\n可以看到\r\n\r\n  U _ZN10fastdeploy35FD_C_CheckAndConvertRKYOLOV5WrapperEP20FD_C_RKYOLOV5Wrapper\r\n\r\n\r\n表示链接有问题，不知道怎么解决，希望帮忙看看，谢谢。",
        "state": "closed",
        "user": "chatop2020",
        "closed_by": "chatop2020",
        "created_at": "2024-01-27T04:52:14+00:00",
        "updated_at": "2024-02-08T13:53:55+00:00",
        "closed_at": "2024-02-08T13:53:54+00:00",
        "comments_count": [
            "rainyfly",
            "chatop2020"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2362,
        "title": "yolov8的实例分割和关键点,OBB旋转框,什么时候能上?",
        "body": "yolov8的实例分割和关键点,OBB旋转框,什么时候能上?",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "heliqi",
        "created_at": "2024-01-27T07:36:52+00:00",
        "updated_at": "2024-03-01T03:20:41+00:00",
        "closed_at": "2024-03-01T03:20:41+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2364,
        "title": "关于deploy.yaml",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： aistudio导出的0.0.0\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) / Windows x64(Windows10)\r\n- 【硬件】：Inter CPU\r\n- 【编译语言】： C++ / Python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n不算bug，应该是一些疑问，我使用paddleSeg训练的pp-liteSeg模型，使用python tools/paddle_infer_shape.py导出可推理的模型，同时也生成了一个deploy.yaml文件，但是该文件似乎有问题（这问题应该是paddleSeg的paddle_infer_shape.py的问题）：\r\ntransforms:\r\n  - target_size:\r\n    - 64\r\n    - 64\r\n    type: Resize\r\n  - mean:\r\n    - 0.78020299\r\n    std:\r\n    - 0.01534636\r\n    type: Normalize\r\n明显type不在它应该的位置。\r\n在使用fastDeploy进行推理时，需要设置一个config文件，我设置为这个有问题的deploy.yaml它也能推理，但是我不知道model = fd.vision.segmentation.PaddleSegModel(model_file, params_file, config_file, runtime_option=runtime_option)是否正确加载了transforms的内容，请问这个文件正确的样子应该是怎样的呢？我使用paddle2onnx转了onnx模型使用onnxruntime进行推理（onnxruntime的数据预处理我确定就是我想要的），使用np.testing.assert_allclose进行比较，整体差距约8%，我感觉问题应该就在数据预处理的位置，但是我不知该如何进行检查，请问是否有办法确认在model.predict内部每一步图像如何变化，或者其他的建议呢\r\n\r\n",
        "state": "closed",
        "user": "rememberBr",
        "closed_by": "heliqi",
        "created_at": "2024-01-29T07:21:47+00:00",
        "updated_at": "2024-03-01T03:20:04+00:00",
        "closed_at": "2024-03-01T03:20:04+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2366,
        "title": "mask_rcnn_r50_fpn_1x_coco.yml训练出来的模型不支持fastdeploy服务部署?",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【FastDeploy版本】:fastdeploy1.0.2-linux-gpu\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： CUDA 11.2 CUDNN 8.\r\n- 【编译语言】：python3.8.10\r\nexamples下的mask_rcnn_r50_1x_coco能够跑通\r\n但是自己训练的mask_rcnn_r50_fpn_1x_coco.yml模型按照examples步骤无法跑通 具体是在启动服务时报以下做错\r\nUNAVAILABLE: Invalid argument: unable to load model 'runtime', configuration expects datatype TYPE_FP32 for output 'concat_9.tmp_0', model provides TYPE_INT32\r\n我在config.obtxt中将output下的oncat_9.tmp_0配置修改为TYPE_INT32后就会报以下这个错误Invalid argument: unexpected inference input 'concat_5.tmp_0', allowed inputs are: concat_13.tmp_0, concat_9.tmp_0, tmp_150\r\n无法解决",
        "state": "closed",
        "user": "wxf764571829",
        "closed_by": "wxf764571829",
        "created_at": "2024-01-31T10:06:46+00:00",
        "updated_at": "2024-02-02T01:25:06+00:00",
        "closed_at": "2024-02-02T01:25:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2367,
        "title": "使用paddle中mask_rcnn_x101_vd_64x4d_fpn_2x_coco训练出的模型部署服务化失败 ",
        "body": "![image](https://github.com/PaddlePaddle/FastDeploy/assets/46362180/b2094d55-a96a-4cfc-ae9a-f6c3e9bf43d1)\r\nInvalid argument: in ensemble ppdet, ensemble tensor RUNTIME_OUTPUT2 is mapped from non-existing output concat_5.tmp_0 in model ",
        "state": "closed",
        "user": "wxf764571829",
        "closed_by": "wxf764571829",
        "created_at": "2024-02-02T01:24:46+00:00",
        "updated_at": "2024-02-02T08:21:06+00:00",
        "closed_at": "2024-02-02T08:21:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2365,
        "title": "代码问题",
        "body": "### `with_nms_` 默认值等于 true, 注释是 \"without nms\".\r\n### `ApplyNMS()` 函数的作用是启用 NMS, 做的事情是设置 `with_nms_` 为 false\r\n![QQ截图20240129174814](https://github.com/PaddlePaddle/FastDeploy/assets/48641295/1a7e089d-11b1-41ca-8708-81c2f33f011a)\r\n#\r\n#\r\n### 这里倒是很合理, 非常清晰\r\n### `with_nms_ == true` 就跳转到 `ProcessWithNMS`\r\n### 否则就跳转到 `ProcessWithoutNMS`\r\n![QQ截图20240129174732](https://github.com/PaddlePaddle/FastDeploy/assets/48641295/6a0791c2-b636-41bf-a2a2-6d2d17438155)\r\n#\r\n#\r\n### 到具体实现, `ProcessWithNMS` 函数名意思是有 NMS, 实际上没有 NMS.\r\n### `ProcessWithoutNMS` 函数名意思是没有 NMS, 实际上有 NMS.\r\n![QQ截图20240129174713](https://github.com/PaddlePaddle/FastDeploy/assets/48641295/e62127af-1612-4038-9c5c-4ce781e29df4)\r\n\r\n### 有点搞笑 ",
        "state": "closed",
        "user": "ThinkWD",
        "closed_by": "ThinkWD",
        "created_at": "2024-01-29T10:03:47+00:00",
        "updated_at": "2024-02-04T02:16:47+00:00",
        "closed_at": "2024-02-04T02:16:47+00:00",
        "comments_count": [
            "ThinkWD",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2368,
        "title": "如何使用python API进行多卡推理",
        "body": "在python脚本中是否能支持多卡推理？比方run option中设置了两张卡，他会自动进行多线程推理。",
        "state": "closed",
        "user": "ChenjieXu",
        "closed_by": "heliqi",
        "created_at": "2024-02-07T02:25:46+00:00",
        "updated_at": "2024-03-01T03:18:38+00:00",
        "closed_at": "2024-03-01T03:18:38+00:00",
        "comments_count": [
            "rainyfly",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2372,
        "title": "./infer_demo: error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory",
        "body": "./infer_demo: error while loading shared libraries: libonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory",
        "state": "closed",
        "user": "chlg",
        "closed_by": "jiangjiajun",
        "created_at": "2024-02-21T03:06:35+00:00",
        "updated_at": "2024-02-26T08:11:47+00:00",
        "closed_at": "2024-02-26T08:11:47+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2373,
        "title": "Orange Pi AIpro , 采用 华为昇腾NPU  , 希望加入硬件支持",
        "body": "http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-AIpro.html\r\n自带 pytorch_npu   CANN\r\n\r\n\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/huawei_ascend.md\r\n华为昇腾NPU 部署环境编译准备\r\n\r\n感觉有点不一样\r\n\r\n希望加入支持",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "jiangjiajun",
        "created_at": "2024-02-21T07:54:22+00:00",
        "updated_at": "2024-02-26T08:11:36+00:00",
        "closed_at": "2024-02-26T08:11:36+00:00",
        "comments_count": [
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2374,
        "title": "fastdeploy预编译库无法使用",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7\r\n- 【系统平台】: win11\r\n- 【硬件】： Nvidia GPU 3060， CUDA 11.7 CUDNN 8.7\r\n- 【编译语言】： C++ \r\n- 【编译器】： msvc2022 \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【官方示例报错】\r\n下载了fastdeploy-win-x64-gpu-1.0.7，以及按照官方示例下载了ppyoloe_crn_l_300e_coco和test_det.jpg，以及代码infer_demo.cc。\r\n目录结构\r\nfastdeploy_test/\r\n                        build\r\n                        ppyoloe_crn_l_300e_coco\r\n                        CmakeLists.txt\r\n                        test_det.jpg\r\n\r\n\r\n按照官方操作\r\nmkdir build\r\ncd build\r\ncmake .. -G \"Visual Studio 17 2022\" -A x64（原版 cmake .. -G \"Visual Studio 16 2019\" -A x64）\r\n出现报错\r\n`\r\nPS D:\\AI_lib\\fastdeploy_test\\build> cmake .. -G \"Visual Studio 17 2022\" -A x64\r\n-- The C compiler identification is MSVC 19.37.32825.0\r\n-- The CXX compiler identification is MSVC 19.37.32825.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- The path of ONNXRuntime is D:/AI_lib/fastdeploy-win-x64-gpu-1.0.7/third_libs/install/onnxruntime/lib.\r\n-- OPENVINO_LIBS = D:/AI_lib/fastdeploy-win-x64-gpu-1.0.7/third_libs/install/openvino/runtime/lib/openvino.lib;TBB::tbb;TBB::tbbmalloc\r\nCMake Error at D:/AI_lib/fastdeploy-win-x64-gpu-1.0.7/FastDeploy.cmake:228 (message):\r\n  [FastDeploy] Cannot find library cudart in /usr/local/cuda, Please define\r\n  CUDA_DIRECTORY, e.g -DCUDA_DIRECTORY=/path/to/cuda\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:4 (include)\r\n\r\n`\r\n\r\n",
        "state": "closed",
        "user": "a819411321",
        "closed_by": "a819411321",
        "created_at": "2024-02-22T03:30:56+00:00",
        "updated_at": "2024-04-23T07:30:02+00:00",
        "closed_at": "2024-02-22T06:11:45+00:00",
        "comments_count": [
            "Wsk0317"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2375,
        "title": "请问有没有基于CUDA 11.8的镜像？",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： CUDA 11.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n请问有没有基于CUDA 11.8的镜像？",
        "state": "closed",
        "user": "neo502721",
        "closed_by": "neo502721",
        "created_at": "2024-02-22T08:31:56+00:00",
        "updated_at": "2024-02-26T07:17:37+00:00",
        "closed_at": "2024-02-26T07:17:37+00:00",
        "comments_count": [
            "rainyfly",
            "neo502721"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2376,
        "title": "libopencv_flann.so.3.4: cannot open shared object file: No such file or directory",
        "body": "开发板: Orange Pi AIpro\r\n编译方式: 源码编译\r\npython 3.9\r\nubuntu 22.0\r\n\r\n ```python\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport WITH_ASCEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n```\r\n\r\n\r\n```python\r\nython infer.py --model yolov5s_infer --image 000000014439.jpg --device ascend\r\nTraceback (most recent call last):\r\n  File \"/home/HwHiAiUser/.conda/envs/FastDeploy/lib/python3.8/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"/home/HwHiAiUser/.conda/envs/FastDeploy/lib/python3.8/site-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/home/HwHiAiUser/.conda/envs/FastDeploy/lib/python3.8/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n\r\n```\r\n\r\n\r\n\r\n如何指定FastDeploy默认安装的opencv目录\r\n\r\n怀疑系统环境没有权限，所以没有加上\r\n\r\n应该要怎么弄\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "heliqi",
        "created_at": "2024-02-23T09:11:31+00:00",
        "updated_at": "2024-04-08T08:36:15+00:00",
        "closed_at": "2024-04-08T08:36:15+00:00",
        "comments_count": [
            "heliqi",
            "monkeycc",
            "jiangjiajun",
            "heliqi",
            "15380831711"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2377,
        "title": "服务端支持的模型列表是只能用哪些模型，比如有一个变化检测的模型p2vnet，怎么使用fastdeploy部署工具？",
        "body": null,
        "state": "open",
        "user": "funny000",
        "closed_by": null,
        "created_at": "2024-02-27T03:38:06+00:00",
        "updated_at": "2024-02-27T09:25:51+00:00",
        "closed_at": null,
        "comments_count": [
            "heliqi",
            "heliqi",
            "funny000"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2381,
        "title": "trt fp16速度和fp32无差异，并没有提速",
        "body": "我用fastdeploy进行trt后端ppocrv4推理，结果fp32的耗时和fp16一样，是什么原有呢？\r\n\r\n以下为我的代码：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/9726e8a0-6096-4123-806e-656d29ee5e59)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/2a0db426-e5b6-47aa-9033-8a7d32da34a1)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/19f00295-6bd2-451b-8679-29b67d7ccfb0)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/bbc0202b-3e63-4879-8c0d-939994db2b5f)\r\n这是结果\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/47891269/234219cf-f583-4924-b997-99bde8eaf1ea)\r\n",
        "state": "closed",
        "user": "txy00001",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-02-28T09:06:57+00:00",
        "updated_at": "2025-03-04T06:48:53+00:00",
        "closed_at": "2025-03-04T06:48:53+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2380,
        "title": "运行stable-diffusion 的c++ 示例报错",
        "body": "### 编译信息：\r\n```bash\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.26.4\r\n--   CMake command             : C:/software/CLion 2022.3.2/bin/cmake/win/x64/bin/cmake.exe\r\n--   System                    : Windows\r\n--   C++ compiler              : C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe\r\n--   C++ standard              : \r\n--   C++ cuda standard         : \r\n--   C++ compiler version      : 19.35.32215.0\r\n--   CXX flags                 : /DWIN32 /D_WINDOWS /W3 /GR /EHsc\r\n--   EXE linker flags          : /machine:x64\r\n--   Shared linker flags       : /machine:x64\r\n--   Build type                : Release\r\n--   Compile definitions       : \r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : C:/Program Files (x86)/main\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : OM\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : \r\n--   Paddle Inference version  : 2.5.0.281761089e\r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   CUDA_DIRECTORY            : C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\r\n--   TRT_DRECTORY              : \r\n--   DEPENDENCY_LIBS           : E:/FastDeploy/build_gpu/install/lib/fastdeploy.lib;E:/FastDeploy/build_gpu/install/third_libs/install/onnxruntime/lib/onnxruntime.lib;E:/FastDeploy/build_gpu/install/third_libs/install/paddle_inference/paddle/lib/paddle_inference.lib;E:/FastDeploy/build_gpu/install/third_libs/install/paddle_inference/third_party/install/mkldnn/lib/mkldnn.lib;E:/FastDeploy/build_gpu/install/third_libs/install/paddle_inference/third_party/install/mklml/lib/libiomp5md.lib;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/lib/x64/cudart.lib;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/lib/x64/nvjpeg.lib;E:/FastDeploy/build_gpu/install/third_libs/install/tensorrt/lib/nvinfer.lib;E:/FastDeploy/build_gpu/install/third_libs/install/tensorrt/lib/nvonnxparser.lib;E:/FastDeploy/build_gpu/install/third_libs/install/tensorrt/lib/nvinfer_plugin.lib;opencv_calib3d;opencv_core;opencv_features2d;opencv_flann;opencv_gapi;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_photo;opencv_stitching;opencv_video;opencv_videoio;E:/FastDeploy/build_gpu/install/third_libs/install/fast_tokenizer/lib/core_tokenizers.lib;E:/FastDeploy/build_gpu/install/third_libs/install/paddle2onnx/lib/paddle2onnx.lib\r\n```\r\n### 报错信息：\r\n```bash\r\n[ERROR] fastdeploy/utils/utils.cc(54)::fastdeploy::ReadBinaryFromFile   Failed to open file: ./stable-diffusion-v1-4\\vae\r\n_encoder\\inference.pdmodel to read.\r\n[ERROR] fastdeploy/runtime/backends/paddle/paddle_backend.cc(209)::fastdeploy::PaddleBackend::InitFromPaddle    Failed t\r\no read file ./stable-diffusion-v1-4\\vae_encoder\\inference.pdmodel.\r\n```",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-02-28T06:23:29+00:00",
        "updated_at": "2025-03-25T06:45:38+00:00",
        "closed_at": "2025-03-25T06:45:38+00:00",
        "comments_count": [
            "heliqi",
            "ChaoII",
            "ChaoII",
            "heliqi",
            "ChaoII",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2382,
        "title": "Building in Jetson Agx is failing while running setup.py",
        "body": "## Environment\r\n\r\nFastDeploy version: develop branch\r\nOS Platform: e.g. Linux aarch64 (jetson)\r\nHardware: e.g. Nvidia Jetson AGX\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\n-- The C compiler identification is GNU 9.4.0\r\n-- The CXX compiler identification is GNU 9.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 16% complete]\r\n-- [download 21% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 42% complete]\r\n-- [download 47% complete]\r\n-- [download 52% complete]\r\n-- [download 57% complete]\r\n-- [download 63% complete]\r\n-- [download 68% complete]\r\n-- [download 73% complete]\r\n-- [download 78% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\n-- Found Python: /usr/bin/python2.7 (found version \"2.7.18\") found components: Interpreter Development \r\n-- Copying /Download/paddle_inference_jetson to /home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/third_libs/install/paddle_inference ...\r\ncp: cannot stat '/Download/paddle_inference_jetson': No such file or directory\r\nCMake Error at cmake/paddle_inference.cmake:272 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\nCMake Error at cmake/paddle_inference.cmake:273 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\nCMake Error at cmake/paddle_inference.cmake:274 (string):\r\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\r\n  command.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\n-- The CUDA compiler identification is NVIDIA 11.4.239\r\n-- Check for working CUDA compiler: /usr/local/cuda-11.4/bin/nvcc\r\n-- Check for working CUDA compiler: /usr/local/cuda-11.4/bin/nvcc -- works\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- CUDA compiler: /usr/local/cuda-11.4/bin/nvcc, version: NVIDIA 11.4.239\r\n-- CUDA detected: 11.4.239\r\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_86,code=sm_86\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/lib/aarch64-linux-gnu/cmake/opencv4/\r\n-- Found OpenCV: /usr (found version \"4.5.4\") \r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.16.3\r\n--   CMake command             : /usr/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ standard              : 11\r\n--   C++ cuda standard         : 11\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : Release\r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;WITH_GPU;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : \r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : /home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/UNDEFINED\r\n--   Python executable         : /usr/bin/python3\r\n--   Python includes           : /usr/include/python3.8\r\n-- Found PythonInterp: /usr/bin/python3 (found version \"3.8.10\") \r\n-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython3.8.so (found version \"3.8.10\") \r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/jetson-agx/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 445, in <module>\r\n    setuptools.setup(\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 144, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.8/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.8/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 308, in run\r\n    self.run_command('cmake_build')\r\n  File \"/usr/lib/python3.8/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"setup.py\", line 294, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.8', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-38-aarch64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DENABLE_TVM_BACKEND=OFF', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DENABLE_ORT_BACKEND=OFF', '-DENABLE_OPENVINO_BACKEND=OFF', '-DENABLE_PADDLE_BACKEND=ON', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=OFF', '-DENABLE_LITE_BACKEND=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=OFF', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=OFF', '-DWITH_IPU=OFF', '-DWITH_OPENCL=OFF', '-DWITH_TIMVX=OFF', '-DWITH_DIRECTML=OFF', '-DWITH_ASCEND=OFF', '-DWITH_KUNLUNXIN=OFF', '-DRKNN2_TARGET_SOC=', '-DTRT_DIRECTORY=UNDEFINED', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson', '-DPADDLEINFERENCE_VERSION=', '-DPADDLEINFERENCE_URL=', '-DPADDLEINFERENCE_API_COMPAT_2_4_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_2_5_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_DEV=OFF', '-DPADDLEINFERENCE_API_CUSTOM_OP=OFF', '-DPADDLE2ONNX_URL=', '-DPADDLELITE_URL=', '-DBUILD_ON_JETSON=ON', '-DBUILD_PADDLE2ONNX=OFF', '/home/jetson-agx/FastDeploy']' returned non-zero exit status 1.\r\n",
        "state": "open",
        "user": "csoham96",
        "closed_by": null,
        "created_at": "2024-02-28T09:54:46+00:00",
        "updated_at": "2024-03-07T07:03:13+00:00",
        "closed_at": null,
        "comments_count": [
            "heliqi",
            "csoham96",
            "jiangjiajun",
            "heliqi",
            "csoham96"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2384,
        "title": "在使用fastdeploy进行推理时",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-gpu-cuda11.4-trt8.5-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python3.8\r\n按照官方给出的教程以及官方给出的模型，在请求时报错\r\n![1315cd777b24376c82d849b434a89b29](https://github.com/PaddlePaddle/FastDeploy/assets/137677731/21c0e4a5-c08b-4668-87b1-0bb6beaaa334)\r\n\r\n",
        "state": "closed",
        "user": "menginbaidu",
        "closed_by": "heliqi",
        "created_at": "2024-02-29T02:16:05+00:00",
        "updated_at": "2024-02-29T09:46:39+00:00",
        "closed_at": "2024-02-29T09:46:39+00:00",
        "comments_count": [
            "heliqi",
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2385,
        "title": "PPYoloe+在PaddleServing上部署报错segmentatioin fault",
        "body": "\r\n用自己的数据训练了ppyoloe+，图片大小大概在1200x1200到1150x1150左右。训练和推理时都正常。\r\n使用pipline的方式部署了ppyoloe+(部署正常未报错)，并用http方式进行预测。预测时直接卡住并报错segmentation fault。\r\n### 测试环境\r\n* paddle-serving-app        0.9.0\r\n* paddle-serving-client     0.9.0\r\n* paddle-serving-server-gpu 0.9.0.post112\r\n* paddlepaddle-gpu          2.5.2.post117\r\n### 模型导出生成的serving_server_conf.prototxt\r\n```\r\nfeed_var {\r\n  name: \"image\"\r\n  alias_name: \"image\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 3\r\n  shape: 1024\r\n  shape: 1024\r\n}\r\nfeed_var {\r\n  name: \"scale_factor\"\r\n  alias_name: \"scale_factor\"\r\n  is_lod_tensor: false\r\n  feed_type: 1\r\n  shape: 2\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_0\"\r\n  alias_name: \"multiclass_nms3_0.tmp_0\"\r\n  is_lod_tensor: false\r\n  fetch_type: 1\r\n  shape: 6\r\n}\r\nfetch_var {\r\n  name: \"multiclass_nms3_0.tmp_2\"\r\n  alias_name: \"multiclass_nms3_0.tmp_2\"\r\n  is_lod_tensor: false\r\n  fetch_type: 2\r\n}\r\n```\r\n### web_service.py\r\n```python\r\nfrom paddle_serving_server.web_service import WebService, Op\r\nimport logging\r\nimport numpy as np\r\nimport sys\r\nimport cv2\r\nfrom paddle_serving_app.reader import *\r\nimport base64\r\n\r\n\r\nclass Ppyoloe(Op):\r\n    def init_op(self):\r\n        self.img_preprocess = Sequential([\r\n            BGR2RGB(), Div(255.0),\r\n            Normalize([0., 0., 0.], [1., 1., 1.], False),\r\n            Resize((1024, 1024)), Transpose((2, 0, 1))\r\n        ])\r\n        self.img_postprocess = RCNNPostprocess(\"label_list.txt\", \"output\")\r\n\r\n    def preprocess(self, input_dicts, data_id, log_id):\r\n        (_, input_dict), = input_dicts.items()\r\n        imgs = []\r\n        #print(\"keys\", input_dict.keys())\r\n        for key in input_dict.keys():\r\n            data = base64.b64decode(input_dict[key].encode('utf8'))\r\n            data = np.fromstring(data, np.uint8)\r\n            im = cv2.imdecode(data, cv2.IMREAD_COLOR)\r\n            print(im.shape)\r\n            im = self.img_preprocess(im)\r\n            # print(im.shape)\r\n            imgs.append({\r\n                \"image\": im[np.newaxis, :],\r\n                # \"im_shape\": np.array(list(im.shape[1:])).reshape(-1)[np.newaxis, :],\r\n                \"scale_factor\": np.array([1., 1.]).reshape(-1)[np.newaxis, :],\r\n            })\r\n        feed_dict = {\r\n            \"image\": np.concatenate(\r\n                [x[\"image\"] for x in imgs], axis=0),\r\n            # \"im_shape\": np.concatenate(\r\n            #     [x[\"im_shape\"] for x in imgs], axis=0),\r\n            \"scale_factor\": np.concatenate(\r\n                [x[\"scale_factor\"] for x in imgs], axis=0)\r\n        }\r\n        for key in feed_dict.keys():\r\n            print(key, feed_dict[key].shape)\r\n        return feed_dict, False, None, \"\"\r\n\r\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\r\n        print(\"fetch_dict = \", fetch_dict)\r\n        res_dict = {\r\n            \"bbox_result\":\r\n            str(self.img_postprocess(\r\n                fetch_dict, visualize=False))\r\n        }\r\n        print(res_dict)\r\n        return res_dict, None, \"\"\r\n\r\n\r\nclass PpyoloeService(WebService):\r\n    def get_pipeline_response(self, read_op):\r\n        ppyoloe_op = Ppyoloe(name=\"ppyoloe\", input_ops=[read_op])\r\n        return ppyoloe_op\r\n\r\nppyoloe_service = PpyoloeService(name=\"ppyoloe\")\r\nppyoloe_service.prepare_pipeline_config(\"config.yml\")\r\nppyoloe_service.run_service()\r\n```\r\n### config.yml\r\n```yaml\r\ndag:\r\n  #op资源类型, True, 为线程模型；False，为进程模型\r\n  is_thread_op: false\r\n  #使用性能分析, True，生成Timeline性能数据，对性能有一定影响；False为不使用\r\n  tracer:\r\n    interval_s: 30\r\n#http端口, rpc_port和http_port不允许同时为空。当rpc_port可用且http_port为空时，不自动生成http_port\r\nhttp_port: 9292\r\nop:\r\n  ppyoloe:\r\n    #并发数，is_thread_op=True时，为线程并发；否则为进程并发\r\n    concurrency: 10\r\n    local_service_conf:\r\n      #client类型，包括brpc, grpc和local_predictor.local_predictor不启动Serving服务，进程内预测\r\n      client_type: local_predictor\r\n      # device_type, 0=cpu, 1=gpu, 2=tensorRT, 3=arm cpu, 4=kunlun xpu\r\n      device_type: 1\r\n      #计算硬件ID，当devices为\"\"或不写时为CPU预测；当devices为\"0\", \"0,1,2\"时为GPU预测，表示使用的GPU卡\r\n      devices: '0'\r\n      #Fetch结果列表，以bert_seq128_model中fetch_var的alias_name为准, 如果没有设置则全部返回\r\n      fetch_list: ['multiclass_nms3_0.tmp_0']\r\n      #模型路径\r\n      model_config: serving_server/\r\n#rpc端口, rpc_port和http_port不允许同时为空。当rpc_port为空且http_port不为空时，会自动将rpc_port设置为http_port+1\r\nrpc_port: 9998\r\n#worker_num, 最大并发数。当build_dag_each_worker=True时, 框架会创建worker_num个进程，每个进程内构建grpcSever和DAG\r\n#当build_dag_each_worker=False时，框架会设置主线程grpc线程池的max_workers=worker_num\r\nworker_num: 20\r\n```\r\n### 报错信息\r\n```\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::AnalysisPredictor::ZeroCopyRun()\r\n1   paddle::framework::NaiveExecutor::Run()\r\n2   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)\r\n3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const\r\n4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&, paddle::framework::RuntimeContext*) const\r\n5   std::_Function_handler<void (paddle::framework::InferShapeContext*), paddle::framework::details::OpInfoFiller<SqueezeInferShapeFunctor, (paddle::framework::details::OpInfoFillType)4>::operator()(char const*, paddle::framework::OpInfo*) const::{lambda(paddle::framework::InferShapeContext*)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::InferShapeContext*&&)\r\n6   SqueezeInferShapeFunctor::operator()(paddle::framework::InferShapeContext*) const\r\n7   phi::SqueezeInferMeta(phi::MetaTensor const&, std::vector<int, std::allocator<int> > const&, phi::MetaTensor*, phi::MetaTensor*)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1706101784 (unix time) try \"date -d @1706101784\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x0) received by PID 17572 (TID 0x7f4bbee78700) from PID 0 ***]\r\n```\r\n### ppyoloe_plus_reader.yml\r\n```yaml\r\nworker_num: 8\r\neval_height: &eval_height 1024\r\neval_width: &eval_width 1024\r\neval_size: &eval_size [*eval_height, *eval_width]\r\n\r\nTrainReader:\r\n  sample_transforms:\r\n    - Decode: {}\r\n    # - RandomDistort: {}\r\n    # - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}\r\n    # - RandomCrop: {}\r\n    - Mixup: {alpha: 1.5, beta: 1.5}\r\n    - RandomFlip: {}\r\n    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}\r\n  batch_transforms:\r\n    # - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768], random_size: True, random_interp: True, keep_ratio: False}\r\n    - NormalizeImage: {mean: [0., 0., 0.], std: [1., 1., 1.], norm_type: none}\r\n    - Permute: {}\r\n    - PadGT: {}\r\n  batch_size: 1\r\n  mixup_epoch: 15\r\n  shuffle: true\r\n  drop_last: true\r\n  use_shared_memory: true\r\n  collate_batch: true\r\n\r\nEvalReader:\r\n  sample_transforms:\r\n    - Decode: {}\r\n    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}\r\n    - NormalizeImage: {mean: [0., 0., 0.], std: [1., 1., 1.], norm_type: none}\r\n    - Permute: {}\r\n  batch_size: 1\r\n\r\nTestReader:\r\n  inputs_def:\r\n    image_shape: [3, *eval_height, *eval_width]\r\n  sample_transforms:\r\n    - Decode: {}\r\n    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}\r\n    - NormalizeImage: {mean: [0., 0., 0.], std: [1., 1., 1.], norm_type: none}\r\n    - Permute: {}\r\n  batch_size: 1\r\n```\r\n",
        "state": "closed",
        "user": "hxuaj",
        "closed_by": "hxuaj",
        "created_at": "2024-02-29T08:15:34+00:00",
        "updated_at": "2024-03-07T01:56:48+00:00",
        "closed_at": "2024-03-07T01:56:48+00:00",
        "comments_count": [
            "heliqi",
            "hxuaj",
            "hxuaj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2386,
        "title": "StructureV2Layout model initialize failed",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "lsx66",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-02-29T09:57:09+00:00",
        "updated_at": "2025-03-04T06:48:54+00:00",
        "closed_at": "2025-03-04T06:48:54+00:00",
        "comments_count": [
            "lsx66",
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2388,
        "title": "PPYOLOE+ 使用apply_nms()后报错",
        "body": "table_sys=fd.vision.detection.PPYOLOE(layout_model_file, layout_params_file, infer_cfg_file,layout_option)\r\ntable_sys.postprocessor.apply_nms()\r\n\r\n报了错误为：\r\n[INFO] fastdeploy/runtime/runtime.cc(384)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\r\n[ERROR] fastdeploy/vision/detection/ppdet/postprocessor.cc(139)::ProcessWithoutNMS\tThe shape of boxes and scores should be [batch, boxes_num, 4], [batch, classes_num, boxes_num]\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(78)::BatchPredict\tFailed to postprocess the inference results by runtime.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\n如果注释掉apply_nms() ,则能正常预测，不知道是哪里配置有误？是因为没有配置model_format的关系吗？\r\n版面分析原有的精度不够，模型我自己训练的PP-YOLOE+，是直接通过python ppstructure/tools/export_model.py -c ppstructure/ml/109/train.yml --output_dir=ppstructure/ml/109/final_inference -o weights=ppstructure/ml/109/checkpoint/best_model exclude_nms=True  导出的",
        "state": "closed",
        "user": "zhuxiaobin",
        "closed_by": "zhuxiaobin",
        "created_at": "2024-03-02T09:35:22+00:00",
        "updated_at": "2024-03-02T15:19:50+00:00",
        "closed_at": "2024-03-02T15:19:50+00:00",
        "comments_count": [
            "zhuxiaobin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2389,
        "title": "在jieson orin nano中编译fastdeploy时报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】cmake -GNinja -DWITH_GPU=ON -DENABLE_ORT_BACKEND=ON -DENABLE_PADDLE_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DENABLE_VISION=ON -DENABLE_FLYCV=ON  -DPADDLEINFERENCE_DIRECTORY=/home/lhou/fastdeploy/paddle  -DPython_EXECUTABLE=/home/lhou/miniconda3/envs/fastdeploy/bin/python -DOPENCV_DIRECTORY=/usr/lib/aarch64-linux-gnu -DTRT_DIRECTORY=/home/lhou/fastdeploy/tensorrt  ..\r\n- 【系统平台】: jetson orin nano jetpack 5.1.1 ubuntu20.04 \r\n- 【硬件】： cuda11.4 cudnn 8.6.0 tensorrt 8.5.2 cmake 3.26\r\n- 【编译语言】：python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n使用上述命令编译FastDeploy时提示有一些库没有安装\r\nCMake Warning (dev) at CMakeLists.txt:15 (PROJECT):\r\n  cmake_minimum_required() should be called prior to this top-level project()\r\n  call.  Please see the cmake-commands(7) manual for usage documentation of\r\n  both commands.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- The C compiler identification is GNU 9.4.0\r\n-- The CXX compiler identification is GNU 9.4.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\nDownloading file from https://bj.bcebos.com/fastdeploy/third_libs/patchelf-0.15.0-aarch64.tar.gz to /home/lhou/fastdeploy/FastDeploy/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- [download 1% complete]\r\n-- [download 6% complete]\r\n-- [download 11% complete]\r\n-- [download 16% complete]\r\n-- [download 21% complete]\r\n-- [download 27% complete]\r\n-- [download 32% complete]\r\n-- [download 37% complete]\r\n-- [download 42% complete]\r\n-- [download 47% complete]\r\n-- [download 52% complete]\r\n-- [download 57% complete]\r\n-- [download 63% complete]\r\n-- [download 68% complete]\r\n-- [download 73% complete]\r\n-- [download 78% complete]\r\n-- [download 83% complete]\r\n-- [download 88% complete]\r\n-- [download 93% complete]\r\n-- [download 99% complete]\r\n-- [download 100% complete]\r\nDecompress file /home/lhou/fastdeploy/FastDeploy/build/patchelf-0.15.0-aarch64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /home/lhou/fastdeploy/FastDeploy/build/third_libs/install/onnxruntime\r\nCannot compile with onnxruntime-gpu while in linux-aarch64 platform, fallback to onnxruntime-cpu\r\nCMake Warning (dev) at /home/lhou/miniconda3/envs/fastdeploy/share/cmake-3.26/Modules/ExternalProject.cmake:3091 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /home/lhou/miniconda3/envs/fastdeploy/share/cmake-3.26/Modules/ExternalProject.cmake:4208 (_ep_add_download_command)\r\n  cmake/onnxruntime.cmake:109 (ExternalProject_Add)\r\n  CMakeLists.txt:230 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Copying /home/lhou/fastdeploy/paddle to /home/lhou/fastdeploy/FastDeploy/build/third_libs/install/paddle_inference ...\r\n-- The CUDA compiler identification is NVIDIA 11.4.315\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /usr/local/cuda-11.4/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- CUDA compiler: /usr/local/cuda-11.4/bin/nvcc, version: NVIDIA 11.4.315\r\nUsing New Release Strategy - All Arches Packge\r\n-- CUDA detected: 11.4.315\r\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86\r\n-- Copying /home/lhou/fastdeploy/tensorrt/lib to /home/lhou/fastdeploy/FastDeploy/build/third_libs/install/tensorrt/lib ...\r\n-- result:0 out:\r\n-- Use the opencv lib specified by user. The OpenCV path: /usr/lib/aarch64-linux-gnu\r\n-- Found OpenCV: /usr (found version \"4.5.4\") \r\nCMake Warning (dev) at /home/lhou/miniconda3/envs/fastdeploy/share/cmake-3.26/Modules/ExternalProject.cmake:3091 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  /home/lhou/miniconda3/envs/fastdeploy/share/cmake-3.26/Modules/ExternalProject.cmake:4208 (_ep_add_download_command)\r\n  cmake/paddle2onnx.cmake:70 (ExternalProject_Add)\r\n  CMakeLists.txt:478 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- \r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.26.4\r\n--   CMake command             : /home/lhou/miniconda3/envs/fastdeploy/bin/cmake\r\n--   System                    : Linux\r\n--   C++ compiler              : /usr/bin/c++\r\n--   C++ standard              : 11\r\n--   C++ cuda standard         : 11\r\n--   C++ compiler version      : 9.4.0\r\n--   CXX flags                 : -Wno-format -g0 -O3\r\n--   EXE linker flags          : \r\n--   Shared linker flags       : \r\n--   Build type                : \r\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;WITH_GPU;ENABLE_NVJPEG;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         : \r\n--   CMAKE_INSTALL_PREFIX      : /usr/local\r\n--   CMAKE_MODULE_PATH         : \r\n-- \r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : OFF\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : OFF\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : OFF\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : 2.5.2\r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   CUDA_DIRECTORY            : /usr/local/cuda\r\n--   TRT_DRECTORY              : /home/lhou/fastdeploy/tensorrt\r\n-- Configuring done (27.8s)\r\n**CMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\nPlease set them or make sure they are set and tested correctly in the CMake files:\r\nNVJPEG_LIB\r\n    linked by target \"fastdeploy\" in directory /home/lhou/fastdeploy/FastDeploy\r\nTRT_INFER_LIB\r\n    linked by target \"fastdeploy\" in directory /home/lhou/fastdeploy/FastDeploy\r\nTRT_ONNX_LIB\r\n    linked by target \"fastdeploy\" in directory /home/lhou/fastdeploy/FastDeploy\r\nTRT_PLUGIN_LIB\r\n    linked by target \"fastdeploy\" in directory /home/lhou/fastdeploy/FastDeploy**\r\n\r\n-- Generating done (0.1s)\r\nCMake Generate step failed.  Build files cannot be regenerated correctly.\r\n\r\n",
        "state": "closed",
        "user": "lhou705",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-02T12:50:33+00:00",
        "updated_at": "2025-04-01T06:44:02+00:00",
        "closed_at": "2025-04-01T06:44:02+00:00",
        "comments_count": [
            "lhou705",
            "jiangjiajun",
            "lhou705",
            "lhou705",
            "WQG6848"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2391,
        "title": "gpu显存不释放",
        "body": "PaddleClasModel 通过gpu启动，后面和教程中的一样，在启动多线程的同时，进行clone()，但是线程执行完后gpu不释放，请问该如何解决？",
        "state": "open",
        "user": "zhuxiaobin",
        "closed_by": null,
        "created_at": "2024-03-03T20:39:59+00:00",
        "updated_at": "2024-10-17T03:13:27+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "zhuxiaobin",
            "rainyfly",
            "wanshichenguang",
            "jiangjiajun",
            "zhuxiaobin",
            "wanshichenguang",
            "zhuxiaobin",
            "sanersbug",
            "wanshichenguang",
            "sanersbug"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2390,
        "title": "gpu镜像部署报错",
        "body": "Error: Failed to initialize NVML\r\nW0303 09:57:36.731008 125 metrics.cc:221] DCGM unable to start: DCGM initialization error\r\nW0303 09:57:36.731207 125 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0303 09:57:36.731222 125 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\n",
        "state": "closed",
        "user": "jiamian2009",
        "closed_by": "jiamian2009",
        "created_at": "2024-03-03T10:04:11+00:00",
        "updated_at": "2024-03-03T14:02:51+00:00",
        "closed_at": "2024-03-03T14:02:50+00:00",
        "comments_count": [
            "jiamian2009"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2392,
        "title": "adaface模型无法正常运行",
        "body": "\r\n## 环境\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【系统平台】: Windows x64(Windows11)\r\n- 【硬件】： 如 Nvidia GPU 1650TI， CUDA 12.0 CUDNN 8.2\r\n- 【编译语言】： Python(3.9）\r\n\r\n- 【模型跑不通】\r\n- - 执行`examples\\vision\\faceid\\adaface\\python`下的部署示例，包括使用examples提供的模型，不能正确执行\r\n  File \"D:\\test_my\\FastDeploy-develop\\examples\\vision\\faceid\\adaface\\python\\infer.py\", line 71, in <module>\r\n    model = fd.vision.faceid.AdaFace(\r\n  File \"E:\\Anaconda\\envs\\paddle25_1282\\lib\\site-packages\\fastdeploy\\vision\\faceid\\contrib\\adaface\\__init__.py\", line 72, in __init__\r\n    self._model = C.vision.faceid.AdaFace(\r\nRuntimeError: No graph was found in the protobuf.\r\n",
        "state": "open",
        "user": "wjjook",
        "closed_by": null,
        "created_at": "2024-03-04T07:09:50+00:00",
        "updated_at": "2024-03-07T07:28:55+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "wjjook"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2393,
        "title": "怎么主动释放已经加载的模型",
        "body": "**问一个小白问题, 为什么所有模型都只有加载的接口, 没有释放的接口.**\r\n\r\n**如果有释放的接口的话请说一下在哪里, 谢谢**",
        "state": "open",
        "user": "ThinkWD",
        "closed_by": null,
        "created_at": "2024-03-04T09:43:16+00:00",
        "updated_at": "2024-10-17T03:06:25+00:00",
        "closed_at": null,
        "comments_count": [
            "ThinkWD",
            "ThinkWD",
            "ThinkWD",
            "rainyfly",
            "ThinkWD",
            "ThinkWD",
            "ThinkWD",
            "Algabeno",
            "ThinkWD",
            "sanersbug",
            "Algabeno"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2394,
        "title": "yaml文件加载失败（Failed to load yaml file , maybe you should check this file.）",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： develop分支\r\n- 【编译命令】```cmake .. -G \"Visual Studio 17 2022\" -A x64 -DENABLE_ORT_BACKEND=ON -DENABLE_TRT_BACKEND=ON -DENABLE_VISION=ON -DWITH_GPU=ON -DTRT_DIRECTORY=\"D:\\Downloads\\TensorRT-8.5.3.1.Windows10.x86_64.cuda-11.8.cudnn8.6\\TensorRT-8.5.3.1\" -DCUDA_DIRECTORY=\"D:\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\" -DCMAKE_INSTALL_PREFIX=\"E:\\cplus\\compiled_fastdeploy\" -DOPENCV_DIRECTORY=\"E:\\cplus\\opencv-win-x64-3.4.16\\build\" -DORT_DIRECTORY=\"E:\\cplus\\onnxruntime-win-x64-gpu-1.12.0\"```\r\n- 【系统平台】: Windows x64(Windows11) \r\n- 【硬件】： Nvidia GPU 3060， CUDA 11.6 CUDNN 8.6\r\n- 【编译语言】： C++ \r\n- 【模型跑不通】\r\n根据 <https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows_build.md>中的**3.3 SDK使用方式三Visual Studio 2019 创建 CMake 工程使用 C++ SDK** 来编译infer_ppyoloe.cpp源文件的，编译也成功了，得到了exe可执行文件：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103496889/6fe31522-c0d6-4ac1-840c-6b566c54d409)\r\n模型权值文件也下载了：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103496889/397123a9-f9b3-4a97-8fbd-79fe82da4667)\r\ninfer_ppyoloe.cpp源文件代码如下，我做了稍微的修改(主要是判断文件是否存在)：\r\n```cpp\r\n#include \"fastdeploy/vision.h\"\r\n\r\n#ifdef WIN32\r\nconst char sep = '\\\\';\r\n#else\r\nconst char sep = '/';\r\n#endif\r\n\r\nvoid CpuInfer(const std::string& model_dir, const std::string& image_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n    auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file,\r\n        config_file);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto im = cv::imread(image_file);\r\n    auto im_bak = im.clone();\r\n\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    std::cout << res.Str() << std::endl;\r\n    auto vis_im = fastdeploy::vision::Visualize::VisDetection(im_bak, res, 0.5);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\nvoid GpuInfer(const std::string& model_dir, const std::string& image_file) {\r\n    auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto im = cv::imread(image_file);\r\n    auto im_bak = im.clone();\r\n\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    std::cout << res.Str() << std::endl;\r\n    auto vis_im = fastdeploy::vision::Visualize::VisDetection(im_bak, res, 0.5);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\nvoid TrtInfer(const std::string& model_dir, const std::string& image_file) {\r\n    /*auto model_file = model_dir + sep + \"model.pdmodel\";\r\n    auto params_file = model_dir + sep + \"model.pdiparams\";\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";*/\r\n    const std::string model_file = \"E:/cplus/fastdeploy_test/infer_ppyoloe/out/build/x64-debug/Debug/ppyoloe_crn_l_300e_coco/model.pdmodel\";\r\n    const std::string params_file = \"E:/cplus/fastdeploy_test/infer_ppyoloe/out/build/x64-debug/Debug/ppyoloe_crn_l_300e_coco/model.pdiparams\";\r\n    const std::string config_file = \"E:/cplus/fastdeploy_test/infer_ppyoloe/out/build/x64-debug/Debug/ppyoloe_crn_l_300e_coco/infer_cfg.yml\";\r\n\r\n    std::ifstream fin(config_file);\r\n    if (!fin) {\r\n        std::cout << config_file + \" no existed.\" << std::endl;\r\n    }\r\n    else {\r\n        std::cout << config_file + \" existed.\" << std::endl;\r\n    }\r\n\r\n    auto option = fastdeploy::RuntimeOption();\r\n    option.UseGpu();\r\n    option.UseTrtBackend();\r\n    auto model = fastdeploy::vision::detection::PPYOLOE(model_file, params_file,\r\n        config_file, option);\r\n    if (!model.Initialized()) {\r\n        std::cerr << \"Failed to initialize.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto im = cv::imread(image_file);\r\n    auto im_bak = im.clone();\r\n\r\n    fastdeploy::vision::DetectionResult res;\r\n    if (!model.Predict(&im, &res)) {\r\n        std::cerr << \"Failed to predict.\" << std::endl;\r\n        return;\r\n    }\r\n\r\n    std::cout << res.Str() << std::endl;\r\n    auto vis_im = fastdeploy::vision::Visualize::VisDetection(im_bak, res, 0.5);\r\n    cv::imwrite(\"vis_result.jpg\", vis_im);\r\n    std::cout << \"Visualized result saved in ./vis_result.jpg\" << std::endl;\r\n}\r\n\r\nstatic void check_cfg(const std::string& model_dir, const std::string& image_file) {\r\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\r\n    std::ifstream fin(config_file);\r\n    if (!fin) {\r\n        std::cout << config_file + \" no existed.\" << std::endl;\r\n    }\r\n    else {\r\n        std::cout << config_file + \" existed.\" << std::endl;\r\n    }\r\n\r\n    std::ifstream fin2(image_file);\r\n    if (!fin2) {\r\n        std::cout << image_file + \" no existed.\" << std::endl;\r\n    }\r\n    else {\r\n        std::cout << image_file + \" existed.\" << std::endl;\r\n    }\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n    if (argc < 4) {\r\n        std::cout\r\n            << \"Usage: infer_demo path/to/model_dir path/to/image run_option, \"\r\n            \"e.g ./infer_model ./ppyoloe_model_dir ./test.jpeg 0\"\r\n            << std::endl;\r\n        std::cout << \"The data type of run_option is int, 0: run with cpu; 1: run \"\r\n            \"with gpu; 2: run with gpu and use tensorrt backend.\"\r\n            << std::endl;\r\n        return -1;\r\n    }\r\n\r\n    check_cfg(argv[1], argv[2]);\r\n\r\n    if (std::atoi(argv[3]) == 0) {\r\n        CpuInfer(argv[1], argv[2]);\r\n    }\r\n    else if (std::atoi(argv[3]) == 1) {\r\n        GpuInfer(argv[1], argv[2]);\r\n    }\r\n    else if (std::atoi(argv[3]) == 2) {\r\n        TrtInfer(argv[1], argv[2]);\r\n    }\r\n    return 0;\r\n}\r\n```\r\n然后运行exe，报**Failed to load yaml file , maybe you should check this file.**错误，运行命令和结果如下：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103496889/baeba594-0bec-4fb0-9726-0640dc0d4135)\r\n\r\n我明明判断了yaml文件是否存在，显示结果是存在的，但fastdeploySDK说不存在，我也看了fastdeploy的源代码\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103496889/5aa9670b-8f69-4a7d-bc78-df537bcf4e31)\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/103496889/eec1ca21-bef2-4ea0-a485-adb3e7abaf04)\r\n触发异常的就是上的代码，但是我明明自己判断文件是存在的，但为什么fastdeploySDK判断文件却不存在。这到底是为啥？\r\n请求大佬指教！\r\n",
        "state": "open",
        "user": "flinzhao",
        "closed_by": null,
        "created_at": "2024-03-04T13:44:16+00:00",
        "updated_at": "2024-05-21T08:06:27+00:00",
        "closed_at": null,
        "comments_count": [
            "jiangjiajun",
            "flinzhao",
            "ChaoII",
            "jiangjiajun",
            "smalie2222",
            "flinzhao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2395,
        "title": "PPyoloe+在FastDeploy服务端部署的http请求报错的问题",
        "body": "- 【FastDeploy版本】： 1.0.7\r\n- 【系统平台】: Linux x64\r\n- 【硬件】： Nvidia T4\r\n\r\n在服务端部署ppyoloe+模型时，遇到http请求报错400的情况。client代码如下：\r\n```python\r\nimport numpy as np\r\nimport requests\r\nimport json\r\nimport cv2\r\nimport base64\r\nimport os\r\n\r\n\r\ndef cv2_to_base64(image):\r\n    return base64.b64encode(image).decode('utf8')\r\n\r\nimg_name = \"0YRDJ012306112017690_22.jpg\"\r\n\r\nurl = \"http://192.168.10.179:8000/v2/models/gs_model/version/1/infer\"\r\n\r\nurl_meta = 'http://localhost:8000/v2/models/gs_model/versions/1'\r\nresponse = requests.get(url_meta)\r\nresponse = response.json()\r\nprint(response)\r\n\r\nimage = cv2.imread(img_name)\r\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)[None]\r\n\r\npayload = {\r\n  \"inputs\" : [\r\n    {\r\n      \"name\" : \"INPUT\",\r\n      \"shape\" : image.shape,\r\n      \"datatype\" : \"UINT8\",\r\n      \"data\" : image.tolist()\r\n    }\r\n  ],\r\n  \"outputs\" : [\r\n    {\r\n      \"name\" : \"DET_RESULT\"\r\n    }\r\n  ]\r\n}\r\ndata = json.dumps(payload)\r\nresponse = requests.post(url, data=data)\r\nresponse = response.json()\r\nprint(response)\r\n\r\n```\r\n请求metadata返回结果正常：\r\n```\r\n{'name': 'gs_model', 'versions': ['1'], 'platform': 'ensemble', 'inputs': [{'name': 'INPUT', 'datatype': 'UINT8', 'shape': [-1, -1, -1, 3]}], 'outputs': [{'name': 'DET_RESULT', 'datatype': 'BYTES', 'shape': [-1]}]}\r\n```\r\n另外使用示例代码的gRPC请求，返回结果“正常”（可以跑通从图片发出到接受到结果，但结果本身任然异常，这个我在另外一个issue中有提到https://github.com/PaddlePaddle/FastDeploy/issues/2385#issuecomment-1978347512 ）:\r\n```json\r\ntm: name: \"INPUT\"\r\ndatatype: \"UINT8\"\r\nshape: -1\r\nshape: -1\r\nshape: -1\r\nshape: 3\r\n\r\noutput_name: DET_RESULT\r\n{'boxes': [[593.3242797851562, 1102.401611328125, 821.515380859375, 1180.915771484375], \r\n[574.4947509765625, 591.4415283203125, 830.2587280273438, 694.104248046875], \r\n[575.1363525390625, 597.9071655273438, 840.8336181640625, 1145.27587890625], \r\n[-11.738316535949707, 701.3428344726562, 33.810340881347656, 1045.60791015625], \r\n[898.4482421875, 1101.2528076171875, 1129.6123046875, 1181.39013671875], \r\n[567.0745239257812, 588.35302734375, 724.93798828125, 677.7369384765625], [\r\n884.1670532226562, 437.82037353515625, 1118.99462890625, 594.2349243164062],  ...此处省略若干bbox, \r\n'label_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12], 'masks': [], 'contain_masks': False} \r\n```\r\n\r\n",
        "state": "closed",
        "user": "hxuaj",
        "closed_by": "rainyfly",
        "created_at": "2024-03-05T10:16:07+00:00",
        "updated_at": "2024-03-07T06:32:53+00:00",
        "closed_at": "2024-03-07T06:32:53+00:00",
        "comments_count": [
            "hxuaj",
            "hxuaj"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2396,
        "title": "在使用fastdeploy时如果不使用root权限就会报错，线上机器没有root如何解决",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\r\n【系统平台】: Linux x64(Ubuntu 18.04)\r\n【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n【编译语言】： Python3.8\r\n使用时给代码添加了权限 还是会报错，显示没有权限\r\n![deeb3c8ee815f12fa9536f90e78a87f5](https://github.com/PaddlePaddle/FastDeploy/assets/137677731/a5c75353-9814-4516-9059-2b8636cdd8bf)\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "closed",
        "user": "menginbaidu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-05T11:27:35+00:00",
        "updated_at": "2025-03-25T06:45:39+00:00",
        "closed_at": "2025-03-25T06:45:38+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2400,
        "title": "示例文档中部署ppyoloe在server端返回结果异常",
        "body": "-  镜像版本registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\r\n-【系统平台】: Linux x64\r\n-【硬件】： Nvidia T4, Driver Version: 545.23.06\r\n\r\n在跑示例文档https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/serving/README_CN.md 部署ppyoloe在服务端时，遇到了返回结果异常的情况（从结果里看似乎是没有nms或者过滤低score的bbox）。\r\n```\r\ntm: name: \"INPUT\"\r\ndatatype: \"UINT8\"\r\nshape: -1\r\nshape: -1\r\nshape: -1\r\nshape: 3\r\n\r\noutput_name: DET_RESULT\r\n{'boxes': [[415.0473327636719, 89.31150817871094, 506.0096130371094, 283.86309814453125], [163.6656951904297, 81.9148941040039, 198.58534240722656, 166.76087951660156], [581.7886352539062, 113.02760314941406, 612.62353515625, 198.5216827392578], [267.21722412109375, 89.77732849121094, 298.7960510253906, 169.3614959716797], [104.46560668945312, 45.4824104309082, 127.68883514404297, 93.53387451171875], [348.90216064453125, 44.059322357177734, 367.54168701171875, 98.40352630615234], [363.8897399902344, 58.385528564453125, 381.3974914550781, 114.65235900878906], [504.84381103515625, 114.53160095214844, 612.2809448242188, 271.2926025390625], [328.225830078125, 38.96531295776367, 347.2108459472656, 80.57064819335938], [379.1754455566406, 40.791236877441406, 394.87908935546875, 83.98038482666016], [25.863767623901367, 117.8653793334961, 61.25945281982422, 153.15689086914062], [279.3775634765625, 80.79803466796875, 296.59808349609375, 109.84254455566406], [464.14312744140625, 15.278188705444336, 472.0542907714844, 33.781959533691406], [169.602783203125, 47.34526062011719, 178.55377197265625, 61.11117172241211], [504.60418701171875, 113.7492446899414, 571.9451904296875, 273.1382141113281], [186.65345764160156, 44.978328704833984, 200.0539093017578, 61.18734359741211], [278.9233093261719, 80.30986785888672, 297.70343017578125, 166.29754638671875], [239.76409912109375, 96.78807830810547, 253.03810119628906, 118.55729675292969], [154.4526824951172, 122.37461853027344, 174.47116088867188, 163.78829956054688], [479.463623046875, 22.919111251831055, 485.5457458496094, 33.30751419067383], [410.3473815917969, 22.929636001586914, 418.0113220214844, 33.90645980834961], [398.67620849609375, 24.442699432373047, 405.7421569824219, 33.88005065917969], [279.5631103515625, 80.63456726074219, 298.11578369140625, 140.30868530273438], [64.93060302734375, 135.47802734375, 85.6537094116211, 154.052978515625], [0.786351203918457, 126.58541107177734, 40.16394805908203, 155.860595703125], [504.5357360839844, 158.92918395996094, 619.2892456054688, 271.0347595214844], [353.5274353027344, 44.34095764160156, 365.78240966796875, 93.13836669921875], [238.58297729492188, 90.62461853027344, 276.26226806640625, 165.94500732421875], [0.740142822265625, 122.25225067138672, 51.19392013549805, 154.63861083984375], [416.4222412109375, 190.34901428222656, 481.7032165527344, 284.4598388671875], [525.2088623046875, 19.898366928100586, 534.68603515625, 33.50472640991211], [504.77154541015625, 113.66645812988281, 555.013916015625, 177.9217529296875], [0.15939617156982422, 126.7978515625, 37.95818328857422, 170.1544647216797], [153.82296752929688, 123.7216567993164, 183.4593963623047, 164.6437225341797], [0.23960399627685547, 126.76113891601562, 8.49870491027832, 171.0194091796875], [239.02842712402344, 95.5167465209961, 258.7713928222656, 128.20510864257812], [536.0523071289062, 22.960216522216797, 544.8829956054688, 33.861820220947266], [195.5425262451172, 124.2804183959961, 212.7749481201172, 155.08444213867188], [239.32289123535156, 94.55370330810547, 264.38543701171875, 153.7139434814453], [57.42528533935547, 152.53089904785156, 104.53608703613281, 173.46681213378906], [0.5161876678466797, 36.44532012939453, 5.782716751098633, 43.749813079833984], [363.78173828125, 58.27681350708008, 377.4848327636719, 90.18199920654297], [217.50582885742188, 83.51508331298828, 252.95803833007812, 118.43180847167969], [279.4331970214844, 80.42427062988281, 302.5350036621094, 151.67153930664062], [584.48828125, 26.752487182617188, 597.4041137695312, 33.81547164916992], [197.61282348632812, 81.76798248291016, 253.2484588623047, 157.53155517578125], [4.529094219207764, 134.43199157714844, 38.82701873779297, 154.2007293701172], [349.8576354980469, 55.54695510864258, 383.3367004394531, 112.15912628173828], [363.1408996582031, 57.4027214050293, 392.0172119140625, 115.85466003417969], [169.24462890625, 46.82175827026367, 181.5938720703125, 61.542266845703125], [-0.1238718032836914, 126.4898681640625, 8.003314971923828, 155.27789306640625], [99.50037384033203, 151.31504821777344, 118.58633422851562, 169.17198181152344], [25.359241485595703, 117.5168228149414, 49.0174560546875, 137.96414184570312], [204.18983459472656, 83.05860137939453, 253.259765625, 120.18955993652344], [168.95484924316406, 47.00589370727539, 178.7480010986328, 73.5147705078125], [273.036376953125, 88.34422302246094, 296.83282470703125, 109.83258819580078], [401.0738220214844, 23.157913208007812, 412.32110595703125, 33.82148742675781], [7.888867378234863, 117.32171630859375, 57.85581970214844, 155.16140747070312], [505.08026123046875, 113.92462921142578, 555.4822998046875, 235.09524536132812], [239.62301635742188, 95.9066390991211, 263.9088439941406, 134.457275390625], [57.121646881103516, 153.02540588378906, 86.1454086303711, 173.2971954345703], [272.8516845703125, 88.57711029052734, 296.7515563964844, 129.9718017578125], [415.3750305175781, 89.75614929199219, 506.9835510253906, 194.9457550048828], [267.9065246582031, 90.82560729980469, 296.7994384765625, 140.1407928466797], [423.9007263183594, 81.24815368652344, 603.3036499023438, 276.6430358886719], [588.5612182617188, 173.52505493164062, 604.80810546875, 197.98690795898438], [584.0293579101562, 113.26741790771484, 612.484375, 177.51014709472656], [164.07598876953125, 124.59485626220703, 191.23179626464844, 165.71856689453125], [105.03042602539062, 45.5959358215332, 127.6837387084961, 74.20264434814453], [169.08651733398438, 47.23845291137695, 178.61878967285156, 65.7249984741211], [535.14111328125, 16.430753707885742, 568.654296875, 34.355892181396484], [64.82987213134766, 135.4317169189453, 97.76400756835938, 157.451171875], [410.95367431640625, 10.481751441955566, 419.670654296875, 28.81130027770996], [548.3133544921875, 112.7360610961914, 614.0767211914062, 218.4713897705078], [75.31558990478516, 122.07091522216797, 101.8942642211914, 155.04481506347656], [416.4981384277344, 102.90975189208984, 485.0086975097656, 284.864501953125], [481.39300537109375, 22.826316833496094, 489.5166931152344, 32.84419250488281], [367.9981689453125, 68.89327239990234, 381.3599548339844, 114.69693756103516], [579.1638793945312, 112.252685546875, 625.4959106445312, 213.7901611328125], [-0.1423358917236328, 122.49160766601562, 25.421987533569336, 170.0618438720703], [556.9065551757812, 27.980899810791016, 568.57568359375, 33.81419372558594], [555.6651000976562, 112.73583221435547, 612.1076049804688, 247.35133361816406], [195.66465759277344, 96.38968658447266, 221.67654418945312, 156.46803283691406], [194.24136352539062, 82.36378479003906, 240.91346740722656, 154.21315002441406], [164.0039520263672, 82.16191864013672, 197.33901977539062, 126.35895538330078], [332.4733581542969, 41.394317626953125, 352.50146484375, 82.90718841552734], [59.18156051635742, 135.38754272460938, 86.11772155761719, 171.28651428222656], [411.5816650390625, 10.556546211242676, 420.6654357910156, 25.59926414489746], [255.8538360595703, 80.89381408691406, 298.96142578125, 163.166259765625], [263.8378601074219, 142.13685607910156, 274.08221435546875, 162.6181640625], [154.25360107421875, 116.09152221679688, 197.474609375, 165.05703735351562], [85.30513763427734, 156.26316833496094, 103.57791137695312, 172.71054077148438], [575.5387573242188, 173.59588623046875, 604.4942626953125, 197.8204345703125], [161.67410278320312, 83.49578857421875, 210.1561279296875, 165.02163696289062], [462.35528564453125, 161.00872802734375, 469.1642150878906, 173.6923065185547], [464.6841735839844, 14.842952728271484, 477.8023681640625, 33.94887161254883], [374.2080993652344, 24.42507553100586, 384.7043151855469, 33.53703308105469], [148.78543090820312, 81.63707733154297, 198.39540100097656, 166.12245178222656], [163.43331909179688, 81.5866928100586, 235.91885375976562, 166.3901824951172], [235.7521514892578, 88.68202209472656, 253.46844482421875, 119.18019104003906], [215.6936798095703, 81.81437683105469, 232.10289001464844, 111.99567413330078], [379.4805908203125, 40.56340408325195, 396.8534240722656, 74.28878784179688], [525.5950927734375, 21.393497467041016, 534.8746948242188, 33.76261520385742], [536.4329223632812, 23.241092681884766, 546.6116943359375, 33.95100402832031], [583.0391235351562, 26.307029724121094, 603.0292358398438, 33.98405456542969], [543.4886474609375, 16.19952392578125, 568.9390258789062, 29.978548049926758], [539.5552978515625, 16.178848266601562, 568.8953857421875, 34.20614242553711], [621.5250244140625, 5.0936384201049805, 638.3634643554688, 11.090214729309082], [570.5025634765625, 26.96520233154297, 602.3051147460938, 34.15656280517578], [556.9065551757812, 27.980899810791016, 568.57568359375, 33.81419372558594], [583.7063598632812, 26.870195388793945, 603.4033813476562, 34.10969161987305], [603.887939453125, 5.724186420440674, 623.5446166992188, 11.43508529663086], [536.0523071289062, 22.960216522216797, 544.8829956054688, 33.861820220947266], [531.5512084960938, 18.56842803955078, 569.04052734375, 33.82920837402344], [585.2420043945312, 27.155885696411133, 598.4056396484375, 33.9611701965332], [410.3473815917969, 22.929636001586914, 418.0113220214844, 33.90645980834961], [525.2088623046875, 19.898366928100586, 534.68603515625, 33.50472640991211], [398.67620849609375, 24.442699432373047, 405.7421569824219, 33.88005065917969], [485.992919921875, 25.199966430664062, 497.2183532714844, 32.6937141418457], [0.284501314163208, 15.109296798706055, 14.804917335510254, 24.112165451049805], [0.15656375885009766, 14.728020668029785, 18.1372013092041, 25.94812774658203], [599.2529907226562, 5.439635276794434, 628.6209716796875, 11.748708724975586], [541.2657470703125, 18.503772735595703, 603.1679077148438, 34.099021911621094], [580.8844604492188, 4.908934593200684, 629.9612426757812, 12.080033302307129], [478.3133239746094, 14.035962104797363, 564.8131103515625, 33.063941955566406], [572.3507690429688, 24.321306228637695, 603.5548706054688, 34.558815002441406], [490.16888427734375, 8.107921600341797, 574.7966918945312, 32.964786529541016], [543.4024658203125, 10.552515029907227, 569.29248046875, 30.699399948120117], [390.0893249511719, 25.00470542907715, 405.44085693359375, 33.68592834472656], [552.7640380859375, 23.427576065063477, 603.1658325195312, 34.204994201660156], [0.2669506072998047, 36.28215408325195, 5.678282737731934, 43.794429779052734], [489.0831604003906, 23.682008743286133, 528.8335571289062, 32.722530364990234], [401.0738220214844, 23.157913208007812, 412.32110595703125, 33.82148742675781], [378.70416259765625, 22.071691513061523, 420.2750244140625, 33.869503021240234], [536.4329223632812, 23.241092681884766, 546.6116943359375, 33.95100402832031], [525.5950927734375, 21.393497467041016, 534.8746948242188, 33.76261520385742], [539.5552978515625, 16.178848266601562, 568.8953857421875, 34.20614242553711], [583.7063598632812, 26.870195388793945, 603.4033813476562, 34.10969161987305], [585.2420043945312, 27.155885696411133, 598.4056396484375, 33.9611701965332], [542.8676147460938, 16.212120056152344, 569.0834350585938, 33.8110466003418], [542.8676147460938, 16.212120056152344, 569.0834350585938, 33.8110466003418], [531.5512084960938, 18.56842803955078, 569.04052734375, 33.82920837402344], [542.4485473632812, 15.827122688293457, 598.1632690429688, 34.231239318847656], [507.1815185546875, 9.61510944366455, 570.3367919921875, 33.46634292602539], [474.8154296875, 6.031200885772705, 572.2814331054688, 33.07912063598633], [570.1296997070312, 27.087892532348633, 600.3795166015625, 33.981056213378906], [556.9065551757812, 27.980899810791016, 568.57568359375, 33.81419372558594], [75.12708282470703, 121.36734771728516, 116.81596374511719, 156.64682006835938], [55.30698776245117, 22.233932495117188, 68.44763946533203, 37.274078369140625], [97.03081512451172, 130.86129760742188, 117.76364135742188, 156.94406127929688], [303.3809509277344, 165.1072998046875, 312.6393737792969, 172.17173767089844], [154.4526824951172, 122.37461853027344, 174.47116088867188, 163.78829956054688], [57.2379150390625, 152.62599182128906, 102.29946899414062, 173.52041625976562], [3.3018360137939453, 150.40457153320312, 38.63814926147461, 172.5203094482422], [57.155303955078125, 152.9988555908203, 87.91175842285156, 173.32557678222656], [64.91835021972656, 135.13156127929688, 85.53469848632812, 153.85238647460938], [57.37481689453125, 152.62535095214844, 101.6075668334961, 173.33860778808594], [100.38810729980469, 152.91757202148438, 119.0905532836914, 168.50799560546875], [3.995237350463867, 135.07254028320312, 38.83803176879883, 154.15782165527344], [85.44366455078125, 156.39337158203125, 103.1622314453125, 172.9759063720703], [1.6192207336425781, 133.44195556640625, 39.62739562988281, 172.09791564941406], [59.18156051635742, 135.38754272460938, 86.11772155761719, 171.28651428222656], [64.82987213134766, 135.4317169189453, 97.76400756835938, 157.451171875], [65.74726867675781, 125.93417358398438, 100.70172882080078, 156.1581573486328], [153.7079620361328, 123.6197280883789, 175.61990356445312, 163.8985137939453], [80.65171813964844, 149.82205200195312, 97.91513061523438, 158.17860412597656], [195.5425262451172, 124.2804183959961, 212.7749481201172, 155.08444213867188], [75.31558990478516, 122.07091522216797, 101.8942642211914, 155.04481506347656], [0.15852975845336914, 127.18214416503906, 9.51118278503418, 171.86131286621094], [1.2652606964111328, 127.315673828125, 39.719757080078125, 153.9408416748047], [96.0339584350586, 132.0460968017578, 118.24811553955078, 158.2288818359375], [575.5387573242188, 173.59588623046875, 604.4942626953125, 197.8204345703125], [279.2477722167969, 80.35539245605469, 297.3070373535156, 109.48333740234375], [13.37531852722168, 118.70643615722656, 60.97172164916992, 153.6507110595703], [25.334667205810547, 118.14696502685547, 59.77709197998047, 150.8802947998047], [0.7877998352050781, 122.44306945800781, 57.55549621582031, 154.12442016601562], [25.344079971313477, 118.12852478027344, 55.271846771240234, 137.6947479248047], [56.031856536865234, 137.55987548828125, 67.42314147949219, 152.74148559570312], [75.03667449951172, 121.65198516845703, 99.5113296508789, 137.90065002441406], [272.8516845703125, 88.57711029052734, 296.7515563964844, 129.9718017578125], [428.25299072265625, 171.7301025390625, 621.9200439453125, 194.07188415527344], [467.5150146484375, 175.83612060546875, 625.7739868164062, 193.64454650878906], [543.4886474609375, 16.19952392578125, 568.9390258789062, 29.978548049926758], [57.33161163330078, 152.28382873535156, 102.84725189208984, 173.32022094726562], [567.9774169921875, 176.03790283203125, 621.5348510742188, 194.01480102539062], [3.2057266235351562, 150.32859802246094, 38.65480041503906, 172.41697692871094], [100.36488342285156, 152.9250946044922, 118.95122528076172, 168.4674835205078], [3.701953887939453, 134.95574951171875, 39.064964294433594, 153.99240112304688], [57.3607177734375, 153.0872802734375, 86.99315643310547, 173.26087951660156], [85.61756134033203, 156.4393768310547, 103.39086151123047, 172.76541137695312], [64.91835021972656, 135.13156127929688, 85.53469848632812, 153.85238647460938], [57.5815544128418, 152.27340698242188, 103.55062866210938, 173.41409301757812], [153.7079620361328, 123.6197280883789, 175.61990356445312, 163.8985137939453], [1.6192207336425781, 133.44195556640625, 39.62739562988281, 172.09791564941406], [74.8982162475586, 121.73531341552734, 100.66050720214844, 153.19483947753906], [80.65171813964844, 149.82205200195312, 97.91513061523438, 158.17860412597656], [64.82987213134766, 135.4317169189453, 97.76400756835938, 157.451171875], [195.5425262451172, 124.2804183959961, 212.7749481201172, 155.08444213867188], [96.0339584350586, 132.0460968017578, 118.24811553955078, 158.2288818359375], [263.8378601074219, 142.13685607910156, 274.08221435546875, 162.6181640625], [65.74726867675781, 125.93417358398438, 100.70172882080078, 156.1581573486328], [575.5387573242188, 173.59588623046875, 604.4942626953125, 197.8204345703125], [59.18156051635742, 135.38754272460938, 86.11772155761719, 171.28651428222656], [0.15852975845336914, 127.18214416503906, 9.51118278503418, 171.86131286621094], [1.2652606964111328, 127.315673828125, 39.719757080078125, 153.9408416748047], [588.5612182617188, 173.52505493164062, 604.80810546875, 197.98690795898438], [56.031856536865234, 137.55987548828125, 67.42314147949219, 152.74148559570312], [75.03667449951172, 121.65198516845703, 99.5113296508789, 137.90065002441406], [2.867422103881836, 150.33497619628906, 38.6275634765625, 172.4726104736328], [3.995237350463867, 135.07254028320312, 38.83803176879883, 154.15782165527344], [100.61165618896484, 152.99896240234375, 119.14982604980469, 168.56565856933594], [57.3607177734375, 153.0872802734375, 86.99315643310547, 173.26087951660156], [57.37481689453125, 152.62535095214844, 101.6075668334961, 173.33860778808594], [64.93060302734375, 135.47802734375, 85.6537094116211, 154.052978515625], [1.6192207336425781, 133.44195556640625, 39.62739562988281, 172.09791564941406], [85.61756134033203, 156.4393768310547, 103.39086151123047, 172.76541137695312], [75.01507568359375, 121.79051208496094, 102.1259765625, 154.90904235839844], [508.3108825683594, 273.7613525390625, 520.3276977539062, 282.66046142578125], [303.3809509277344, 165.1072998046875, 312.6393737792969, 172.17173767089844], [428.25299072265625, 171.7301025390625, 621.9200439453125, 194.07188415527344], [508.3108825683594, 273.7613525390625, 520.3276977539062, 282.66046142578125], [303.3809509277344, 165.1072998046875, 312.6393737792969, 172.17173767089844], [57.316429138183594, 153.08230590820312, 87.27027893066406, 173.30284118652344], [325.30987548828125, 168.3092041015625, 332.55853271484375, 172.6632080078125], [239.77330017089844, 96.85023498535156, 253.08148193359375, 118.15531921386719], [162.664306640625, 83.0866928100586, 626.038818359375, 344.3745422363281], [428.25299072265625, 171.7301025390625, 621.9200439453125, 194.07188415527344], [280.5286865234375, 166.3562774658203, 620.8729248046875, 196.15675354003906], [467.5150146484375, 175.83612060546875, 625.7739868164062, 193.64454650878906], [578.5392456054688, 174.22592163085938, 620.5628662109375, 196.18894958496094], [413.2127990722656, 89.3641586303711, 504.73431396484375, 285.7040710449219], [198.6156005859375, 82.13031005859375, 253.29318237304688, 154.3547821044922], [505.1615905761719, 144.55958557128906, 619.9993896484375, 271.35223388671875], [496.8830871582031, 150.49493408203125, 620.6256713867188, 243.82904052734375], [303.3809509277344, 165.1072998046875, 312.6393737792969, 172.17173767089844], [325.30987548828125, 168.3092041015625, 332.55853271484375, 172.6632080078125], [556.1092529296875, 174.62442016601562, 621.22021484375, 194.7135772705078], [568.2611694335938, 174.31715393066406, 620.9566040039062, 190.2695770263672], [162.91616821289062, 89.9115982055664, 313.25054931640625, 339.2209777832031], [309.51019287109375, 165.91964721679688, 332.6920471191406, 172.8731689453125], [397.8562316894531, 167.2297821044922, 621.8546752929688, 196.08470153808594], [462.35528564453125, 161.00872802734375, 469.1642150878906, 173.6923065185547], [214.46356201171875, 83.74333190917969, 253.50112915039062, 119.47327423095703], [206.93936157226562, 81.50543975830078, 253.0297393798828, 128.26141357421875], [508.2390441894531, 273.49432373046875, 520.796142578125, 283.7806396484375], [239.5054931640625, 96.62606811523438, 253.23989868164062, 117.77973175048828], [196.41265869140625, 82.59197998046875, 266.7341613769531, 167.2433624267578], [196.31582641601562, 157.23931884765625, 617.1111450195312, 281.3926086425781], [162.37554931640625, 87.99073791503906, 376.2151794433594, 344.1787109375], [603.3372802734375, 174.2715301513672, 620.865478515625, 182.57684326171875], [358.2396545410156, 166.86898803710938, 538.33984375, 193.40248107910156], [597.4798583984375, 174.28016662597656, 620.765625, 193.84425354003906], [302.42791748046875, 163.75335693359375, 332.7991027832031, 172.88201904296875], [214.77488708496094, 82.15776824951172, 238.85952758789062, 118.74676513671875], [162.3896942138672, 83.24432373046875, 479.7991943359375, 344.2940979003906], [423.9007263183594, 81.24815368652344, 603.3036499023438, 276.6430358886719], [341.1839599609375, 31.56324577331543, 620.8245239257812, 277.17059326171875], [195.0897674560547, 91.05286407470703, 273.1888427734375, 190.49461364746094], [215.72523498535156, 82.23905944824219, 231.90724182128906, 113.28080749511719], [176.26893615722656, 83.75300598144531, 289.9616394042969, 191.69102478027344], [191.8516387939453, 101.30970764160156, 294.5722961425781, 189.14883422851562], [161.67518615722656, 84.08781433105469, 479.53826904296875, 263.02728271484375], [583.7063598632812, 26.870195388793945, 603.4033813476562, 34.10969161987305], [328.28802490234375, 156.73843383789062, 522.84814453125, 248.9593963623047], [162.72613525390625, 87.22933959960938, 312.1281433105469, 267.5166015625], [428.25299072265625, 171.7301025390625, 621.9200439453125, 194.07188415527344], [428.25299072265625, 171.7301025390625, 621.9200439453125, 194.07188415527344], [508.3108825683594, 273.7613525390625, 520.3276977539062, 282.66046142578125], [508.3108825683594, 273.7613525390625, 520.3276977539062, 282.66046142578125], [75.27662658691406, 121.6022720336914, 116.64280700683594, 156.69175720214844], [97.03081512451172, 130.86129760742188, 117.76364135742188, 156.94406127929688], [75.07342529296875, 121.65553283691406, 102.75826263427734, 155.1343536376953], [153.3013916015625, 123.23374938964844, 177.13055419921875, 164.558349609375], [75.03667449951172, 121.65198516845703, 99.5113296508789, 137.90065002441406], [3.8897933959960938, 134.78652954101562, 41.27455139160156, 154.36416625976562], [65.74726867675781, 125.93417358398438, 100.70172882080078, 156.1581573486328], [65.15660858154297, 135.39845275878906, 85.76416015625, 154.011962890625], [99.59290313720703, 151.08938598632812, 118.30513000488281, 168.4336395263672], [26.025135040283203, 117.55677795410156, 60.54419708251953, 153.05181884765625], [3.014322280883789, 150.19363403320312, 38.83381652832031, 172.21328735351562], [0.7877998352050781, 122.44306945800781, 57.55549621582031, 154.12442016601562], [195.82814025878906, 124.68016052246094, 212.9248046875, 157.302734375], [152.92080688476562, 122.9360122680664, 191.67984008789062, 165.82211303710938], [1.2652606964111328, 127.315673828125, 39.719757080078125, 153.9408416748047], [158.6168212890625, 83.91439056396484, 198.076171875, 165.9413299560547], [578.5392456054688, 174.22592163085938, 620.5628662109375, 196.18894958496094], [13.37531852722168, 118.70643615722656, 60.97172164916992, 153.6507110595703], [164.07598876953125, 124.59485626220703, 191.23179626464844, 165.71856689453125], [239.5054931640625, 96.62606811523438, 253.23989868164062, 117.77973175048828], [263.7712097167969, 142.0758056640625, 274.40960693359375, 162.3542938232422], [1.6192207336425781, 133.44195556640625, 39.62739562988281, 172.09791564941406], [11.675285339355469, 114.3095932006836, 134.0594024658203, 163.92750549316406], [217.50582885742188, 83.51508331298828, 252.95803833007812, 118.43180847167969], [56.73722839355469, 151.35101318359375, 105.21092224121094, 173.88116455078125], [57.121646881103516, 153.02540588378906, 86.1454086303711, 173.2971954345703], [80.65171813964844, 149.82205200195312, 97.91513061523438, 158.17860412597656], [-0.14708852767944336, 126.65233612060547, 8.333593368530273, 170.780517578125], [55.30698776245117, 22.233932495117188, 68.44763946533203, 37.274078369140625], [153.3013916015625, 123.23374938964844, 177.13055419921875, 164.558349609375], [505.8875732421875, 140.91958618164062, 523.167236328125, 151.8753204345703]], \r\n'scores': [0.9504230618476868, 0.896433413028717, 0.8425963521003723, 0.8379513621330261, 0.7733482718467712, \r\n0.7671267986297607, 0.7564674615859985, 0.7141281962394714, 0.6813817620277405, 0.657216489315033, \r\n0.574505627155304, 0.4586893320083618, 0.4316045939922333, 0.3914433717727661, 0.382770836353302, \r\n0.3580609858036041, 0.28445056080818176, 0.2698679566383362, 0.24611839652061462, 0.21509209275245667, \r\n0.204181507229805, 0.19484460353851318, 0.19334666430950165, 0.18015803396701813, 0.16743536293506622, \r\n0.15897995233535767, 0.15554530918598175, 0.14924687147140503, 0.14597395062446594, 0.1453094631433487, \r\n0.14321254193782806, 0.1430281549692154, 0.14108918607234955, 0.13885091245174408, 0.13692179322242737, \r\n0.13145697116851807, 0.13011828064918518, 0.12933190166950226, 0.12562662363052368, 0.12274627387523651, \r\n0.12187842279672623, 0.11965461820363998, 0.11348088085651398, 0.11247064173221588, 0.10909152030944824, \r\n0.10901464521884918, 0.10768859088420868, 0.10430868715047836, 0.10229603946208954, 0.10180152207612991, \r\n0.10088793933391571, 0.09864998608827591, 0.09163130819797516, 0.09111356735229492, 0.0908743366599083, \r\n0.08917637169361115, 0.08785522729158401, 0.08557935804128647, 0.08504784107208252, 0.08497358858585358, \r\n0.08347504585981369, 0.08222542703151703, 0.08163080364465714, 0.08075252920389175, 0.08032017946243286, \r\n0.0793798416852951, 0.07917174696922302, 0.07908519357442856, 0.07893947511911392, 0.07844094932079315, \r\n0.07599174976348877, 0.07445529103279114, 0.07366783171892166, 0.07330697774887085, 0.07236965745687485, \r\n0.07230524718761444, 0.07205937057733536, 0.07166958600282669, 0.07121387124061584, 0.07085146009922028, \r\n0.06871296465396881, 0.06863636523485184, 0.06824524700641632, 0.06811398267745972, 0.06802220642566681, \r\n0.06793830543756485, 0.06782685220241547, 0.06740063428878784, 0.06664621829986572, 0.06660506129264832, \r\n0.06387868523597717, 0.06337273865938187, 0.0628015324473381, 0.06258857250213623, 0.06239909678697586, \r\n0.05987631157040596, 0.05969928950071335, 0.059461746364831924, 0.05939899757504463, 0.059371959418058395, \r\n0.05788147822022438, 0.057543762028217316, 0.08629918843507767, 0.07789531350135803, 0.06289315223693848, \r\n0.2688209116458893, 0.20444510877132416, 0.15873807668685913, 0.1545199155807495, 0.14963024854660034, 0.14170537889003754, 0.1412048488855362, 0.12670932710170746, 0.11597608774900436, 0.11518345028162003, 0.1003512516617775, 0.09286744147539139, 0.08626268059015274, 0.08561965823173523, 0.08482491970062256, 0.08235416561365128, 0.08029723167419434, 0.07917913049459457, 0.07278425991535187, 0.07108897715806961, 0.07005442678928375, 0.06783794611692429, 0.06596836447715759, 0.06325803697109222, 0.06277843564748764, 0.060479309409856796, 0.06027490273118019, 0.059480175375938416, 0.05928025767207146, 0.07808676362037659, 0.07540538907051086, 0.07430747896432877, 0.07193569839000702, 0.06483762711286545, 0.10275454819202423, 0.29064205288887024, 0.08417706191539764, 0.07489804178476334, 0.0697244182229042, 0.06454786658287048, 0.06124381721019745, 0.06107869744300842, 0.0895213708281517, 0.08465585112571716, 0.06466830521821976, 0.06062478572130203, 0.09730815142393112, 0.05790483206510544, 0.4211689829826355, 0.39655646681785583, 0.3541761040687561, 0.3321775197982788, 0.3305021822452545, 0.25672513246536255, 0.24709761142730713, 0.1546103060245514, 0.14012297987937927, 0.12830907106399536, 0.0965692549943924, 0.09451515972614288, 0.08201250433921814, 0.0799773633480072, 0.079134501516819, 0.07461892068386078, 0.07261565327644348, 0.07206861674785614, 0.0672830268740654, 0.06664462387561798, 0.06459251046180725, 0.06458000838756561, 0.05990254133939743, 0.059083424508571625, 0.05861629545688629, 0.05852464959025383, 0.057474855333566666, 0.0715019628405571, 0.06949001550674438, 0.06698913872241974, 0.06279070675373077, 0.06110743060708046, 0.3939814865589142, 0.2858927845954895, 0.24853378534317017, 0.21157626807689667, 0.20773963630199432, 0.19155652821063995, 0.1637057662010193, 0.16218167543411255, 0.1387106031179428, 0.089652881026268, 0.08688339591026306, 0.08130300045013428, 0.07992308586835861, 0.07908309251070023, 0.07849936932325363, 0.0772915706038475, 0.0732429251074791, 0.07175003737211227, 0.06962233036756516, 0.06445857882499695, 0.06383044272661209, 0.06075025349855423, 0.06071286275982857, 0.3357076644897461, 0.21045157313346863, 0.1394304484128952, 0.12997393310070038, 0.12853772938251495, 0.1110009104013443, 0.10603033751249313, 0.09767460823059082, 0.0619550459086895, 0.070225290954113, 0.059668079018592834, 0.05747557058930397, 0.11265211552381516, 0.09124837070703506, 0.05876699462532997, 0.05798572301864624, 0.05725397542119026, 0.963882327079773, 0.2040756642818451, 0.19884897768497467, 0.17713536322116852, 0.14737965166568756, 0.13058440387248993, 0.12522003054618835, 0.12342987209558487, 0.11750625818967819, 0.11224118620157242, 0.10828551650047302, 0.1052493080496788, 0.09532453864812851, 0.08916952461004257, 0.08652466535568237, 0.08642707020044327, 0.08582153916358948, 0.08443481475114822, 0.08296214044094086, 0.08231735974550247, 0.08138302713632584, 0.07960137724876404, 0.07844948023557663, 0.07660599797964096, 0.07411625981330872, 0.07400055229663849, 0.0722406879067421, 0.07118921726942062, 0.06956096738576889, 0.0654514878988266, 0.0645882785320282, 0.0634094774723053, 0.06231240555644035, 0.06196978688240051, 0.06159641593694687, 0.060153793543577194, 0.05948561429977417, 0.05931799113750458, 0.05857465788722038, 0.05809867009520531, 0.06530751287937164, 0.07071242481470108, 0.19873422384262085, 0.073960080742836, 0.6765272617340088, 0.46706095337867737, 0.4648328423500061, 0.15904517471790314, 0.15531767904758453, 0.15004900097846985, 0.14588068425655365, 0.14012248814105988, 0.120736263692379, 0.10348795354366302, 0.0919198989868164, 0.088162362575531, 0.08746687322854996, 0.08712983876466751, 0.08515707403421402, 0.08508922159671783, 0.07716631144285202, 0.07103769481182098, 0.06988956779241562, 0.06846390664577484, 0.06566493213176727, 0.06443235278129578, 0.06195981800556183, 0.05932055413722992, 0.05901051312685013, 0.058313701301813126, 0.05752068758010864, 0.05748004838824272, 0.06946082413196564, 0.06669697165489197, 0.08491163700819016],\r\n'label_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \r\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \r\n0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 5, 7, 7, 7, 7, 7, 7, 7, 13, 13, 13, \r\n14, 16, 16, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, \r\n25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, \r\n29, 30, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, \r\n33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 36, 37, 41, 45, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, \r\n56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 58, 60, 67], 'masks': [], 'contain_masks': False}\r\n\r\n```\r\n附上ppdet使用相同模型参数的推理结果：\r\n```\r\n-----------  Running Arguments -----------\r\naction_file: None\r\nbatch_size: 1\r\ncamera_id: -1\r\ncollect_trt_shape_info: False\r\ncombine_method: nms\r\ncpu_threads: 1\r\ndevice: gpu\r\nenable_mkldnn: False\r\nenable_mkldnn_bfloat16: False\r\nimage_dir: None\r\nimage_file: /home/FastDeploy/examples/vision/detection/paddledetection/serving/000000014439.jpg\r\nmatch_metric: ios\r\nmatch_threshold: 0.6\r\nmodel_dir: /home/FastDeploy/examples/vision/detection/paddledetection/serving/ppyoloe_crn_l_300e_coco\r\noutput_dir: output\r\noverlap_ratio: [0.25, 0.25]\r\nrandom_pad: False\r\nreid_batch_size: 50\r\nreid_model_dir: None\r\nrun_benchmark: False\r\nrun_mode: paddle\r\nsave_images: True\r\nsave_mot_txt_per_img: False\r\nsave_mot_txts: False\r\nsave_results: False\r\nscaled: False\r\nslice_infer: False\r\nslice_size: [640, 640]\r\nthreshold: 0.5\r\ntracker_config: None\r\ntrt_calib_mode: False\r\ntrt_max_shape: 1280\r\ntrt_min_shape: 1\r\ntrt_opt_shape: 640\r\ntuned_trt_shape_file: shape_range_info.pbtxt\r\nuse_coco_category: False\r\nuse_dark: True\r\nuse_fd_format: False\r\nuse_gpu: False\r\nvideo_file: None\r\nwindow_size: 50\r\n------------------------------------------\r\n-----------  Model Configuration -----------\r\nModel Arch: YOLO\r\nTransform Order: \r\n--transform op: Resize\r\n--transform op: NormalizeImage\r\n--transform op: Permute\r\n--------------------------------------------\r\nclass_id:0, confidence:0.9504, left_top:[415.05,89.31],right_bottom:[506.01,283.86]\r\nclass_id:0, confidence:0.8964, left_top:[163.67,81.91],right_bottom:[198.59,166.76]\r\nclass_id:0, confidence:0.8426, left_top:[581.79,113.03],right_bottom:[612.62,198.52]\r\nclass_id:0, confidence:0.8380, left_top:[267.22,89.78],right_bottom:[298.80,169.36]\r\nclass_id:0, confidence:0.7733, left_top:[104.47,45.48],right_bottom:[127.69,93.53]\r\nclass_id:0, confidence:0.7671, left_top:[348.90,44.06],right_bottom:[367.54,98.40]\r\nclass_id:0, confidence:0.7565, left_top:[363.89,58.39],right_bottom:[381.40,114.65]\r\nclass_id:0, confidence:0.7141, left_top:[504.84,114.53],right_bottom:[612.28,271.29]\r\nclass_id:0, confidence:0.6814, left_top:[328.23,38.97],right_bottom:[347.21,80.57]\r\nclass_id:0, confidence:0.6572, left_top:[379.18,40.79],right_bottom:[394.88,83.98]\r\nclass_id:0, confidence:0.5745, left_top:[25.86,117.87],right_bottom:[61.26,153.16]\r\nclass_id:33, confidence:0.9639, left_top:[162.66,83.09],right_bottom:[626.04,344.37]\r\nclass_id:56, confidence:0.6765, left_top:[75.28,121.60],right_bottom:[116.64,156.69]\r\nsave result to: output/000000014439.jpg\r\nTest iter 0\r\n------------------ Inference Time Info ----------------------\r\ntotal_time(ms): 1451.3, img_num: 1\r\naverage latency time(ms): 1451.30, QPS: 0.689037\r\npreprocess_time(ms): 44.30, inference_time(ms): 1407.00, postprocess_time(ms): 0.00\r\n```",
        "state": "closed",
        "user": "hxuaj",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-06T10:35:11+00:00",
        "updated_at": "2025-03-25T06:45:39+00:00",
        "closed_at": "2025-03-25T06:45:39+00:00",
        "comments_count": [
            "hxuaj",
            "jiangjiajun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2397,
        "title": "加载uie模型后是否可以动态修改配置",
        "body": "uie = UIEModel(\r\n    model_path,\r\n    param_path,\r\n    vocab_path,\r\n    position_prob=0.5,\r\n    max_length=256, \r\n    schema=schema,\r\n    runtime_option=runtime_option)\r\n是否可以动态修改position_prob、max_length、schema\r\n而不需要去初始化模型",
        "state": "closed",
        "user": "wanshichenguang",
        "closed_by": "rainyfly",
        "created_at": "2024-03-06T06:34:45+00:00",
        "updated_at": "2024-03-07T06:30:45+00:00",
        "closed_at": "2024-03-07T06:30:45+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2401,
        "title": "How to load Adaface model if I only have pdparams?",
        "body": "Basically what the title says, I only have .pdparams and also a .yaml, but the tutorial shows that you meed both .pdiparams and .pdmodel for it to work. Any way to work just with .pdparams?\r\n",
        "state": "open",
        "user": "angelabr",
        "closed_by": null,
        "created_at": "2024-03-07T15:19:24+00:00",
        "updated_at": "2024-03-11T11:31:04+00:00",
        "closed_at": null,
        "comments_count": [
            "juncaipeng",
            "angelabr"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2402,
        "title": "fastdeployserver 不支持ocrv4",
        "body": "替换ocrv3的推理文件为ocrV4 server版本, ch_PP-OCRv4_det_server_infer和ch_PP-OCRv4_rec_server_infer ，然后在容器中启动fastdeployserver --model-repository=serving/models\r\n无法启动，会提示\r\ndet_runtime    UNAVAILABLE: Invalid argument: unexpected inference input 'sigmoid_0.tmp_0', allowed inputs are: sigmoid_11.tmp_0, tmp_36\r\nrec_runtime   UNAVAILABLE: Invalid argument: unexpected inference input 'softmax_5.tmp_0', allowed inputs are: softmax_2.tmp_0",
        "state": "closed",
        "user": "zhuxiaobin",
        "closed_by": "zhuxiaobin",
        "created_at": "2024-03-09T22:26:28+00:00",
        "updated_at": "2024-03-09T22:47:13+00:00",
        "closed_at": "2024-03-09T22:47:12+00:00",
        "comments_count": [
            "zhuxiaobin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2403,
        "title": "不能debug的原因，以及有无解决计划",
        "body": "现象：\r\n1. 目前Windows下不能debug，编译debug版本会出错\r\n2. vs直接debug其fastdeploy的release版本，能够编译成功，但是会运行失败，会报错\r\n\r\n问题：\r\n1. 为什么fastdeploy不支持debug，推理引擎、opencv都支持debug，为什么fastdeploy是基于他们，为什么没有debug版本？\r\n2. 有无增加debug的计划，或者有没有相关的方向、方法，想进行尝试。 \r\n",
        "state": "closed",
        "user": "zhoujun0715",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-11T03:25:27+00:00",
        "updated_at": "2025-03-25T06:45:40+00:00",
        "closed_at": "2025-03-25T06:45:40+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2404,
        "title": "cascade mask rcnn get empty mask(all 0), but python give right result.",
        "body": "infer_mask_rcnn.cc get empty mask(all 0),the detection box is right, but python test bed gives right mask result. Thank you a lot!",
        "state": "open",
        "user": "newforrestgump001",
        "closed_by": null,
        "created_at": "2024-03-13T05:15:05+00:00",
        "updated_at": "2024-03-14T01:21:38+00:00",
        "closed_at": null,
        "comments_count": [
            "newforrestgump001"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2405,
        "title": "docker容器部署后，每次调用gpu显存不释放，直到溢出，这个问题有很多issues，但没找到解决方案",
        "body": "容器镜像：registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\r\n调用一万次后，显存直接爆了\r\nW0314 04:50:46.438977 62225 memory.cc:135] Failed to allocate CUDA memory with byte size 79027200 on GPU 1: CNMEM_STATUS_OUT_OF_MEMORY, falling back to pinned system memory\r\n0314 05:01:17.338640 62420 pb_stub.cc:402] Failed to process the request(s) for model 'det_postprocess_0_0', message: TritonModelException: in ensemble 'rec_pp', softmax_2.tmp_0: failed to perform CUDA copy: an illegal memory access was encountered\r\n",
        "state": "closed",
        "user": "zhuxiaobin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-14T05:11:04+00:00",
        "updated_at": "2025-03-25T06:45:41+00:00",
        "closed_at": "2025-03-25T06:45:41+00:00",
        "comments_count": [
            "zhuxiaobin",
            "zhuxiaobin",
            "KyleWang-Hunter"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2406,
        "title": "--use_trt True 每一次build都要花费大量时间",
        "body": "能否build一次后，有缓存，可以直接读取\r\nhttps://github.com/PaddlePaddle/FastDeploy/tree/cc8d1f3c9f31ea4919972f14ee3eaaa9a09966e2/examples/vision/detection/yolov5lite/python\r\n用的是这里的代码",
        "state": "closed",
        "user": "kissablemt",
        "closed_by": "kissablemt",
        "created_at": "2024-03-14T11:11:52+00:00",
        "updated_at": "2024-03-25T08:19:17+00:00",
        "closed_at": "2024-03-25T08:19:17+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2407,
        "title": "opencv4.7.0推理ppocr报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-cpu-0.0.0  自编译opencv4.7.0\r\n- 【编译命令】cmake .. -DCMAKE_INSTALL_PREFIX=${PWD}/ppocr_fastdeploy_sdk -DOPENCV_DIRECTORY=/home/cxj/opencv/opencv4.7.0 -DENABLE_VISION=ON -DENABLE_TEXT=ON -DENABLE_PADDLE_BACKEND=ON\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： intel core i7-10700 CPU @ 2.90GHz × 4\r\n- 【编译语言】： C++\r\n\r\n能够成功编译出sdk    ppocr cpp也能成功编译      opencv4.5.2编译的sdk可以检测识别处结果   \r\nopencv4.7.0报错[1]  86146 segmentation faullt (core dumped)   ./infer_demo ../B.jpg 0\r\n\r\n定位到\r\nif(!ppocr_v3.Predict(&im, &result)) {}  \r\n--》》ppocr_v2.h   virtual bool Predict(cv::Mat* img, fastdeploy::vision::OCRResult* result);    \r\n--》》ppocr_v2.cc    if(!detect_->BatchPredict(images, &batch_boxes)) 之前",
        "state": "closed",
        "user": "Zomcxj",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-15T02:32:41+00:00",
        "updated_at": "2025-03-25T06:45:42+00:00",
        "closed_at": "2025-03-25T06:45:42+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "Question"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2408,
        "title": "F0320 08:45:55.725492 11628 memory_optimize_pass.cc:95] Check failed: i >= 0 (-1 vs. 0) ",
        "body": "\r\nfrom pathlib import Path\r\nimport soundfile as sf\r\nimport os\r\nfrom paddlespeech.t2s.exps.syn_utils import get_am_output\r\nfrom paddlespeech.t2s.exps.syn_utils import get_frontend\r\nfrom paddlespeech.t2s.exps.syn_utils import get_predictor\r\nfrom paddlespeech.t2s.exps.syn_utils import get_voc_output\r\n\r\n\r\ndef get_text_dict(name: str, txtname: str):\r\n    ff = open(txtname, \"r\", encoding=\"utf-8\")\r\n    msg = ff.read()\r\n    ff.close()\r\n    text_list = msg.split(\"\\n\")\r\n    text_dict = {}\r\n    num = 0\r\n    for i in text_list:\r\n        text_dict[name + str(num)] = i\r\n        num += 1\r\n        print(f\"{name}text:{num}\")\r\n    return text_dict\r\n\r\n\r\ndef the_main(text_dict):\r\n    # frontend\r\n    frontend = get_frontend(\r\n        lang=\"mix\",\r\n        phones_dict=os.path.join(am_inference_dir, \"phone_id_map.txt\"),\r\n        tones_dict=None\r\n    )\r\n\r\n    # am_predictor\r\n    am_predictor = get_predictor(\r\n        model_dir=am_inference_dir,\r\n        model_file=\"fastspeech2_mix\" + \".pdmodel\",\r\n        params_file=\"fastspeech2_mix\" + \".pdiparams\",\r\n        device=device)\r\n    # voc_predictor\r\n    voc_predictor = get_predictor(\r\n        model_dir=voc_inference_dir,\r\n        model_file=\"pwgan_aishell3\" + \".pdmodel\",  # 这里以 pwgan_aishell3 为例子，其它模型记得修改此处模型名称\r\n        params_file=\"pwgan_aishell3\" + \".pdiparams\",\r\n        device=device)\r\n    output_dir = Path(wav_output_dir)\r\n    output_dir.mkdir(parents=True, exist_ok=True)\r\n    sentences = list(text_dict.items())\r\n    merge_sentences = True\r\n    fs = 24000\r\n    for utt_id, sentence in sentences:\r\n        am_output_data = get_am_output(\r\n            input=sentence,\r\n            am_predictor=am_predictor,\r\n            am=\"fastspeech2_mix\",\r\n            frontend=frontend,\r\n            lang=\"mix\",\r\n            merge_sentences=merge_sentences,\r\n            speaker_dict=os.path.join(am_inference_dir, \"phone_id_map.txt\"),\r\n            spk_id=0, )\r\n        wav = get_voc_output(\r\n            voc_predictor=voc_predictor, input=am_output_data)\r\n        # 保存文件\r\n        sf.write(output_dir / (utt_id + \".wav\"), wav, samplerate=fs)\r\n    return\r\n\r\n\r\nif __name__ == '__main__':\r\n    # 模型路径\r\n    am_inference_dir = \"doubao_01\"\r\n    # 声码器路径，这里以 pwgan_aishell3 为例子\r\n    voc_inference_dir = \"pwgan_aishell3_static_1.1.0\"\r\n    # 音频生成的路径，修改成你音频想要保存的路径\r\n    wav_output_dir = \"output\"\r\n    # 选择设备[gpu / cpu]，这里以GPU为例子，\r\n    device = \"cpu\"\r\n    # 想要生成的文本文档对应文件名\r\n    txt_name = \"demo.txt\"\r\n    the_main(get_text_dict(name=am_inference_dir, txtname=txt_name))\r\n\r\n\r\n\r\n\r\n\r\nD:\\ruanjian\\ad3\\envs\\FJ_DEMO\\python.exe D:\\work\\py\\feijiang\\main.py \r\nD:\\ruanjian\\ad3\\envs\\FJ_DEMO\\lib\\site-packages\\lazy_loader\\__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\r\n  warnings.warn(msg, RuntimeWarning)\r\nD:\\ruanjian\\ad3\\envs\\FJ_DEMO\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\ndoubao_01text:1\r\ndoubao_01text:2\r\n[2024-03-20 08:45:52,493] [    INFO] - Already cached C:\\Users\\Administrator\\.paddlenlp\\models\\bert-base-chinese\\bert-base-chinese-vocab.txt\r\n[2024-03-20 08:45:52,516] [    INFO] - tokenizer config file saved in C:\\Users\\Administrator\\.paddlenlp\\models\\bert-base-chinese\\tokenizer_config.json\r\n[2024-03-20 08:45:52,516] [    INFO] - Special tokens file saved in C:\\Users\\Administrator\\.paddlenlp\\models\\bert-base-chinese\\special_tokens_map.json\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\nI0320 08:45:54.643479 11628 executor.cc:187] Old Executor is Running.\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [layer_norm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [attention_lstm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass]e[0m\r\ne[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass]e[0m\r\ne[32m--- Running IR pass [mul_lstm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fc_gru_fuse_pass]e[0m\r\ne[32m--- Running IR pass [mul_gru_fuse_pass]e[0m\r\ne[32m--- Running IR pass [seq_concat_fc_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [matmul_v2_scale_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]e[0m\r\nI0320 08:45:54.957129 11628 fuse_pass_base.cc:59] ---  detected 37 subgraphs\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]e[0m\r\nI0320 08:45:54.967124 11628 fuse_pass_base.cc:59] ---  detected 17 subgraphs\r\ne[32m--- Running IR pass [matmul_scale_fuse_pass]e[0m\r\nI0320 08:45:54.977124 11628 fuse_pass_base.cc:59] ---  detected 8 subgraphs\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\nI0320 08:45:55.071126 11628 fuse_pass_base.cc:59] ---  detected 37 subgraphs\r\ne[32m--- Running IR pass [repeated_fc_relu_fuse_pass]e[0m\r\ne[32m--- Running IR pass [squared_mat_sub_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_transpose_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [is_test_pass]e[0m\r\ne[32m--- Running IR pass [constant_folding_pass]e[0m\r\nI0320 08:45:55.720499 11628 fuse_pass_base.cc:59] ---  detected 171 subgraphs\r\ne[1me[35m--- Running analysis [save_optimized_model_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nF0320 08:45:55.725492 11628 memory_optimize_pass.cc:95] Check failed: i >= 0 (-1 vs. 0) \r\n*** Check failure stack trace: ***\r\n    @   00007FFF7B895025  PyInit__eval_frame\r\n    @   00007FFF7B8945D2  PyInit__eval_frame\r\n    @   00007FFF7D7D90F8  class paddle::Tensor __cdecl paddle::from_blob(void * __ptr64,class paddle::experimental::IntArrayBase<class phi::DenseTensor> const & __ptr64,enum phi::DataType,enum common::DataLayout,class phi::Place const & __ptr64,class std::function<void __cdecl(v\r\n    @   00007FFF7D7DA5D6  class paddle::Tensor __cdecl paddle::from_blob(void * __ptr64,class paddle::experimental::IntArrayBase<class phi::DenseTensor> const & __ptr64,enum phi::DataType,enum common::DataLayout,class phi::Place const & __ptr64,class std::function<void __cdecl(v\r\n    @   00007FFF7D7CF609  class paddle::Tensor __cdecl paddle::from_blob(void * __ptr64,class paddle::experimental::IntArrayBase<class phi::DenseTensor> const & __ptr64,enum phi::DataType,enum common::DataLayout,class phi::Place const & __ptr64,class std::function<void __cdecl(v\r\n    @   00007FFF7B9AC455  class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl paddle_infer::GetVersion(void)\r\n    @   00007FFF7B9AB539  class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl paddle_infer::GetVersion(void)\r\n    @   00007FFF7B99C7CF  class std::unique_ptr<class paddle::PaddlePredictor,struct std::default_delete<class paddle::PaddlePredictor> > __cdecl paddle::CreatePaddlePredictor<struct paddle::AnalysisConfig,2>(struct paddle::AnalysisConfig const & __ptr64)\r\n    @   00007FFF7B9A3C5B  public: __cdecl paddle_infer::Predictor::Predictor(struct paddle::AnalysisConfig const & __ptr64) __ptr64\r\n    @   00007FFF7B634C7D  public: class paddle::ZeroCopyTensor & __ptr64 __cdecl paddle::ZeroCopyTensor::operator=(class paddle::ZeroCopyTensor const & __ptr64) __ptr64\r\n    @   00007FFF7B60E1B6  PyInit_libpaddle\r\n    @   00007FFF7B2FE469  public: class paddle::OpMetaInfoBuilder & __ptr64 __cdecl paddle::OpMetaInfoBuilder::operator=(class paddle::OpMetaInfoBuilder const & __ptr64) __ptr64\r\n    @   00007FFF885882C6  PyCFunction_GetFlags\r\n    @   00007FFF8854551C  _PyObject_MakeTpCall\r\n    @   00007FFF8863E6C2  PyEval_GetFuncDesc\r\n    @   00007FFF8863AE6F  _PyEval_EvalFrameDefault\r\n    @   00007FFF8863CE4B  _PyEval_EvalFrameDefault\r\n    @   00007FFF8854582E  _PyFunction_Vectorcall\r\n    @   00007FFF88636179  _PyOS_URandomNonblock\r\n    @   00007FFF8863E6C2  PyEval_GetFuncDesc\r\n    @   00007FFF8863AED8  _PyEval_EvalFrameDefault\r\n    @   00007FFF8863CE4B  _PyEval_EvalFrameDefault\r\n    @   00007FFF8854582E  _PyFunction_Vectorcall\r\n    @   00007FFF88636179  _PyOS_URandomNonblock\r\n    @   00007FFF8863E6C2  PyEval_GetFuncDesc\r\n    @   00007FFF8863A8B2  _PyEval_EvalFrameDefault\r\n    @   00007FFF8863CE4B  _PyEval_EvalFrameDefault\r\n    @   00007FFF88637892  PyEval_EvalCode\r\n    @   00007FFF886AE05E  PyRun_FileExFlags\r\n    @   00007FFF886AE138  PyRun_FileExFlags\r\n    @   00007FFF886ADCE8  PyRun_StringFlags\r\n    @   00007FFF886ABF45  _PyRun_SimpleFileObject\r\n\r\n进程已结束，退出代码为 -1073740791 (0xC0000409)\r\n\r\n\r\n请问这个报错是为什么？找不到解决办法了。",
        "state": "closed",
        "user": "xn3469010",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-20T00:55:46+00:00",
        "updated_at": "2025-03-25T06:45:43+00:00",
        "closed_at": "2025-03-25T06:45:43+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2409,
        "title": "layout fastdeploy_serving方式部署 Failed to process the request(s) for model 'postprocess_0', message: TypeError: run(): incompatible function arguments.",
        "body": "版面分析算法, fastdeploy_serving方式部署,在StructureV2LayoutPostprocessor后处理时报错\r\nFailed to process the request(s) for model 'postprocess_0', message: TypeError: run(): incompatible function arguments. The following argument types are supported:\r\n    1. (self: fastdeploy.libs.fastdeploy_main.vision.ocr.StructureV2LayoutPostprocessor, arg0: List[fastdeploy.libs.fastdeploy_main.FDTensor], arg1: List[List[int[4]]]) -> List[fastdeploy.libs.fastdeploy_main.vision.DetectionResult]\r\nInvoked with: <fastdeploy.libs.fastdeploy_main.vision.ocr.StructureV2LayoutPostprocessor object at 0x7f788b5eb4f0>, array([[[ 3.0430562 ,  2.9620893 , -0.92005384, ..., -1.1082802 ,\r\n         -1.694324  , -1.3905903 ],\r\n        [-0.8745545 ,  3.1247966 ,  3.0257077 , ..., -1.0160857 ,\r\n         -1.3037924 , -0.8528714 ],\r\n        [-1.1111182 , -1.1563158 ,  3.6807263 , ..., -0.8917918 ,\r\n         -1.5667745 , -1.2615712 ],\r\n        ...,\r\n        [ 2.574394  ,  2.0140955 , -0.02018397, ..., -1.5280299 ,\r\n         -1.9861596 , -1.8610322 ],\r\n        [ 0.99233127,  1.3739115 ,  0.8655137 , ..., -1.4873394 ,\r\n         -1.9895219 , -1.8228513 ],\r\n        [ 0.2792613 ,  0.42100033,  1.0559945 , ..., -1.313761  ,\r\n         -1.8969777 , -1.7097901 ]]], dtype=float32), [array([640, 404, 608, 800], dtype=object)]\r\n\r\n\r\nAt:\r\n  /usr/local/lib/python3.8/dist-packages/fastdeploy/vision/ocr/ppocr/__init__.py(778): run\r\n  /ocr_serving/vision/ocr/PP-OCR/serving/fastdeploy_serving/models-layout/postprocess/1/model.py(109): execute\r\n\r\n\r\n版面分析后处理model.py报错行\r\nresults = self.postprocessor_.run([infer_outputs], [im_infos])\r\n参数打印出\r\ninfer_outputs shape is  (1, 130, 32)\r\nim_infos: [640 404 608 800]\r\n\r\nppocr/__init__.py  中StructureV2LayoutPostprocessor类\r\nclass StructureV2LayoutPostprocessor:\r\n    def __init__(self):\r\n        \"\"\"Create a postprocessor for StructureV2Layout Model\r\n        \"\"\"\r\n        self._postprocessor = C.vision.ocr.StructureV2LayoutPostprocessor()\r\n\r\n    def run(self, runtime_results, ims_info):\r\n        \"\"\"Postprocess the runtime results for StructureV2Layout Model\r\n        :param: runtime_results: (list of FDTensor or list of pyArray)The output FDTensor results from runtime\r\n        :return: list of Result(If the runtime_results is predict by batched samples, the length of this list equals to the batch size)\r\n        \"\"\"\r\n        return self._postprocessor.run(runtime_results, ims_info)\r\n",
        "state": "closed",
        "user": "lsx66",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-20T10:01:17+00:00",
        "updated_at": "2025-03-25T06:45:44+00:00",
        "closed_at": "2025-03-25T06:45:44+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "lsx66",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2412,
        "title": "how to use tracking such as bytetrack with pptracking",
        "body": "how to use SDE tracking such as bytetrack with ppyoloe models along with fastdeploy and pptracking class?",
        "state": "closed",
        "user": "mahesh11T",
        "closed_by": "heliqi",
        "created_at": "2024-03-26T04:01:51+00:00",
        "updated_at": "2024-05-17T04:05:59+00:00",
        "closed_at": "2024-05-17T04:05:59+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2410,
        "title": "sophon se7 编译安装 执行失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n采用算能编译部署：\r\n# Download the latest source code\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport ENABLE_SOPHGO_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\ncd dist\r\npip install fastdeploy_python-0.0.0-cp38-cp38-linux_aarch64.whl\r\n\r\n\r\n\r\n- 【FastDeploy版本】：\r\n-fastapi             0.110.0\r\nfastdeploy-python   0.0.0\r\nfastdeploy-tools    0.0.5\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： sophon se7\r\n-\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n\r\nroot@bm1684:/home/paddle/ppliteseg# python3 ppliteseg_fastdeploy_sophon.py\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"ppliteseg_fastdeploy_sophon.py\", line 6, in <module>\r\n    import fastdeploy as fd\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n运行报错\r\n\r\n",
        "state": "closed",
        "user": "SweatLin",
        "closed_by": "heliqi",
        "created_at": "2024-03-25T03:44:46+00:00",
        "updated_at": "2024-04-15T07:07:14+00:00",
        "closed_at": "2024-04-15T07:07:14+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2411,
        "title": "fastdeploy 服务化部署paddleocr如何调整输入图片为base64编码",
        "body": "我现在试着用fastdeploy服务化部署的方式部署paddleocrv3模型，但是输入的图片是一个list，这也太慢了吧。有没有可以将输入数据转换为base64的参考资料？",
        "state": "closed",
        "user": "KyleWang-Hunter",
        "closed_by": "heliqi",
        "created_at": "2024-03-25T09:27:16+00:00",
        "updated_at": "2024-11-28T10:15:22+00:00",
        "closed_at": "2024-05-17T04:06:11+00:00",
        "comments_count": [
            "heliqi",
            "ouerum"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2413,
        "title": "文档中的链接失效",
        "body": "[文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/README_CN.md)中关于PPOCR的[链接](https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/README_CN.md)失效：\r\n\r\n\r\n<img width=\"488\" alt=\"image\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/12560511/31737d42-c606-478a-9eba-3353ef092b2b\">",
        "state": "closed",
        "user": "TingquanGao",
        "closed_by": "heliqi",
        "created_at": "2024-03-27T03:54:47+00:00",
        "updated_at": "2024-04-01T09:22:29+00:00",
        "closed_at": "2024-04-01T09:22:29+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2414,
        "title": "ortbackend内存管理的问题咨询",
        "body": "请问一下：https://github.com/PaddlePaddle/FastDeploy/blob/cc8d1f3c9f31ea4919972f14ee3eaaa9a09966e2/fastdeploy/runtime/backends/ort/ort_backend.cc#L203C22-L203C38\r\n这里的`std::vector<char*> disable_fp16_ops;`是怎么管理内存的\r\n还有https://github.com/PaddlePaddle/FastDeploy/blob/cc8d1f3c9f31ea4919972f14ee3eaaa9a09966e2/fastdeploy/runtime/backends/ort/ort_backend.cc#L255\r\n`char* model_content_ptr`的内存是由ort 的session管理的吗？",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "heliqi",
        "created_at": "2024-03-27T06:19:52+00:00",
        "updated_at": "2024-04-01T09:22:01+00:00",
        "closed_at": "2024-04-01T09:22:01+00:00",
        "comments_count": [
            "heliqi",
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2415,
        "title": "picodet to rknn result in large size (117MB)",
        "body": "## Environment\r\n\r\nFastDeploy version: develop branch\r\nOS Platform: e.g. Linux x64 \r\nHardware: e.g. AMD CPU\r\nProgram Language: e.g. Python 3.6.15\r\n\r\n## Problem description\r\nUsing https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.7/deploy/fastdeploy/rockchip/rknpu2 \r\nWhy the exported rknn model \"picodet_s_416_coco_lcnet_rk3588_unquantized.rknn\" size at 117mb??\r\nThe downloaded picodet_s_416_coco_lcnet_rk3588_unquantized.rknn only 4.4mb.\r\ninfer is ok, but cost much more time.\r\n\r\n`wget https://bj.bcebos.com/paddlehub/fastdeploy/rknpu2/picodet_s_416_coco_lcnet.zip`\r\n\r\npicodet_s_416_coco_lcnet_unquantized.yaml\r\n```\r\nmean:\r\n  -\r\n    - 123.675\r\n    - 116.28\r\n    - 103.53\r\nstd:\r\n  -\r\n    - 58.395\r\n    - 57.12\r\n    - 57.375\r\nmodel_path: /opt/picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx\r\noutputs_nodes:\r\n  - 'p2o.Mul.179'\r\n  - 'p2o.Concat.9'\r\ndo_quantization: False\r\ndataset:\r\noutput_folder: \"/opt/picodet_s_416_coco_lcnet/t\"\r\n```\r\n\r\n```\r\n!python /opt/FastDeploy/tools/rknpu2/export.py --config_path /opt/FastDeploy/tools/rknpu2/config/picodet_s_416_coco_lcnet_unquantized.yaml \\\r\n                              --target_platform rk3588\r\n```\r\n[com.log](https://github.com/PaddlePaddle/FastDeploy/files/14771853/com.log)\r\n\r\n",
        "state": "closed",
        "user": "shao77622",
        "closed_by": "shao77622",
        "created_at": "2024-03-27T10:30:44+00:00",
        "updated_at": "2024-03-28T02:24:44+00:00",
        "closed_at": "2024-03-28T02:24:44+00:00",
        "comments_count": [
            "shao77622"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2416,
        "title": "多模型共用一个infer_cfg.yaml，扩展Label_list字段后，预测返回label_id",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-python-1.0.7\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】： i5-6400 CPU\r\n- 【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【DET_RESULT】\r\n    我使用ensemble启动两个模型（picodet_l_640_coco_lcnet_cpu、picodet_l_640_coco_lcnet_hd）共用postprocess/1/infer_cfg.ymal，并将两个模型的label_list合并保存，但无论是调用picodet_l_640_coco_lcnet_cpu还是picodet_l_640_coco_lcnet_hd，返回的预测结果abel_ids字段总为0。是我没有弄对，还是说本身就不可以这样做？\r\n\r\n- 【模型仓库】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/de110225-5512-4251-874a-ad4aae5b3ef5)\r\n-  【infer_cfg.yaml内容】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/4de5c2a0-5560-47db-b141-5160861754a7)\r\n-  【预测及返回结果】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/1e7e1623-6839-4b6d-8817-5c1864e0dff2)\r\n\r\n\r\n新手上路，还请大佬赐教。\r\n",
        "state": "closed",
        "user": "njflove",
        "closed_by": "njflove",
        "created_at": "2024-03-28T02:46:26+00:00",
        "updated_at": "2024-04-03T06:34:02+00:00",
        "closed_at": "2024-04-03T06:34:02+00:00",
        "comments_count": [
            "heliqi",
            "njflove",
            "heliqi",
            "njflove"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2424,
        "title": "Jetson部署库编译安装后运行错误",
        "body": "- 【系统平台】: Ubuntu 18.04\r\n- 【硬件】： Jetson NX，jetpack4.6  CUDA 10.2 CUDNN 8.2\r\n- 【编译语言】： C++ / Python(3.8）\r\n按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/jetson.md在Jetson NX上以源码方式编译安装fastDeploy,安装成功，但是在python中运行出错。\r\n\r\n- 【编译命令】\r\n- 先编译C＋＋\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -DBUILD_ON_JETSON=ON \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF \\ # 可选项，如若不需要Paddle Inference后端，可关闭\r\n         -DPADDLEINFERENCE_DIRECTORY=/Download/paddle_inference_jetson \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/installed_fastdeploy\r\nmake -j8\r\nmake install\r\n - Python编译安装：\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport BUILD_ON_JETSON=ON\r\nexport ENABLE_VISION=ON\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\nwheel包安装成功。\r\n\r\n\r\n测试代码：\r\nimport cv2\r\nimport fastdeploy.vision as vision\r\n\r\nmodel = vision.detection.PPYOLOE(\"ppyoloe_crn_l_300e_coco/model.pdmodel\",\r\n                                 \"ppyoloe_crn_l_300e_coco/model.pdiparams\",\r\n                                 \"ppyoloe_crn_l_300e_coco/infer_cfg.yml\")\r\n\r\nim = cv2.imread(\"ppocr_img/imgs/14.jpg\")\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\n错误提示：\r\nTraceback (most recent call last):\r\n  File \"/home/nvidia/miniconda3/envs/camera/lib/python3.8/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nModuleNotFoundError: No module named 'fastdeploy.libs.fastdeploy_main'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/nvidia/PycharmProjects/untitled/fastdeploy_demo.py\", line 2, in <module>\r\n    import fastdeploy.vision as vision\r\n  File \"/home/nvidia/miniconda3/envs/camera/lib/python3.8/site-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/home/nvidia/miniconda3/envs/camera/lib/python3.8/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: No module named 'fastdeploy.libs.fastdeploy_main'\r\n\r\nProcess finished with exit code 1\r\n\r\n",
        "state": "open",
        "user": "lvrh2018",
        "closed_by": null,
        "created_at": "2024-04-07T11:05:44+00:00",
        "updated_at": "2024-04-07T11:36:54+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2417,
        "title": "在执行examples下的关键点rknn模型推理测试时出现未知输入参数。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy 3.0.11\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】： Nvidia GPU 2080TI， CUDA 12.2 \r\n- 【编译语言】： python 3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n在执行examples下的关键点rknn模型推理测试时出现未知输入参数。\r\n即输入python pptinypose_infer.py --tinypose_model_dir PP_TinyPose_256x192_infer --image hrnet_demo.jpg后报以下错误。\r\n![QQ图片20240328113244](https://github.com/PaddlePaddle/FastDeploy/assets/52063950/03224efd-4adf-4410-aff8-cfa0e59a07de)\r\n\r\n",
        "state": "closed",
        "user": "xianningblog",
        "closed_by": "heliqi",
        "created_at": "2024-03-28T03:33:22+00:00",
        "updated_at": "2024-04-15T07:02:17+00:00",
        "closed_at": "2024-04-15T07:02:17+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2419,
        "title": "预提交验证失败remote: Repository not found.",
        "body": "在执行`commit-prepare.sh`后\r\n执行`git commit` 的时候验证不通过，报错信息：\r\n```bash\r\nremote: Repository not found.\r\n    fatal: repository 'https://github.com/PaddlePaddle/mirrors-yapf.git/' not found\r\n```\r\n这个仓库貌似失效了，必须加上`--no-verify`\r\n麻烦修复一下预提交的配置\r\n",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "heliqi",
        "created_at": "2024-03-29T01:10:53+00:00",
        "updated_at": "2024-04-01T09:22:59+00:00",
        "closed_at": "2024-04-01T09:22:59+00:00",
        "comments_count": [
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2423,
        "title": "Fastdeploy下使用ppvehicle的模型推理汽车的属性分类出现bug",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 2004) \r\n- 【硬件】： CPU\r\n- 【编译语言】： C++ \r\n在https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.7/deploy/pipeline/docs/tutorials/ppvehicle_attribute.md下载了车辆检测模型以及车辆属性识别模型，基于fastdeploy部署了车辆目标检测以及车辆属性识别模型，其中车辆检测模型识别正常推理，但是车辆属性识别出现问题，算法流程：目标检测图像中的车辆，然后将车辆roi扣出来，单独送到车辆属性检测器推理，但是出现失败，具体log如下：\r\nterminate called after throwing an instance of 'ov::Exception'\r\n  what():  Can't set input blob with name: x, because model input (shape=[?,3,192,256]) and blob (shape=(1.765.1360.3)) are incompatible\r\nAborted\r\nroi图像没有问题，测试保存下来是正常的车辆图像，也尝试过单独使用fastdeploy推理ppvehicle的属性识别模型，也是报一样的错误\r\n\r\n",
        "state": "open",
        "user": "hunagjingwei",
        "closed_by": null,
        "created_at": "2024-04-03T09:23:53+00:00",
        "updated_at": "2024-04-08T02:51:58+00:00",
        "closed_at": null,
        "comments_count": [
            "rainyfly",
            "hunagjingwei",
            "hunagjingwei"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2422,
        "title": "picodet grpc客户端推理，如何保存检测结果图片？",
        "body": "## 环境\r\n\r\n【FastDeploy版本】：fastdeploy-python-1.0.7\r\n【系统平台】: Windows x64(Windows10)\r\n【硬件】： i5-6400 CPU\r\n【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【DET_RESULT】\r\n         使用fastdeploy部署模型之后，如果保存标注好的图片？在Response中没有找相关信息，需要从哪里配置？\r\n- 【模型仓库】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/bccc7c31-5d44-4b69-9f10-3483599161ed)\r\n\r\n- 【模型配置文件】\r\n    \r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/a87b9037-582f-43cb-a575-d18d4c922d2b)\r\n\r\n\r\n- 【postprocess配置文件】\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/41934985/548ce4d2-6245-417d-ba67-c821023f4cb6)\r\n\r\n",
        "state": "closed",
        "user": "njflove",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-02T09:25:32+00:00",
        "updated_at": "2025-04-08T06:43:58+00:00",
        "closed_at": "2025-04-08T06:43:58+00:00",
        "comments_count": [
            "rainyfly"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2425,
        "title": "使用瑞芯微RK3588部署PaddleOCRV3模型，识别模型只能识别到第一个字符，后面的字符都没有识别",
        "body": "使用瑞芯微RK3588部署PaddleOCRV3模型，识别模型只能识别到第一个字符，后面的字符都没有识别。检测模型可以正常检测到文字，分类模型也正常使用，但是识别结果不对。是哪个参数影响？还是文件影响？",
        "state": "closed",
        "user": "yjszyd0813",
        "closed_by": "yjszyd0813",
        "created_at": "2024-04-08T03:15:50+00:00",
        "updated_at": "2024-04-10T11:39:43+00:00",
        "closed_at": "2024-04-10T11:39:12+00:00",
        "comments_count": [
            "yjszyd0813"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2427,
        "title": "atlas 300i pro跑yolov5.onnx模型推理出错",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python ==0.0.0\r\n- 【系统平台】: Linux aarch64(Ubuntu 18.04) \r\n- 【硬件】： atlas 300i pro推理卡  cann6.0.1  \r\n- 【编译语言】： Python3.7.5\r\n\r\n## 出现问题的操作流程\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，使用examples提供的模型，可以执行成功\r\n- - `examples`下的示例python infer.py --model yolov5s_infer --image 000000014439.jpg --device ascend可以运行，换成了paddle提供的yolov5的onnx文件(yolov5n.onnx)，代码报错\r\n- - 这是测试代码 \r\n\r\n```\r\nimport fastdeploy as fd\r\nimport cv2\r\n\r\noption = fd.RuntimeOption()\r\noption.use_ascend()\r\nmodel = fd.vision.detection.YOLOv5(\r\n    \"./yolov5n.onnx\",\r\n    params_file=\"\",\r\n    runtime_option=option,\r\n    model_format=fd.ModelFormat.ONNX)\r\nimage = './1.jpg'\r\nim = cv2.imread(image)\r\nresult = model.predict(im, conf_threshold=0.25, nms_iou_threshold=0.5)\r\nvis_im = fd.vision.vis_detection(im, result)\r\ncv2.imwrite(\"visualized_result_monkey.jpg\", vis_im)\r\nprint(\"Visualized result save in ./visualized_result_monkey.jpg\")\r\n```\r\n--报错信息\r\n```\r\n[ERROR] fastdeploy/fastdeploy_model.cc(385)::CreateASCENDBackend        There's no valid ascend backends for model: yolov5\r\n[ERROR] fastdeploy/vision/detection/contrib/yolov5/yolov5.cc(45)::Initialize    Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 10, in <module>\r\n    model_format=fd.ModelFormat.ONNX)\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy/vision/detection/contrib/yolov5.py\", line 184, in __init__\r\n    assert self.initialized, \"YOLOv5 initialize failed.\"\r\nAssertionError: YOLOv5 initialize failed.\r\n```\r\n",
        "state": "open",
        "user": "PeterFive",
        "closed_by": null,
        "created_at": "2024-04-09T12:12:34+00:00",
        "updated_at": "2024-04-15T13:48:56+00:00",
        "closed_at": null,
        "comments_count": [
            "juncaipeng",
            "PeterFive"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2428,
        "title": "fastdeploy编译问题：1.用预编译的提示RuntimeOption不存在。2.自己编译，编译成功了安装失败。提示不支持当前平台 #11905",
        "body": "\r\n问题1：用预编译的提示RuntimeOption不存在\r\n 通过如下命令安装的预编译版本.提示RuntimeOption不存在\r\n```sh\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n```\r\n出错信息：\r\n```python git:(main) ✗ python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu --backend paddle\r\n\r\nfd <module 'fastdeploy' (namespace)>\r\nTraceback (most recent call last):\r\n  File \"/Users/leo/PaddleOCR/deploy/fastdeploy/cpu-gpu/python/infer.py\", line 185, in <module>\r\n    det_option, cls_option, rec_option = build_option(args)\r\n  File \"/Users/che3vinci/PaddleOCR/deploy/fastdeploy/cpu-gpu/python/infer.py\", line 75, in build_option\r\n    det_option = fd.RuntimeOption()\r\nAttributeError: module 'fastdeploy' has no attribute 'RuntimeOption'\r\n\r\n```\r\n\r\n\r\n问题2：2.自己编译，编译成功了安装失败。提示不支持当前平台\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Mac OSX intel(14.4.1) X86-64\r\n- 【硬件】： CPU\r\n- 【编译语言】：  Python 3.9\r\n- 编译方法：\r\n\r\n```sh\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_OPENVINO_BACKEND=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n```\r\n编译完成后，安装过程中提示错误\r\n```sh\r\nERROR: fastdeploy_python-0.0.0-cp39-cp39-macosx_12_0_x86_64.whl is not a supported wheel on this platform.\r\n\r\n```\r\n\r\n意思是说不支持我现在的osx 14 系统吗？只支持osx12的？\r\n\r\n\r\n",
        "state": "closed",
        "user": "coding-jimmy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-10T02:57:31+00:00",
        "updated_at": "2025-04-29T06:46:03+00:00",
        "closed_at": "2025-04-29T06:46:03+00:00",
        "comments_count": [
            "juncaipeng",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2430,
        "title": "structurev2_ser_vi_layoutxlm python 什么时候提供？",
        "body": "structurev2_ser_vi_layoutxlm python 什么时候提供？\r\nc++的版本10个月之前就有了。",
        "state": "closed",
        "user": "cumthxy",
        "closed_by": "cumthxy",
        "created_at": "2024-04-11T09:03:29+00:00",
        "updated_at": "2024-04-12T04:13:08+00:00",
        "closed_at": "2024-04-12T04:13:08+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2429,
        "title": "vision::OCRResult missing necessary fields `table_boxes` and `table_structure`",
        "body": "## Environment\r\n\r\nFastDeploy version:  1.0.7\r\nOS Platform:  Windows x64 intel\r\nHardware: Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: . Python 3.10\r\n\r\n## Problem description\r\nThe `vision::OCRResult` actually include `table_boxes` and `table_structure` fields, but the pybing11 part doesn't include them. We can find the both fields in the first code snippet below, but not in the second one.\r\n\r\n```C\r\n// FastDeploy\\fastdeploy\\vision\\common\\result.h\r\nstruct FASTDEPLOY_DECL OCRResult : public BaseResult {\r\n  std::vector<std::array<int, 8>> boxes;\r\n\r\n  std::vector<std::string> text;\r\n  std::vector<float> rec_scores;\r\n\r\n  std::vector<float> cls_scores;\r\n  std::vector<int32_t> cls_labels;\r\n\r\n  std::vector<std::array<int, 8>> table_boxes;\r\n  std::vector<std::string> table_structure;\r\n  std::string table_html;\r\n\r\n  ResultType type = ResultType::OCR;\r\n\r\n  void Clear();\r\n\r\n  std::string Str();\r\n};\r\n```\r\n\r\n```C\r\n  pybind11::class_<vision::OCRResult>(m, \"OCRResult\")\r\n      .def(pybind11::init())\r\n      .def_readwrite(\"boxes\", &vision::OCRResult::boxes)\r\n      .def_readwrite(\"text\", &vision::OCRResult::text)\r\n      .def_readwrite(\"rec_scores\", &vision::OCRResult::rec_scores)\r\n      .def_readwrite(\"cls_scores\", &vision::OCRResult::cls_scores)\r\n      .def_readwrite(\"cls_labels\", &vision::OCRResult::cls_labels)\r\n      .def(\"__repr__\", &vision::OCRResult::Str)\r\n      .def(\"__str__\", &vision::OCRResult::Str);\r\n```\r\n\r\n## Solution\r\n\r\n* The directly way to solve it is to add the two fields to pybind11 part. Or we can derive a TableOCRResult class. Thanks.",
        "state": "closed",
        "user": "bingoabs",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-10T11:48:20+00:00",
        "updated_at": "2025-04-15T06:43:38+00:00",
        "closed_at": "2025-04-15T06:43:38+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2432,
        "title": "python 3.10 fastdeploy-gpu-python  C++ Traceback",
        "body": "pip install fastdeploy-gpu-python==0.0.0 -f https://www.paddlepaddle.org.cn/whl/fastdeploy_nightly_build.html\r\n\r\npython =3.10 \r\ncuda =11.2 \r\n报错。\r\n\r\n自己编译的也是这样的错误。 使用官方提供的 whl 也是错误。\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n1   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n2   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n3   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n4   paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n5   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\r\n6   paddle::inference::analysis::IrGraphBuildPass::RunImpl(paddle::inference::analysis::Argument*)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Erroneous arithmetic operation` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1712889656 (unix time) try \"date -d @1712889656\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGFPE (@0x7f268c23bda5) received by PID 337444 (TID 0x7f26fd303740) from PID 18446744071765736869 ***]\r\n\r\n",
        "state": "closed",
        "user": "cumthxy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-12T02:46:22+00:00",
        "updated_at": "2025-04-22T06:44:22+00:00",
        "closed_at": "2025-04-22T06:44:22+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2435,
        "title": "Linux aarch64预编译版无法下载",
        "body": "Linux aarch641.0.7预编译版无法下载",
        "state": "closed",
        "user": "lkocok",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-17T08:49:57+00:00",
        "updated_at": "2025-04-22T06:44:23+00:00",
        "closed_at": "2025-04-22T06:44:23+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2431,
        "title": "fastdeploy_model.h(140)::Clone   doesn't support Cone() now.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux x64(Ubuntu 20.04) \r\n- 【硬件】：  Nvidia GPU 3080TI， CUDA 11.5 CUDNN 8.6\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\nfastdeploy 提供的yolov8  不支持clone吗\r\n[ERROR] FastDeploy-develop/build/compiled_fastdeploy_sdk/include/fastdeploy/fastdeploy_model.h(140)::Clone   doesn't support Cone() now.\r\n\r\n请问有计划支持吗\r\n",
        "state": "closed",
        "user": "294978174",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-04-11T10:18:19+00:00",
        "updated_at": "2025-04-22T06:44:21+00:00",
        "closed_at": "2025-04-22T06:44:21+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2433,
        "title": "ser_vi_layoutxlm/vqa_utils.py 中代码是否有必要？默认值为-100",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/python/fastdeploy/vision/ocr/ppocr/utils/ser_vi_layoutxlm/vqa_utils.py#L316\r\n此行代码是否有必要？\r\npaddle.nn.CrossEntropyLoss().ignore_index 这个值不是默认值-100么？\r\n",
        "state": "open",
        "user": "cumthxy",
        "closed_by": null,
        "created_at": "2024-04-12T04:08:39+00:00",
        "updated_at": "2024-04-15T08:12:37+00:00",
        "closed_at": null,
        "comments_count": [
            "juncaipeng",
            "cumthxy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2436,
        "title": "module 'fastdeploy' has no attribute 'RuntimeOption'",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fast-tokenizer-python 1.0.2\r\n\r\n通过如下命令安装的.提示RuntimeOption不存在\r\n\r\npip install fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\r\n出错信息：\r\n\r\n```\r\nfd <module 'fastdeploy' (namespace)>\r\nTraceback (most recent call last):\r\n  File \"/Users/che3vinci/PaddleOCR/deploy/fastdeploy/cpu-gpu/python/infer.py\", line 185, in <module>\r\n    det_option, cls_option, rec_option = build_option(args)\r\n  File \"/Users/che3vinci/PaddleOCR/deploy/fastdeploy/cpu-gpu/python/infer.py\", line 75, in build_option\r\n    det_option = fd.RuntimeOption()\r\nAttributeError: module 'fastdeploy' has no attribute 'RuntimeOption'\r\n```\r\n",
        "state": "open",
        "user": "GaoYuelong",
        "closed_by": null,
        "created_at": "2024-04-17T09:00:51+00:00",
        "updated_at": "2024-08-24T10:26:48+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "will-bug",
            "jackyzzy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2439,
        "title": "特定图像会导致OCR例子内存报错core",
        "body": "这张图像，跑python ppocr 默认配置例子，正常推理，python程序退出时直接core，转成jpg后不会，其它图像暂时不会。",
        "state": "open",
        "user": "Noobzm",
        "closed_by": null,
        "created_at": "2024-04-22T10:15:13+00:00",
        "updated_at": "2024-04-25T09:42:15+00:00",
        "closed_at": null,
        "comments_count": [
            "Noobzm",
            "Noobzm",
            "heliqi",
            "Noobzm"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2438,
        "title": "c#编译fastdeploy，在nuget restore时出现报错：无法计算表达式“[MSBuild]::NormalizePath('', '')”。参数“path”的长度不能为零。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n\r\n- 【编译命令】\r\n- cmake .. -G \"Visual Studio 16 2019\" -A x64 ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_PADDLE_BACKEND=ON ^\r\n         -DENABLE_OPENVINO_BACKEND=ON ^\r\n         -DENABLE_TRT_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DENABLE_TEXT=ON ^\r\n         -DWITH_GPU=ON ^\r\n         -DWITH_CSHARPAPI=ON ^\r\n         -DTRT_DIRECTORY=\"D:\\TensorRT-8.6.1.6\" ^\r\n         -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\" ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\paddle_fastdeploy\"\r\n\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】：  Nvidia GPU 3060ti， CUDA 11.7 CUDNN 8.6.0  tensorRT GA 8.6（8.6.1.6）\r\n- 【编译语言】：c#\r\n\r\n错误信息如下图：\r\n<img width=\"628\" alt=\"EE175819-1525-47b6-B9FA-47D3473CEE2B\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/73456895/aba04f09-1ffc-49ed-83ab-504486681d36\">\r\n",
        "state": "closed",
        "user": "gunh4mmer",
        "closed_by": "gunh4mmer",
        "created_at": "2024-04-22T07:56:27+00:00",
        "updated_at": "2024-04-23T01:59:55+00:00",
        "closed_at": "2024-04-23T01:59:55+00:00",
        "comments_count": [
            "heliqi",
            "gunh4mmer"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2437,
        "title": "CAPI 重OneDimDetectionResult 调用释放内存释方法后崩溃",
        "body": "```c\r\nvoid FD_C_DestroyDetectionResult(\r\n    __fd_take FD_C_DetectionResult* fd_c_detection_result) {\r\n  if (fd_c_detection_result == nullptr) return;\r\n  // delete boxes\r\n  for (size_t i = 0; i < fd_c_detection_result->boxes.size; i++) {\r\n    delete[] fd_c_detection_result->boxes.data[i].data;\r\n  }\r\n  delete[] fd_c_detection_result->boxes.data;\r\n  // delete scores\r\n  delete[] fd_c_detection_result->scores.data;\r\n  // delete label_ids\r\n  delete[] fd_c_detection_result->label_ids.data;\r\n  // delete masks\r\n  for (size_t i = 0; i < fd_c_detection_result->masks.size; i++) {\r\n    delete[] fd_c_detection_result->masks.data[i].data.data;\r\n    delete[] fd_c_detection_result->masks.data[i].shape.data;\r\n  }\r\n  delete fd_c_detection_result;\r\n}\r\n```\r\n我添加一个释放方法\r\n```c\r\nvoid FD_C_DestroyOneDimDetectionResult(\r\n    __fd_take FD_C_OneDimDetectionResult* fd_c_one_dim_detection_result) {\r\n  for (int i = 0; i < fd_c_one_dim_detection_result->size; i++) {\r\n    FD_C_DestroyDetectionResult(fd_c_one_dim_detection_result->data + i);\r\n  }\r\n  delete[] fd_c_one_dim_detection_result->data;\r\n  delete fd_c_one_dim_detection_result;\r\n}\r\n```\r\n##### 在调用`FD_C_DestroyOneDimDetectionResult`的时候，走到`FD_C_DestroyDetectionResult`的最后一行`  delete fd_c_detection_result;`出问题了，具体问题：\r\n- 如果OneDimDetectionResult是里面有俩结果，在for循环中调用释放方法的时候i为0的时候没问题，i为1的时候报错了。直接崩溃崩溃在`delete fd_c_detection_result;`\r\n\r\n有大伙知道啥情况吗？有没有人测过？\r\n",
        "state": "open",
        "user": "ChaoII",
        "closed_by": null,
        "created_at": "2024-04-19T07:03:11+00:00",
        "updated_at": "2024-04-20T15:31:04+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII",
            "Jiang-Jia-Jun",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2440,
        "title": "FastDeploy 是否有适配 paddleocr box_type='poly' 的计划",
        "body": "paddleocr 目前支持 box_type='poly' 的文本框处理方式，在fastdeploy 的源码中并不支持，后续是否会跟进？",
        "state": "closed",
        "user": "RubinCarter",
        "closed_by": "heliqi",
        "created_at": "2024-04-22T11:12:07+00:00",
        "updated_at": "2024-05-17T04:05:37+00:00",
        "closed_at": "2024-05-17T04:05:37+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2441,
        "title": "c#部署fastdeploy时fastdeploy.vision.segmentation.PaddleSegModel model读取config文件时报错 Unexcepted preprocess operator: LoadImages.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【编译命令】\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\nmkdir build && cd build\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64 ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_PADDLE_BACKEND=ON ^\r\n         -DENABLE_OPENVINO_BACKEND=ON ^\r\n         -DENABLE_TRT_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DENABLE_TEXT=ON ^\r\n         -DWITH_GPU=ON ^\r\n         -DWITH_CAPI=ON ^\r\n         -DWITH_CSHARPAPI=ON ^\r\n         -DTRT_DIRECTORY=\"D:\\TensorRT-8.5.2.2\" ^\r\n         -DCUDA_DIRECTORY=\"D:\\cuda\\v112\" ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\paddle_fastdeploy\"\r\nnuget restore\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\nmsbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】：  Nvidia GPU 3060TI， CUDA 11.2 CUDNN 8.4.1.50  tensorRT8.5.2.2\r\n- 【编译语言】： C\r\n\r\n我将在下方详细地将编译部署过程解释\r\n（1）：clone fastdeploy代码\r\n（2）：在build文件夹中按照上方cmake，结果如下，无报错\r\n![1](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/428f1097-5cc6-4080-8b94-78e4e576a8be)\r\n（3）：进行nuget restore，报错如下\r\n![2](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/65f1cfe9-26e5-45e1-b5f9-e62635ff5356)\r\n原因是opencvsharp版本过高，与项目.net framework4.0不适配，要么将.net framework升至4.8，要么指定低版本opencvsharp，因为之前已经尝试过升级framework，这次尝试指定opencvsharp版本。更改FastDeploy\\csharp中的cmakelist.txt的内容，将PROPERTY VS_PACKAGE_REFERENCES  \"OpenCvSharp4_4.7.0.20230115\"中的版本更改为“OpenCvSharp4_4.2.0.20200108”，重新cmake，并nuget restore，无报错。\r\n（4）：进行msbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64操作，报错如下图\r\n![3](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/c53b2e03-aa16-4886-ad43-f327f8bbe7a6)\r\n原因是该项目与默认的c#10.0版本并不适配，因此修改FastDeploy\\build\\csharp\\fastdeploy_csharp.csproj，将其中的LangVersion版本均改为9.0（因为改为7.3时会报错部分语句仅在高于c#8.0时才存在，因此改为的9.0），重新进行上述语句操作，无报错如下图\r\n![4](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/600da1eb-8d4e-4ed6-8e17-9bd1777d2400)\r\n（5）：进行msbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64操作，无报错，如下图\r\n![5](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/e320d434-2cba-4d43-ae46-dca04f63b74a)\r\n（6）进入paddle_fastdeploy文件夹，新建bin文件夹，并在cmd中执行fastdeploy_init.bat install %cd% bin语句，将dll全部打包到bin文件夹，完成，结果如下图，总共44个文件，2.19g大小\r\n![6](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/01627755-a4f8-4407-84ea-aa8f9ddc37cc)\r\n（7）**使用语句python tools/export.py --config configs/ppmattingv2/ppmattingv2-stdc1-human_512.yml --model_path pretrained_models/ppmattingv2-stdc1-human_512.pdparams --save_dir output/inference_model --input_shape 1 3 1374 918\r\n对已训练的paddle模型进行动转静操作，该模型转换后的deploy.yaml信息如下图**\r\n![7](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/913d3c1d-e426-451f-92ed-6e81f6d71086)\r\n使用netron可视化查看模型输入输出结果如下\r\n![8](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/db013266-734d-4de4-8382-9ff25bcf46b8)\r\n（8）新建vs工程文件，将工程修改为release且x64，添加如图所示的相关引用，并将bin文件夹中所有dll复制到项目文件夹下的bin\\x64\\Release中，并写下如下测试代码，测试后得知箭头处为报错点，似乎为不识别配置文件中的loadimage\r\n![9](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/847d9685-328f-4555-a7ff-048db6a0b664)\r\n![10](https://github.com/PaddlePaddle/FastDeploy/assets/73456895/c6527bfa-3ab8-42a6-947a-e971defe85ae)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "state": "closed",
        "user": "gunh4mmer",
        "closed_by": "heliqi",
        "created_at": "2024-04-24T02:36:55+00:00",
        "updated_at": "2024-05-17T04:04:07+00:00",
        "closed_at": "2024-05-17T04:04:07+00:00",
        "comments_count": [
            "heliqi",
            "gunh4mmer",
            "gunh4mmer",
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2442,
        "title": "pyinstaller打包python项目时，遇到fastdeploy总是提示dll加载不了的问题，但是原始依赖包中就没有fastdeploy_main.dll文件",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： windows环境 cpu版  fastdeploy-python 1.0.7\r\n- 【编译命令】pip 安装\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- fastdeploy无法init启动\r\n-具体报错如下\r\n- Traceback (most recent call last):\r\n  File \"E:\\new_cell\\CellCount\\dist\\final\\fastdeploy\\c_lib_wrap.py\", line 165, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed while importing fastdeploy_main: 找不到指定的程序。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"InstanceProcess.py\", line 19, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 493, in exec_module\r\n  File \"cell_count_jetson\\infer_paddle_class.py\", line 14, in <module>\r\n    import fastdeploy as fd\r\n  File \"E:\\new_cell\\CellCount\\dist\\final\\fastdeploy\\__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"E:\\new_cell\\CellCount\\dist\\final\\fastdeploy\\c_lib_wrap.py\", line 168, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: DLL load failed while importing fastdeploy_main: 找不到指定的程序。\r\n[11364] Failed to execute script 'InstanceProcess' due to unhandled exception!\r\n",
        "state": "closed",
        "user": "zhly000",
        "closed_by": "heliqi",
        "created_at": "2024-04-26T02:24:32+00:00",
        "updated_at": "2024-06-08T09:36:02+00:00",
        "closed_at": "2024-05-17T04:03:44+00:00",
        "comments_count": [
            "heliqi",
            "monkeycc",
            "Firestick-Xia"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2443,
        "title": "2024了,希望支持3.11",
        "body": "2024了,\r\nwhl希望增加3.11\r\n\r\nhttps://www.paddlepaddle.org.cn/whl/fastdeploy.html",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "heliqi",
        "created_at": "2024-04-26T04:06:16+00:00",
        "updated_at": "2024-05-17T04:02:35+00:00",
        "closed_at": "2024-05-17T04:02:35+00:00",
        "comments_count": [
            "heliqi",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2444,
        "title": "FastDeploy预计啥时候支持PP OCR V4的模型？",
        "body": "FastDeploy预计啥时候支持PP OCR V4的模型？另外请教下，现阶段在线的星河PaddleX的模型，是否均支持使用FastDeploy进行推理，目前测试的是支持C++和Py，如果使用C#编译版是否也能支持使用？",
        "state": "closed",
        "user": "GentlerMan",
        "closed_by": "heliqi",
        "created_at": "2024-04-26T07:00:42+00:00",
        "updated_at": "2024-05-17T04:02:04+00:00",
        "closed_at": "2024-05-17T04:02:04+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2445,
        "title": "YOLO8Cls 支持不",
        "body": "YOLO8Cls 支持不\r\n\r\n使用 YOLOv5Cls Python 推理 YOLO8Cls模型\r\n结果和yolov8结果不一致",
        "state": "open",
        "user": "monkeycc",
        "closed_by": null,
        "created_at": "2024-04-29T10:38:32+00:00",
        "updated_at": "2024-04-29T10:38:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2448,
        "title": "在rk3588上推理insightface模型只能使用cpu不能用npu加速",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：\r\n- 【编译命令】rk3588按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rknpu2.md#%E7%BC%96%E8%AF%91fastdeploy-python-sdk编译\r\n- 【系统平台】: Linux version 5.10.110 (root@jensen) (aarch64-linux-gcc (ctng-1.25.0-119g-FA) 11.3.0\r\n- 【硬件】： friendly r6c\r\n- 【编译语言】： Python 3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/faceid/insightface/rknpu2/python/README_CN.md文档测试insightface，使用https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/faceid/insightface/rknpu2/python/infer_arcface.py脚本，python infer_arcface.py --model ms1mv3_arcface_r100.onnx                         --face face_0.jpg                         --face_positive face_1.jpg                         --face_negative face_2.jpg                         --device npu  命令推理onnx模型时报错：\r\n    [INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend     Runtime initialized with Backend::ORT in Device::CPU.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(401)::Infer      Failed to Infer: Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))\r\n[ERROR] fastdeploy/vision/faceid/contrib/insightface/base.cc(70)::BatchPredict  Failed to inference by runtime.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(401)::Infer      Failed to Infer: Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))\r\n[ERROR] fastdeploy/vision/faceid/contrib/insightface/base.cc(70)::BatchPredict  Failed to inference by runtime.\r\n[ERROR] fastdeploy/runtime/backends/ort/ort_backend.cc(401)::Infer      Failed to Infer: Unexpected input data type. Actual: (tensor(uint8)) , expected: (tensor(float))\r\n[ERROR] fastdeploy/vision/faceid/contrib/insightface/base.cc(70)::BatchPredict  Failed to inference by runtime.\r\n/home/pi/FastDeploy/examples/vision/faceid/insightface/python/infer_npu.py:12: RuntimeWarning: invalid value encountered in scalar divide\r\n  return mul_ab / (mul_a * mul_b)\r\nFaceRecognitionResult: [Empty Result]FaceRecognitionResult: [Empty Result]FaceRecognitionResult: [Empty Result]Cosine 01:  nan\r\nCosine 02:  nan\r\nRuntimeOption(\r\n  backend : Backend.ORT\r\n  cpu_thread_num : -1\r\n  device : Device.CPU\r\n  device_id : 0\r\n  external_stream : None\r\n  model_file : ms1mv3_arcface_r100.onnx\r\n  model_format : ModelFormat.ONNX\r\n  model_from_memory : False\r\n  openvino_option : <fastdeploy.libs.fastdeploy_main.OpenVINOBackendOption object at 0x7f8c2e48f0>\r\n  ort_option : <fastdeploy.libs.fastdeploy_main.OrtBackendOption object at 0x7f8c2e48f0>\r\n  paddle_infer_option : <fastdeploy.libs.fastdeploy_main.PaddleBackendOption object at 0x7f8c2e48f0>\r\n  paddle_lite_option : <fastdeploy.libs.fastdeploy_main.LiteBackendOption object at 0x7f8c2e48f0>\r\n  params_file : \r\n  poros_option : <fastdeploy.libs.fastdeploy_main.PorosBackendOption object at 0x7f8c2e48f0>\r\n  trt_option : <fastdeploy.libs.fastdeploy_main.TrtBackendOption object at 0x7f8c2e48f0>\r\n)\r\n",
        "state": "open",
        "user": "Rsndmmm",
        "closed_by": null,
        "created_at": "2024-05-03T15:30:17+00:00",
        "updated_at": "2024-05-03T15:30:21+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2447,
        "title": "C#不能和DirectML一起编译",
        "body": "## 环境\r\n- 【编译命令】\r\n```\r\ncmake .. -G \"Visual Studio 17 2022\" -A x64 ^\r\n\t -DWITH_DIRECTML=ON ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DWITH_CSHARPAPI=ON ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\Paddle\\compiled_fastdeploy\"\r\n```\r\n- 【系统平台】:  Windows x64(Windows11)\r\n- 【硬件】： Nvidia GTX 1650， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ / C#\r\n## 问题\r\nC#不能和DirectML一起编译\r\n会出现以下报错：\r\n```\r\nD:\\[Hided]\\FastDeploy\\csharp\\fastdeploy\\vision\\detection\\ppdet\\model.cs\r\nerror CS024: The type or namespace name 'Mat' could not be found (are you missing a using directive or an assembly reference?\r\n```\r\n```\r\nD:\\[Hided]\\FastDeploy\\csharp\\fastdeploy\\vision\\detection\\contrib\\model.cs\r\nerror CS024: The type or namespace name 'Mat' could not be found (are you missing a using directive or an assembly reference?\r\n```\r\n\r\n",
        "state": "open",
        "user": "luckystar5408",
        "closed_by": null,
        "created_at": "2024-05-01T08:21:36+00:00",
        "updated_at": "2024-05-01T08:21:40+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2446,
        "title": "yolo系列多batch推理有误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： release-v1.0.7\r\n- 【编译命令】:\r\n```\r\ncmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=ON \\\r\n         -DENABLE_OPENVINO_BACKEND=ON \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DWITH_GPU=ON \\\r\n         -DTRT_DIRECTORY=${Path/to/TensorRT} \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\r\n         -DENABLE_VISION=ON \\\r\n         -DOPENCV_DIRECTORY=${/usr/local/lib/cmake/opencv4}\r\nmake -j12\r\nmake install\r\n```\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： Nvidia GPU 3090， CUDA 11.8 CUDNN 8.3\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【yolo系列多batch推理有误】\r\n- 问题详细描述：例如在使用yolov5.onnx(yolov8.onnx也尝试过了，是一样的问题)推理时，当同一个batch中，有一张图没有推理结果时，那张图之后的图即使有推理结果，推理结果也为空。例如一共有四张图片，当batch size = 1时，第0,2,3张有推理结果，第1张没有推理结果，结果正确。当batch size = 4时，仅有第0张有推理结果，第1,2,3均没有推理结果，结果与batch size=1时不一致，结果错误。\r\n我的多batch推理代码：\r\n```\r\nvoid Model::ProcessBatchImage(std::vector<cv::Mat>& batch_images, std::vector<fastdeploy::vision::DetectionResult>* batch_results){\r\n    if (!model->BatchPredict(batch_images, batch_results)) {\r\n        std::cerr << \"Failed to predict batch.\" << std::endl;\r\n        return;\r\n    }\r\n    batch_images.clear();\r\n    batch_results->clear();\r\n}\r\n\r\nvoid Model::InferImagesBatch(const std::vector<std::string>& batch_files){\r\n    std::vector<cv::Mat> batch_images;\r\n    std::vector<std::string> batch_names;\r\n    std::vector<fastdeploy::vision::DetectionResult> batch_results;\r\n    batch_images.reserve(cfg.bs);\r\n\r\n    for (const auto& file_path : batch_files) {\r\n        auto image = cv::imread(file_path);\r\n        if (image.empty()) {\r\n            std::cerr << \"Failed to load image: \" << file_path << std::endl;\r\n            continue; // Skip this image and continue with the next\r\n        }\r\n        batch_images.push_back(image);\r\n        if (batch_images.size() == cfg.bs)\r\n            ProcessBatchImage(batch_images, &batch_results);\r\n    }\r\n\r\n    // 处理剩余的图片（如果有）\r\n    if (!batch_images.empty())\r\n        ProcessBatchImage(batch_images, &batch_results);\r\n}\r\n```\r\n更多代码可以参考：[https://github.com/LorenzoSun-V/LorenzoDeploy/tree/main/fd/src](https://github.com/LorenzoSun-V/LorenzoDeploy/tree/main/fd/src)中的model.cc，核心的就是`model->BatchPredict`这个代码。",
        "state": "open",
        "user": "LorenzoSun-V",
        "closed_by": null,
        "created_at": "2024-04-30T08:21:40+00:00",
        "updated_at": "2024-12-24T05:45:10+00:00",
        "closed_at": null,
        "comments_count": [
            "jia0511",
            "hhx518",
            "wuxie-k"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2449,
        "title": "yolov8目标检测如何修改iou",
        "body": "`model.postprocessor.iuo = 0.2`\r\nAttributeError: 'fastdeploy.libs.fastdeploy_main.vision.detection.YOLOv8Postprocessor' object has no attribute 'iuo'",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-07T06:57:32+00:00",
        "updated_at": "2025-05-13T06:50:20+00:00",
        "closed_at": "2025-05-13T06:50:20+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2450,
        "title": "paddleseg模型GPU部署推理结果有问题",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，fastdeploy-linux-gpu-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，Nvidia GPU GeForce 940MX， CUDA 11.6 CUDNN 8.4\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型精度问题】\r\n-- 按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/use_sdk_on_windows.md的操作步骤进行编译，使用paddleseg及ppmatting中的示例代码，对人像分割或抠图结果出错（CPU推理结果正确）\r\n![matting_input](https://github.com/PaddlePaddle/FastDeploy/assets/51151402/b76d7c03-e438-4a8a-a426-4e6a85088373)\r\n![visualized_result_fg](https://github.com/PaddlePaddle/FastDeploy/assets/51151402/8c41e521-5476-4c24-8f91-d80006ae2bd5)\r\n\r\n",
        "state": "closed",
        "user": "yunwuhen",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-07T07:57:20+00:00",
        "updated_at": "2025-05-13T06:50:21+00:00",
        "closed_at": "2025-05-13T06:50:21+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2451,
        "title": "支持yolov9不",
        "body": "请问最近有计划支持yolov9不",
        "state": "closed",
        "user": "SonwYang",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-10T08:38:19+00:00",
        "updated_at": "2025-05-20T06:44:45+00:00",
        "closed_at": "2025-05-20T06:44:45+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2452,
        "title": "FastDeploy有计划推通用Java平台的SDK吗",
        "body": "- 通用Java平台SDK有计划推出吗？如有，大概什么时候？\r\n- 现在准备用DJL暂时实现模型加载与推理，但前置处理如何做，或者说有无Python的参考示例呢？",
        "state": "open",
        "user": "TommysLee",
        "closed_by": null,
        "created_at": "2024-05-13T01:10:13+00:00",
        "updated_at": "2024-05-20T00:17:21+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "TommysLee"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2453,
        "title": "FastDeploy PPYoloE Plus 模型的前置处理实现在哪里可以查看源码",
        "body": "FastDeploy PPYoloE Plus 模型的前置处理实现在哪里可以查看源码\r\n把这个搞明白，准备基于DJL实现Java版的模型预测",
        "state": "closed",
        "user": "TommysLee",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-13T08:11:47+00:00",
        "updated_at": "2025-05-20T06:44:46+00:00",
        "closed_at": "2025-05-20T06:44:46+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2454,
        "title": "FastDeploy目前支持cyclegan的C++sdk部署吗",
        "body": "\r\n- 【FastDeploy版本】：fastdeploy-linux-aarch64-1.0.0\r\n- 【硬件】： 飞腾D2000\r\n- 【编译语言】： C++ \r\n\r\nFastDeploy目前支持cyclegan的C++或python版本的部署吗，在example和源码中没有看到相关的类\r\n",
        "state": "closed",
        "user": "duenyuduenyu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-14T12:16:04+00:00",
        "updated_at": "2025-05-20T06:44:47+00:00",
        "closed_at": "2025-05-20T06:44:47+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2455,
        "title": "hopper架构上安装后使用报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy_gpu_python-1.0.7-cp310-cp310-manylinux1_x86_64.whl\r\n- 【编译命令】pip install\r\n- 【系统平台】: Linux x64(centOS 7)\r\n- 【硬件】： Nvidia Hopper GPU， CUDA 12.0 CUDNN 8.3\r\n- 【编译语言】：Python 3.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n代码：https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/quick_start/models/python.md#1-%E8%8E%B7%E5%8F%96%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F\r\n\r\n`import fastdeploy as fd\r\n\r\nmodel_url = \"https://bj.bcebos.com/paddlehub/fastdeploy/ppyoloe_crn_l_300e_coco.tgz\"\r\nimage_url = \"https://bj.bcebos.com/fastdeploy/tests/test_det.jpg\"\r\nfd.download_and_decompress(model_url, path=\".\")\r\nfd.download(image_url, path=\".\")`\r\n\r\n## 报错信息\r\n(paddle-2.6.1) [root@Hopper-Paddle-BBC test]# python fd1.py \r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/paddle-2.6.1/lib/python3.10/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/test/fd1.py\", line 1, in <module>\r\n    import fastdeploy as fd\r\n  File \"/root/miniconda3/envs/paddle-2.6.1/lib/python3.10/site-packages/fastdeploy/__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"/root/miniconda3/envs/paddle-2.6.1/lib/python3.10/site-packages/fastdeploy/c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
        "state": "open",
        "user": "onecatcn",
        "closed_by": null,
        "created_at": "2024-05-16T06:19:34+00:00",
        "updated_at": "2024-08-12T13:15:14+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "tymons"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2456,
        "title": "希望增加pp-shitu部署的更多示例",
        "body": "pp-shitu只有det和rec单独使用的demo\r\n希望增加Pipeline完整示例",
        "state": "closed",
        "user": "lemon3853",
        "closed_by": "lemon3853",
        "created_at": "2024-05-16T09:44:23+00:00",
        "updated_at": "2024-06-10T07:59:17+00:00",
        "closed_at": "2024-06-10T07:59:17+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "lemon3853"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2457,
        "title": "TensorRT后端能否生成engin文件，下次启动直接打开engin文件，不用每次生成。",
        "body": "借鉴Nvidia DeepStream框架的思路，第一次生成TensorRT的engin文件，下次找到文件就直接打开使用，程序启动就快多了。现在每次启动都要生成一遍，等待时间太长。",
        "state": "closed",
        "user": "lvrh2018",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-17T02:38:32+00:00",
        "updated_at": "2025-06-10T06:50:04+00:00",
        "closed_at": "2025-06-10T06:50:04+00:00",
        "comments_count": [
            "lvrh2018",
            "bltcn"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2461,
        "title": "Does fastdeploy have a docker image about the python version?",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 1.0.7 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3070 CUDA 11.8 CUDNN 8.3\r\nProgram Language: e.g. Python 3.9\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\nI want to deploy a python version of fastdeploy-gpu mirror environment, which contains paddlenlp, paddlepaddle, and the paddle mirror is too large",
        "state": "open",
        "user": "watertianyi",
        "closed_by": null,
        "created_at": "2024-05-28T03:50:21+00:00",
        "updated_at": "2024-05-28T03:50:24+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2458,
        "title": "RK3588上部署FastDeploy，模型推理无法推理，提示\"DBDetector\" object has no attribute 'preprocessor''",
        "body": "一、前置环境：在x86环境的Ubuntu中，将paddleocr模型转换为onnx，并将onnx转换为rknn模型一切顺利。\r\n二、问题：\r\n在rk3588上执行ocr的rknn模型时提示：[ERROR]:fastdeploy/fastdeploy_model.cc(183)::CreateRKNPUBackend There's no valid npu backends for model: ppocr/ocr_det\r\n\r\n1. 推理环境：RK3588 + Ubuntu20.04\r\n\r\n2. FastDeploy部署：\r\n【编译命令】通过官方指导文档安装部署FastDeploy\r\n【系统平台】:RK3588\r\n【硬件】： NPU\r\n【编译语言】： Python 3.8\r\n编译方法：\r\n```\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy\r\n\r\ncd python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_RKNPU2_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport RKNN2_TARGET_SOC=RK3588\r\n\r\npython3 setup.py build\r\npython3 setup.py bdist_wheel\r\ncd dist\r\npip3 install fastdeploy_python-0.0.0-cp38-cp38-linux_aarch64.whl\r\n```\r\n执行完提示错误：不论是cpu还是npu都存在问题。\r\n![微信图片_20240520235431](https://github.com/PaddlePaddle/FastDeploy/assets/57511184/01f8e98d-ade7-42b5-b8a1-18a7940699cb)\r\n\r\n`/usr/local/python3/bin/python3.8 /opt/content_detecion/FastDeploy-develop/examples/vision/ocr/PP-OCR/rockchip/python/infer.py --det_model /opt/rknn_model/ch_PP-OCRv3_det_infer/ch_PP-OCRv3_det_infer_rk3588_unquantized.rknn --rec_model /opt/rknn_model/ch_PP-OCRv3_rec_infer/ch_PP-OCRv3_rec_infer_rk3588_unquantized.rknn --cls_model /opt/rknn_model/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v20_cls_infer_rk3588_unquantized.rknn --rec_label_file /opt/content_detecion/paddleOCR/ppocr.txt --image /opt/content_detecion/paddleOCR/12.jpg --device npu\r\n\r\n\r\n[ERROR] fastdeploy/fastdeploy_model.cc(183)::CreateRKNPUBackend There's no valid npu backends for model: ppocr/ocr_det\r\n[ERROR] fastdeploy/vision/ocr/ppocr/dbdetector.cc(47)::Initialize       Failed to initialize fastdeploy backend.\r\nTraceback (most recent call last):\r\n  File \"/opt/content_detecion/FastDeploy-develop/examples/vision/ocr/PP-OCR/rockchip/python/infer.py\", line 94, in <module>\r\n    det_model = fd.vision.ocr.DBDetector(\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/fastdeploy/vision/ocr/ppocr/__init__.py\", line 41, in __init__\r\n    assert self.initialized, \"DBDetector initialize failed.\"\r\nAssertionError: DBDetector initialize failed.`",
        "state": "closed",
        "user": "carakiacc",
        "closed_by": "carakiacc",
        "created_at": "2024-05-20T16:17:57+00:00",
        "updated_at": "2024-05-24T06:05:15+00:00",
        "closed_at": "2024-05-24T06:05:15+00:00",
        "comments_count": [
            "carakiacc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2459,
        "title": "deeplabv3p的onnx模型使用GPU+FP16推理耗时增加",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：release 1.0.0\r\n- 【编译命令】\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： Nvidia GPU 3060， CUDA 11.8 CUDNN 8.5\r\n- 【编译语言】： C++\r\n\r\n【性能问题】\r\ndeeplabv3p的onnx模型使用GPU+FP16推理耗时增加，将近翻倍。\r\n",
        "state": "closed",
        "user": "zyz207",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-22T08:19:17+00:00",
        "updated_at": "2025-05-27T06:44:33+00:00",
        "closed_at": "2025-05-27T06:44:33+00:00",
        "comments_count": [
            "heliqi",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2463,
        "title": "README 文档在黑色的 Github 主题下，无法看清“推理后端及能力”的表格表头。",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【系统平台】: Github 主题颜色设置成黑色。\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 访问 https://github.com/PaddlePaddle/FastDeploy/blob/develop/README_CN.md 并在预览（preview）的状态下观看 README.md 文档\r\n- \"推理后端及能力\" 表格的表头如下截图。\r\n- 第一次看见表格时，完全不知道表格中的内容是什么意思。无意中发现表头中有文字并手动切换成浅色主题，才能清晰看见表头。\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/22042246/1295de0b-8e78-49b1-8fcb-0962def2e322)\r\n",
        "state": "open",
        "user": "Atom2004",
        "closed_by": null,
        "created_at": "2024-05-30T07:49:53+00:00",
        "updated_at": "2024-05-30T07:49:57+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2460,
        "title": "fastdeploy的sophgo开发案例，ppyoloe正常转模型及推理，转picodet模型为bmodel正常，推理结果出错，yolov8还未开始调试",
        "body": "请各位大佬进行指教，具体信息及操作如下：\r\n\r\n## 环境\r\n- 【FastDeploy版本】: 自行编译\r\n- 【编译命令】自行编译的FastDeploy，参考[SOPHGO 部署库编译](https://github.com/PaddlePaddle/FastDeploy/blob/release/1.0.7/docs/cn/build_and_install/sophgo.md)  进行的python版本编译，SOPHGO 的环境版本统一为0.4.9\r\n- 【系统平台】: 系统--Ubuntu 20.04.4  硬件架构--arm64\r\n- 【硬件】： 具体硬件型号，sophgo  bm1684 aarch64\r\n- 【编译语言】：  Python 3.8.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n1. 按照ppyoloe转模型案例，跑通ppyoloe模型的转换及推理，print打印的结果及result图片正常显示\r\n2. 根据ppyoloe转模型案例，picodet模型的bmodel转换成功，但是推理结果不正确。转模型具体操作命令如下：\r\n- paddle2onnx --model_dir picodet_s_320_coco_lcnet --model_filename model.pdmodel --params_filename model.pdiparams --save_file picodet_s_320_coco_lcnet.onnx --enable_dev_version True\r\n- python -m paddle2onnx.optimize --input_model picodet_s_320_coco_lcnet.onnx --output_model picodet_s_320_coco_lcnet.onnx --input_shape_dict \"{'image':[1,3,640,640]}\"\r\n- model_transform.py --model_name picodet_s_320_coco_lcnet --model_def ../picodet_s_320_coco_lcnet.onnx --input_shapes [[1,3,640,640],[1,2]] --keep_aspect_ratio --pixel_format rgb --output_names p2o.Div.79,p2o.Concat.9 --mlir picodet_s_320_coco_lcnet.mlir\r\n- model_deploy.py --mlir picodet_s_320_coco_lcnet.mlir --quantize F32 --chip bm1684 --model picodet_s_320_coco_lcnet_f32.bmodel\r\n\r\n3.推理部分代码如下图：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/96225888/bf6501f9-8d9c-4852-b432-b49bc0d029b4)\r\n\r\n4.推理print的结果如下图：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/96225888/a23f8f01-d792-4a72-80df-ad5d2468273c)\r\n\r\n5.推理后的sophgo_result_picodet.jpg无改变：\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/96225888/bb5e4f7e-5446-4fbf-966f-28891ba25c6b)\r\n\r\n\r\n",
        "state": "closed",
        "user": "yscolc",
        "closed_by": "yscolc",
        "created_at": "2024-05-24T08:15:04+00:00",
        "updated_at": "2024-05-29T10:03:46+00:00",
        "closed_at": "2024-05-29T10:03:45+00:00",
        "comments_count": [
            "heliqi",
            "yscolc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2462,
        "title": "Error [ERR_REQUIRE_ESM]: require() of ES Module d3-polygon from @paddle-js-models\\ocr not supported",
        "body": "## Environment\r\n\r\n**Paddle OCR JS**\r\n\r\n1. Using this create sveltekit app\r\n```\r\nnpm create vite@latest\r\n```\r\n2. Install `paddlejs`\r\n\r\n```\r\nnpm i \"@paddlejs/paddlejs-core\"\r\nnpm i \"@paddle-js-models/ocr\"\r\n```\r\n\r\n3. Use in app in `Counter.svelte`\r\n\r\n```\r\nimport * as ocr from \"@paddle-js-models/ocr\";\r\n```\r\n\r\nNow following error occurs.\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n\r\n```\r\nError [ERR_REQUIRE_ESM]: require() of ES Module C:\\Projects\\ocr-test\\node_modules\\d3-polygon\\src\\index.js from C:\\Projects\\ocr-test\\node_modules\\@paddle-js-models\\ocr\\lib\\index.js not supported.\r\nInstead change the require of C:\\Projects\\ocr-test\\node_modules\\d3-polygon\\src\\index.js in C:\\Projects\\ocr-test\\node_modules\\@paddle-js-models\\ocr\\lib\\index.js to a dynamic import() which is available in all CommonJS modules.\r\n```",
        "state": "open",
        "user": "kryptoniancode",
        "closed_by": null,
        "created_at": "2024-05-28T15:00:20+00:00",
        "updated_at": "2024-08-06T07:46:11+00:00",
        "closed_at": null,
        "comments_count": [
            "RomanczuG",
            "kryptoniancode"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2464,
        "title": "编译成功，CPU运行成功  GPU运行失败",
        "body": "question",
        "state": "open",
        "user": "huoxicai",
        "closed_by": null,
        "created_at": "2024-05-31T04:29:27+00:00",
        "updated_at": "2024-06-01T05:10:05+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2465,
        "title": "在华为昇腾Atlas 300推理卡上编译安装FastDeploy，后尝试部署uie模型出现问题。[ERROR] fastdeploy/runtime/runtime_option.cc(181)::UsePaddleBackend\tThe FastDeploy didn't compile with Paddle Inference.",
        "body": "\r\n## 环境\r\n\r\n- 【FastDeploy版本】： npu自行编译\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Linux arm(Ubuntu 18.04) \r\n- 【硬件】： 昇腾Atlas 300\r\n- 【编译语言】： Python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\nroot@ecs-1e0c:/Work# cd /Work/FastDeploy/examples/text/uie/python\r\nroot@ecs-1e0c:/Work/FastDeploy/examples/text/uie/python# python infer.py --model_dir ./uie-base --device cpu\r\n[ERROR] fastdeploy/runtime/runtime_option.cc(181)::UsePaddleBackend\tThe FastDeploy didn't compile with Paddle Inference.\r\nAborted (core dumped)\r\nroot@ecs-1e0c:/Work/FastDeploy/examples/text/uie/python# \r\n",
        "state": "open",
        "user": "Piterang",
        "closed_by": null,
        "created_at": "2024-05-31T08:23:29+00:00",
        "updated_at": "2024-07-25T11:12:22+00:00",
        "closed_at": null,
        "comments_count": [
            "Piterang",
            "Irisnotiris"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2466,
        "title": "rk3588部署飞浆ocr使用命令行代码可以运行，但是使用python脚本调用出现问题",
        "body": "\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-python-0.0.0\r\n- 【编译命令】官方教程编译\r\n- 【系统平台】: Linux version5.10,110-rockchip-rk3588(root@orangepi5)(aarch64-linux-gnu-gcc）\r\n- 【硬件】： rk 588\r\n- 【编译语言】： C++ / Python3.9\r\n\r\n问题描述\r\n\r\n大佬们好，我是小白，我用c++和python编译都成功了，用命令行的形式也能够成功运行，我想用pycharm运行infer.py脚本，我将模型图片等都直接在代码中修改了，然后报错RuntimeError: FastDeploy initalized failed! Error: libopencv_flann.so.3.4: cannot open shared object file: No such file or directory\r\n这个问题我在用命令行调用的时候也出现过，我通过添加环境变量解决了这个问题，我也想用同样的方式解决脚本这个问题，通过百度到的方法，例如在代码中用os库添加环境变量，用vim改写~/.bashrc，都试过了，还是无法解决这个问题，请问大佬们还有什么解决方式吗\r\n",
        "state": "open",
        "user": "sobrilliant1",
        "closed_by": null,
        "created_at": "2024-06-04T10:38:08+00:00",
        "updated_at": "2024-10-21T03:33:02+00:00",
        "closed_at": null,
        "comments_count": [
            "sobrilliant1",
            "13307790006",
            "13307790006",
            "sobrilliant1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2469,
        "title": "FastDeploy-release-1.0.7  单个模型在70ms作用，piplin需要300-500毫秒",
        "body": "版本FastDeploy-release-1.0.7/examples/\r\n运行的示例：examples/vision/keypointdetection\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in \r\n[FastDeploy] PPDet in RKNPU2 duration = 0.443722s.\r\n硬件 rk3588模型是 rknn\r\ntinypose_256x192\r\npicodet_s_416_coco\r\n单个模型在70ms作用，piplin需要300-500毫秒",
        "state": "open",
        "user": "13fengfeng",
        "closed_by": null,
        "created_at": "2024-06-06T08:31:37+00:00",
        "updated_at": "2024-06-06T08:49:10+00:00",
        "closed_at": null,
        "comments_count": [
            "13fengfeng",
            "13fengfeng",
            "13fengfeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2467,
        "title": "Failed to build on Ubuntu 20",
        "body": "## Environment\r\n\r\nFastDeploy version: 1.0.7\r\nOS Platform: Linux x64 (Ubuntu 20.04)\r\nHardware: AMD Ryzen 5, GeForce RTX 3060\r\nProgram Language: C#\r\n\r\n## Problem description\r\nAfter compiling FastDeploy, application is built successfully, but failed to run\r\n\r\nlibonnxruntime.so.1.12.0: cannot open shared object file: No such file or directory\r\n/usr/share/dotnet/shared/Microsoft.NETCore.App/8.0.4/fastdeploy.dll.so: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/fastdeploy.dll.so: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/runtimes/linux-x64/native/libfastdeploy.dll.so: cannot open shared object file: No such file or directory\r\n/usr/share/dotnet/shared/Microsoft.NETCore.App/8.0.4/libfastdeploy.dll.so: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/libfastdeploy.dll.so: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/runtimes/linux-x64/native/fastdeploy.dll: cannot open shared object file: No such file or directory\r\n/usr/share/dotnet/shared/Microsoft.NETCore.App/8.0.4/fastdeploy.dll: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/fastdeploy.dll: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/runtimes/linux-x64/native/libfastdeploy.dll: cannot open shared object file: No such file or directory\r\n/usr/share/dotnet/shared/Microsoft.NETCore.App/8.0.4/libfastdeploy.dll: cannot open shared object file: No such file or directory\r\n/project_path/bin/Debug/net8.0/libfastdeploy.dll: cannot open shared object file: No such file or directory\r\n\r\nCould you suggest a detailed document to compile and link dynamic library to the current project?",
        "state": "closed",
        "user": "danhdoan",
        "closed_by": "danhdoan",
        "created_at": "2024-06-05T04:34:08+00:00",
        "updated_at": "2024-06-27T03:36:02+00:00",
        "closed_at": "2024-06-27T03:36:02+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "danhdoan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2468,
        "title": "pp-ocr服务化部署问题",
        "body": "之前生产环境用的PaddleOCR的hubserving，现在切换到fastdeploy，部署的pp-ocr服务，8000端口也起来了， 想在业务端http调用，发现request参数是需要做图片处理的，要用到cv库，如果我只想传图片b64，是否要自己用py再写一个“客户端服务”",
        "state": "closed",
        "user": "yss0729",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-05T15:22:31+00:00",
        "updated_at": "2025-06-17T06:49:52+00:00",
        "closed_at": "2025-06-17T06:49:52+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2473,
        "title": "服务化部署。http调用返回识别结果正常，grpc调用特殊符号部分会乱码",
        "body": "## 环境\r\n- 【FastDeploy版本】： fastdeploy:1.0.4-cpu-only-21.10\r\n## 问题日志及出现问题的操作流程\r\n都是在同一个java类文件里执行识别同一张图片，http能正常显示应该不是我这边的编码问题吧。还有grpc提供的一个校验btyeString是否为utf-8的方法也是返回true的，但是我使用对应的toStringUtf8()方法输出字符串却是识别内容部分正常，其它部分乱码，这返回结果我没办法进一步处理了。我想请教一下是我初学grpc没搞明白原理，还是确实是服务化部署代码grpc接口这块就是有问题的需要修正。\r\n\r\n![QQ截图20240615232630](https://github.com/PaddlePaddle/FastDeploy/assets/41407267/4b4c7cf4-0d54-4992-8864-27cf9cdfec59)\r\n\r\n",
        "state": "open",
        "user": "zhouyiminga",
        "closed_by": null,
        "created_at": "2024-06-15T16:10:58+00:00",
        "updated_at": "2024-06-15T16:11:03+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2471,
        "title": "C#编译完成后部署异常返回值为0xc00000374",
        "body": "按照教程下载fastdeploy-develop正常编译了C#版本的FastDeploy，在部署使用的过程中，同一台设备，同样的运行环境，无论是使用PPOCR还是官网的PPYoloE示例，有时候推理正常显示，有时候就无故异常退出了，在VS里面显示返回值为0xc00000374，这个问题在issues#1993 里面曾提出，但时隔一年，使用最新版的源码编译还是存在这个问题，烦请看下是否可以修复？",
        "state": "open",
        "user": "GentlerMan",
        "closed_by": null,
        "created_at": "2024-06-13T00:53:53+00:00",
        "updated_at": "2024-08-01T07:53:51+00:00",
        "closed_at": null,
        "comments_count": [
            "GentlerMan",
            "julyff",
            "GentlerMan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2470,
        "title": "RuntimeError: FastDeploy initalized failed! Error: DLL load failed while importing fastdeploy_main: 找不到指定的程序。",
        "body": "版本兼容，报错\r\npaddlepaddle-gpu    2.6.1.post117\r\nfastdeploy-gpu-python   1.0.7 \r\n`import paddle\r\nimport fastdeploy`\r\n报错RuntimeError: FastDeploy initalized failed! Error: DLL load failed while importing fastdeploy_main: 找不到指定的程序。\r\n单独只是用`import fastdeploy` 代码正常运行",
        "state": "closed",
        "user": "Firestick-Xia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-09T11:41:24+00:00",
        "updated_at": "2025-06-17T06:49:54+00:00",
        "closed_at": "2025-06-17T06:49:54+00:00",
        "comments_count": [
            "juncaipeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2472,
        "title": "FastDeploy推理PPOCR过程加载字库文件ppocr_keys_v1.txt出现的编码报错的异常",
        "body": "在Windows使用C#版本编译的fastDeploy中的PPOCRV3推理过程，按照教程里面加载字典模型，官方提供的字库链接（https://gitee.com/paddlepaddle/PaddleOCR/raw/release/2.6/ppocr/utils/ppocr_keys_v1.txt）\r\n但是有个问题是，默认这个文件是UTF-8编码的，使用该模式直接加载文件，使用fastDeploy推理时，会出现以下报错：\r\n\r\n![image](https://github.com/PaddlePaddle/FastDeploy/assets/48674724/45e8c597-6904-4e76-96a2-c9d2b5222da5)\r\n\r\n\r\n当把文件使用记事本工具打开并以ANSI编码保存后重新加载，才能正常推理，请问下fastDeploy是否提供了读取该TXT的编码选型，方便直接读取UTF8编码模式的字库？\r\n",
        "state": "closed",
        "user": "GentlerMan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-13T01:13:22+00:00",
        "updated_at": "2025-06-17T06:49:55+00:00",
        "closed_at": "2025-06-17T06:49:55+00:00",
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2474,
        "title": "模型服务化部署是否支持yolov8",
        "body": "你好，在文档里看到可以支持yolov5服务化部署，请问是否支持v8呢？多谢",
        "state": "closed",
        "user": "Syorst",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-17T09:12:15+00:00",
        "updated_at": "2025-06-24T06:46:19+00:00",
        "closed_at": "2025-06-24T06:46:19+00:00",
        "comments_count": [
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2477,
        "title": "Serving encrypted models",
        "body": "Hello! Is it possible to run an encrypted model using fastdeploy server?\r\nI have a few guesses.\r\n1. The fastdeploy backend looks for 'model.onnx' or 'model.pdmodel'. Perhaps registry \"paddlepaddle/fastdeploy:1.0.7-cpu-only-21.10\" is compiled with ENABLE_ENTRYPTION=OFF and if I rebuild the image with ENABLE_ENTRYPTION=ON it will work. Or should I just rename the model \"__model__.encrypted\" -> \"model.onnx\"? Then how can I specify the key in the configuration (config.pbtxt)?\r\n2. This is only possible through the Python Triton Model.\r\nThank you in advance.",
        "state": "open",
        "user": "ErshovVE",
        "closed_by": null,
        "created_at": "2024-06-20T09:21:24+00:00",
        "updated_at": "2024-06-20T09:21:29+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2475,
        "title": "C++使用预编译库预测pp_liteseg模型时结果内score_map为什么是空值，图像可视化已经正确标准，到处模型是也设置--output_op none,测试好几个模型score_map都是空值，不明白是为什么",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n- 【FastDeploy版本】：fastdeploy-windows-gpu-1.0.7\r\n- 【编译命令】预编译库\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4060， CUDA 11.4 CUDNN 8.2\r\n- 【编译语言】： C++ \r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "shilizhongyuan",
        "closed_by": null,
        "created_at": "2024-06-17T09:39:49+00:00",
        "updated_at": "2024-07-23T09:55:52+00:00",
        "closed_at": null,
        "comments_count": [
            "viwe-monai"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2481,
        "title": "How to read camera for super Resolution",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux aarch64\r\nHardware: e.g. Jetson xavier NX \r\nProgram Language: e.g. C++\r\n\r\n## Problem description\r\nHow to read usb camera and put each frame into super Resolution algorithm instead of using std::vector<cv::Mat>\r\n",
        "state": "open",
        "user": "vongocminh778",
        "closed_by": null,
        "created_at": "2024-06-27T13:39:40+00:00",
        "updated_at": "2024-06-27T13:39:44+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2480,
        "title": "在运行contrib的分类模型（非ppcls）的模型结果不对",
        "body": "版本：develop c++\r\n问题：\r\n在运行resnet18分类模型时[fastdeploy/vision/classification/contrib/resnet.cc](https://github.com/PaddlePaddle/FastDeploy/blob/cd0ee79c91d4ed1103abdc65ff12ccadd23d0827/fastdeploy/vision/classification/contrib/resnet.cc#L91) 使用function::SoftMax()后结果异常，score全部为1，如果删除softmax调用，分类结果正常只是score不是0-1之间的概率。",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "ChaoII",
        "created_at": "2024-06-24T09:57:54+00:00",
        "updated_at": "2024-06-27T00:48:04+00:00",
        "closed_at": "2024-06-27T00:48:03+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2476,
        "title": "如何修改使http请求能支持传入图片的base64编码？",
        "body": "\r\n- 【FastDeploy版本】： 最新版镜像\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【硬件】： Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n\r\n```\r\npayload = {\r\n  \"inputs\" : [\r\n    {\r\n      \"name\" : \"INPUT\",\r\n      \"shape\" : image.shape,\r\n      \"datatype\" : \"UINT8\",\r\n      \"data\" : image.tolist()\r\n    }\r\n  ],\r\n  \"outputs\" : [\r\n    {\r\n      \"name\" : \"\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n在http服务中传递list太慢了，如果希望datatype是base64编码，要如何修改代码？",
        "state": "closed",
        "user": "ignore1999",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-06-19T06:13:38+00:00",
        "updated_at": "2025-06-24T06:46:20+00:00",
        "closed_at": "2025-06-24T06:46:20+00:00",
        "comments_count": [
            "KyleWang-Hunter",
            "ignore1999",
            "ignore1999",
            "KyleWang-Hunter"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2482,
        "title": "编译android的aar包失败，希望能集成RKNPU2，能否给出更详细的操作步骤",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 目标\r\n编译集成RKNPU2的android部署环境\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-rknpu-develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n```\r\ncmake ..  -DCMAKE_C_COMPILER=/home/lemon/FastDeploy/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc \\\r\n          -DCMAKE_CXX_COMPILER=/home/lemon/FastDeploy/gcc-linaro-6.3.1-2017.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++ \\\r\n          -DCMAKE_TOOLCHAIN_FILE=./../cmake/toolchain.cmake \\\r\n          -DENABLE_LITE_BACKEND=ON \\\r\n          -DTARGET_ABI=arm64 \\\r\n          -DENABLE_ORT_BACKEND=OFF \\\r\n\t      -DENABLE_RKNPU2_BACKEND=ON \\\r\n\t      -DENABLE_VISION=ON \\\r\n\t      -DRKNN2_TARGET_SOC=RK356X \\\r\n          -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.0\r\n```\r\n- 【系统平台】: Linux x64(Ubuntu 20.04)\r\n- 【硬件】： 说明具体硬件型号，如 CPU\r\n- 【编译语言】： C++ / Python(3.7或3.8等）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【编译缺失依赖】\r\n同此issue，官方说会更新，但是至今没有解决\r\n`https://github.com/PaddlePaddle/FastDeploy/issues/2215`\r\n`https://bj.bcebos.com/fastdeploy/third_libs/lite-linux-arm64-20230316.tgz`\r\n- 【替换成旧版本lite-linux-arm64后，打包aar报错】\r\n```\r\n  [22/22] Linking CXX shared library C:\\Users\\lemon\\Documents\\java\\android\\fastdeploy\\build\\intermediates\\cxx\\Debug\\5f16o4i4\\obj\\arm64-v8a\\libfastdeploy_jni.so\r\n  FAILED: C:/Users/lemon/Documents/java/android/fastdeploy/build/intermediates/cxx/Debug/5f16o4i4/obj/arm64-v8a/libfastdeploy_jni.so \r\n  cmd.exe /C \"cd . && C:\\Users\\lemon\\AppData\\Local\\Android\\Sdk\\ndk\\25.1.8937393\\toolchains\\llvm\\prebuilt\\windows-x86_64\\bin\\clang++.exe --target=aarch64-none-linux-android21 --sysroot=C:/Users/lemon/AppData/Local/Android/Sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/windows-x86_64/sysroot -fPIC -Wno-format -ffast-math -Ofast -DNDEBUG -fomit-frame-pointer -fno-asynchronous-unwind-tables -fno-unwind-tables -fvisibility=hidden -fvisibility-inlines-hidden -fdata-sections -ffunction-sections -fno-limit-debug-info  -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Wl,--gc-sections -Wl,--no-undefined -Qunused-arguments  -Wl,--gc-sections -Wl,-z,nocopyreloc -shared -Wl,-soname,libfastdeploy_jni.so -o C:\\Users\\lemon\\Documents\\java\\android\\fastdeploy\\build\\intermediates\\cxx\\Debug\\5f16o4i4\\obj\\arm64-v8a\\libfastdeploy_jni.so CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/bitmap_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/assets_loader_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/runtime_option_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/results_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/visualize_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/pipeline/ppocr_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/pipeline/pipeline_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/detection/picodet_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/detection/detection_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/classification/paddleclas_model_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/classification/classification_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/segmentation/paddleseg_model_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/segmentation/segmentation_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/facedet/scrfd_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/facedet/yolov5face_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/facedet/facedet_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/keypointdetection/pptinypose_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/vision/keypointdetection/keypointdetection_utils_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/text/text_results_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/text/uie/uie_model_jni.cc.o CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/text/uie/uie_utils_jni.cc.o  -ljnigraphics  C:/Users/lemon/Documents/java/android/fastdeploy/libs/fastdeploy-android-latest-shared-dev/lib/arm64-v8a/libfastdeploy.so  C:/Users/lemon/Documents/java/android/fastdeploy/libs/fastdeploy-android-latest-shared-dev/third_libs/install/rknpu2_runtime/lib/librknnrt.so  C:/Users/lemon/Documents/java/android/fastdeploy/libs/fastdeploy-android-latest-shared-dev/third_libs/install/paddlelite/lib/arm64-v8a/libpaddle_full_api_shared.so  C:/Users/lemon/Documents/java/android/fastdeploy/libs/fastdeploy-android-latest-shared-dev/third_libs/install/opencv/sdk/native/libs/arm64-v8a/libopencv_java4.so  C:/Users/lemon/AppData/Local/Android/Sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/windows-x86_64/sysroot/usr/lib/aarch64-linux-android/21/liblog.so  -lGLESv2  -lEGL  C:/Users/lemon/AppData/Local/Android/Sdk/ndk/25.1.8937393/toolchains/llvm/prebuilt/windows-x86_64/sysroot/usr/lib/aarch64-linux-android/21/liblog.so  -lGLESv2  -lEGL  -latomic -lm && cd .\"\r\n  ld: error: undefined symbol: fastdeploy::RuntimeOption::SetLiteOptimizedModelDir(std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)\r\n  >>> referenced by runtime_option_jni.cc\r\n  >>>               CMakeFiles/fastdeploy_jni.dir/fastdeploy_jni/runtime_option_jni.cc.o:(fastdeploy::jni::NewCxxRuntimeOption(_JNIEnv*, _jobject*))\r\n```\r\n",
        "state": "open",
        "user": "lemon3853",
        "closed_by": null,
        "created_at": "2024-06-30T12:49:15+00:00",
        "updated_at": "2024-06-30T12:49:20+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2478,
        "title": "在rk3588上部署paddleseg模型，发现在性能十分不如意，有没有办法提高npu的占用率或者是多核调用？",
        "body": "\r\n在瑞芯微rk3588上部署paddleseg的模型（如pp-liteseg），发现在npu只调用Core0，且占用率只有5%-15%左右，有没有办法提高npu的占用率或者是多核调用？python语言下,预测代码如下所示：\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\n\r\ndef parse_arguments():\r\n    import argparse\r\n    import ast\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \"--model_file\", required=True, help=\"Path of PaddleSeg model.\")\r\n    parser.add_argument(\r\n        \"--config_file\", required=True, help=\"Path of PaddleSeg config.\")\r\n    parser.add_argument(\r\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\r\n    return parser.parse_args()\r\n\r\n\r\ndef build_option(args):\r\n    option = fd.RuntimeOption()\r\n    option.use_rknpu2()\r\n    return option\r\n\r\nargs = parse_arguments()\r\n\r\n# setup runtime\r\nruntime_option = build_option(args)\r\nmodel_file = args.model_file\r\nparams_file = \"\"\r\nconfig_file = args.config_file\r\nmodel = fd.vision.segmentation.PaddleSegModel(\r\n    model_file,\r\n    params_file,\r\n    config_file,\r\n    runtime_option=runtime_option,\r\n    model_format=fd.ModelFormat.RKNN)\r\n\r\nmodel.preprocessor.disable_normalize()\r\nmodel.preprocessor.disable_permute()\r\n\r\n# predict\r\nim = cv2.imread(args.image)\r\nresult = model.predict(im)\r\nprint(result)\r\n\r\n# visualize\r\nvis_im = fd.vision.vis_segmentation(im, result, weight=0.5)\r\ncv2.imwrite(\"vis_img.png\", vis_im)",
        "state": "closed",
        "user": "Gary-zyt",
        "closed_by": "Gary-zyt",
        "created_at": "2024-06-21T06:04:23+00:00",
        "updated_at": "2024-12-09T14:07:47+00:00",
        "closed_at": "2024-06-25T03:56:20+00:00",
        "comments_count": [
            "Gary-zyt",
            "Jiang-Jia-Jun",
            "Gary-zyt",
            "NyquistBodeTu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2479,
        "title": "从内存中加载模型时报错：进程已结束，退出代码为 -1073741819 (0xC0000005)",
        "body": "## 环境\r\n- 【FastDeploy版本】：fastdeploy-gpu-python 1.0.7\r\n- 【系统平台】:  Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 3090TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n```python\r\nmodel_file = Path(r\"D:\\workspace\\ZhousfLib\\model\\inference.pdmodel\")\r\nparams_file = Path(r\"D:\\workspace\\ZhousfLib\\model\\inference.pdiparams\")\r\nconfig_file = Path(r\"D:\\workspace\\ZhousfLib\\model\\inference_cls.yaml\")\r\ntry:\r\n    with model_file.open(\"rb\") as model_buffer, params_file.open(\"rb\") as params_buffer:\r\n        # runtime_option read model form memory\r\n        runtime_option.set_model_buffer(model_buffer.read(), params_buffer.read())\r\n        runtime_option.use_gpu(0)\r\n        runtime_option.use_paddle_backend()\r\n        # Initialize model without model path and params path\r\n        model = fd.vision.classification.PaddleClasModel(\"\", \"\", str(config_file), runtime_option=runtime_option)\r\nexcept Exception as e:\r\n    print(e)\r\n```\r\n\r\n## 直接报错：\r\n```python\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n\r\n进程已结束，退出代码为 -1073741819 (0xC0000005)\r\n```\r\n看报错是内存非法访问，不知道哪里出了问题，代码参考 #1073\r\n@rainyfly 麻烦帮忙看下，感谢~\r\n\r\n\r\n",
        "state": "open",
        "user": "MrZhousf",
        "closed_by": null,
        "created_at": "2024-06-24T07:12:46+00:00",
        "updated_at": "2024-06-25T01:24:29+00:00",
        "closed_at": null,
        "comments_count": [
            "MrZhousf"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2483,
        "title": "CMake Error: The following variables are used in this project, but they are set to NOTFOUND.",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如FastDeploy Windows C++ SDK编译安装\r\n- 【编译命令】cmake .. -G \"Visual Studio 16 2019\" -A x64 `\r\n         -DENABLE_ORT_BACKEND=ON `\r\n         -DENABLE_PADDLE_BACKEND=ON `\r\n         -DENABLE_OPENVINO_BACKEND=ON `\r\n         -DENABLE_TRT_BACKEND=ON `\r\n         -DENABLE_VISION=ON `\r\n         -DENABLE_TEXT=ON `\r\n         -DWITH_CSHARPAPI=ON `\r\n         -DWITH_GPU=ON `\r\n         -DTRT_DIRECTORY=\"E:\\Work\\Dependency_Library\\TensorRT-8.6.1.6\" `\r\n         -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\" `\r\n         -DCMAKE_INSTALL_PREFIX=\"E:\\Work\\OurDevelop\\OCR\\compiled_fastdeploy\"\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 4060， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】： C++ (编译Csharp接口使用）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 编译记录\r\n```\r\nCMake Warning (dev) at D:/SoftWare/Anaconda3/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/ExternalProject.cmake:3239 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  D:/SoftWare/Anaconda3/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/ExternalProject.cmake:4465 (_ep_add_download_command)\r\n  cmake/fast_tokenizer.cmake:110 (ExternalProject_Add)\r\n  CMakeLists.txt:459 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at D:/SoftWare/Anaconda3/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/ExternalProject.cmake:3239 (message):\r\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\r\n  not set.  The policy's OLD behavior will be used.  When using a URL\r\n  download, the timestamps of extracted files should preferably be that of\r\n  the time of extraction, otherwise code that depends on the extracted\r\n  contents might not be rebuilt if the URL changes.  The OLD behavior\r\n  preserves the timestamps from the archive instead, but this is usually not\r\n  what you want.  Update your project to the NEW behavior or specify the\r\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\r\n  robustness issue.\r\nCall Stack (most recent call first):\r\n  D:/SoftWare/Anaconda3/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/ExternalProject.cmake:4465 (_ep_add_download_command)\r\n  cmake/paddle2onnx.cmake:70 (ExternalProject_Add)\r\n  CMakeLists.txt:478 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- The CSharp compiler identification is Microsoft Visual Studio 2019\r\n-- The CSharp compiler version is 2019\r\n-- Check for working C# compiler: D:/SoftWare/Microsoft Visual Studio/2019/Community/MSBuild/Current/Bin/Roslyn/csc.exe\r\n-- Check for working C# compiler: D:/SoftWare/Microsoft Visual Studio/2019/Community/MSBuild/Current/Bin/Roslyn/csc.exe - works\r\nfastdeploy_csharp_SOURCE_DIR: E:/Work/OurDevelop/OCR/FastDeploy/csharp\r\n--\r\n-- *************FastDeploy Building Summary**********\r\n--   CMake version             : 3.29.6\r\n--   CMake command             : D:/SoftWare/Anaconda3/Lib/site-packages/cmake/data/bin/cmake.exe\r\n--   System                    : Windows\r\n--   C++ compiler              : D:/SoftWare/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.29.30133/bin/Hostx64/x64/cl.exe\r\n--   C++ standard              :\r\n--   C++ cuda standard         : 11\r\n--   C++ compiler version      : 19.29.30152.0\r\n--   CXX flags                 : /DWIN32 /D_WINDOWS /W3 /GR /EHsc\r\n--   EXE linker flags          : /machine:x64\r\n--   Shared linker flags       : /machine:x64\r\n--   Build type                :\r\n--   Compile definitions       : YAML_CPP_DLL;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;EIGEN_STRONG_INLINE=inline;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;PADDLEINFERENCE_API_COMPAT_2_5_x;PADDLE_ON_INFERENCE;PADDLE_NO_PYTHON;PADDLE_WITH_CUDA;ENABLE_OPENVINO_BACKEND;WITH_GPU;ENABLE_NVJPEG;ENABLE_TRT_BACKEND;ENABLE_VISION;ENABLE_TEXT;ENABLE_PADDLE2ONNX\r\n--   CMAKE_PREFIX_PATH         :\r\n--   CMAKE_INSTALL_PREFIX      : E:/Work/OurDevelop/OCR/compiled_fastdeploy\r\n--   CMAKE_MODULE_PATH         :\r\n--\r\n--   FastDeploy version        : 0.0.0\r\n--   ENABLE_ORT_BACKEND        : ON\r\n--   ENABLE_RKNPU2_BACKEND     : OFF\r\n--   ENABLE_HORIZON_BACKEND    : OFF\r\n--   ENABLE_SOPHGO_BACKEND     : OFF\r\n--   ENABLE_PADDLE_BACKEND     : ON\r\n--   ENABLE_LITE_BACKEND       : OFF\r\n--   ENABLE_POROS_BACKEND      : OFF\r\n--   ENABLE_TRT_BACKEND        : ON\r\n--   ENABLE_OPENVINO_BACKEND   : ON\r\n--   ENABLE_TVM_BACKEND        : OFF\r\n--   ENABLE_BENCHMARK          : OFF\r\n--   ENABLE_VISION             : ON\r\n--   ENABLE_TEXT               : ON\r\n--   ENABLE_ENCRYPTION         : OFF\r\n--   ENABLE_FLYCV              : OFF\r\n--   ENABLE_CVCUDA             : OFF\r\n--   WITH_GPU                  : ON\r\n--   WITH_IPU                  : OFF\r\n--   WITH_OPENCL               : OFF\r\n--   WITH_TESTING              : OFF\r\n--   WITH_ASCEND               : OFF\r\n--   WITH_DIRECTML             : OFF\r\n--   WITH_TIMVX                : OFF\r\n--   WITH_KUNLUNXIN            : OFF\r\n--   WITH_CAPI                 : OFF\r\n--   WITH_CSHARPAPI            : ON\r\n--   ONNXRuntime version       : 1.12.0\r\n--   Paddle Inference version  : 2.5.0.281761089e\r\n--   PADDLE_WITH_ENCRYPT       : OFF\r\n--   PADDLE_WITH_AUTH          : OFF\r\n--   OpenVINO version          : 2022.2.0.dev20220829\r\n--   CUDA_DIRECTORY            : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3\r\n--   TRT_DRECTORY              : E:/Work/Dependency_Library/TensorRT-8.6.1.6\r\n-- Configuring done (105.0s)\r\nCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\nPlease set them or make sure they are set and tested correctly in the CMake files:\r\nCUDA_LIB\r\n    linked by target \"fastdeploy\" in directory E:/Work/OurDevelop/OCR/FastDeploy\r\nNVJPEG_LIB\r\n    linked by target \"fastdeploy\" in directory E:/Work/OurDevelop/OCR/FastDeploy\r\n\r\n-- Generating done (0.6s)\r\nCMake Generate step failed.  Build files cannot be regenerated correctly.\r\n```\r\n- 【出现问题：编译出错】\r\n CMake Error: The following variables are used in this project, but they are set to NOTFOUND\r\nPlease set them or make sure they are set and tested correctly in the CMake files:\r\nCUDA_LIB\r\n    linked by target \"fastdeploy\" in directory E:/Work/OurDevelop/OCR/FastDeploy\r\nNVJPEG_LIB\r\n    linked by target \"fastdeploy\" in directory E:/Work/OurDevelop/OCR/FastDeploy\r\nCMake Generate step failed.  Build files cannot be regenerated correctly.\r\n所以请问我编译过程中是有不正常操作吗？再或者这个问题该怎么解决？在此求助大家。\r\n\r\n",
        "state": "closed",
        "user": "1wang11lijian1",
        "closed_by": "1wang11lijian1",
        "created_at": "2024-07-01T06:56:29+00:00",
        "updated_at": "2024-07-05T06:53:40+00:00",
        "closed_at": "2024-07-05T06:53:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2484,
        "title": "在使用onnxruntime作为推理后端时，如何设置cudnn_conv_algo_search = DEFAULT",
        "body": "我开发的程序用于工业场景的视觉检测，但在模型运行时会出现耗时过长的问题。通过设置 CUDAExecutionProvider = DEFAULT 可以解决这些耗时的情况，但在FastDeploy中找不到设置该参数的入口。\r\n\r\n## 环境\r\n\r\n- 【编译命令】官方编译好的dll\r\n- 【系统平台】:Windows x64(Windows10) \r\n- 【模型类型】:onnx\r\n- 【后端】:onnxruntime\r\n\r\n## 问题描述及复现步骤\r\n- 【性能问题】\r\n以下两种场景会出现耗时过长的问题：\r\n\r\n模型已加载的情况下，第一次运行；\r\n模型运行多次后，输入的Tensor批次或尺寸发生变化，紧接着的第一次运行。\r\n以1600*3200*3的图像分割为例，这两种情况的耗时为6000ms，而正常运行时的耗时在200ms以内。\r\n\r\n## 解决问题的急迫性：\r\n1.设备安全性: 工业软件通常有超时设定，若在规定时间内未执行完毕，会及时停机保护设备。由于模型第一次运行耗时过长，导致超时停机时间远超最大限定。\r\n2.非人性化操作: 为了衔接上下游设备，必须在设备正式运行前先跑一次程序。如果操作人员忘记这一步骤，设备衔接可能会出问题。\r\n3.使用受限: 处理的图像有大有小，批次处理并不比每张单独推理快多少。对于小图像推理时间更短，但受制于该问题，不得不将小图缩放到指定尺寸，导致整体推理时间增加。\r\n4.软件启动慢: 为了确保流程顺畅，设备运行前需要执行一次推理。设备使用多个模型，单个模型加载耗时1s，第一次推理耗时6s。如果有10个模型，模型加载耗时10s，推理耗时60s。在调整相机和算法参数时，频繁关闭重开软件来查看效果会耗费大量时间，现场人员很难接受。\r\n\r\n## 过往解决办法\r\n最初使用C#调用onnxrutime时，通过设置 cudnn_conv_algo_search = \"DEFAULT\" 解决了这个问题，使得模型第一次运行和连续运行的耗时一致。\r\n\r\n## 期待解决办法\r\n现在使用C++调用Fastdeploy，希望能提供设置接口以便设置该参数。\r\n`   providers=[\r\n        (\"CUDAExecutionProvider\", {\r\n            # \"cudnn_conv_algo_search\": \"DEFAULT\",\r\n            # \"cudnn_conv_algo_search\": \"HEURISTIC\",\r\n            \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\r\n        }),\r\n        # \"CPUExecutionProvider\",\r\n      ]`\r\n\r\n",
        "state": "closed",
        "user": "wujingxin521",
        "closed_by": "wujingxin521",
        "created_at": "2024-07-03T03:30:35+00:00",
        "updated_at": "2025-06-06T06:27:18+00:00",
        "closed_at": "2025-06-06T06:27:18+00:00",
        "comments_count": [
            "wujingxin521"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2485,
        "title": "double free or corruption (out)",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\r\n\r\n使用 fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10 或者 /fastdeploy:1.0.7-cpu-only-21.10 这个镜像部署的Docker 服务器。\r\n![error](https://github.com/PaddlePaddle/FastDeploy/assets/9401364/12b03183-040f-41af-a5d9-bf1c42578b22)\r\n这个图片100%抛出异常:\r\n\r\ndouble free or corruption (out)\r\n\r\nE0704 02:28:19.958359 7 python.cc:1942] Stub process is unhealthy and it will be restarted.\r\n",
        "state": "open",
        "user": "tianv",
        "closed_by": null,
        "created_at": "2024-07-04T02:30:59+00:00",
        "updated_at": "2024-07-04T03:17:26+00:00",
        "closed_at": null,
        "comments_count": [
            "tianv",
            "tianv"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2486,
        "title": "运行编译出来的Csharp示例代码./FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/csharp/build/infer_demo.sln，出现推理报错闪退问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 自行编译FastDeploy SDK 的Csharp API接口\r\n- 【编译命令】\r\n- C++ SDK编译安装（Windows）\r\ncd E:\\Work\\OurDevelop\\OCR\\FastDeploy\\build\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64 `\r\n         -DENABLE_ORT_BACKEND=ON `\r\n         -DENABLE_PADDLE_BACKEND=ON `\r\n         -DENABLE_OPENVINO_BACKEND=ON `\r\n         -DENABLE_TRT_BACKEND=ON `\r\n         -DENABLE_VISION=ON `\r\n         -DENABLE_TEXT=ON `\r\n         -DWITH_GPU=ON `\r\n         -DWITH_CAPI=ON `\r\n         -DWITH_CSHARPAPI=ON `\r\n         -DTRT_DIRECTORY=\"E:\\Work\\Dependency_Library\\TensorRT-8.6.1.6\" `\r\n         -DCUDA_DIRECTORY=\"D:\\SoftWare\\CUDA\\v11.3\" `\r\n         -DCMAKE_INSTALL_PREFIX=\"E:\\Work\\OurDevelop\\OCR\\compiled_fastdeploy\"\r\n- 编译Csharp示例代码\r\ncd E:\\Work\\OurDevelop\\OCR\\FastDeploy\\examples\\vision\\ocr\\PP-OCR\\cpu-gpu\\csharp\\build\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64 -DFASTDEPLOY_INSTALL_DIR=E:\\Work\\OurDevelop\\OCR\\compiled_fastdeploy -DCUDA_DIRECTORY=\"D:/SoftWare/CUDA/v11.3\"\r\nnuget restore\r\nmsbuild infer_demo.sln /m:4 /p:Configuration=Release /p:Platform=x64\r\n- FastDeploy依赖的库拷贝至可执行程序所在目录\r\n./fastdeploy_init.bat install %cd% E:\\Work\\OurDevelop\\OCR\\FastDeploy\\examples\\vision\\ocr\\PP-OCR\\cpu-gpu\\csharp\\build\\Release\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： Nvidia GPU 4060， CUDA 11.3 CUDNN 8.2\r\n- 【编译语言】：Csharp\r\n\r\n## 问题日志及出现问题的操作流程\r\n运行Csharp示例代码`./FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/csharp/build/infer_demo.sln`项目\r\nCsharp运行代码如下\r\n```\r\nusing System;\r\nusing System.IO;\r\nusing System.Runtime.InteropServices;\r\nusing OpenCvSharp;\r\nusing fastdeploy;\r\n\r\nnamespace Test\r\n{\r\n    public class TestPPOCRv3\r\n    {\r\n        public static void Main(string[] args)\r\n        {\r\n            string det_model_dir = \"E:\\\\model_en\\\\en_PP-OCRv3_det_infer\";\r\n            string cls_model_dir = \"E:\\\\model_en\\\\ch_ppocr_mobile_v2.0_cls_infer\";\r\n            string rec_model_dir = \"E:\\\\model_en\\\\en_PP-OCRv3_rec_infer\";\r\n            string rec_label_file = \"E:\\\\model_en\\\\en_dict.txt\";\r\n            string image_path = \"E:\\\\model\\\\img_12.jpg\";\r\n            RuntimeOption runtimeoption = new RuntimeOption();\r\n            int device_option = 1;\r\n            if(device_option==0){\r\n                runtimeoption.UseCpu();\r\n            }else{\r\n                runtimeoption.UseGpu();\r\n            }\r\n            string sep = \"\\\\\";\r\n            string det_model_file = det_model_dir + sep + \"inference.pdmodel\";\r\n            string det_params_file = det_model_dir + sep + \"inference.pdiparams\";\r\n\r\n            string cls_model_file = cls_model_dir + sep + \"inference.pdmodel\";\r\n            string cls_params_file = cls_model_dir + sep + \"inference.pdiparams\";\r\n\r\n            string rec_model_file = rec_model_dir + sep + \"inference.pdmodel\";\r\n            string rec_params_file = rec_model_dir + sep + \"inference.pdiparams\";\r\n\r\n            fastdeploy.vision.ocr.DBDetector dbdetector = new fastdeploy.vision.ocr.DBDetector(det_model_file, det_params_file, runtimeoption, ModelFormat.PADDLE);\r\n            fastdeploy.vision.ocr.Classifier classifier = new fastdeploy.vision.ocr.Classifier(cls_model_file, cls_params_file, runtimeoption, ModelFormat.PADDLE);\r\n            fastdeploy.vision.ocr.Recognizer recognizer = new fastdeploy.vision.ocr.Recognizer(rec_model_file, rec_params_file, rec_label_file, runtimeoption, ModelFormat.PADDLE);\r\n            fastdeploy.pipeline.PPOCRv3 model = new fastdeploy.pipeline.PPOCRv3(dbdetector, classifier, recognizer);\r\n            if(!model.Initialized()){\r\n                Console.WriteLine(\"Failed to initialize.\\n\");\r\n            }\r\n            Mat image = Cv2.ImRead(image_path);\r\n            fastdeploy.vision.OCRResult res = model.Predict(image);\r\n            Console.WriteLine(res.ToString());\r\n            Mat res_img = fastdeploy.vision.Visualize.VisOcr(image, res);\r\n            Cv2.ImShow(\"result.png\", res_img);\r\n            Cv2.ImWrite(\"result.png\", res_img);\r\n            Cv2.WaitKey(0);\r\n        }\r\n    }\r\n}\r\n```\r\n在这行`fastdeploy.vision.OCRResult res = model.Predict(image);`出现报错\r\nSystem.ArgumentOutOfRangeException:“在多字节的目标代码页中，没有此 Unicode 字符可以映射到的字符。 (异常来自 HRESULT:0x80070459)”\r\n在我的电脑上是一直都会报这个问题，或者直接闪退./FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/csharp/build/Debug/infer_demo.exe (进程 31404)已退 出，代码为 -1073740940。\r\n在我伙伴的电脑上有时图片正常计算（偶尔出现中文乱码问题），有时也报这个错误\r\n如下图（偶尔出现中文乱码问题）\r\n![微信图片_20240705161958](https://github.com/PaddlePaddle/FastDeploy/assets/101158990/9bb918bc-0a84-4ec8-bfab-daa6ab2183ec)\r\n如下图（计算报错闪退）\r\n![微信图片_20240705160728](https://github.com/PaddlePaddle/FastDeploy/assets/101158990/1ba3f16a-51b5-4c78-88b0-27ced9060eff)\r\n\r\n所以想请问各位这是由于什么原因导致这两个问题，特在此求助大家！\r\n",
        "state": "open",
        "user": "1wang11lijian1",
        "closed_by": null,
        "created_at": "2024-07-05T08:23:34+00:00",
        "updated_at": "2024-07-12T07:25:03+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII",
            "1wang11lijian1",
            "ChaoII",
            "1wang11lijian1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2491,
        "title": "sophgo-tpu编译",
        "body": "算能bm1688只有soc模式，怎么部署环境",
        "state": "open",
        "user": "Z2C9808",
        "closed_by": null,
        "created_at": "2024-07-11T06:43:03+00:00",
        "updated_at": "2024-07-11T06:43:08+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2492,
        "title": "能否提供一下 Android 端部署 Ernie-3.0-tiny 意图识别模型的示例",
        "body": "您好 在B站上有看到咱们部署 Ernie-3.0-tiny 意图识别模型的示例，但是在 Github 上找不到响应的代码，能否提供一下部署 Erinie-3.0-tiny 模型的示例",
        "state": "open",
        "user": "GJYWorld",
        "closed_by": null,
        "created_at": "2024-07-15T06:41:33+00:00",
        "updated_at": "2024-07-15T06:54:35+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2487,
        "title": "关于FastDeploy文档中关于RKNN的百度网盘链接无法打开",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: ARM\r\n- 【硬件】： rk3568\r\n- 【编译语言】： Python3.7\r\n\r\n我想在ARM平台使用NPU推理模型，芯片为rk3568，但是需要使用rknn-toolkit2的驱动，文档里面写的使用1.4.2版本，右边附带了一个链接，是个百度网盘，但是那个链接失效了，我在别的地方也找不到这个版本，只有1.4.0和1.5.0，没有1.4.2版本\r\n![微信截图_20240709144819](https://github.com/PaddlePaddle/FastDeploy/assets/34666329/362d9c56-c7ce-41c8-bbe2-97687836586c)\r\n能不能更新一下这个链接，让我下载下来这个1.4.2版本的库",
        "state": "open",
        "user": "ltj19900609",
        "closed_by": null,
        "created_at": "2024-07-09T06:52:55+00:00",
        "updated_at": "2024-07-16T09:21:22+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "Zheng-Bicheng",
            "Zheng-Bicheng",
            "ltj19900609",
            "Zheng-Bicheng",
            "ltj19900609"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2489,
        "title": "PaddleSeg (OCRNet and PPliteSeg) output is not correct",
        "body": "## Environment\r\n\r\nFastDeploy version:1.0.7\r\nOS Platform: e.g. Linux x64\r\nHardware: e.g. Nvidia GPU 4090  CUDA 12.4 CUDNN 8.3\r\nProgram Language: e.g. Python 3.10\r\n\r\n## Problem description\r\nI have try different options:\r\n\r\n\"option.use_gpu()\r\noption.use_paddle_backend() # Paddle Inference\r\noption.use_trt_backend() # TensorRT\r\noption.use_openvino_backend() # OpenVINO\r\noption.use_ort_backend() # ONNX Runtime\"\r\n\r\nAll the output is zero.\r\n\r\n<img width=\"607\" alt=\"1720575196977\" src=\"https://github.com/PaddlePaddle/FastDeploy/assets/13444641/0430549e-25ec-4c8e-96d3-6c897a9b483f\">\r\n",
        "state": "closed",
        "user": "universea",
        "closed_by": "universea",
        "created_at": "2024-07-10T01:34:44+00:00",
        "updated_at": "2024-07-13T23:59:37+00:00",
        "closed_at": "2024-07-13T23:59:37+00:00",
        "comments_count": [
            "priyankavgalagali",
            "universea"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2488,
        "title": "一个愚蠢的问题 关于 OpenVino/OnnxRuntime 版本问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-windows-0.0.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）openvino\r\n- 【系统平台】:Windows x64(Windows10) \r\n- 【硬件】： intel +4090\r\n- 【编译语言】： C++ \r\n\r\n一个愚蠢问题, 我看到说 win下 只支持cpu 推理，我有看到推理后端部分有判断only cpu. \r\n有计划更新openvino to 2024.2 利用上intel npu  这个是因为目前没人贡献吗？还是有啥坑。\r\n如果是没人适配。 等我弄完 npu 提交上去     amd/onnxruntime->amd ryzen ai 等intel弄完也搞起来\r\n",
        "state": "open",
        "user": "MCapricorns",
        "closed_by": "MCapricorns",
        "created_at": "2024-07-09T06:58:22+00:00",
        "updated_at": "2024-07-24T07:43:51+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "bltcn",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2493,
        "title": "fastDeploy服务部署支持windows吗",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: e.g. Linux x64 / Windows x64 / Mac OSX 12.1(arm or intel)\r\nHardware: e.g. Nvidia GPU 3080Ti  CUDA 11.2 CUDNN 8.3\r\nProgram Language: e.g. Python 3.8\r\n\r\n## Problem description\r\nPlease attach the log file if there's problem happend.\r\n",
        "state": "open",
        "user": "aiwenzhu",
        "closed_by": null,
        "created_at": "2024-07-15T10:35:05+00:00",
        "updated_at": "2024-07-26T05:38:09+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2494,
        "title": "Using celery is very slow",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: Huawei\r\nHardware: Atlas300I pro\r\nProgram Language:  Python 3.9\r\n\r\nfastdeploy-python      0.0.0\r\nfastdeploy-tools       0.0.5\r\n\r\n![d919468e04f5bb805f4be20deacabb9](https://github.com/user-attachments/assets/f3b1475b-7e6a-4e63-b70c-3c5f80d4a9a4)\r\n\r\n\r\n\r\n## Problem description\r\nI am using celery + fastapi + fastdeploy on Huawei qilin arm64 Docker. But if I do not use celery, the inference time is 2s while 6s with celery. Does anyone know how to solve this?\r\n\r\nps: The time count is minimum (no redundant codes).\r\n\r\n",
        "state": "open",
        "user": "a01163125",
        "closed_by": null,
        "created_at": "2024-07-16T02:49:57+00:00",
        "updated_at": "2024-07-16T02:50:02+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2501,
        "title": "能提供一份fastdeploy serving 部署 UIE 的 config.pbtxt 么？",
        "body": "您好，我是新手，想用 fastdeploy serving 部署 UIE, 能一个一份标准的 config.pbtxt 参考一下么？",
        "state": "open",
        "user": "empty2enrich",
        "closed_by": null,
        "created_at": "2024-08-05T06:13:33+00:00",
        "updated_at": "2024-08-05T06:13:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2498,
        "title": "fastdeploy C++ 编译问题，cmake 中勾选ENABLE_TEXT编译报错（取消勾选编译通过）",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：FastDeploy-develop （CPU，GPU均报错）\r\n- 【编译命令】:ENBALE_TEXT （取消勾选，编译通过）\r\n- 【系统平台】: Windows x64(Windows11) ，cuda-11.3，cudnn-8.9.7，opencv-4.7.0，tensorRT-8.6\r\n- 【编译语言】： C++ （cmake + vs2019）\r\n\r\n- 【编译报错】\r\n- 【报错代码】：std::vector<std::string> texts_pair_ds = {\"收钱码，对花呗支付的金额有限制吗\",\"为什么友付宝不支持花呗付款\"};”\r\n- 【vs2019报错】：C2001 常量中有换行符\r\n- 【修改】：在最后一个汉字之后增加一个空格，C2001报错消失。但是又爆出连接错误\r\n- 【vs2019报错】：LNK2019\t无法解析的外部符号 \"public: static bool fastdeploy::FDLogger::enable_info\" (?enable_info@FDLogger@fastdeploy@@2_NA)，函数 \"bool __cdecl CreateRuntimeOption(struct fastdeploy::RuntimeOption *)\" (?CreateRuntimeOption@@YA_NPEAURuntimeOption@fastdeploy@@@Z) 中引用了该符号\ttext_text_ernie-3.0_seq_cls_infer\tD:\\sdk\\FastDeploy_gpu\\build\\examples\\seq_cls_infer.obj\t1\r\n",
        "state": "open",
        "user": "zjbistu",
        "closed_by": null,
        "created_at": "2024-08-01T10:06:18+00:00",
        "updated_at": "2024-08-01T10:08:10+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2500,
        "title": "fastdeploy是否在rk3588上支持tinypose",
        "body": "我在rk3588上编译安装好了python版本\r\nmodel = fd.vision.keypointdetection.PPTinyPose(model_file, \"\", config_file, runtime_option=runtime_option, model_format=fd.ModelFormat.RKNN)\r\n报错  PPTinyPose model only support model format of ModelFormat.Paddle now.\r\n并且示例代码runtime_option.use_rknpu()，在当前版本没有use_rknpu()，只有use_rknpu2()\r\n\r\n当前最新版的fastdeploy是否在rk3588上支持tinypose",
        "state": "closed",
        "user": "11spring",
        "closed_by": "11spring",
        "created_at": "2024-08-02T08:23:47+00:00",
        "updated_at": "2024-12-26T06:33:25+00:00",
        "closed_at": "2024-12-26T06:33:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2502,
        "title": "jetson上pptinypose输出异常",
        "body": "环境：jetpack5.1.3,包括cuda11.4， cudnn8.6.0,  tensorrt5.2.2\r\n编译按照官方文档，编译时没有加上paddle_backend，编译过程正常\r\n在jetson上使用fastdeploy跑tinypose模型输出关键点全为0.0000000，置信度很低\r\n同样的代码在windows上跑正常，windows环境cuda11.6, cudnn8.6, tensorrt5.3.1，windows上的fastdeploy是官方下的\r\n我逐步排查发现可能是没有permute的原因，在windows上使用参数disable_permute()后输出结果与jetson上一致，也就是说本来模型需要的是[1,3,256,192]但是现在传的是[1,256,192,3]\r\n我又查看fastdploy源码，添加代码打印了在传入推理前的数据维度并编译，发现输出是[1,3,256,192]，这个是正常的\r\n\r\n同样的，我部署mot模型，里面也有permute操作，但是在jetson上输出正常\r\n\r\n个人猜测是不是tensorrt转算子不支持permute(这方面不是太了解)，希望得到大神解答",
        "state": "open",
        "user": "11spring",
        "closed_by": null,
        "created_at": "2024-08-05T08:49:43+00:00",
        "updated_at": "2024-08-05T08:49:48+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2496,
        "title": "fd.vision.ocr.PPOCRv3初始化模型后调用predict(im)无返回，报错：Process finished with exit code -1073741819 (0xC0000005)",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-python-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： CPU\r\n- 【编译语言】：Python(3.8）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- 问题说明：fd.vision.ocr.PPOCRv3(det_model=build_det_model(), rec_model=build_rec_model())；初始化模型后调用predict(im)无返回\r\n- 报错：Process finished with exit code -1073741819 (0xC0000005)\r\n下面是我的代码：\r\n```python\r\n\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\ndef build_det_model():\r\n    det_model_file = \"./model/det_db_inference/inference.pdmodel\"\r\n    det_params_file = \"./model/det_db_inference/inference.pdiparams\"\r\n    # Set the runtime option\r\n    det_option = fd.RuntimeOption()\r\n    det_option.use_cpu()\r\n    # det_option.use_openvino_backend()\r\n\r\n    # Create the det_model\r\n    det_model = fd.vision.ocr.DBDetector(\r\n        det_model_file, det_params_file, runtime_option=det_option)\r\n\r\n    # Set the preporcessing parameters\r\n    det_model.preprocessor.max_side_len = 960\r\n    det_model.postprocessor.det_db_thresh = 0.3\r\n    det_model.postprocessor.det_db_box_thresh = 0.6\r\n    det_model.postprocessor.det_db_unclip_ratio = 1.5\r\n    det_model.postprocessor.det_db_score_mode = \"slow\"\r\n    det_model.postprocessor.use_dilation = False\r\n    return det_model\r\n\r\n\r\ndef build_rec_model():\r\n    rec_model_file = \"./model/rec_ppocr_v3_distillation/Teacher/inference.pdmodel\"\r\n    rec_params_file = \"./model/rec_ppocr_v3_distillation/Teacher/inference.pdiparams\"\r\n    rec_label_file = \"./model/rec_ppocr_v3_distillation/lesson_letter.txt\"\r\n    # Set the runtime option\r\n    rec_option = fd.RuntimeOption()\r\n    rec_option.use_cpu()\r\n    # rec_option.use_openvino_backend()\r\n\r\n    rec_model = fd.vision.ocr.Recognizer(\r\n        rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option)\r\n    rec_model.rec_image_shape = [3, 48, 48]\r\n    return rec_model\r\n\r\n\r\nif __name__ == '__main__':\r\n    im = cv2.imread(\"./imgs/19.png\")\r\n    ppocr_v3 = fd.vision.ocr.PPOCRv3(det_model=build_det_model(), rec_model=build_rec_model())\r\n    result = ppocr_v3.predict(im)\r\n    print(result)\r\n```\r\n",
        "state": "open",
        "user": "RlonLL11FE",
        "closed_by": null,
        "created_at": "2024-07-18T08:40:12+00:00",
        "updated_at": "2024-07-18T08:45:40+00:00",
        "closed_at": null,
        "comments_count": [
            "RlonLL11FE"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2495,
        "title": "关于如何切换为摄像头输入",
        "body": "\r\n## 环境\r\n\r\n\r\n- 【系统平台】: \r\nLinux topeet 4.19.232 #13 SMP Sat Mar 23 16:01:21 CST 2024 aarch64 aarch64 aarch64 GNU/Linux\r\n- 【硬件】： 讯为RK3568开发板\r\n- 【编译语言】： C++ \r\n图片输入模型运行正常，不知道如何切换为板载USB免驱摄像头输入\r\n## 问题日志及出现问题的操作流程\r\n-[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(81)::GetSDKAndDeviceVersion rknpu2 runtime version: 1.4.0 (a10f100eb@2022-09-09T09:07:14)\r\n[INFO] fastdeploy/runtime/backends/rknpu2/rknpu2_backend.cc(82)::GetSDKAndDeviceVersion rknpu2 driver version: 0.7.2\r\nindex=0, name=x.1, n_dims=4, dims=[1, 640, 640, 3], n_elems=1228800, size=1228800, fmt=NHWC, type=INT8, qnt_type=AFFINE, zp=-128, scale=0.003922, pass_through=0\r\nindex=0, name=172, n_dims=4, dims=[1, 255, 80, 80], n_elems=1632000, size=1632000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\nindex=1, name=173, n_dims=4, dims=[1, 255, 40, 40], n_elems=408000, size=408000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\nindex=2, name=174, n_dims=4, dims=[1, 255, 20, 20], n_elems=102000, size=102000, fmt=NCHW, type=FP32, qnt_type=AFFINE, zp=-128, scale=0.003921, pass_through=0\r\n[INFO] fastdeploy/runtime/runtime.cc(367)::CreateRKNPU2Backend  Runtime initialized with Backend::RKNPU2 in Device::RKNPU.\r\n[ WARN:0@0.200] imread_('dev21'): can't open/read file: check file path/integrity\r\n[ERROR] fastdeploy/vision/common/processors/pad.cc(30)::ImplByOpenCV    Pad: Require input channels equals to size of padding value, but now channels = 1, the size of padding values = 3.\r\nterminate called after throwing an instance of 'cv::Exception'\r\n  what():  OpenCV(3.4.20-dev) /home/topeet/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\r\n",
        "state": "closed",
        "user": "13307790006",
        "closed_by": "13307790006",
        "created_at": "2024-07-16T13:16:16+00:00",
        "updated_at": "2024-07-17T08:21:12+00:00",
        "closed_at": "2024-07-17T08:21:12+00:00",
        "comments_count": [
            "13307790006"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2503,
        "title": "模型加载速度太慢",
        "body": "fastdeploy有像paddlelite一样的config.add_discarded_pass(\"__xpu__multi_encoder_fuse_pass\") 函数吗，C++里怎么做，目前加载一个模型的时间感觉太长了，我用的是yolov8 c++版本，一致找不到add_discarded_pass这个方法",
        "state": "open",
        "user": "sanersbug",
        "closed_by": null,
        "created_at": "2024-08-05T17:35:51+00:00",
        "updated_at": "2024-08-06T03:41:40+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2504,
        "title": "fastdeploy使用openvino推理后端在ppocr-v3模型上的推理速度问题",
        "body": "按照官方的部署方法搭建了环境，仅cpu运行推理，目前参数设置如下\r\n```\r\nimport fastdeploy as fd\r\nimport cv2\r\nimport os\r\n\r\ndet_option = fd.RuntimeOption()\r\ncls_option = fd.RuntimeOption()\r\nrec_option = fd.RuntimeOption()\r\nprint(det_option)\r\n\r\ndet_option.use_openvino_backend()\r\ncls_option.use_openvino_backend()\r\nrec_option.use_openvino_backend()\r\n\r\ndet_model_file = os.path.join(\"ch_PP-OCRv3_det_infer\", \"inference.pdmodel\")\r\ndet_params_file = os.path.join(\"ch_PP-OCRv3_det_infer\", \"inference.pdiparams\")\r\n\r\ncls_model_file = os.path.join(\"ch_ppocr_mobile_v2.0_cls_infer\", \"inference.pdmodel\")\r\ncls_params_file = os.path.join(\"ch_ppocr_mobile_v2.0_cls_infer\", \"inference.pdiparams\")\r\n\r\nrec_model_file = os.path.join(\"ch_PP-OCRv3_rec_infer\", \"inference.pdmodel\")\r\nrec_params_file = os.path.join(\"ch_PP-OCRv3_rec_infer\", \"inference.pdiparams\")\r\nrec_label_file = \"ppocr_keys_v1.txt\"\r\n\r\n\r\ndet_model = fd.vision.ocr.DBDetector(\r\n    det_model_file, det_params_file, runtime_option=det_option)\r\n\r\ncls_model = fd.vision.ocr.Classifier(\r\n    cls_model_file, cls_params_file, runtime_option=cls_option)\r\n\r\nrec_model = fd.vision.ocr.Recognizer(\r\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option)\r\n\r\n\r\n#设置输入图像的最大边长。\r\ndet_model.preprocessor.max_side_len = 960\r\n#只有置信度高于这个值的文本框才会被认为是有效的文本框\r\ndet_model.postprocessor.det_db_thresh = 0.3\r\n#只有置信度高于这个值的文本框才会被保留下来\r\ndet_model.postprocessor.det_db_box_thresh = 0.6\r\n#检测到的文本框会按照这个比例进行扩展，以确保完整包含整个文本区域\r\ndet_model.postprocessor.det_db_unclip_ratio = 1.5\r\n#设置文本框得分模式\r\ndet_model.postprocessor.det_db_score_mode = \"slow\"\r\n#设置是否在后处理中使用膨胀操作。膨胀操作可以扩大文本区域\r\ndet_model.postprocessor.use_dilation = False\r\n#设置文本方向分类的阈值。这个值用于控制分类结果的置信度阈值，只有置信度高于这个值的分类结果才会被认为是有效的\r\ncls_model.postprocessor.cls_thresh = 0.9\r\n\r\nprint(det_option)\r\nppocr_v3 = fd.vision.ocr.PPOCRv3(\r\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model)\r\n\r\nppocr_v3.cls_batch_size = 1\r\nppocr_v3.rec_batch_size = 6\r\n\"\"\"\r\nppocr_v3.predict()返回结果\r\nOCRResult代码定义在fastdeploy/vision/common/result.h中，用于表明图像检测和识别出来的文本框，文本框方向分类，以及文本框内的文本内容.\r\nAPI:fastdeploy.vision.OCRResult, 该结果返回:\r\nboxes(list of list(int)): 成员变量，表示单张图片检测出来的所有目标框坐标，boxes.size()表示单张图内检测出的框的个数，每个框以8个int数值依次表示框的4个坐标点，顺序为左下，右下，右上，左上.\r\ntext(list of string): 成员变量，表示多个文本框内被识别出来的文本内容，其元素个数与boxes.size()一致.\r\nrec_scores(list of float): 成员变量，表示文本框内识别出来的文本的置信度，其元素个数与boxes.size()一致.\r\ncls_scores(list of float): 成员变量，表示文本框的分类结果的置信度，其元素个数与boxes.size()一致.\r\ncls_labels(list of int): 成员变量，表示文本框的方向分类类别，其元素个数与boxes.size()一致.\r\n\"\"\"\r\ndef fd_ocr_predict(img):\r\n    result = ppocr_v3.predict(img)\r\n    return result.boxes,result.text,result.rec_scores\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    im = cv2.imread(\"picture/1.png\")\r\n    print(im)\r\n    text_boxes, recognized_texts,confidence= fd_ocr_predict(im)\r\n    print(text_boxes)\r\n    print(recognized_texts)\r\n    print(confidence)\r\n    print(ppocr_v3.predict(im))\r\n    #vis_im = fd.vision.vis_ppocr(im, result)\r\n    #cv2.imwrite(\"visualized_result.jpg\", vis_im)\r\n    #print(\"Visualized result save in ./visualized_result.jpg\")\r\n```\r\n\r\n请问应该如何设置接口调用参数，或采取某些方法加快推理速度",
        "state": "open",
        "user": "uniqlo1",
        "closed_by": null,
        "created_at": "2024-08-08T07:16:03+00:00",
        "updated_at": "2024-08-08T07:16:07+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2508,
        "title": "cpu部署库移植报错",
        "body": "## 环境\r\n- 【FastDeploy版本】： 官方fastdeploy-linux-x64-1.0.7+自行编译版本\r\n- 【编译命令】cmake.. -DCMAKE\r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n先在第一台编译的包含paddle和ort后端的部署库，成功编译也能在ocr代码中编译推理；将代码＋部署库移植到另一台电脑上opencv版本一致，路径也对；编译源码cmake.. 成功 但是make时出现libfastdeploy.so对opencv一些函数未定义的引用\r\n类似于../../lib/libfastdeploy.so:对cv::String::allocate(unsigned long)未定义的引用 cv::getTextSIze(cv::String const&, int, double, int, int*)\r\n\r\n直接只用官方部署库也出现对opencv函数的未定义的引用 但是为infer.cc:(text+0x2662):对cv::imwrite(std::_cxx11::basic_string<char, std::char_straits<char>, std::allocator<char>....>未定义的使用)\r\n\r\n\r\n好像不能离线安装 把需要的库下载再编译 还是会覆盖从网页下载\r\n",
        "state": "closed",
        "user": "Zomcxj",
        "closed_by": "Zomcxj",
        "created_at": "2024-08-22T01:33:53+00:00",
        "updated_at": "2024-08-24T01:40:36+00:00",
        "closed_at": "2024-08-24T01:40:36+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2505,
        "title": "python编译安装fastdeploy,出现错误",
        "body": "## Environment\r\n\r\nFastDeploy version: e.g 0.8.0 or the latest code in develop branch\r\nOS Platform: ARM64\r\nHardware: RK3588\r\nProgram Language: e.g. Python 3.8\r\n## Problem description\r\n当运行python3 setup.py build 出现报错：\r\ngmake[2]: *** [CMakeFiles/fastdeploy_main.dir/build.make:104：CMakeFiles/fastdeploy_main.dir/fastdeploy/pybind/fastdeploy_model.cc.o] 错误 1\r\nIn file included from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/../attr.h:13,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/class.h:12,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/pybind11.h:13,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/numpy.h:12,\r\n                 from /home/linaro/FastDeploy/./fastdeploy/pybind/main.h:17,\r\n                 from /home/linaro/FastDeploy/fastdeploy/pybind/fd_tensor.cc:19:\r\n/home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/../detail/common.h:211:10: fatal error: Python.h: 没有那个文件或目录\r\n  211 | #include &lt;Python.h&gt;\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\ngmake[2]: *** [CMakeFiles/fastdeploy_main.dir/build.make:118：CMakeFiles/fastdeploy_main.dir/fastdeploy/pybind/fd_tensor.cc.o] 错误 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:259：CMakeFiles/fastdeploy_main.dir/all] 错误 2\r\ngmake: *** [Makefile:156：all] 错误 2\r\nTraceback (most recent call last):\r\n  File \"/home/linaro/FastDeploy/python/setup.py\", line 445, in &lt;module&gt;\r\n    setuptools.setup(\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 153, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.10/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.10/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.10/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.10/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.10/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\ngmake[2]: *** [CMakeFiles/fastdeploy_main.dir/build.make:104：CMakeFiles/fastdeploy_main.dir/fastdeploy/pybind/fastdeploy_model.cc.o] 错误 1\r\nIn file included from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/../attr.h:13,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/class.h:12,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/pybind11.h:13,\r\n                 from /home/linaro/FastDeploy/third_party/pybind11/include/pybind11/numpy.h:12,\r\n                 from /home/linaro/FastDeploy/./fastdeploy/pybind/main.h:17,\r\n                 from /home/linaro/FastDeploy/fastdeploy/pybind/fd_tensor.cc:19:\r\n/home/linaro/FastDeploy/third_party/pybind11/include/pybind11/detail/../detail/common.h:211:10: fatal error: Python.h: 没有那个文件或目录\r\n  211 | #include &lt;Python.h&gt;\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\ngmake[2]: *** [CMakeFiles/fastdeploy_main.dir/build.make:118：CMakeFiles/fastdeploy_main.dir/fastdeploy/pybind/fd_tensor.cc.o] 错误 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:259：CMakeFiles/fastdeploy_main.dir/all] 错误 2\r\ngmake: *** [Makefile:156：all] 错误 2\r\nTraceback (most recent call last):\r\n  File \"/home/linaro/FastDeploy/python/setup.py\", line 445, in &lt;module&gt;\r\n    setuptools.setup(\r\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 153, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.10/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.10/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.10/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/lib/python3.10/distutils/command/build.py\", line 135, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/lib/python3.10/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n****",
        "state": "open",
        "user": "wzd129",
        "closed_by": null,
        "created_at": "2024-08-11T09:21:48+00:00",
        "updated_at": "2024-08-12T05:30:13+00:00",
        "closed_at": null,
        "comments_count": [
            "wzd129",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2507,
        "title": "使用FD部署Paddleseg生成的模型时报错",
        "body": "![image](https://github.com/user-attachments/assets/8484c600-f4a4-4485-b197-ec1275c18549)\r\n\r\nDeeplabv3p的模型\r\n\r\n",
        "state": "open",
        "user": "CashBai",
        "closed_by": null,
        "created_at": "2024-08-19T07:14:35+00:00",
        "updated_at": "2024-08-20T02:42:43+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2506,
        "title": "使用Fastdeploy之后就无法导入Taskflow？另外同一进程无法加载两个Fastdeploy模型？",
        "body": "环境windows, x86, cpu, paddle2.6.1, paddlenlp2.6.1, fastdeploy-python 1.07\r\n\r\n我想在本地部署三个模型脚本，其中两个只依赖fastdeploy，第三个fastdeploy示例里没有因此用的是paddlenlp的Taskflow。\r\n\r\n首先感谢fastdeploy团队，大大加速我的模型在cpu上的推理速度。但是现在遇到三个问题，其中前两个已经避开了，第三个有点麻烦：\r\n\r\n第一，我的两个fastdeploy脚本就是使用example里的ocr和uie python脚本进行简单封装，创建模型对象，调用预测函数。这两个单独都能正常运行工作，但是两个不能运行在一个进程，否则第一个预测时会失效，表现为无返回结果不报错。这个我通过多进程调用解决了，两个都能正常工作。请问这是正常现象嘛，还是是bug？虽然我知道可能服务化部署多进程调用才是正确的，但是小规模使用还是希望一个进程直接调用比较方便。\r\n\r\n第二，fastdeploy加载的模型内存对象如果出了作用域，即使用python默认的复制、return把赋值给别的变量，fastdeploy的模型也会失效，表现为无返回结果不报错。这个我通过创建对象，把模型保存在对象成员变量里把这个问题解决了。不过感觉这个行为和一般的python绑定cpp对象行为不一致，比如numpy的对象不管怎么复制还是在一个函数里创建，返回到函数外面也能用，不会失效。不知道会不会改进。\r\n\r\n```python\r\ndef create_model():\r\n    uie_model = UIEModel(...)\r\n    uie_model.predict(...) # 这里predict有效\r\n    return uie_mode\r\nuie_model = create_model()\r\nuie_model.predict(...) # 这里predict失效，无报错，不返回结果  \r\n```\r\n\r\n第三，导入fastdeploy相关库后，无法导入Taskflow。反之，导入Taskflow后，无法导入fastdeploy。这个问题也没办法通过单个python解释器开启多进程脚本解决。我再wsl里测试也是类似效果。\r\n\r\n```python\r\nimport fastdeploy\r\nfrom paddlenlp import Taskflow\r\n```\r\n\r\n报错\r\n```\r\nError: Can not import paddle core while this file exists: d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\libpaddle.pyd\r\nTraceback (most recent call last):\r\n  File \"d:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py\", line 2, in <module>\r\n    from paddlenlp import Taskflow\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddlenlp\\__init__.py\", line 33, in <module>\r\n    import paddle\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\__init__.py\", line 28, in <module>\r\n    from .base import core  # noqa: F401\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\__init__.py\", line 36, in <module>\r\n    from . import core\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 380, in <module>\r\n    raise e\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 268, in <module>\r\n    from . import libpaddle\r\nImportError: DLL load failed while importing libpaddle: 找不到指定的程序。\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton> ^C\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton>\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton>  d:; cd 'd:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton'; & 'd:\\miniconda\\envs\\fastdeploy\\python.exe' 'c:\\Users\\zhaoy\\.vscode\\extensions\\ms-python.debugpy-2024.10.0-win32-x64\\bundled\\libs\\debugpy\\adapter/../..\\debugpy\\launcher' '56080' '--' 'd:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py' \r\nError: Can not import paddle core while this file exists: d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\libpaddle.pyd\r\nTraceback (most recent call last):\r\n  File \"d:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py\", line 2, in <module>\r\n    from paddlenlp import Taskflow\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddlenlp\\__init__.py\", line 33, in <module>\r\n    import paddle\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\__init__.py\", line 28, in <module>\r\n    from .base import core  # noqa: F401\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\__init__.py\", line 36, in <module>\r\n    from . import core\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 380, in <module>\r\n    raise e\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 268, in <module>\r\n    from . import libpaddle\r\nImportError: DLL load failed while importing libpaddle: 找不到指定的程序。\r\n```\r\n\r\n如果反过来，如下：\r\n```python\r\nfrom paddlenlp import Taskflow\r\nimport fastdeploy\r\n```\r\n报错\r\n```\r\nError: Can not import paddle core while this file exists: d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\libpaddle.pyd\r\nTraceback (most recent call last):\r\n  File \"d:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py\", line 2, in <module>\r\n    from paddlenlp import Taskflow\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddlenlp\\__init__.py\", line 33, in <module>\r\n    import paddle\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\__init__.py\", line 28, in <module>\r\n    from .base import core  # noqa: F401\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\__init__.py\", line 36, in <module>\r\n    from . import core\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 380, in <module>\r\n    raise e\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\paddle\\base\\core.py\", line 268, in <module>\r\n    from . import libpaddle\r\nImportError: DLL load failed while importing libpaddle: 找不到指定的程序。\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton> ^C\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton>\r\n(fastdeploy) PS D:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton>  d:; cd 'd:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton'; & 'd:\\miniconda\\envs\\fastdeploy\\python.exe' 'c:\\Users\\zhaoy\\.vscode\\extensions\\ms-python.debugpy-2024.10.0-win32-x64\\bundled\\libs\\debugpy\\adapter/../..\\debugpy\\launcher' '56160' '--' 'd:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py' \r\nd:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\_distutils_hack\\__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: DLL load failed while importing fastdeploy_main: 找不到指定的程序。\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\SeafileData\\rush1993\\My Libraries\\OneDrive\\CodeLearning\\Paddle\\general_card_uie_singleton\\test_scripts\\test_import.py\", line 3, in <module>\r\n    import fastdeploy\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\__init__.py\", line 122, in <module>\r\n    from .c_lib_wrap import (\r\n  File \"d:\\miniconda\\envs\\fastdeploy\\lib\\site-packages\\fastdeploy\\c_lib_wrap.py\", line 166, in <module>\r\n    raise RuntimeError(f\"FastDeploy initalized failed! Error: {e}\")\r\nRuntimeError: FastDeploy initalized failed! Error: DLL load failed while importing fastdeploy_main: 找不到指定的程序。\r\n```",
        "state": "open",
        "user": "Strepsiades",
        "closed_by": null,
        "created_at": "2024-08-15T10:20:25+00:00",
        "updated_at": "2025-06-10T06:22:52+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "Strepsiades"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2509,
        "title": "运行python代码报错condition: bindings[x] || nullBindingOK",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： deploy自己编译\r\n- 【编译命令】export ENABLE_ORT_BACKEND=ON\r\n                        export ENABLE_PADDLE_BACKEND=OFF\r\n                        export ENABLE_OPENVINO_BACKEND=OFF\r\n                        export ENABLE_VISION=ON\r\n                        export ENABLE_TEXT=OFF\r\n                        export ENABLE_TRT_BACKEND=ON\r\n                        export WITH_GPU=ON\r\n                        export CUDA_DIRECTORY=/usr/local/cuda\r\n                        export TRT_DIRECTORY=/home/sh/soft/TensorRT-8.5.3.1/\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： 笔记本 Nvidia GPU 1050， CUDA 11.4 CUDNN 8.6\r\n- 【编译语言】：Python3.10\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- - examples/vision/detection/paddledetection/python/infer_ppyoloe.py部署示例可以跑通，用的是ppyoloe_crn_l_300e_coco模型。\r\n-但是当使用pphuman提供的ppyoloe_crn_s_36e_crowdhuman模型就会报错，模型连接为https://paddledet.bj.bcebos.com/models/ppyoloe_crn_s_36e_crowdhuman.pdparams\r\n- 下载的模型也经过pp-detection(v2.7)的模型转换，加了trt=true\r\n- 报错内容如下\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(731)::CreateTrtEngineFromOnnx\tCannot build engine right now, because there's dynamic input shape exists, list as below,\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(735)::CreateTrtEngineFromOnnx\tInput 0: TensorInfo(name: image, shape: [-1, 3, 640, 640], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(735)::CreateTrtEngineFromOnnx\tInput 1: TensorInfo(name: scale_factor, shape: [-1, 2], dtype: FDDataType::FP32)\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(737)::CreateTrtEngineFromOnnx\tFastDeploy will build the engine while inference with input data, and will also collect the input shape range information. You should be noticed that FastDeploy will rebuild the engine while new input shape is out of the collected shape range, this may bring some time consuming problem, refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/runtime/runtime.cc(339)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\nstart det\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(40)::Update\t[New Shape Out of Range] input name: image, shape: [1, 3, 640, 640], The shape range before: min_shape=[-1, 3, 640, 640], max_shape=[-1, 3, 640, 640].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(52)::Update\t[New Shape Out of Range] The updated shape range now: min_shape=[1, 3, 640, 640], max_shape=[1, 3, 640, 640].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(40)::Update\t[New Shape Out of Range] input name: scale_factor, shape: [1, 2], The shape range before: min_shape=[-1, 2], max_shape=[-1, 2].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/utils.cc(52)::Update\t[New Shape Out of Range] The updated shape range now: min_shape=[1, 2], max_shape=[1, 2].\r\n[WARNING] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(330)::Infer\tTensorRT engine will be rebuilt once shape range information changed, this may take lots of time, you can set a proper shape range before loading model to avoid rebuilding process. refer https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/faq/tensorrt_tricks.md for more details.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(572)::BuildTrtEngine\tStart to building TensorRT Engine...\r\n\r\n\r\n\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log\t3: [executionContext.cpp::enqueueInternal::629] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::enqueueInternal::629, condition: bindings[x] || nullBindingOK\r\n)\r\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(348)::Infer\tFailed to Infer with TensorRT.\r\n[ERROR] fastdeploy/vision/detection/ppdet/base.cc(73)::BatchPredict\tFailed to inference by runtime.\r\nDetectionResult: [xmin, ymin, xmax, ymax, score, label_id]\r\n\r\nFPS =  32.92232339089482\r\n\r\n\r\n-主要代码：\r\nmodel_dir = './infer_weights/ppyoloe_crn_l_300e_coco/'\r\nimage = './demo/test_img.jpg'\r\n\r\noption = fd.RuntimeOption()\r\noption.use_gpu()\r\noption.use_trt_backend()\r\noption.trt_option.serialize_file = './model_cache.trt'\r\noption.enable_trt_fp16()    \r\n\r\nmodel_file = os.path.join(model_dir, \"model.pdmodel\")\r\nparams_file = os.path.join(model_dir, \"model.pdiparams\")\r\nconfig_file = os.path.join(model_dir, \"infer_cfg.yml\")\r\n\r\nmodel = fd.vision.detection.PPYOLOE(\r\n    model_file, params_file, config_file, runtime_option=option)\r\n\r\nim = cv2.imread(image)\r\nwhile True:\r\n\ttime1 = time.time()\r\n\tresult = model.predict(im)\r\n\ttime2 = time.time()\r\n\ttime_use = time2 - time1\r\n\tprint(result)\r\n\t# cv2.imwrite(\"visualized_result.jpg\", vis_im)\r\n\tprint(\"FPS = \", str(1.0 / time_use))\r\n",
        "state": "closed",
        "user": "MichaelSunEngineer",
        "closed_by": "MichaelSunEngineer",
        "created_at": "2024-08-25T16:55:16+00:00",
        "updated_at": "2024-08-28T03:20:07+00:00",
        "closed_at": "2024-08-28T03:20:07+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "MichaelSunEngineer",
            "MichaelSunEngineer",
            "MichaelSunEngineer",
            "Jiang-Jia-Jun",
            "Jiang-Jia-Jun",
            "MichaelSunEngineer",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2511,
        "title": "重复刷新模型造成的内存泄露问题",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n【FastDeploy版本】： deploy自己编译\r\n【编译命令】export ENABLE_ORT_BACKEND = OFF\r\nexport ENABLE_PADDLE_BACKEND = OFF\r\nexport ENABLE_OPENVINO_BACKEND = OFF\r\nexport ENABLE_VISION = ON\r\nexport ENABLE_TEXT = OFF\r\nexport ENABLE_TRT_BACKEND = OFF\r\nexport WITH_GPU = OFF\r\n【系统平台】: Windows 11\r\n【硬件】： 笔记本 intel i7-1260p\r\n【编译语言】：C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n由于工程要求，每次刷新相机之后都需要重新刷新一次AI模型。经过测试发现，如果重复调用多次模型初始化函数，会造成内存泄漏，每隔10s~20s会泄露0.3M。下面是我的初始化函数\r\nbool OcrModel::InitModel(OcrModelConfig& config) {\r\n    // 将 GBK 路径转换为 UTF-8 路径\r\n    std::string model_dir = config.strNetPath;\r\n\r\n    // 构建模型文件的路径\r\n    std::filesystem::path model_path = std::filesystem::path(model_dir) / \"inference.pdmodel\";\r\n    std::filesystem::path params_path = std::filesystem::path(model_dir) / \"inference.pdiparams\";\r\n    std::filesystem::path config_path = std::filesystem::path(model_dir) / \"inference.yaml\";\r\n\r\n    std::string& strError = config.strError;\r\n\r\n    // 检查文件是否存在\r\n    if (!fs::exists(model_path) || !fs::exists(params_path)) {\r\n        config.strError = \"Model or parameter file does not exist.\";\r\n        return false;\r\n    }\r\n\r\n    // 初始化模型\r\n    option.UseCpu();  // 使用 CPU\r\n    try {\r\n        model = fastdeploy::vision::ocr::DBDetector(model_path.string(), params_path.string(), option);\r\n    }\r\n    catch (const std::exception& e) {\r\n        strError = \"Failed to create the OCR model: \" + std::string(e.what());\r\n        return false;\r\n    }\r\n\r\n    if (!model.Initialized()) {\r\n        strError = \"Failed to initialize the OCR model.\";\r\n        return false;\r\n    }\r\n\r\n    try {\r\n        model.GetPostprocessor().SetDetDBThresh(config.det_db_thresh);\r\n        model.GetPostprocessor().SetDetDBBoxThresh(config.det_db_box_thresh);\r\n        model.GetPostprocessor().SetDetDBUnclipRatio(config.det_db_unclip_ratio);\r\n    }\r\n    catch (const std::exception& e) {\r\n        strError = \"Failed to set post-processing parameters: \" + std::string(e.what());\r\n        return false;\r\n    }\r\n    return true;\r\n}\r\n",
        "state": "open",
        "user": "Algabeno",
        "closed_by": null,
        "created_at": "2024-08-26T08:38:11+00:00",
        "updated_at": "2025-06-10T06:22:53+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "Algabeno",
            "Jiang-Jia-Jun",
            "Algabeno",
            "Hommoner",
            "Algabeno"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2512,
        "title": "Fastdeploy服务化部署时failed to load model 'cls_runtime': at least one version must be available under the version policy of model 'cls_runtime'",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.7-cpu-only-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【编译语言】：Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n```bash\r\ndocker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-cpu-only-21.10\r\n\r\ndocker run --name fdocr -dit -p 8000:8000 -p 8002:8002 -p 8001:8001  --shm-size=\"1g\" -v $PWD:/ocr_serving registry.baidubce.com/paddlepaddle/fastdeploy:1.0.4-cpu-only-21.10 bash\r\n\r\ndocker exec -it -u root fdocr bash\r\n```\r\n使用CPU部署，运行docker时出错\r\n![image](https://github.com/user-attachments/assets/3a263728-1493-4960-978b-ad62fede9200)\r\n![image](https://github.com/user-attachments/assets/e0a3e7c8-df44-46b4-b284-90da7f2e32a5)\r\n![image](https://github.com/user-attachments/assets/71421fa4-05bb-458c-93b3-ba4e3d79015d)\r\n![image](https://github.com/user-attachments/assets/08aae6b3-46d6-4812-a6e5-017c5de16751)\r\n![image](https://github.com/user-attachments/assets/7bbc30dd-0f96-4497-a0da-113f92fe2fcf)\r\n![image](https://github.com/user-attachments/assets/433ae1af-d18c-484a-a5f2-546acebab327)\r\n",
        "state": "closed",
        "user": "IceHowe",
        "closed_by": "IceHowe",
        "created_at": "2024-08-27T01:50:14+00:00",
        "updated_at": "2024-09-05T06:49:07+00:00",
        "closed_at": "2024-08-27T02:19:50+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2510,
        "title": "测试rvm Pfld Pipnet 感觉都有点问题",
        "body": "- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-osx-arm64-1.0.7\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Mac OSX arm(14.0)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\r\n- 【编译语言】： C++ \r\n\r\n- 【模型精度问题】\r\n使用rvm明显把头像切了部分，手出现也不完整，和原版比质量差了很多\r\n![image](https://github.com/user-attachments/assets/d6fb3b75-85c7-4ae6-ac53-4d4479c7ff4b)\r\nPfld Pipnet 人脸识别点位置都乱了，而且乱跳，是不是有问题需要确认下\r\n<img width=\"336\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6d8116ce-40ba-4c59-9439-57e452b7aff5\">\r\n\r\n还有 [MobileNetV2SE68](https://github.com/cunjian/pytorch_face_landmark)  这个能都加下效果更好的",
        "state": "open",
        "user": "chfeizy",
        "closed_by": null,
        "created_at": "2024-08-26T07:23:51+00:00",
        "updated_at": "2025-06-10T06:22:52+00:00",
        "closed_at": null,
        "comments_count": [
            "chfeizy",
            "chfeizy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2513,
        "title": "Fastdeploy服务化部署模型后，客户端请求失败",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy:1.0.7-cpu-only-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 22.04) \r\n- 【硬件】： VM虚拟机中\r\n- 【编译语言】：Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n## 下面这是docker服务端\r\n![image](https://github.com/user-attachments/assets/f3687588-4756-4429-9ab0-34b170cef884)\r\n![image](https://github.com/user-attachments/assets/9e2e4ee6-12db-4252-92f3-bef0072c4470)\r\n![image](https://github.com/user-attachments/assets/a4205a96-8aa9-4abb-a90a-b74152b93473)\r\n![image](https://github.com/user-attachments/assets/354909fe-3ddd-4a15-b559-6dacb25e0c77)\r\n![image](https://github.com/user-attachments/assets/942aa346-0a9c-4cf7-8ca7-028c57604028)\r\n![image](https://github.com/user-attachments/assets/2c88b060-f32f-4f98-a898-922cf463f8b7)\r\n![image](https://github.com/user-attachments/assets/c778cceb-525d-478d-88b1-596e4ae667b7)\r\n\r\n## 运行示例客户端代码：\r\n![image](https://github.com/user-attachments/assets/6d89b875-742e-4050-b3ee-22dae939b052)\r\n\r\n![image](https://github.com/user-attachments/assets/6bd93ca5-1469-4300-b374-5f90cd8b468b)\r\n请问这种情况如何解决？是因为性能差超时吗？如果是的话该如何修改服务端的超时时间（30s）？",
        "state": "closed",
        "user": "IceHowe",
        "closed_by": "IceHowe",
        "created_at": "2024-08-27T05:45:45+00:00",
        "updated_at": "2024-08-27T17:45:50+00:00",
        "closed_at": "2024-08-27T17:45:50+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2514,
        "title": "模型直接推理没错，服务化部署后，客户端请求后结果不对",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy:1.0.7-cpu-only-21.10\r\n- 【系统平台】: Linux x64(Ubuntu 22.04)\r\n- 【硬件】： VM虚拟机中\r\n- 【编译语言】： C++ / Python3.9\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 这是用别人写的Http请求，结果错误：\r\n![image](https://github.com/user-attachments/assets/65a596e5-1b44-4a77-a23b-ec43ed4be353)\r\n如果用官方示例的rpc请求，报错，没有结果，好像是越界之类的\r\n",
        "state": "open",
        "user": "IceHowe",
        "closed_by": null,
        "created_at": "2024-08-27T17:48:53+00:00",
        "updated_at": "2024-08-27T17:49:11+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2521,
        "title": "希望能够支持华为910b3，cann8.0",
        "body": "华为的驱动和算子库已经升级到24.0和cann8.0了，希望能够支持这个版本的产品",
        "state": "open",
        "user": "bltcn",
        "closed_by": null,
        "created_at": "2024-09-07T08:07:54+00:00",
        "updated_at": "2024-09-20T01:42:13+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2517,
        "title": "华为昇腾支持cann7.0吗？",
        "body": "想请教一下华为昇腾支持cann7.0吗？",
        "state": "open",
        "user": "intjun",
        "closed_by": null,
        "created_at": "2024-09-03T02:11:51+00:00",
        "updated_at": "2024-09-20T01:43:25+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2519,
        "title": "OCR Cpu与Gpu推理结果不一致，Cpu错误",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-develop\r\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3090， CUDA 11.7 CUDNN 8.9\r\n- 【编译语言】： C++ \r\n- 【PaddleOCR版本】：v2.8\r\n\r\n【模型来源】：https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar\r\n【模型导出】： python3 tools/export_model.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml -o Global.pretrained_model=\"pretrain/en_PP-OCRv3_rec_train/best_accuracy\" Global.save_inference_dir=\"pretrain/en_PP-OCRv3_rec_train/exportold/\"\r\n【python版本CPU推理正确】：python tools/infer/predict_rec.py --rec_model_dir=\"pretrain/en_PP-OCRv3_rec_train/exportold/\" --image_dir=\"testimg/3/\" --rec_char_dict_path=\"ppocr/utils/en_dict.txt\" --rec_image_shape=\"3,48,320\" --use_gpu=false\r\n\r\n[2024/09/03 16:12:09] ppocr INFO: Predicts of testimg/3/2C_105_58_52_0088288__train1_1.jpg:('70', 0.997565746307373)\r\n[2024/09/03 16:12:09] ppocr INFO: Predicts of testimg/3/2C_105_58_52_0088288__train1_2.jpg:('61', 0.8275187015533447)\r\n[2024/09/03 16:12:09] ppocr INFO: Predicts of testimg/3/long_00009204_error_train_1.jpg:('T3O', 0.2974623739719391)\r\n【python版本GPU推理正确】python tools/infer/predict_rec.py --rec_model_dir=\"pretrain/en_PP-OCRv3_rec_train/exportold/\" --image_dir=\"testimg/3/\" --rec_char_dict_path=\"ppocr/utils/en_dict.txt\" --rec_image_shape=\"3,48,320\" --use_gpu=true\r\n\r\n[2024/09/03 16:13:21] ppocr INFO: Predicts of testimg/3/2C_105_58_52_0088288__train1_1.jpg:('70', 0.9975684881210327)\r\n[2024/09/03 16:13:21] ppocr INFO: Predicts of testimg/3/2C_105_58_52_0088288__train1_2.jpg:('61', 0.8278857469558716)\r\n[2024/09/03 16:13:21] ppocr INFO: Predicts of testimg/3/long_00009204_error_train_1.jpg:('T3O', 0.2983874976634979)\r\n\r\n【C++FastDeployGPU推理正确-ubuntu/windows】保证recshape设置正确，dict也正确，模型一致\r\n结果\r\ntestimg/3/2C_105_58_52_0088288__train1_1.jpg:text=70 rec_prob=0.997568\r\nPredicts of testimg/3/2C_105_58_52_0088288__train1_2.jpg:text=61 rec_prob=0.827548\r\nPredicts of testimg/3/long_00009204_error_train_1.jpg:text=T3O        rec_prob=0.297802\r\n\r\n【C++FastDeployCPU推理错误-ubuntu/windows】保证recshape设置正确，dict也正确，模型一致\r\n结果\r\ntestimg/3/2C_105_58_52_0088288__train1_1.jpg:text=70 rec_prob=0.745858\r\nPredicts of testimg/3/2C_105_58_52_0088288__train1_2.jpg:text=   rec_prob=0\r\nPredicts of testimg/3/long_00009204_error_train_1.jpg:text=   rec_prob=0\r\n\r\n请问CPU推理如何改进可以正确推理呢？",
        "state": "open",
        "user": "panshuaisharon",
        "closed_by": null,
        "created_at": "2024-09-03T08:31:26+00:00",
        "updated_at": "2024-09-26T02:07:28+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "panshuaisharon",
            "Jiang-Jia-Jun",
            "panshuaisharon"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2518,
        "title": "Csharp 推理异常闪退",
        "body": "C# Release代码\r\n\r\nopencv4.8版本\r\nopencvsharp4.7 指定版本\r\ncuda 12.1版本\r\n\r\n\r\n运行时使用Cpu进行推理\r\n  RuntimeOption runtimeoption = new RuntimeOption();\r\n  runtimeoption.UseCpu();\r\n  fastdeploy.vision.detection.PPYOLOE model = new fastdeploy.vision.detection.PPYOLOE(model_file, params_file, config_file, runtimeoption, ModelFormat.PADDLE);\r\n  if (!model.Initialized())\r\n  {\r\n      Console.WriteLine(\"Failed to initialize.\\n\");\r\n  }\r\n  Mat image = Cv2.ImRead(image_path);\r\n   fastdeploy.vision.DetectionResult res = model.Predict(image);\r\n  Console.WriteLine(res.ToString());\r\n  Mat res_img = fastdeploy.vision.Visualize.VisDetection(image, res, 0, 1, 0.5f);\r\n  Cv2.ImShow(\"result.png\", res_img);\r\n  Cv2.WaitKey(0);\r\n\r\n正常加载读取模型\r\n运行到Predict推理，就异常闪退。调试后DetectionResult detection_result =ConvertResult.ConvertCResultToDetectionResult(fd_detection_result); 问题，可以检测到目标300个，然后在return后直接异常报错，包括下方的Visualize.VisDetection(image, res, 0, 1, 0.5f);中涉及到的反转换也都是异常，最后运行完结果\r\n![image](https://github.com/user-attachments/assets/4ce5d733-be70-43e5-9ec2-38bdbe4be005)\r\n\r\nC#的exe程序不使用兼容模式的话应该直接到predict就闪退了，使用兼容模式能到最后结果显示，但是程序中的错误肯定还是如上面描述一样存在的，该如何处理",
        "state": "open",
        "user": "hjm950118",
        "closed_by": null,
        "created_at": "2024-09-03T08:03:48+00:00",
        "updated_at": "2024-09-20T01:44:50+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2527,
        "title": "部署llm是否使用paddlepaddle2.6也可以？和使用nightly的区别是什么？",
        "body": "看到 https://github.com/PaddlePaddle/FastDeploy/tree/develop/llm\r\n想知道 部署llm是否使用paddlepaddle2.6也可以？和使用nightly的区别是什么？",
        "state": "open",
        "user": "Aganlengzi",
        "closed_by": null,
        "created_at": "2024-09-29T05:33:15+00:00",
        "updated_at": "2024-10-08T09:09:07+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2528,
        "title": "镜像Dockerfile问题请教",
        "body": "请问\r\nregistry.baidubce.com/paddlepaddle/fastdeploy:llm-base-gcc12.3-cuda11.8-cudnn8-nccl2.15.5\r\n\r\n的dockerfile方便提供一下吗？",
        "state": "open",
        "user": "Aganlengzi",
        "closed_by": null,
        "created_at": "2024-09-29T08:58:30+00:00",
        "updated_at": "2024-10-08T09:08:30+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2530,
        "title": "linux_aarch64的安装包1.0.2之后没有了，希望能够支持后续版本",
        "body": "\r\nhttps://www.paddlepaddle.org.cn/whl/fastdeploy.html",
        "state": "open",
        "user": "walaiwalai",
        "closed_by": null,
        "created_at": "2024-10-03T03:10:23+00:00",
        "updated_at": "2024-10-08T09:00:42+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2529,
        "title": "yolov11 能增加不",
        "body": "https://docs.ultralytics.com/models/yolo11/\r\n\r\nyolov11 能增加不",
        "state": "open",
        "user": "monkeycc",
        "closed_by": null,
        "created_at": "2024-10-01T07:34:58+00:00",
        "updated_at": "2024-12-19T00:27:18+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "zhoujun0715",
            "Jiang-Jia-Jun",
            "zhoujun0715",
            "zhoujun0715",
            "zhoujun0715",
            "zhoujun0715",
            "ChaoII",
            "ChaoII",
            "zhoujun0715",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2532,
        "title": "请问一下fastdeploy_serving支持ascend（310/910）系列部署吗？",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/serving/docs/zh_CN\r\n看了文档好像仅支持XPU，如果使用晟腾系列的显卡该如何编译fastdeploy_serving？",
        "state": "open",
        "user": "cgq0816",
        "closed_by": null,
        "created_at": "2024-10-10T08:58:22+00:00",
        "updated_at": "2024-10-10T08:58:26+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2531,
        "title": "相同的模型结构，不同的参数，预测时间差异很大",
        "body": "私有训练了一个OCR识别模型，训出来两个版本，模型结构一模一样，保存到硬盘上大小也没有区别。\r\n\r\n现象是：\r\nfastdeploy输入paddle模型，同样的代码，\r\n选择onnx cpu推理，thread=1，先后进行了多次测试，速度测试稳定。\r\n但结果差异巨大，一个是10s，一个是20s，区别只是识别模型用了一个结构的不同版本，也就是网络结构一致，weights值或者训练时的epoch不同。\r\n\r\n请问大家遇到过这种问题吗？",
        "state": "open",
        "user": "terrencew",
        "closed_by": null,
        "created_at": "2024-10-10T07:32:36+00:00",
        "updated_at": "2024-10-10T07:48:28+00:00",
        "closed_at": null,
        "comments_count": [
            "terrencew",
            "terrencew"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2533,
        "title": "自行编译的fastdeploy无法使用gpu进行推理",
        "body": "## 环境\r\n\r\n- 【编译命令】自行编译的FastDeploy，（参数命令）：\r\n- \r\ncd FastDeploy/python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=ON\r\nexport ENABLE_OPENVINO_BACKEND=ON\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TEXT=ON\r\nexport ENABLE_TRT_BACKEND=ON\r\nexport WITH_GPU=ON\r\nexport TRT_DIRECTORY=/usr/local/TensorRT-8.5.2.2\r\nexport CUDA_DIRECTORY=/usr/local/cuda-11.7\r\nexport OPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \r\n\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n\r\n- 【系统平台】: Linux x64(Ubuntu 20.04.1) \r\n- 【硬件】： Nvidia GPU 3060， CUDA 11.7 CUDNN 8.6.0\r\n- 【编译语言】：Python3.8\r\n\r\n## 问题日志及出现问题的操作流程\r\ncd FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python\r\n python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device gpu --backend paddle\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption    Will inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption    Will inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption    Will inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::GPU.\r\nfree(): invalid size\r\n已放弃 (核心已转储)\r\n问题总结：当--device gpu 时会中断推理，当--device cpu时不存在问题：\r\n python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image 12.jpg --device cpu --backend paddle\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend  Runtime initialized with Backend::PDINFER in Device::CPU.\r\ndet boxes: [[42,413],[483,391],[484,428],[43,450]]rec text: 上海斯格威铂尔大酒店 rec score:0.980085 cls label: 0 cls score: 1.000000\r\ndet boxes: [[187,456],[399,448],[400,480],[188,488]]rec text: 打浦路15号 rec score:0.964993 cls label: 0 cls score: 1.000000\r\ndet boxes: [[23,507],[513,488],[515,529],[24,548]]rec text: 绿洲仕格维花园公寓 rec score:0.993727 cls label: 0 cls score: 1.000000\r\ndet boxes: [[74,553],[427,542],[428,571],[75,582]]rec text: 打浦路252935号 rec score:0.947723 cls label: 0 cls score: 1.000000\r\n\r\n当我把自行编译的fastdeploy卸载之后安装：pip install fastdeploy-gpu-python==1.0.7 -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html，这时gpu和cpu都可以推理，我为什么非要重新编译fastdeploy是因为我想要添加如下内容到fastdeploy/vision/vision_pybind.cc文件中。\r\n.def_readwrite(\"table_html\", &vision::OCRResult::table_structure)\r\n .def_readwrite(\"table_boxes\", &vision::OCRResult::table_boxes)\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "tammy-na",
        "closed_by": null,
        "created_at": "2024-10-11T06:34:38+00:00",
        "updated_at": "2024-10-15T11:54:29+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2534,
        "title": "关于算法后处理设计疑问，以yolov5为例",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Ubuntu 20.04\r\n- 【硬件】： sophon 1688\r\n- 【编译语言】： C++ \r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【后处理复用问题】\r\n- - sophon平台上基于yolov5算法实现了几个模型，后处理稍微有些不同。有的后处理可以在硬件加速，有的不能，因此不同的模型会涉及到对后处理方法重写的地方。但发现FastDeploy框架中没有对外提供对后处理的方式。\r\n考虑重载一个YOLOv5::Predict(cv::Mat* im, DetectionResult* result, YOLOv5PostprocessorImpl* post);\r\n方法来使用YOLOv5PostprocessorImpl继承YOLOv5Postprocessor，实现多态，如：\r\n```\r\nfastdeploy::vision::DetectionResult res;\r\n//重写后处理的子类\r\nfastdeploy::vision::detection::YOLOv5PostprocessorImpl post;\r\nmodel.Predict(&det_image, &res, &post));\r\n```\r\n但发现YOLOv5的postprocessor_成员不是指针，\r\n```\r\n protected:\r\n  bool Initialize();\r\n  YOLOv5Preprocessor preprocessor_;\r\n  YOLOv5Postprocessor postprocessor_;\r\n```\r\n因此无法实现多态。如果修改postprocessor的成员为指针，其他方法的访问也需要修改。请问有什么更好的方法么？\r\n从个人理解来看，每一个算法的后处理部分应该变化较多，做成一个可继承接口是不是更好些？\r\n",
        "state": "open",
        "user": "mailliw2010",
        "closed_by": null,
        "created_at": "2024-10-12T06:05:07+00:00",
        "updated_at": "2024-10-15T11:53:23+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2539,
        "title": "在安卓端实现目标跟踪",
        "body": "您好！请问在安卓端可以实现目标跟踪吗？",
        "state": "open",
        "user": "Edward-YS",
        "closed_by": null,
        "created_at": "2024-10-24T07:46:23+00:00",
        "updated_at": "2024-10-24T07:46:27+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2535,
        "title": "调用uie的c++代码，循环调用predict函数，出现内存一直增长",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： FastDeploy-release-1.0.7\r\n- 【编译语言】： C++\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【内存泄漏】\r\n- - 先执行`examples`下的部署示例，测试通过，但是循环调用预测函数，内存一直增加。\r\n\r\n",
        "state": "open",
        "user": "lhx150926",
        "closed_by": null,
        "created_at": "2024-10-12T11:23:28+00:00",
        "updated_at": "2025-05-13T04:07:11+00:00",
        "closed_at": null,
        "comments_count": [
            "bert-y"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2536,
        "title": "Python统计耗时明显高于模型内置统计耗时",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】：fastdeploy-linux-gpu-0.0.0\r\n- 【系统平台】: Linux x64 (Ubuntu  20.04)\r\n- 【硬件】： Nvidia GPU 4060Ti,  conda-forge: CUDA 11.7, CUDNN 8.4\r\n- 【编译语言】： Python (3.10）\r\n\r\n## 性能疑问\r\n\r\n- FastDeploy模型内置的压测统计耗时和Python层面的统计耗时不一致的问题。\r\n- 如何缩小内置耗时`57.6966ms`和Python耗时`118ms`的差距。\r\n\r\n```python\r\nimport time\r\nimport fastdeploy as fd\r\nimport numpy as np\r\nimport statistics\r\n\r\n\r\nif __name__ == '__main__':\r\n    option = fd.RuntimeOption()\r\n    option.use_gpu(0)\r\n    option.use_trt_backend()\r\n    option.trt_option.enable_fp16 = True\r\n    option.trt_option.set_shape('images', [1, 3, 640, 640], [1, 3, 640, 640], [40, 3, 640, 640])\r\n    option.trt_option.serialize_file = 'weights/yolov8m.engine'\r\n    model = fd.vision.detection.YOLOv8('weights/yolov8m.onnx', runtime_option=option)\r\n\r\n    ims = [np.random.randint(0, 256, (360, 640, 3), dtype=np.uint8) for _ in range(20)]\r\n\r\n    model.enable_record_time_of_runtime()\r\n    costs = []\r\n    for i in range(500):\r\n        if 100 <= i:\r\n            begin = time.perf_counter()\r\n        results = model.batch_predict(ims)\r\n        if 100 <= i:\r\n            costs.append(time.perf_counter() - begin)\r\n    model.print_statis_info_of_runtime()\r\n\r\n    print(f'{int(1000 * statistics.mean(costs))}ms')\r\n```\r\n\r\n```python\r\n$ python benchmark.py \r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(719)::CreateTrtEngineFromOnnx\tDetect serialized TensorRT Engine file in weights/yolov8m.engine, will load it directly.\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(108)::LoadTrtCache\tBuild TensorRT Engine from cache file: weights/yolov8m.engine with shape range information as below,\r\n[INFO] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(111)::LoadTrtCache\tInput name: images, shape=[-1, 3, -1, -1], min=[1, 3, 640, 640], max=[40, 3, 640, 640]\r\n\r\n[INFO] fastdeploy/runtime/runtime.cc(339)::CreateTrtBackend\tRuntime initialized with Backend::TRT in Device::GPU.\r\n============= Runtime Statis Info(yolov8) =============\r\nTotal iterations: 500\r\nTotal time of runtime: 29.7184s.\r\nWarmup iterations: 100\r\nTotal time of runtime in warmup step: 6.63981s.\r\nAverage time of runtime exclude warmup step: 57.6966ms.\r\n118ms\r\n```",
        "state": "closed",
        "user": "ucsk",
        "closed_by": "ucsk",
        "created_at": "2024-10-15T02:20:57+00:00",
        "updated_at": "2025-01-22T01:32:19+00:00",
        "closed_at": "2024-11-18T06:24:05+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "ucsk",
            "wuxie-k"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2538,
        "title": "自训练了ocr的det模型，用的v3，但是在Android中运行报错，直接用demo能够正常运行，求解答",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\nWindows\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10)\r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4090\r\n- 【编译语言】： Python(3.6)\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - `examples`下的小程序部分的代码可以运行，但自己的模型，不能运行\r\n模型如下，转换后的json，另外.dat不知道是什么用（只单独训练了det模型）：\r\n[model.json](https://github.com/user-attachments/files/17471363/model.json)\r\n- - - 错误log\r\n`WAServiceMainContext.js:2 WebGL: INVALID_OPERATION: texImage2D: ArrayBufferView not big enough for request\r\nindex.js? [sm]:497 [Violation] 'load' handler took 1204ms\r\nWAServiceMainContext.js:2 WebGL: INVALID_OPERATION: texImage2D: ArrayBufferView not big enough for request\r\nindex.js? [sm]:497 [Violation] 'load' handler took 1026ms\r\nWAServiceMainContext.js:2 WebGL: INVALID_OPERATION: texImage2D: ArrayBufferView not big enough for request\r\nindex.js? [sm]:497 [Violation] 'load' handler took 1020ms\r\nWAServiceMainContext.js:2 WebGL: INVALID_OPERATION: texImage2D: ArrayBufferView not big enough for request\r\nindex.js? [sm]:497 [Violation] 'load' handler took 1021ms\r\nworker.js?libName=WA…celerateWorker.js:1 [worker] reportRealtimeAction:fail not support\r\nWAServiceMainContext.js:2 Invalid context type [webgl2] for Canvas#getContext\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nWAServiceMainContext.js:2 Invalid context type [webgl2] for Canvas#getContext\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d_depthwise -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\n2\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\n2\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d_depthwise -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\n2\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d_depthwise -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\n2\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\n2\r\nindex.js? [sm]:1 webgl createProgram: elementwise_mul -- Error: Error: compile: ERROR: 0:131: '[]' : vector field selection out of range\r\n\r\n(env: macOS,mp,1.06.2303220; lib: 2.23.4)\r\nindex.js? [sm]:1 webgl createProgram: conv2d_depthwise -- Error: Error: compile: ERROR: 0:186: 'undefined' : undeclared identifier\r\nERROR: 0:186: '' : boolean expression expected\r\n`\r\n- 【模型精度问题】\r\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\r\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n- 【性能问题】描述清楚对比的方式\r\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\r\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\r\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\r\n",
        "state": "open",
        "user": "mozeqiu123",
        "closed_by": null,
        "created_at": "2024-10-22T06:48:56+00:00",
        "updated_at": "2024-11-01T01:29:42+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "mozeqiu123",
            "mozeqiu123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2537,
        "title": "加载PPYOLOE+模型时报错",
        "body": "![image](https://github.com/user-attachments/assets/c479c3ad-b53c-4d22-aa41-fd1f9ab7481e)\r\n![image](https://github.com/user-attachments/assets/d96dcd61-2937-42ca-96a4-acf3a15aa0f2)\r\n安装模式用的FD文档中的Conda指令“conda config --add channels conda-forge && conda install cudatoolkit=11.2 cudnn=8.2”\r\nPPYOLOE+的模型是用的Paddledetection训练的，develop版本\r\n![image](https://github.com/user-attachments/assets/05a93a1c-a69c-4cae-b8ab-34876257981e)\r\n",
        "state": "open",
        "user": "CashBai",
        "closed_by": null,
        "created_at": "2024-10-22T01:25:17+00:00",
        "updated_at": "2024-12-13T06:55:20+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2540,
        "title": " cannot open shared object file: No such file or directory",
        "body": "跑官方例子的时候直接就报错了,华为昇腾的卡 找不到libhuawei_ascend_npu.so, libascendcl.so\r\nmulticlass_nms3:tmp_17,concat_14.tmp_0:multiclass_nms3_0.tmp_1,multiclass_nms3_0.tmp_2,multiclass_nms3_0.tmp_0\r\nfetch:multiclass_nms3_0.tmp_0:fetch\r\nfetch:multiclass_nms3_0.tmp_2:fetch\r\n\r\n[INFO] fastdeploy/runtime/runtime.cc(354)::CreateLiteBackend    Runtime initialized with Backend::PDLITE in Device::ASCEND.\r\n[F 10/28 12:31:52. 73 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libascendcl.so: cannot open shared object file: No such file or directory\r\n[F 10/28 12:31:52. 73 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libascendcl.so: cannot open shared object file: No such file or directory\r\n\r\nTraceback (most recent call last):\r\n  File \"infer_ppyoloe.py\", line 69, in <module>\r\n    result = model.predict(im)\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/fastdeploy_python-0.0.0-py3.7-linux-aarch64.egg/fastdeploy/vision/detection/ppdet/__init__.py\", line 126, in predict\r\n    return self._model.predict(im)\r\nRuntimeError: NNAdapter C++ Exception: \r\n**[F 10/28 12:31:52. 73 ...nadapter/nnadapter/src/runtime/device.cc:529 Find] Failed to load the nnadapter device HAL library for 'huawei_ascend_npu' from libhuawei_ascend_npu.so, libascendcl.so: cannot open shared object file: No such file or directory**",
        "state": "open",
        "user": "jo-dean",
        "closed_by": null,
        "created_at": "2024-10-28T12:35:19+00:00",
        "updated_at": "2024-10-28T12:36:27+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2543,
        "title": "华为Ascend310P卡死了",
        "body": "[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:78 Context] profiling path: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:88 Context] dump model path: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:98 Context] precision mode: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:120 Context] op select impl mode: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:130 Context] op type list for impl mode: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:140 Context] enable compressw weight: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:150 Context] auto tune mode: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:160 Context] enable dynamic shape range: \r\n[I 10/30  9:32:46. 24 ...r/src/driver/huawei_ascend_npu/engine.cc:176 Context] initial buffer length of dynamic shape range: -1\r\n[W 10/30  9:32:46. 24 ...ter/nnadapter/src/runtime/compilation.cc:334 Finish] Warning: Failed to create a program, No model and cache is provided.\r\n[W 10/30  9:32:46. 24 ...le-Lite/lite/kernels/nnadapter/engine.cc:149 LoadFromCache] Warning: Build model failed(3) !\r\n[W 10/30  9:32:46. 49 ...nnadapter/nnadapter/src/runtime/model.cc:86 GetSupportedOperations] Warning: Failed to get the supported operations for device 'huawei_ascend_npu', because the HAL interface 'validate_program' is not implemented!\r\n[W 10/30  9:32:46. 49 ...kernels/nnadapter/converter/converter.cc:171 Apply] Warning: Failed to get the supported operations for the selected devices, one or more of the selected devices are not supported!\r\n[I 10/30  9:32:46. 49 ...r/src/driver/huawei_ascend_npu/driver.cc:70 CreateProgram] Create program for huawei_ascend_npu.\r\n[F 10/30  9:35:41.268 ...driver/huawei_ascend_npu/model_client.cc:54 InitAclClientEnv] Check failed: (reinterpret_cast<aclError>(aclrtSetDevice(device_id_)) == ACL_ERROR_NONE): 507033!==0 507033 Unknown ACL error code(507033)\r\n[F 10/30  9:35:41.268 ...driver/huawei_ascend_npu/model_client.cc:54 InitAclClientEnv] Check failed: (reinterpret_cast<aclError>(aclrtSetDevice(device_id_)) == ACL_ERROR_NONE): 507033!==0 507033 Unknown ACL error code(507033)\r\n\r\n[F 10/30  9:35:41.270 ...ter/nnadapter/src/runtime/compilation.cc:98 ~Program] Check failed: device_context: No device found.\r\n[F 10/30  9:35:41.270 ...ter/nnadapter/src/runtime/compilation.cc:98 ~Program] Check failed: device_context: No device found.\r\n\r\nterminate called after throwing an instance of 'nnadapter::logging::Exception'\r\n  what():  NNAdapter C++ Exception: \r\n[F 10/30  9:35:41.270 ...ter/nnadapter/src/runtime/compilation.cc:98 ~Program] Check failed: device_context: No device found.\r\n\r\nAborted (core dumped)\r\nroot@devserver-1183:/Work/FastDeploy/examples/vision/detection/paddledetection/python# /usr/local/python3.7.5/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 91 leaked semaphores to clean up at shutdown\r\n  len(cache))\r\n卡死在这里了",
        "state": "open",
        "user": "jo-dean",
        "closed_by": null,
        "created_at": "2024-10-30T09:39:06+00:00",
        "updated_at": "2024-10-30T09:39:11+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2544,
        "title": "TrtBackend::LoadTrtCache加载tensorrt模型时报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF \\\r\n         -DENABLE_OPENVINO_BACKEND=OFF \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DCMAKE_BUILD_TYPE=DEBUG \\\r\n         -DWITH_GPU=ON \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_INSTALL_PREFIX=/usr/local/ \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON \\\r\n         -DBUILD_EXAMPLES=ON \\\r\n         -DTRT_DIRECTORY=${PWD}/../../thirdlibs-x86/tensorrt/TensorRT-8.6.1.6 \\\r\n         -DOPENCV_DIRECTORY=/usr/local/share/OpenCV/ \\\r\n         -DOPENVINO_DIRECTORY=${PWD}/../../thirdlibs-x86/openvino \r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： \r\n- nvidia 2060, \r\n- NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 11.1\r\n- TensorRT-8.6.1.6\r\n- \r\n- 【编译语言】： C++ ,python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n模型转换参数\r\n～/tensorrt/TensorRT-8.6.1.6/bin/trtexec \\\r\n--onnx=fusionmodel_final_inNHWC-float32-255_outNHWC-float32-1.onnx  \\\r\n--saveEngine=x86/fusionmodel_final_inNHWC-float32-255_outNHWC-float32-1.trt  \\\r\n--sparsity=enable \\\r\n--noTF32 \\\r\n--fp16 \\\r\n--int8 \\\r\n--skipInference \\\r\n--minShapes=img_vis:1x100x100x3,img_ir:1x100x100x1 \\\r\n--optShapes=img_vis:1x480x640x3,img_ir:1x480x640x1 \\\r\n--maxShapes=img_vis:1x480x640x3,img_ir:1x480x640x1 \\\r\n--builderOptimizationLevel=0\r\n\r\n加载tensorrt模型时报错：\r\n![Screenshot from 2024-10-31 15-31-35](https://github.com/user-attachments/assets/d450b85e-9de0-413d-ac0f-d2841cb29cac)\r\n![Screenshot from 2024-10-31 15-31-12](https://github.com/user-attachments/assets/cb53be87-427e-4fa9-aa4f-a365950534a3)\r\n![Screenshot from 2024-10-31 15-29-48](https://github.com/user-attachments/assets/4eca3d30-7267-4fb2-aa8f-8ee5a578acd5)\r\n\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "aidenyao",
        "closed_by": null,
        "created_at": "2024-10-31T07:33:36+00:00",
        "updated_at": "2024-11-04T06:39:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2552,
        "title": "FastDeploy c++部署ppyoloe_r 旋转框预测异常",
        "body": "参考 #1872  问题解决了，在导出ppyoloe_r 模型时要加上 export_onnx=True  \r\n",
        "state": "closed",
        "user": "ifluck",
        "closed_by": "ifluck",
        "created_at": "2024-11-12T02:54:38+00:00",
        "updated_at": "2024-11-13T13:46:59+00:00",
        "closed_at": "2024-11-13T13:46:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2546,
        "title": "web demo替换自训练det模型出问题，求大佬解答（看过issue里的关于web demo的问题，没有解决）",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 训练：使用的是paddleOCR最新版训练det模型，验证测试没问题。转换inference：根据webdemo以及issue，修改了tools/export_model.py中infer_shape = [3, 960, 960]，然后转换了det模型。转换paddlejs：在本项目中通过以下命令（FastDeploy\\examples\\application\\js\\converter> paddlejsconverter --modelPath=det_20241101_new/inference.pdmodel --paramPath=det_20241101_new/inference.pdiparams --outputDir=det_20241101_new）生成了model.json和chunk_1.dat \r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- 【系统平台】: Windows x64(Windows10) \r\n- 【硬件】： 训练：4090。webdemo：集显\r\n- 【编译语言】： nodejs：v22.11.0   npm：10.9.0\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【操作流程】\r\n- - 我先把模型上传阿里云的oss存储，开启了跨域访问（我不确定是否的是必须要百度的存储才行）\r\n- - 然后修改代码配置参数和模型路径：\r\nconst detConfig = {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]};\r\nawait ocr.init({modelPath:\"https://***/paddlejs/model.json\"},detConfig);\r\n- 【运行】\r\n- - 运行没有报错\r\n- -点击文本识别一直显示模型加载中，F12 console信息如下：\r\n![image](https://github.com/user-attachments/assets/10209e37-1156-41c9-9c7e-a1ffb8ab60b7)\r\n在network里面我没看到下载model.json模型成功。可能必须要用百度的云存储？？\r\n\r\n",
        "state": "open",
        "user": "mozeqiu123",
        "closed_by": null,
        "created_at": "2024-11-04T06:58:50+00:00",
        "updated_at": "2025-06-10T06:22:07+00:00",
        "closed_at": null,
        "comments_count": [
            "mozeqiu123",
            "mozeqiu123"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2547,
        "title": "TrtBackend::LoadTrtCache加载tensorrt模型时报错",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： develop\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\n- cmake .. -DENABLE_ORT_BACKEND=ON \\\r\n         -DENABLE_PADDLE_BACKEND=OFF \\\r\n         -DENABLE_OPENVINO_BACKEND=OFF \\\r\n         -DENABLE_TRT_BACKEND=ON \\\r\n         -DCMAKE_BUILD_TYPE=DEBUG \\\r\n         -DWITH_GPU=ON \\\r\n         -DCUDA_DIRECTORY=/usr/local/cuda \\\r\n         -DCMAKE_INSTALL_PREFIX=/usr/local/ \\\r\n         -DENABLE_VISION=ON \\\r\n         -DENABLE_TEXT=ON \\\r\n         -DBUILD_EXAMPLES=ON \\\r\n         -DTRT_DIRECTORY=${PWD}/../../thirdlibs-x86/tensorrt/TensorRT-8.6.1.6 \\\r\n         -DOPENCV_DIRECTORY=/usr/local/share/OpenCV/ \\\r\n         -DOPENVINO_DIRECTORY=${PWD}/../../thirdlibs-x86/openvino \r\n- 【系统平台】: Linux x64(Ubuntu 18.04)\r\n- 【硬件】： \r\n- nvidia 2060, \r\n- NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 11.1\r\n- TensorRT-8.6.1.6\r\n- \r\n- 【编译语言】： C++ ,python3.7\r\n\r\n## 问题日志及出现问题的操作流程\r\n\r\n模型转换参数\r\n～/tensorrt/TensorRT-8.6.1.6/bin/trtexec \\\r\n--onnx=fusionmodel_final_inNHWC-float32-255_outNHWC-float32-1.onnx  \\\r\n--saveEngine=x86/fusionmodel_final_inNHWC-float32-255_outNHWC-float32-1.trt  \\\r\n--sparsity=enable \\\r\n--noTF32 \\\r\n--fp16 \\\r\n--int8 \\\r\n--skipInference \\\r\n--minShapes=img_vis:1x100x100x3,img_ir:1x100x100x1 \\\r\n--optShapes=img_vis:1x480x640x3,img_ir:1x480x640x1 \\\r\n--maxShapes=img_vis:1x480x640x3,img_ir:1x480x640x1 \\\r\n--builderOptimizationLevel=0\r\n\r\n加载tensorrt模型时报错：\r\n\r\n![Uploading Screenshot from 2024-10-31 15-31-35.png…]()\r\n![Uploading Screenshot from 2024-10-31 15-31-12.png…]()\r\n\r\n\r\n",
        "state": "open",
        "user": "aidenyao",
        "closed_by": null,
        "created_at": "2024-11-04T07:07:43+00:00",
        "updated_at": "2025-06-10T06:22:08+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2545,
        "title": "OCSort里的iou_batch与Python版本算法不一致",
        "body": "BreadcrumbsFastDeploy/streamer/src/gstreamer/plugin/fdtracker/src/ocsort.cpp\r\nhttps://github.com/PaddlePaddle/FastDeploy/blob/cd0ee79c91d4ed1103abdc65ff12ccadd23d0827/streamer/src/gstreamer/plugin/fdtracker/src/ocsort.cpp#L243\r\n\r\nhttps://github.com/PaddlePaddle/PaddleDetection/blob/666f597fd00d3157e84dd9fb2dd3f40bf9918ebc/deploy/pptracking/python/mot/matching/ocsort_matching.py#L22\r\n\r\n准备用这个代码的人，大家考虑改改代码\r\n\r\n参考修改如下：\r\nvoid iou_batch(cv::Mat &bboxes1, cv::Mat &bboxes2, cv::Mat &iou_matrix) {\r\n  for (int row = 0; row < bboxes1.rows; row++) {\r\n    cv::Mat box1 = bboxes1(cv::Rect(2, row, 4, 1));\r\n    float box1_area = (box1.at<float>(0, 2) - box1.at<float>(0, 0)) * \\\r\n      (box1.at<float>(0, 3) - box1.at<float>(0, 1));\r\n    for (int col = 0; col < bboxes2.rows; col++) {\r\n      cv::Mat box2 = bboxes2(cv::Rect(0, col, 4, 1));\r\n      float box2_area = (box2.at<float>(0, 2) - box2.at<float>(0, 0)) * \\\r\n        (box2.at<float>(0, 3) - box2.at<float>(0, 1));\r\n      float inner_xmin = std::max(box1.at<float>(0, 0), box2.at<float>(0, 0));\r\n      float inner_ymin = std::max(box1.at<float>(0, 1), box2.at<float>(0, 1));\r\n      float inner_xmax = std::min(box1.at<float>(0, 2), box2.at<float>(0, 2));\r\n      float inner_ymax = std::min(box1.at<float>(0, 3), box2.at<float>(0, 3));\r\n      float w = std::max(0.f, inner_xmax - inner_xmin);\r\n      float h = std::max(0.f, inner_ymax - inner_ymin);\r\n      float inner_area = w * h;\r\n      float outer_area = box1_area + box2_area - inner_area;\r\n\r\n      float iou = inner_area / outer_area;\r\n      iou_matrix.at<float>(row, col) = iou;\r\n    }\r\n  }\r\n}",
        "state": "closed",
        "user": "terrencew",
        "closed_by": "terrencew",
        "created_at": "2024-11-01T01:40:18+00:00",
        "updated_at": "2024-11-01T01:46:00+00:00",
        "closed_at": "2024-11-01T01:46:00+00:00",
        "comments_count": [
            "terrencew"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2549,
        "title": "yolov 8推理输出的目标检测框大小不正确",
        "body": "我使用yolov8做目标检测。目标检测框大小输出不对，其他的评分，和类别都正确。为什么呢",
        "state": "closed",
        "user": "huangguifeng",
        "closed_by": "huangguifeng",
        "created_at": "2024-11-08T02:49:54+00:00",
        "updated_at": "2024-11-08T06:29:34+00:00",
        "closed_at": "2024-11-08T06:11:53+00:00",
        "comments_count": [
            "huangguifeng",
            "huangguifeng",
            "huangguifeng"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2556,
        "title": "开启paddlelite支持在aarch64架构上编译不成功",
        "body": "## 环境\r\naarch64架构cpu 鲲鹏920 麒麟linux\r\n\r\n\r\n## 问题说明：\r\n想要编译一个在aarch64架构的上支持ppocr系列模型的fastdeploy-python。按文档，需要开启paddlelite支持。\r\n\r\n按如下命令编译：\r\n'''\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport ENABLE_ORT_BACKEND=ON\r\nexport ENABLE_LITE_BACKEND=ON\r\nexport ENABLE_PADDLE_BACKEND=OFF\r\nexport ENABLE_OPENVINO_BACKEND=OFF\r\nexport ENABLE_VISION=ON\r\nexport ENABLE_TEXT=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n'''\r\n发现编译不通过。\r\n\r\n初步排查有如下问题：\r\n1. 链接paddlelite的so文件失败。看了下代码，在编译时，会下载lite-linux-arm64-20221209.tgz，然后其中lite-linux-arm64-20221209\\lib里包含一个._libpaddle_full_api_shared.so文件，这个文件是苹果电脑的索引文件，会导致构建脚本误以为它是共享库：\r\n’‘’\r\nAssertionError: patchelf /data/ai/fastdeploy/FastDeploy/python/.setuptools-cmake-build/third_libs/install/paddlelite/lib/._lib\r\npaddle_full_api_shared.so failed, the command: /data/ai/fastdeploy/FastDeploy/python/.setuptools-cmake-build/third_libs/patchelf/bin/patchelf --set-rpath '$ORIGIN:$ORIGIN/mklml/lib/' /data/ai/fastdeploy/FastDeploy/python/.setuptools-cmake-build/third_l\r\nibs/install/paddlelite/lib/._libpaddle_full_api_shared.so\r\n‘’‘\r\n这个问题是1.02引入的，之前是lite-linux-arm64-20220920.tgz\r\n2. 我把paddlelite的.so文件替换掉，可以编译成功（1.07版本），但是现在运行程序时报：\r\n'''\r\n2024-11-14 17:26:17,794 - uvicorn.error - ERROR - Traceback (most recent call last):\r\n  File \"/root/miniconda3/envs/fastdeploy/lib/python3.10/site-packages/fastdeploy/c_lib_wrap.py\", line 164, in <module>\r\n    from .libs.fastdeploy_main import *\r\nImportError: /root/miniconda3/envs/fastdeploy/lib/python3.10/site-packages/fastdeploy/libs/libfastdeploy.so.1.0.7: undefined s\r\nymbol: _ZN6paddle8lite_api6Tensor11CopyFromCpuIlLNS0_10TargetTypeE1EEEvPKT_\r\n'''\r\n\r\n3. 由于还是未能成功运行。尝试用1.02版本使用lite后端运行ppocrv4，推理过程仍有错误。这部分错误没保存下来。\r\n4. 最后发现文档里写的，ppocr在aarch64架构linux只支持paddlelite是不对的，实际上支持onnx_runtime，我直接用1.02版本开启onnx_runtime后端就成功运行了。\r\n",
        "state": "open",
        "user": "Strepsiades",
        "closed_by": null,
        "created_at": "2024-11-16T01:44:26+00:00",
        "updated_at": "2025-06-10T06:22:08+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2550,
        "title": "华为昇腾310p编译安装出现CANN版本不匹配的问题",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： 编译安装develop分支\r\n- 【编译命令】cmake -DWITH_ASCEND=ON  -DCMAKE_INSTALL_PREFIX=fastdeploy-ascend  -DENABLE_VISION=ON   ..\r\n- 【系统平台】: Linux worker-12 4.19.90-23.8.v2101.ky10.x86_64\r\n- 【硬件】： Ascend 310I Pro（310P3）\r\n- 【编译语言】：Python 3.10\r\n- 【CANN版本】：CANN 6.4.12.1.241:6.3.RC2\r\n- 【npu 驱动版本】：Version: 23.0.rc2\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n-- 首先编译安装Paddle-Lite，编译命令：$./lite/tools/build_linux.sh --arch=x86 --with_extra=ON --with_log=ON --with_exception=ON --with_nnadapter=ON --nnadapter_with_huawei_ascend_npu=ON --nnadapter_huawei_ascend_npu_sdk_root=/usr/local/Ascend/ascend-toolkit/latest full_publish\r\n-- 修改FastDeploy/cmake/paddlelite.cmake中的内容：set(PADDLELITE_DIRECTORY /home/xxx/Paddle-Lite/build.lite.linux.x86.gcc/inference_lite_lib/cxx CACHE PATH \"Directory of custom Paddle-Lite library\")设置为编译好的Paddle-Lite库\r\n-- 根据示例进行C++和python编译安装Fastdeploy（https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/huawei_ascend.md）\r\n-- 运行example示例（https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/detection/paddledetection/python/README_CN.md）\r\n-- 执行命令：\r\n```shell\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/xxx/Paddle-Lite/build.lite.linux.x86.gcc/inference_lite_lib/third_party/mklml/lib/\r\npython infer_ppyoloe.py --model_dir ppyoloe_crn_l_300e_coco --image 000000014439.jpg --device ascend\r\n```\r\n\r\n无法完成昇腾芯片中的推理，错误信息如下：\r\nI1108 13:48:41.355319 730851 generate_program_pass.h:41] insts.size: 1\r\n[INFO] fastdeploy/runtime/runtime.cc(354)::CreateLiteBackend    Runtime initialized with Backend::PDLITE in Device::ASCEND.\r\n[W 11/ 8 13:48:41.490 .../src/driver/huawei_ascend_npu/utility.cc:57 InitializeAscendCL] CANN version mismatch. The build version is 0.0.0, but the current environment version is 6.3.2.\r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:42 Context] properties: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:67 Context] selected device ids: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:69 Context] 0\r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:79 Context] profiling path: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:89 Context] dump model path: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:99 Context] precision mode: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:121 Context] op select impl mode: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:131 Context] op type list for impl mode: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:141 Context] enable compressw weight: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:151 Context] auto tune mode: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:161 Context] enable dynamic shape range: \r\n[I 11/ 8 13:48:41.677 ...r/src/driver/huawei_ascend_npu/engine.cc:177 Context] initial buffer length of dynamic shape range: -1\r\n[W 11/ 8 13:48:41.678 ...ter/nnadapter/src/runtime/compilation.cc:334 Finish] Warning: Failed to create a program, No model and cache is provided.\r\nW1108 13:48:41.678025 730851 engine.cc:149] Warning: Build model failed(3) !\r\n[W 11/ 8 13:48:41.693 ...nnadapter/nnadapter/src/runtime/model.cc:86 GetSupportedOperations] Warning: Failed to get the supported operations for device 'huawei_ascend_npu', because the HAL interface 'validate_program' is not implemented!\r\nW1108 13:48:41.693092 730851 converter.cc:171] Warning: Failed to get the supported operations for the selected devices, one or more of the selected devices are not supported!\r\n[I 11/ 8 13:48:41.693 ...r/src/driver/huawei_ascend_npu/driver.cc:70 CreateProgram] Create program for huawei_ascend_npu.\r\n[F 11/ 8 13:48:42.749 .../src/driver/huawei_ascend_npu/utility.cc:315 BuildOMModelToBuffer] Check failed: (reinterpret_cast<ge::graphStatus>(aclgrphBuildModel(ir_graph, options, om_buffer)) == ge::GRAPH_SUCCESS): 1343266818!==0 1343266818 Unknown ATC error code(1343266818)\r\n[F 11/ 8 13:48:42.749 .../src/driver/huawei_ascend_npu/utility.cc:315 BuildOMModelToBuffer] Check failed: (reinterpret_cast<ge::graphStatus>(aclgrphBuildModel(ir_graph, options, om_buffer)) == ge::GRAPH_SUCCESS): 1343266818!==0 1343266818 Unknown ATC error code(1343266818)\r\n\r\n[F 11/ 8 13:48:42.854 ...ter/nnadapter/src/runtime/compilation.cc:98 ~Program] Check failed: device_context: No device found.\r\n[F 11/ 8 13:48:42.854 ...ter/nnadapter/src/runtime/compilation.cc:98 ~Program] Check failed: device_context: No device found.\r\n\r\n已放弃 (核心已转储)\r\n\r\n",
        "state": "open",
        "user": "Hakstar",
        "closed_by": null,
        "created_at": "2024-11-08T06:09:58+00:00",
        "updated_at": "2025-06-10T06:22:08+00:00",
        "closed_at": null,
        "comments_count": [
            "guzichen",
            "xujiang1",
            "xujiang1",
            "mytk2012",
            "Hakstar"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2555,
        "title": "关于C# FastDeploy部署，编译后找不到C# 动态库文件 fastdeploy_csharp.dll ",
        "body": "求大佬救救\r\n需要在VS C#中部署目标检测模型。但是编译FastDeploy后没有生成fastdeploy_csharp.dll 动态库文件。编译没有报错。\r\ncmake .. -G \"Visual Studio 16 2019\" -A x64 ^\r\n         -DENABLE_ORT_BACKEND=ON ^\r\n         -DENABLE_PADDLE_BACKEND=ON ^\r\n         -DENABLE_OPENVINO_BACKEND=ON ^\r\n         -DENABLE_TRT_BACKEND=ON ^\r\n         -DENABLE_VISION=ON ^\r\n         -DENABLE_TEXT=ON ^\r\n         -WITH_CAPI=ON ^\r\n         -WITH_CSHARPAPI=ON ^\r\n         -DWITH_GPU=ON ^\r\n         -DTRT_DIRECTORY=\"D:\\VM\\TensorRT-8.6.1.6\" ^\r\n         -DCUDA_DIRECTORY=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\" ^\r\n         -DCMAKE_INSTALL_PREFIX=\"D:\\paddle\\compiled_fastdeploy\"\r\n\r\n% nuget restore  （please execute it when WITH_CSHARPAPI=ON to prepare dependencies in C#)\r\nmsbuild fastdeploy.sln /m /p:Configuration=Release /p:Platform=x64\r\nmsbuild INSTALL.vcxproj /m /p:Configuration=Release /p:Platform=x64",
        "state": "closed",
        "user": "caojun226",
        "closed_by": "caojun226",
        "created_at": "2024-11-14T04:49:11+00:00",
        "updated_at": "2024-11-15T07:45:56+00:00",
        "closed_at": "2024-11-15T07:45:56+00:00",
        "comments_count": [
            "caojun226"
        ],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 2569
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2558,
        "title": "无法正常运行ppocrv4_rec_server，其他可以正常运行",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\r\n    - fastdeploy-gpu-python  1.0.7\r\n       fastdeploy-tools       0.0.5\r\n-  运行指令：\r\n代码为示例代码：pipeline/multi_thread_process_ocr.py\r\n运行方法：\r\n`python multi_thread_process_ocr.py --det_model '/home/ubuntu/shawn/ocr-split/ocr_mix/models/ch_PP-OCRv4_det_infer' --cls_model 'ch_ppocr_mobile_v2.0_cls_infer' --rec_model '/home/ubuntu/shawn/ocr-split/ocr_mix/models/ch_PP-OCRv4_rec_server_infer' --rec_label_file '/home/ubuntu/shawn/FastDeploy/tutorials/multi_thread/python/pipeline/ppocr_keys_v1.txt' --image_path '/home/ubuntu/shawn/test_imgs/sda.jpg' --device gpu --use_multi_process True --process_num 3 --backend paddle`\r\n\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 附上详细的问题日志有助于快速定位分析\r\n- 【模型跑不通】\r\n- - ppocrv4的server版本rec 无法运行，其他模型都可以正常运行，但是只要切换到server版本的识别模型就会报错\r\n\r\n\r\n报错日志：\r\nmultiprocessing.pool.RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/miniconda3/envs/ocr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/ubuntu/miniconda3/envs/ocr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\r\n    return list(map(*args))\r\n  File \"/home/ubuntu/shawn/FastDeploy/tutorials/multi_thread/python/pipeline/multi_thread_process_ocr.py\", line 219, in process_predict\r\n    result = ppocr_v3.predict(im)\r\n  File \"/home/ubuntu/miniconda3/envs/ocr/lib/python3.10/site-packages/fastdeploy/vision/ocr/ppocr/__init__.py\", line 958, in predict\r\n    return self.system_.predict(input_image)\r\nRuntimeError: \r\n\r\n  Compile Traceback (most recent call last):\r\n    File \"tools/export_model.py\", line 288, in <module>\r\n      main()\r\n    File \"tools/export_model.py\", line 284, in main\r\n      model, arch_config, save_path, logger, input_shape=input_shape)\r\n    File \"tools/export_model.py\", line 197, in export_single_model\r\n      paddle.jit.save(model, save_path)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/jit.py\", line 629, in wrapper\r\n      func(layer, path, input_spec, **configs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/jit.py\", line 857, in save\r\n      inner_input_spec, with_hook=with_hook)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 528, in concrete_program_specify_input_spec\r\n      *desired_input_spec, with_hook=with_hook)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 436, in get_concrete_program\r\n      concrete_program, partial_program_layer = self._program_cache[cache_key]\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 801, in __getitem__\r\n      self._caches[item_id] = self._build_once(item)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 790, in _build_once\r\n      **cache_key.kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 733, in from_func_spec\r\n      outputs = static_func(*inputs)\r\n    File \"/tmp/tmp18grrv7q.py\", line 28, in forward\r\n      false_fn_1, (x,), (x,), (x,))\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 211, in convert_ifelse\r\n      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 257, in _run_py_ifelse\r\n      return true_fn(*true_args) if pred else false_fn(*false_args)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/architectures/base_model.py\", line 86, in forward\r\n      x = self.backbone(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/tmp/tmparp3ri44.py\", line 45, in forward\r\n      for_loop_condition_0, for_loop_body_0, [i, __for_loop_var_index_0, x])\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 45, in convert_while_loop\r\n      loop_vars = _run_py_while(cond, body, loop_vars)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 59, in _run_py_while\r\n      loop_vars = body(*loop_vars)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 252, in forward\r\n      x = stage(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 172, in forward\r\n      x = self.blocks(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/container.py\", line 98, in forward\r\n      input = layer(input)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 125, in forward\r\n      x = self.att(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 76, in forward\r\n      x = self.conv(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/conv.py\", line 678, in forward\r\n      use_cudnn=self._use_cudnn)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/nn/functional/conv.py\", line 169, in _conv_nd\r\n      type=op_type, inputs=inputs, outputs=outputs, attrs=attrs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/layer_helper.py\", line 44, in append_op\r\n      return self.main_program.current_block().append_op(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py\", line 3621, in append_op\r\n      attrs=kwargs.get(\"attrs\", None))\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py\", line 2635, in __init__\r\n      for frame in traceback.extract_stack():\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::AnalysisPredictor::ZeroCopyRun()\r\n1   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)\r\n2   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const\r\n3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&, paddle::framework::RuntimeContext*) const\r\n4   void phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::KernelCallHelper<paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::TypeTag<int> >::Compute<1, 3, 0, 0, phi::GPUContext const, phi::DenseTensor const, phi::DenseTensor const, phi::DenseTensor const>(phi::KernelContext*, phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&)\r\n5   void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)\r\n6   phi::DnnWorkspaceHandle::RunFunc(std::function<void (void*)> const&, unsigned long)\r\n7   std::_Function_handler<void (void*), phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)::{lambda(void*)#4}>::_M_invoke(std::_Any_data const&, void*&&)\r\n8   phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n9   phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. \r\n  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnStatus_t) to get Nvidia's official solution and advice about CUDNN Error.] (at /home/fastdeploy/develop/paddle_build/v0.0.0/Paddle/paddle/phi/kernels/fusion/gpu/conv_fusion_kernel.cu:611)\r\n  [operator < conv2d_fusion > error]\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/shawn/FastDeploy/tutorials/multi_thread/python/pipeline/multi_thread_process_ocr.py\", line 250, in <module>\r\n    pool.map(process_predict, imgs_list)\r\n  File \"/home/ubuntu/miniconda3/envs/ocr/lib/python3.10/multiprocessing/pool.py\", line 367, in map\r\n    return self._map_async(func, iterable, mapstar, chunksize).get()\r\n  File \"/home/ubuntu/miniconda3/envs/ocr/lib/python3.10/multiprocessing/pool.py\", line 774, in get\r\n    raise self._value\r\nRuntimeError: \r\n\r\n  Compile Traceback (most recent call last):\r\n    File \"tools/export_model.py\", line 288, in <module>\r\n      main()\r\n    File \"tools/export_model.py\", line 284, in main\r\n      model, arch_config, save_path, logger, input_shape=input_shape)\r\n    File \"tools/export_model.py\", line 197, in export_single_model\r\n      paddle.jit.save(model, save_path)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/jit.py\", line 629, in wrapper\r\n      func(layer, path, input_spec, **configs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/jit.py\", line 857, in save\r\n      inner_input_spec, with_hook=with_hook)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 528, in concrete_program_specify_input_spec\r\n      *desired_input_spec, with_hook=with_hook)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 436, in get_concrete_program\r\n      concrete_program, partial_program_layer = self._program_cache[cache_key]\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 801, in __getitem__\r\n      self._caches[item_id] = self._build_once(item)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 790, in _build_once\r\n      **cache_key.kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/decorator.py\", line 232, in fun\r\n      return caller(func, *(extras + args), **kw)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 733, in from_func_spec\r\n      outputs = static_func(*inputs)\r\n    File \"/tmp/tmp18grrv7q.py\", line 28, in forward\r\n      false_fn_1, (x,), (x,), (x,))\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 211, in convert_ifelse\r\n      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 257, in _run_py_ifelse\r\n      return true_fn(*true_args) if pred else false_fn(*false_args)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/architectures/base_model.py\", line 86, in forward\r\n      x = self.backbone(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/tmp/tmparp3ri44.py\", line 45, in forward\r\n      for_loop_condition_0, for_loop_body_0, [i, __for_loop_var_index_0, x])\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 45, in convert_while_loop\r\n      loop_vars = _run_py_while(cond, body, loop_vars)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py\", line 59, in _run_py_while\r\n      loop_vars = body(*loop_vars)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 252, in forward\r\n      x = stage(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 172, in forward\r\n      x = self.blocks(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/container.py\", line 98, in forward\r\n      input = layer(input)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 125, in forward\r\n      x = self.att(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/workspace/gry/docker/code/luolixin/baidu/paddle/paddle-uapi/uapi/PaddleOCR/ppocr/modeling/backbones/rec_hgnet.py\", line 76, in forward\r\n      x = self.conv(x)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 930, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py\", line 915, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/conv.py\", line 678, in forward\r\n      use_cudnn=self._use_cudnn)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/nn/functional/conv.py\", line 169, in _conv_nd\r\n      type=op_type, inputs=inputs, outputs=outputs, attrs=attrs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/layer_helper.py\", line 44, in append_op\r\n      return self.main_program.current_block().append_op(*args, **kwargs)\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py\", line 3621, in append_op\r\n      attrs=kwargs.get(\"attrs\", None))\r\n    File \"/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py\", line 2635, in __init__\r\n      for frame in traceback.extract_stack():\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::AnalysisPredictor::ZeroCopyRun()\r\n1   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, phi::Place const&)\r\n2   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&) const\r\n3   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, phi::Place const&, paddle::framework::RuntimeContext*) const\r\n4   void phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::KernelCallHelper<paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, phi::TypeTag<int> >::Compute<1, 3, 0, 0, phi::GPUContext const, phi::DenseTensor const, phi::DenseTensor const, phi::DenseTensor const>(phi::KernelContext*, phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&)\r\n5   void phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)\r\n6   phi::DnnWorkspaceHandle::RunFunc(std::function<void (void*)> const&, unsigned long)\r\n7   std::_Function_handler<void (void*), phi::fusion::ConvFusionKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, std::string const&, bool, std::vector<int, std::allocator<int> > const&, int, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)::{lambda(void*)#4}>::_M_invoke(std::_Any_data const&, void*&&)\r\n8   phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)\r\n9   phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nExternalError: CUDNN error(9), CUDNN_STATUS_NOT_SUPPORTED. \r\n  [Hint: Please search for the error code(9) on website (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnStatus_t) to get Nvidia's official solution and advice about CUDNN Error.] (at /home/fastdeploy/develop/paddle_build/v0.0.0/Paddle/paddle/phi/kernels/fusion/gpu/conv_fusion_kernel.cu:611)\r\n  [operator < conv2d_fusion > error]",
        "state": "open",
        "user": "sunzx8",
        "closed_by": null,
        "created_at": "2024-11-18T05:41:23+00:00",
        "updated_at": "2025-06-10T06:22:09+00:00",
        "closed_at": null,
        "comments_count": [
            "Alex37882388"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2565,
        "title": "咨询：fd是否支持pp-yoloe-r旋转框检测？",
        "body": null,
        "state": "open",
        "user": "njflove",
        "closed_by": null,
        "created_at": "2024-11-22T11:34:38+00:00",
        "updated_at": "2025-06-10T06:22:09+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2568,
        "title": "FastDeploy对AMD GPU / 海光DCU的支持问题",
        "body": "请问FastDeploy在AMD GPU及海光DCU硬件环境下，是用哪个runtime驱动对应硬件设备？是onnx DirectML吗？",
        "state": "open",
        "user": "cerasumat",
        "closed_by": null,
        "created_at": "2024-11-29T02:04:42+00:00",
        "updated_at": "2025-06-10T06:22:10+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2566,
        "title": "这个仓库后续还继续更新吗？另外如果更新的话，可以考虑目前的大语言模型、视觉大模型的部署那就更好了",
        "body": null,
        "state": "closed",
        "user": "GLaStu",
        "closed_by": "GLaStu",
        "created_at": "2024-11-25T09:11:27+00:00",
        "updated_at": "2024-12-20T01:00:33+00:00",
        "closed_at": "2024-12-20T01:00:33+00:00",
        "comments_count": [
            "qianbin1989228",
            "GLaStu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2567,
        "title": "使用官方文字cls模型转onnx正常再转rknn报错",
        "body": "- 【系统平台】: Linux x64(Ubuntu 18.04) \r\n- 【编译语言】：Python(3.6）\r\n\r\npython export.py --config_path config/ppocrv3_cls.yaml --target_platform rk3588 \r\n{'mean': [[127.5, 127.5, 127.5]], 'std': [[127.5, 127.5, 127.5]], 'model_path': './onnx/ch_ppocr_mobile_v2.0_cls_infer.onnx', 'outputs_nodes': None, 'do_quantization': False, 'dataset': None, 'output_folder': './ch_ppocr_mobile_v2.0_cls_infer'}\r\nW __init__: rknn-toolkit2 version: 1.5.0+1fa95b5c\r\nE load_onnx: Catch exception when loading onnx model: /home/cxj/cxj/CPP/PPOCR/Base/rockchip/rknpu2_tools/onnx/ch_ppocr_mobile_v2.0_cls_infer.onnx!\r\nE load_onnx: Traceback (most recent call last):\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 1382, in rknn.api.rknn_base.RKNNBase.load_onnx\r\nE load_onnx:   File \"rknn/api/rknn_base.py\", line 658, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 58, in rknn.api.ir_graph.IRGraph.__init__\r\nE load_onnx:   File \"rknn/api/ir_graph.py\", line 503, in rknn.api.ir_graph.IRGraph.rebuild\r\nE load_onnx:   File \"/home/cxj/anaconda3/envs/rknn/lib/python3.6/site-packages/onnx/checker.py\", line 106, in check_model\r\nE load_onnx:     C.check_model(protobuf_string)\r\nE load_onnx: onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of type is required but missing.\r\nW If you can't handle this error, please try updating to the latest version of the toolkit2 and runtime from:\r\n  https://eyun.baidu.com/s/3eTDMk6Y (Pwd: rknn)  Path: RK_NPU_SDK / RK_NPU_SDK_1.X.0 / develop /\r\n  If the error still exists in the latest version, please collect the corresponding error logs and the model,\r\n  convert script, and input data that can reproduce the problem, and then submit an issue on:\r\n  https://redmine.rock-chips.com (Please consult our sales or FAE for the redmine account)\r\nTraceback (most recent call last):\r\n  File \"export.py\", line 52, in <module>\r\n    assert ret == 0, \"Load model failed!\"\r\nAssertionError: Load model failed!\r\n\r\n",
        "state": "open",
        "user": "Zomcxj",
        "closed_by": null,
        "created_at": "2024-11-26T06:53:42+00:00",
        "updated_at": "2025-06-10T06:22:09+00:00",
        "closed_at": null,
        "comments_count": [
            "Zomcxj",
            "x53151231",
            "x53151231",
            "Zomcxj",
            "x53151231"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2570,
        "title": "请问下使用fastdeploy部署paddleOCRv3的性能问题",
        "body": "按照官方文档部署ppOCRv3 fastdeploy服务，使用jmeter压测性能一直卡在10QPS左右，修改instance_group里面的count数值也没有提高并发。\r\n1. 推理后端使用tensorrt，精度是trt_fp8。\r\n2. 从原有的矩阵输入改成了图片base64输入。\r\n\r\n参考文档连接：https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/README.md\r\n\r\n部署环境\r\n【GPU】NVIDIA T4\r\n【docker镜像】fastdeploy:1.0.1-gpu-cuda11.4-trt8.4-21.10\r\n\r\n",
        "state": "open",
        "user": "ouerum",
        "closed_by": null,
        "created_at": "2024-12-04T09:33:53+00:00",
        "updated_at": "2025-06-10T06:22:10+00:00",
        "closed_at": null,
        "comments_count": [
            "ouerum",
            "Jiang-Jia-Jun",
            "ouerum",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2571,
        "title": "PP-OCRv4,什么时候增加",
        "body": "PP-OCRv4,什么时候增加",
        "state": "open",
        "user": "monkeycc",
        "closed_by": null,
        "created_at": "2024-12-05T08:11:37+00:00",
        "updated_at": "2025-06-10T06:22:10+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2574,
        "title": "PP-OCRv4 在没检测到文本的时候，结果出错",
        "body": "当PP-OCRv4没检测到结果的时候结果相矛盾，直接上图：\r\n\r\n![QQ_1734084433011](https://github.com/user-attachments/assets/2a6540fe-e15c-4711-92e1-b74bbbf3afc0)\r\n执行完\r\n![QQ_1734084450211](https://github.com/user-attachments/assets/14027001-2b67-4f9d-a05c-9b642cab587e)\r\ntext size 为1\r\nbox size 为0\r\n这是咋回事？",
        "state": "open",
        "user": "ChaoII",
        "closed_by": null,
        "created_at": "2024-12-13T10:08:06+00:00",
        "updated_at": "2025-06-10T06:22:11+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2572,
        "title": "C++SDK如何关闭日志啊",
        "body": null,
        "state": "open",
        "user": "monster-arch",
        "closed_by": null,
        "created_at": "2024-12-11T06:19:26+00:00",
        "updated_at": "2025-06-10T06:22:10+00:00",
        "closed_at": null,
        "comments_count": [
            "monster-arch"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2575,
        "title": "OCR的REC模型,能用RCNN的吗,比如rec_r50_fpn_srn",
        "body": "我只需要REC模型\r\n\r\nOCR的REC模型,能用RCNN的吗\r\n比如rec_r50_fpn_srn\r\n\r\n这边只有PP-OCR\r\n使用报错\r\n```\r\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::fastdeploy::PaddleBackend::BuildOption Will inference_precision float32\r\n[INFO] fastdeploy/runtime/runtime.cc(286)::fastdeploy::Runtime::CreatePaddleBackend     Runtime initialized with Backend::PDINFER in Device::GPU.\r\n[ERROR] fastdeploy/runtime/backends/paddle/paddle_backend.cc(392)::fastdeploy::PaddleBackend::Infer     [PaddleBackend] Size of inputs(1) should keep same with the inputs of this model(5).\r\n[ERROR] fastdeploy/vision/ocr/ppocr/recognizer.cc(121)::fastdeploy::vision::ocr::Recognizer::BatchPredict       Failed to inference by runtime.\r\n\r\n```\r\n",
        "state": "open",
        "user": "monkeycc",
        "closed_by": null,
        "created_at": "2024-12-17T01:37:54+00:00",
        "updated_at": "2025-06-10T06:22:11+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2573,
        "title": "FastDeploy在昇腾部署PaddleOCR（文字检测+文字识别）发现存在耗时的问题",
        "body": "我在昇腾使用FastDeploy部署PaddleOCR，文字识别和文字检测，测试发现存在耗时问题，后来排查发现在文字识别的时候，不同尺寸的图像会生成一个缓存文件，导致耗时的问题，又遇到相同问题的朋友吗，请问如何解决\r\n",
        "state": "open",
        "user": "zznnxx",
        "closed_by": null,
        "created_at": "2024-12-13T08:03:06+00:00",
        "updated_at": "2025-06-10T06:22:11+00:00",
        "closed_at": null,
        "comments_count": [
            "xujiang1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2576,
        "title": "fastdeploy模型支持滞后于paddlex3.0，后续部署逐步过渡到以paddlex为主，还是fastdeploy会同步更新",
        "body": null,
        "state": "open",
        "user": "bigkun",
        "closed_by": null,
        "created_at": "2024-12-20T07:07:42+00:00",
        "updated_at": "2025-06-10T06:22:12+00:00",
        "closed_at": null,
        "comments_count": [
            "bigkun",
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2578,
        "title": "可以部署到高通SNPE框架吗？在高通DSP上面可以跑吗？",
        "body": "有部署到高通DSP上面运行的教程吗？我看到的好像都是cpu和gpu的使用方法",
        "state": "open",
        "user": "ziFan99",
        "closed_by": null,
        "created_at": "2024-12-26T06:33:28+00:00",
        "updated_at": "2025-06-10T06:22:12+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2579,
        "title": "请问paddle.js还在维护吗？",
        "body": null,
        "state": "open",
        "user": "AIDeepx",
        "closed_by": null,
        "created_at": "2024-12-26T07:18:01+00:00",
        "updated_at": "2025-06-10T06:22:13+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2582,
        "title": "UNAVAILABLE: Invalid argument: unexpected inference input 'sigmoid_0.tmp_0', allowed inputs are: save_infer_model/scale_0.tmp_0",
        "body": "## 环境\r\n\r\n- 【FastDeploy版本】： docker pull registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\r\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\r\nDocker内启动服务时：fastdeployserver --model-repository=./examples/vision/ocr/PP-OCR/serving/fastdeploy_serving/models\r\n- 【系统平台】: Linux x64(centos) \r\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU A10\r\n- 【编译语言】： Python(3.6）\r\n\r\n## 问题日志及出现问题的操作流程\r\n- 【模型跑不通】\r\n- - 先执行`examples`下的部署示例，可以正确执行,但自己的模型不能运行\r\n| Model           | Version | Status                                                                                                                          |\r\n| cls_postprocess | 1       | READY                                                                                                                           |\r\n| cls_pp          | 1       | READY                                                                                                                           |\r\n| cls_runtime     | 1       | READY                                                                                                                           |\r\n| det_postprocess | 1       | READY                                                                                                                           |\r\n| det_preprocess  | 1       | READY                                                                                                                           |\r\n| det_runtime     | 1       | UNAVAILABLE: Invalid argument: unexpected inference input 'sigmoid_0.tmp_0', allowed inputs are: save_infer_model/scale_0.tmp_0 |\r\n| rec_postprocess | 1       | READY                                                                                                                           |\r\n| rec_pp          | 1       | READY                                                                                                                           |\r\n| rec_runtime     | 1       | READY                                                                                                                           |\r\n\r\n\r\n\r\n",
        "state": "open",
        "user": "pjd206",
        "closed_by": null,
        "created_at": "2024-12-30T06:33:22+00:00",
        "updated_at": "2025-06-10T06:22:14+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2577,
        "title": "# 预测结果可视化 vis_im = fd.vision.vis_detection(im, result, score_threshold=0.5) windows 下写出的文件为空白",
        "body": "*********************************************\r\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\r\n*********************************************\r\n\r\n## 环境\r\n\r\n- 【FastDeploy版本】： fastdeploy-python      1.0.7   fastdeploy-tools       0.0.5\r\n- 【系统平台】:  Windows x64(Windows10) \r\n- 【硬件】： CPU\r\n- 【编译语言】：python 3.10\r\n\r\n目标识别可以成功 但是 可视化结果为空白\r\n",
        "state": "open",
        "user": "xujiang1",
        "closed_by": null,
        "created_at": "2024-12-23T03:48:53+00:00",
        "updated_at": "2025-06-10T06:22:12+00:00",
        "closed_at": null,
        "comments_count": [
            "Tansong666",
            "Tansong666"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2581,
        "title": "AI快车道-FastDeploy推理部署套件系列直播课 里的文档无法下载",
        "body": "初步了解到fastdeploy工具，可以直接将paddleOCR模型转换成RK板子可用的形式，现在项目中需要将paddleOCR部署于瑞芯微RK3588板子上，需要详细的资料支持，看到有直播课的视频回放，但是文档找不到了，显示失败，希望官方帮忙找一下，或者有新的链接。",
        "state": "open",
        "user": "Wonddy",
        "closed_by": null,
        "created_at": "2024-12-27T07:34:14+00:00",
        "updated_at": "2025-06-10T06:22:13+00:00",
        "closed_at": null,
        "comments_count": [
            "Wonddy"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2580,
        "title": "npu 编译后无法使用 ",
        "body": "https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/huawei_ascend.md#%E4%B8%80%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87\r\n按照教程\r\n\r\n# Download the latest source code\r\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\r\ncd FastDeploy/python\r\nexport WITH_ASCEND=ON\r\nexport ENABLE_VISION=ON\r\n\r\npython setup.py build\r\npython setup.py bdist_wheel\r\n在 华为 arm  npu 上编译  无法使用 报错 \r\n```\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::FuseNormalizeCast  Normalize and Cast are fused to Normalize in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::FuseNormalizeHWC2CHW       Normalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\r\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::FuseNormalizeColorConvert BGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\r\n[ERROR] Cannot found attribute beta in op: swish\r\n```\r\n\r\npip list\r\n```\r\nPackage                Version\r\n---------------------- --------------------\r\nabsl-py                2.1.0\r\nannotated-types        0.7.0\r\nanyio                  4.7.0\r\nascendebug             0.1.0\r\nasgiref                3.8.1\r\nastor                  0.8.1\r\nastroid                3.2.2\r\nasttokens              2.4.1\r\nattrs                  23.2.0\r\naudioread              3.0.1\r\nauto-tune              0.1.0\r\nautograd               1.4\r\nBabel                  2.15.0\r\nbce-python-sdk         0.9.17\r\nblinker                1.8.2\r\ncertifi                2024.12.14\r\ncffi                   1.16.0\r\ncfgv                   3.4.0\r\nchardet                3.0.4\r\ncharset-normalizer     3.4.1\r\nclang-format           13.0.0\r\nclick                  8.1.8\r\ncloudpickle            3.0.0\r\ncoloredlogs            15.0.1\r\ncomm                   0.2.2\r\nconcurrent-log-handler 0.9.25\r\ncontourpy              1.2.1\r\ncoverage               5.5\r\ncryptography           42.0.8\r\ncycler                 0.12.1\r\nCython                 3.0.10\r\ndataflow               0.0.1\r\ndbus-python            1.2.16\r\ndebugpy                1.8.2\r\ndecorator              5.1.1\r\nDeprecated             1.2.14\r\ndill                   0.3.8\r\ndistlib                0.3.8\r\ndistro                 1.4.0\r\ndistro-info            0.23+ubuntu1.1\r\ndocker-pycreds         0.4.0\r\nexceptiongroup         1.2.2\r\nexecuting              2.0.1\r\nFarama-Notifications   0.0.4\r\nfastapi                0.115.6\r\nfastdeploy-python      0.0.0\r\nfastdeploy-tools       0.0.5\r\nfilelock               3.15.4\r\nFlask                  3.0.3\r\nflask-babel            4.0.0\r\nflatbuffers            24.3.25\r\nfonttools              4.53.0\r\nfuture                 1.0.0\r\ngitdb                  4.0.11\r\nGitPython              3.1.43\r\nGPUtil                 1.4.0\r\ngymnasium              1.0.0a2\r\nh11                    0.14.0\r\nhccl                   0.1.0\r\nhccl-parser            0.1\r\nhttpcore               1.0.5\r\nhttptools              0.3.0\r\nhttpx                  0.27.0\r\nhumanfriendly          10.0\r\nhypothesis             6.104.1\r\nidentify               2.5.36\r\nidna                   3.10\r\niniconfig              2.0.0\r\nipykernel              6.29.4\r\nipython                8.26.0\r\nisort                  5.13.2\r\nitsdangerous           2.2.0\r\njedi                   0.19.1\r\nJinja2                 3.1.4\r\njoblib                 1.4.2\r\njupyter_client         8.6.2\r\njupyter_core           5.7.2\r\nkiwisolver             1.4.5\r\nllm-engine             0.0.1\r\nllvmlite               0.43.0\r\nMarkupSafe             2.1.5\r\nmatplotlib             3.9.0\r\nmatplotlib-inline      0.1.7\r\nmccabe                 0.7.0\r\nmock                   5.1.0\r\nmpmath                 1.3.0\r\nmsadvisor              1.0.0\r\nmypy                   1.10.0\r\nmypy-extensions        1.0.0\r\nnest-asyncio           1.6.0\r\nnetworkx               3.3\r\nnodeenv                1.9.1\r\nnumpy                  2.2.1\r\nop-compile-tool        0.1.0\r\nop-gen                 0.1\r\nop-test-frame          0.1\r\nopc-tool               0.1.0\r\nopencv-contrib-python  4.10.0.84\r\nopencv-python          4.10.0.84\r\nopt-einsum             3.3.0\r\npackaging              24.1\r\npaddle-custom-npu      3.0.0b2\r\npaddle2onnx            1.2.4\r\npaddlepaddle           3.0.0b2\r\npandas                 2.2.2\r\nparameterized          0.9.0\r\nparso                  0.8.4\r\npathlib2               2.3.7.post1\r\npexpect                4.9.0\r\npillow                 11.0.0\r\npip                    24.1.1\r\nplatformdirs           4.2.2\r\npluggy                 1.5.0\r\npooch                  1.8.2\r\nportalocker            3.0.0\r\npre-commit             2.17.0\r\nprettytable            3.10.0\r\nprompt_toolkit         3.0.47\r\nprotobuf               5.27.2\r\npsutil                 6.0.0\r\nptyprocess             0.7.0\r\npure-eval              0.2.2\r\npycparser              2.22\r\npycrypto               2.6.1\r\npycryptodome           3.20.0\r\npydantic               2.10.4\r\npydantic_core          2.27.2\r\npygame                 2.5.2\r\nPyGithub               2.3.0\r\nPygments               2.18.0\r\nPyGObject              3.36.0\r\nPyJWT                  2.8.0\r\npylint                 3.2.4\r\nPyNaCl                 1.5.0\r\npyparsing              3.1.2\r\npytest                 8.2.2\r\npython-apt             2.0.1+ubuntu0.20.4.1\r\npython-dateutil        2.9.0.post0\r\npython-dotenv          1.0.1\r\npython-multipart       0.0.20\r\npytz                   2024.1\r\nPyYAML                 6.0.2\r\npyzmq                  26.0.3\r\nqtconsole              5.5.2\r\nQtPy                   2.4.1\r\nrarfile                4.2\r\nrequests               2.32.3\r\nrequests-unixsocket    0.2.0\r\nschedule-search        0.0.1\r\nscikit-learn           1.5.0\r\nscipy                  1.14.0\r\nsentry-sdk             2.7.1\r\nsetproctitle           1.3.3\r\nsetuptools             70.1.1\r\nsix                    1.14.0\r\nsmmap                  5.0.1\r\nsniffio                1.3.1\r\nsortedcontainers       2.4.0\r\nsoundfile              0.12.1\r\nssh-import-id          5.10\r\nstack-data             0.6.3\r\nstarlette              0.41.3\r\nsympy                  1.12.1\r\nte                     0.4.0\r\nthreadpoolctl          3.5.0\r\ntoml                   0.10.2\r\ntomli                  2.0.1\r\ntomlkit                0.12.5\r\ntornado                6.4.1\r\ntqdm                   4.67.1\r\ntraitlets              5.14.3\r\ntyping_extensions      4.12.2\r\ntzdata                 2024.1\r\nubelt                  1.3.3\r\nunattended-upgrades    0.1\r\nurllib3                2.3.0\r\nuvicorn                0.16.0\r\nuvloop                 0.21.0\r\nvirtualenv             20.26.3\r\nvisualdl               2.5.3\r\nwandb                  0.17.3\r\nwcwidth                0.2.13\r\nwebsockets             14.1\r\nWerkzeug               3.0.3\r\nwheel                  0.45.1\r\nwrapt                  1.16.0\r\nxdoctest               1.1.1\r\nXlsxWriter             3.0.9\r\n\r\n```\r\n使用 registry.baidubce.com/device/paddle-npu:cann80T13-ubuntu20-aarch64-gcc84-py310 这个镜像的docker\r\n\r\nnpu 信息\r\n```\r\n| npu-smi 23.0.6                   Version: 23.0.6                                               |\r\n+---------------------------+---------------+----------------------------------------------------+\r\n| NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|\r\n| Chip                      | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |\r\n+===========================+===============+====================================================+\r\n| 0     910B3               | OK            | 89.5        50                0    / 0             |\r\n| 0                         | 0000:C1:00.0  | 0           0    / 0          5627 / 65536         |\r\n\r\n```\r\n\r\ncpu 信息\r\n```\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 192\r\n  On-line CPU(s) list:  0-191\r\nVendor ID:              HiSilicon\r\n  BIOS Vendor ID:       HiSilicon\r\n  Model name:           Kunpeng-920\r\n    BIOS Model name:    HUAWEI Kunpeng 920 5250\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 48\r\n    Socket(s):          4\r\n    Stepping:           0x1\r\n    BogoMIPS:           200.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm ssbs\r\nCaches (sum of all):\r\n  L1d:                  12 MiB (192 instances)\r\n  L1i:                  12 MiB (192 instances)\r\n  L2:                   96 MiB (192 instances)\r\n  L3:                   192 MiB (8 instances)\r\nNUMA:\r\n  NUMA node(s):         8\r\n  NUMA node0 CPU(s):    0-23\r\n  NUMA node1 CPU(s):    24-47\r\n  NUMA node2 CPU(s):    48-71\r\n  NUMA node3 CPU(s):    72-95\r\n  NUMA node4 CPU(s):    96-119\r\n  NUMA node5 CPU(s):    120-143\r\n  NUMA node6 CPU(s):    144-167\r\n  NUMA node7 CPU(s):    168-191\r\nVulnerabilities:\r\n  Itlb multihit:        Not affected\r\n  L1tf:                 Not affected\r\n  Mds:                  Not affected\r\n  Meltdown:             Not affected\r\n  Mmio stale data:      Not affected\r\n  Retbleed:             Not affected\r\n  Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:           Mitigation; __user pointer sanitization\r\n  Spectre v2:           Not affected\r\n  Srbds:                Not affected\r\n  Tsx async abort:      Not affected\r\n\r\n```",
        "state": "open",
        "user": "xujiang1",
        "closed_by": null,
        "created_at": "2024-12-26T10:26:14+00:00",
        "updated_at": "2025-06-10T06:22:13+00:00",
        "closed_at": null,
        "comments_count": [
            "xujiang1"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2583,
        "title": "rk3588现在可以跑paddlespeech模型吗",
        "body": "我找不到paddlespeech的语音识别模型在哪里下载，以及如何转成onnx",
        "state": "open",
        "user": "Anbingsong",
        "closed_by": null,
        "created_at": "2025-01-03T09:42:30+00:00",
        "updated_at": "2025-06-10T06:22:14+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2585,
        "title": "在gpu上进行python编译报错",
        "body": "参照文档：https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/gpu.md#python%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85\r\n编译报错\r\n命令如下：\r\npython -m  setup.py build\r\n报错信息如下：\r\nCMake Error at /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPackageHandleStandardArgs.cmake:165 (message):\r\n  Could NOT find Python (missing: Python_EXECUTABLE Python_INCLUDE_DIRS\r\n  Python_LIBRARIES Interpreter Development Development.Module\r\n  Development.Embed)\r\nCall Stack (most recent call first):\r\n  /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPackageHandleStandardArgs.cmake:458 (_FPHSA_FAILURE_MESSAGE)\r\n  /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPython.cmake:436 (find_package_handle_standard_args)\r\n  CMakeLists.txt:411 (find_package)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/work/zyc/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\nTraceback (most recent call last):\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 445, in <module>\r\n    setuptools.setup(\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/__init__.py\", line 103, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/command/build.py\", line 131, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 308, in run\r\n    self.run_command('cmake_build')\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 294, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.10/subprocess.py\", line 369, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/home/cmake-3.18.0-Linux-x86_64/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.10', '-DPYTHON_EXECUTABLE=/usr/bin/python', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-310-x86_64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DENABLE_TVM_BACKEND=OFF', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DENABLE_ORT_BACKEND=ON', '-DENABLE_OPENVINO_BACKEND=ON', '-DENABLE_PADDLE_BACKEND=ON', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=ON', '-DENABLE_LITE_BACKEND=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=ON', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=ON', '-DWITH_IPU=OFF', '-DWITH_OPENCL=OFF', '-DWITH_TIMVX=OFF', '-DWITH_DIRECTML=OFF', '-DWITH_ASCEND=OFF', '-DWITH_KUNLUNXIN=OFF', '-DRKNN2_TARGET_SOC=', '-DTRT_DIRECTORY=/Paddle/TensorRT-8.4.1.5', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=', '-DPADDLEINFERENCE_VERSION=', '-DPADDLEINFERENCE_URL=', '-DPADDLEINFERENCE_API_COMPAT_2_4_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_2_5_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_DEV=OFF', '-DPADDLEINFERENCE_API_CUSTOM_OP=OFF', '-DPADDLE2ONNX_URL=', '-DPADDLELITE_URL=', '-DBUILD_ON_JETSON=OFF', '-DBUILD_PADDLE2ONNX=OFF', '/work/zyc/FastDeploy']' returned non-zero exit status 1.\r\nλ instance-3bwob41y-10 /work/zyc/FastDeploy/python python -V\r\nPython 3.10.14\r\nλ instance-3bwob41y-10 /work/zyc/FastDeploy/python git branch\r\n* develop\r\nλ instance-3bwob41y-10 /work/zyc/FastDeploy/python python setup.py build\r\n/work/zyc/FastDeploy/python/setup.py:28: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\r\n  from distutils.spawn import find_executable\r\n/work/zyc/FastDeploy/python/setup.py:29: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead\r\n  from distutils import sysconfig, log\r\n/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\nrunning build\r\nrunning build_py\r\nrunning create_version\r\nrunning cmake_build\r\nDecompress file /work/zyc/FastDeploy/python/.setuptools-cmake-build/patchelf-0.15.0-x86_64.tar.gz ...\r\n-- Use the default onnxruntime lib. The ONNXRuntime path: /work/zyc/FastDeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\r\nCMake Warning (dev) in cmake/paddle_inference.cmake:\r\n  A logical block opening on the line\r\n\r\n    /work/zyc/FastDeploy/cmake/paddle_inference.cmake:71 (if)\r\n\r\n  closes on the line\r\n\r\n    /work/zyc/FastDeploy/cmake/paddle_inference.cmake:132 (endif)\r\n\r\n  with mis-matching arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning at cmake/paddle_inference.cmake:311 (message):\r\n  You are using PADDLEINFERENCE_API_COMPAT_2_5_x:2.5.0.558ae9cd11, force\r\n  PADDLEINFERENCE_API_CUSTOM_OP=ON\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:245 (include)\r\n\r\n\r\nDecompress file /work/zyc/FastDeploy/python/.setuptools-cmake-build/openvino-linux-x64-20230302.tgz ...\r\n-- Could NOT find TBB (missing: TBB_DIR)\r\nOPENVINO_LIBS = /work/zyc/FastDeploy/python/.setuptools-cmake-build/third_libs/install/openvino/runtime/lib/libopenvino.so;/work/zyc/FastDeploy/python/.setuptools-cmake-build/third_libs/install/openvino/runtime/3rdparty/omp/lib/libiomp5.so\r\n-- CUDA compiler: /usr/local/cuda/bin/nvcc, version: NVIDIA 11.8.89\r\nUsing New Release Strategy - All Arches Packge\r\n-- CUDA detected: 11.8.89\r\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86\r\nCMake Error at /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPackageHandleStandardArgs.cmake:165 (message):\r\n  Could NOT find Python (missing: Python_EXECUTABLE Python_INCLUDE_DIRS\r\n  Python_LIBRARIES Interpreter Development Development.Module\r\n  Development.Embed)\r\nCall Stack (most recent call first):\r\n  /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPackageHandleStandardArgs.cmake:458 (_FPHSA_FAILURE_MESSAGE)\r\n  /home/cmake-3.18.0-Linux-x86_64/share/cmake-3.18/Modules/FindPython.cmake:436 (find_package_handle_standard_args)\r\n  CMakeLists.txt:411 (find_package)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/work/zyc/FastDeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\r\nTraceback (most recent call last):\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 445, in <module>\r\n    setuptools.setup(\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/__init__.py\", line 103, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/core.py\", line 185, in setup\r\n    return run_commands(dist)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/core.py\", line 201, in run_commands\r\n    dist.run_commands()\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/command/build.py\", line 131, in run\r\n    self.run_command(cmd_name)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 308, in run\r\n    self.run_command('cmake_build')\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/dist.py\", line 989, in run_command\r\n    super().run_command(command)\r\n  File \"/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/setuptools/_distutils/dist.py\", line 988, in run_command\r\n    cmd_obj.run()\r\n  File \"/work/zyc/FastDeploy/python/setup.py\", line 294, in run\r\n    subprocess.check_call(cmake_args)\r\n  File \"/usr/lib/python3.10/subprocess.py\", line 369, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/home/cmake-3.18.0-Linux-x86_64/bin/cmake', '-DPYTHON_INCLUDE_DIR=/usr/include/python3.10', '-DPYTHON_EXECUTABLE=/usr/bin/python', '-DBUILD_FASTDEPLOY_PYTHON=ON', '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON', '-DONNX_NAMESPACE=paddle2onnx', '-DPY_EXT_SUFFIX=.cpython-310-x86_64-linux-gnu.so', '-DCMAKE_BUILD_TYPE=Release', '-DLIBRARY_NAME=fastdeploy', '-DPY_LIBRARY_NAME=fastdeploy_main', '-DENABLE_TVM_BACKEND=OFF', '-DENABLE_RKNPU2_BACKEND=OFF', '-DENABLE_SOPHGO_BACKEND=OFF', '-DENABLE_ORT_BACKEND=ON', '-DENABLE_OPENVINO_BACKEND=ON', '-DENABLE_PADDLE_BACKEND=ON', '-DENABLE_POROS_BACKEND=OFF', '-DENABLE_TRT_BACKEND=ON', '-DENABLE_LITE_BACKEND=OFF', '-DENABLE_VISION=ON', '-DENABLE_ENCRYPTION=OFF', '-DENABLE_FLYCV=OFF', '-DENABLE_CVCUDA=OFF', '-DENABLE_TEXT=ON', '-DENABLE_BENCHMARK=OFF', '-DWITH_GPU=ON', '-DWITH_IPU=OFF', '-DWITH_OPENCL=OFF', '-DWITH_TIMVX=OFF', '-DWITH_DIRECTML=OFF', '-DWITH_ASCEND=OFF', '-DWITH_KUNLUNXIN=OFF', '-DRKNN2_TARGET_SOC=', '-DTRT_DIRECTORY=/Paddle/TensorRT-8.4.1.5', '-DCUDA_DIRECTORY=/usr/local/cuda', '-DOPENCV_DIRECTORY=', '-DORT_DIRECTORY=', '-DPADDLEINFERENCE_DIRECTORY=', '-DPADDLEINFERENCE_VERSION=', '-DPADDLEINFERENCE_URL=', '-DPADDLEINFERENCE_API_COMPAT_2_4_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_2_5_x=OFF', '-DPADDLEINFERENCE_API_COMPAT_DEV=OFF', '-DPADDLEINFERENCE_API_CUSTOM_OP=OFF', '-DPADDLE2ONNX_URL=', '-DPADDLELITE_URL=', '-DBUILD_ON_JETSON=OFF', '-DBUILD_PADDLE2ONNX=OFF', '/work/zyc/FastDeploy']' returned non-zero exit status 1.",
        "state": "open",
        "user": "a31413510",
        "closed_by": null,
        "created_at": "2025-01-08T09:02:16+00:00",
        "updated_at": "2025-06-10T06:22:14+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2587,
        "title": "mask_rtdetr 模型部署，希望能尽快支持呢？或者有教程吗？",
        "body": "rtdetr 模型部署，希望能尽快支持呢？或者有教程吗？\n",
        "state": "open",
        "user": "happybear1015",
        "closed_by": null,
        "created_at": "2025-01-16T03:27:00+00:00",
        "updated_at": "2025-06-10T06:22:15+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2591,
        "title": "瑞芯微RK3588板子问题",
        "body": "您好,请教下我想基于FD在板子上部署paddleOCR的模型,请问支持嘛",
        "state": "open",
        "user": "fanruifeng",
        "closed_by": null,
        "created_at": "2025-02-07T08:37:02+00:00",
        "updated_at": "2025-06-10T06:21:43+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2595,
        "title": "编译fastdeploy中FastDeploy/fastdeploy /function/tranpose.cc 文件编译耗时特别多",
        "body": "## 环境\n\n- 【FastDeploy版本】： develop版\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\n- 【系统平台】:  Windows x64(Windows11) \n- 【硬件】： 说明具体硬件型号，i5-14600KF\n- 【编译语言】： C++ \n- 【编译器】：VS 2022\n\n其它文件编译都很快就transpose.cc 文件编译需要大量的时间，需要5分钟左右\n",
        "state": "open",
        "user": "ChaoII",
        "closed_by": null,
        "created_at": "2025-02-26T00:53:35+00:00",
        "updated_at": "2025-06-10T06:21:43+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2588,
        "title": "使用fastdeploy paddleOCR部署后，如何一次识别多张图片？",
        "body": "## 环境\n\n- 【FastDeploy版本】： fastdeploy:1.0.7-cpu-only-21.10\n- 【编译命令】无\n- 【系统平台】: Linux x64(Ubuntu 22.04) \n- 【硬件】： 未使用\n- 【编译语言】： Python 3.8\n\n## 问题日志及出现问题的操作流程\n- 【模型配置问题】\n- - 已执行`examples`下的部署示例，包括使用examples提供的模型，python client.py可以正确执行\n- - 如何配置使可以一次性识别多张图片\n![Image](https://github.com/user-attachments/assets/a994dfba-6c5c-4d03-a63d-dfd4f2937151)\n",
        "state": "closed",
        "user": "pioneer12345",
        "closed_by": "pioneer12345",
        "created_at": "2025-01-16T07:11:14+00:00",
        "updated_at": "2025-06-18T00:51:02+00:00",
        "closed_at": "2025-06-18T00:51:02+00:00",
        "comments_count": [
            "pioneer12345"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2589,
        "title": "C++版本使用FastDeploy和PaddleDetection时，关于异常捕获问题",
        "body": "// 创建模型\nmodel = std::make_shared<fastdeploy::vision::detection::PaddleDetectionModel>(model_buffer, params_buffer, config_file, option);\n\n\n问题：\nmodel_buffer和params_buffer是解密后输入的参数，但是这两个值可能会因为密码错误的情况下报错，报错信息：\n[Paddle2ONNX] Failed to parse PaddlePaddle model from memory buffer.\n[Paddle2ONNX] Failed to load program of PaddlePaddle model from memory.\n[ERROR] Failed to parse PaddlePaddle model.\n\n请问这样的异常信息怎么进行捕获呢，避免程序直接异常退出？\n catch (const std::exception& e) { // 捕获所有标准异常\n        init_model_status = 0;\n        std::cerr << \"Standard exception: \" << e.what() << std::endl;\n    }\n    catch (...) { // 捕获所有其他异常（非标准异常）\n        init_model_status = 0;\n        std::cerr << \"Unknown exception occurred\" << std::endl;\n    }\n两种方式不能捕获到异常信息。设置option.DisablePaddleLogInfo()也无效。",
        "state": "open",
        "user": "sunshine-app",
        "closed_by": null,
        "created_at": "2025-01-16T09:48:00+00:00",
        "updated_at": "2025-06-10T06:21:42+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2593,
        "title": "在安卓中使用 ppocr 如果只想使用文本检测是否可以呢？？",
        "body": " ppocr  在安卓中使用时如果我只想使用其中的文本识别模型，怎么实现呢，我看初始化模型时，必须有目标检测，和方向分类，目标检测修改成了自己的模型，只需要文本识别模型。",
        "state": "open",
        "user": "huangguifeng",
        "closed_by": null,
        "created_at": "2025-02-23T07:26:41+00:00",
        "updated_at": "2025-06-10T06:21:43+00:00",
        "closed_at": null,
        "comments_count": [
            "liyaozong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2596,
        "title": "RKYolo 预测推理耗时不断增加",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-rknn2-0.0.3\n- 【编译命令】如果您是自行编译的FastDeploy，cmake ..  -DENABLE_ORT_BACKEND=ON \\\n\t      -DENABLE_RKNPU2_BACKEND=ON  -DENABLE_VISION=ON -DRKNN2_TARGET_SOC=RK3588    -DCMAKE_INSTALL_PREFIX=${PWD}/fastdeploy-0.0.3\n- 【系统平台】: Linux x64(Ubuntu 18.04)\n- 【硬件】： RK3588\n- 【编译语言】： C++\n\n\n- 【性能问题】描述清楚对比的方式\n   /opt/pp/FastDeploy/fastdeploy/vision/detection/contrib/rknpu2/rkyolo.cc\n   同一个(fastdeploy::vision::detection::RKYOLOV5对象实例   RKYOLO 多次推理耗时不断增加从100多毫秒不断增加4秒多离了大普\n调试打印发现：\n  void RKYOLOPreprocessor::LetterBox(FDMat* mat) {\n         ...\n         scale_.push_back(scale);\n        pad_hw_values_.push_back({pad_h, pad_w});\n  }  \n两个向量scale_,pad_hw_values_ 没有清理的地方，向量大小会不断增长，隐藏引起对象拷贝耗时不断增加\n\n  postprocessor_.SetPadHWValues(preprocessor_.GetPadHWValues());\n  postprocessor_.SetScale(preprocessor_.GetScale());\n\nrkyolo.cc(109)::BatchPredict  RKYOLO sn:11232,Infer:0.057892 s\nrkyolo.cc(109)::BatchPredict  RKYOLO sn:11232,Infer:0.075516 s\nrkyolo.cc(109)::BatchPredict  RKYOLO sn:11232,Infer:0.092575 s\nrkyolo.cc(114)::BatchPredict  RKYOLO sn:11232,GetPadHWValues time:0.067186 s,size:3760\nrkyolo.cc(114)::BatchPredict  RKYOLO sn:11232,GetPadHWValues time:0.065356 s,size:3727\nrkyolo.cc(114)::BatchPredict  RKYOLO sn:11232,GetPadHWValues time:0.067844 s,size:3750\nrkyolo.cc(119)::BatchPredict  RKYOLO sn:11232,SetPadHWValues: GetPadHWValues time:0.260485 s,size:3760\nrkyolo.cc(125)::BatchPredict  RKYOLO sn:11232,GetScale & SetScale time:0.000106 s,size:3760\nrkyolo.cc(119)::BatchPredict  RKYOLO sn:11232,SetPadHWValues: GetPadHWValues time:0.341411 s,size:3727\nrkyolo.cc(136)::BatchPredict  RKYOLO sn:11232,postprocessor_:0.028234 s\nrkyolo.cc(139)::BatchPredict  RKYOLO sn:11232,all:0.665357 s\nrkyolo.cc(119)::BatchPredict  RKYOLO sn:11232,SetPadHWValues: GetPadHWValues time:0.476271 s,size:3750\nrkyolo.cc(125)::BatchPredict  RKYOLO sn:11232,GetScale & SetScale time:0.000125 s, size:3727\n\n两个操作耗时增加\ndeepseek 给出修改建议：\n1  使用局部变量替代成员变量；\n2. 清空向量\n       如果 pad_hw_values_ 和 scale_ 必须在类的成员变量中保留，可以在每次调用 Preprocess 或 LetterBox 之前清空它们，以避免它们不断增长。\n3. 使用引用传递\n    在 SetPadHWValues 和 SetScale 中，可以使用引用传递来避免不必要的拷贝。\n",
        "state": "open",
        "user": "erroot",
        "closed_by": null,
        "created_at": "2025-03-10T05:17:25+00:00",
        "updated_at": "2025-06-10T06:21:44+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2594,
        "title": "paddlejs-module/ocr在较新版本的vite项目中导入的问题",
        "body": "我能够正常运行web_demo，但是在最新版本的vite项目中导入@paddle-js-models/ocr都会得到错误Module is not defined\n测试了react+vite和vue+vite，均通过`yarn create vite .`创建模板\n测试了@paddle-js-models/ocr的4.0.0版本（与web demo中一致）和4.1.1（yarn安装的最新版本）\n尝试了在vite配置文件中定义转换ES模块：\n```ts\n  optimizeDeps: {\n    include: ['@paddle-js-models/ocr']\n  },\n  build: {\n    commonjsOptions: {\n      transformMixedEsModules: true\n    },\n  }\n```\n以及使用@rollup/plugin-commonjs插件指定模块目录：\n```ts\n  plugins: [\n    react(),\n    commonjs({\n      include: [/node_modules\\/@paddle-js-models\\/ocr/, /node_modules\\/@paddle-js-models\\/ocrdet/],\n    }),\n  ],\n```\n但是都没有办法处理Module未定义的问题😭\n```\nUncaught ReferenceError: Module is not defined\n```",
        "state": "closed",
        "user": "Need-an-AwP",
        "closed_by": "Need-an-AwP",
        "created_at": "2025-02-24T08:51:45+00:00",
        "updated_at": "2025-03-15T12:15:00+00:00",
        "closed_at": "2025-03-15T12:14:59+00:00",
        "comments_count": [
            "Toni-Stark",
            "Need-an-AwP"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2599,
        "title": "fastdeploy服务化部署mask输出mask值全为0",
        "body": "## 环境\n\n- 【FastDeploy版本】： fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10)\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\n- 【系统平台】: Linux x64(Ubuntu 20.04) \n- 【硬件】： 说明具体硬件型号，T4， CUDA 11.4\n- 【编译语言】： Python\n\n\n- 【模型精度问题】\n使用mask_rcnn_x101_vd_64x4d_fpn_1x_coco训练出的模型转化后部署fastdeploy 推理出来的图像结果中mask有值，但是输出结果全为零，而使用paddleDecetion推理结果为正常。\n以下为fastdeploy推理结果：\n{'model_name': 'ppdet', 'model_version': '1', 'parameters': {'sequence_id': 0, 'sequence_start': False, 'sequence_end': False}, 'outputs': [{'name': 'DET_RESULT', 'datatype': 'BYTES', 'shape': [1], 'data': ['{\"boxes\": [[499.6636657714844, 430.0556335449219, 515.0889282226562, 448.396240234375], [322.3255615234375, 200.67718505859375, 353.9615478515625, 226.19215393066406]], \"scores\": [0.16571356356143951, 0.4343606233596802], \"label_ids\": [2, 2], \"masks\": [\"{\\\\\"data\\\\\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \\\\\"shape\\\\\": [18, 15]}\", \"{\\\\\"data\\\\\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \\\\\"shape\\\\\": [25, 32]}\"], \"contain_masks\": true}']}]}",
        "state": "open",
        "user": "wxf764571829",
        "closed_by": null,
        "created_at": "2025-03-13T05:59:20+00:00",
        "updated_at": "2025-06-10T06:21:44+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2597,
        "title": "什么时候适配paddle inference 3.0",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： fastdeploy-develop\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\n- 【编译语言】： C++ / Python(3.7或3.8等）\n- \n- 【模型跑不通】\n目前咋们最新的DocLayout模型需要依赖paddle 3.0版本，咋们这块预计什么时候可以适配？\n",
        "state": "open",
        "user": "liuwqiang",
        "closed_by": null,
        "created_at": "2025-03-10T07:54:58+00:00",
        "updated_at": "2025-06-10T06:21:44+00:00",
        "closed_at": null,
        "comments_count": [
            "monkeycc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2601,
        "title": "在NVIDIA 50系列显卡上使用fastdeploy",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： libfastdepoly.0.0.0\n- 【编译命令】未知\n- 【系统平台】: Linux x86_64(Ubuntu 20.04) \n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 5080， CUDA 11.2 \n- 【编译语言】： C++ \n\n## 问题日志及出现问题的操作流程\n- 附上详细的问题日志有助于快速定位分析\n-加载模型报错\n\n![Image](https://github.com/user-attachments/assets/f15a392a-7f87-4ef9-a2ab-8a75324c4068)\n",
        "state": "open",
        "user": "mingxingpu",
        "closed_by": null,
        "created_at": "2025-03-17T09:20:08+00:00",
        "updated_at": "2025-06-10T06:21:45+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2598,
        "title": "paddledetection使用ppyoloe导出的模型无法正常使用",
        "body": "问题描述 Please describe your issue\n完全无法识别，训练命令：python tools/train.py -c configs/ppyoloe/ppyoloe_plus_crn_s_80e_coco.yml --eval\n导出命令：python tools/export_model.py -c configs/ppyoloe/ppyoloe_plus_crn_s_80e_coco.yml -o weights=output/best_model/model.pdparams。\n\n使用config = paddle_infer.Config(model_file, params_file)这种模式没有问题。\n代码如下：\ntime1 = time.time()\n\n模型路径\nmodel_dir = \"output_inference/ppyoloe_plus_crn_s_80e_coco\"\nmodel_file = os.path.join(model_dir, \"model.pdmodel\")\nparams_file = os.path.join(model_dir, \"model.pdiparams\")\n\n初始化推理配置\nconfig = paddle_infer.Config(model_file, params_file)\nconfig.enable_use_gpu(200, 0)\nconfig.use_gpu()\nconfig.rt\n\n使用 GPU，设置初始显存大小为 100 MB，设备 ID 为 0\n如果使用 CPU，可以启用以下代码：\nconfig.disable_gpu()\n创建预测器\npredictor = paddle_infer.create_predictor(config)\n\n图片预处理\ndef preprocess(image_path, target_size=(640, 640)):\n# 读取图片并转换为 RGB 格式\nimage = Image.open(image_path).convert(\"RGB\")\n# 调整图片尺寸为目标尺寸\nimage = image.resize(target_size, Resampling.BILINEAR)\n# 归一化\nimage_np = np.array(image).astype(\"float32\") / 255.0\nmean = np.array([0, 0, 0], dtype=\"float32\").reshape((1, 1, 3))\nstd = np.array([1, 1, 1], dtype=\"float32\").reshape((1, 1, 3))\nimage_np = (image_np - mean) / std\n# 转换为 CHW 格式\nimage_np = image_np.transpose((2, 0, 1)) # HWC -> CHW\nimage_np = np.expand_dims(image_np, axis=0) # 增加 batch 维度\nreturn image_np\n\n解析输出结果\ndef parse_output(output_data, threshold=0.75):\nresults = []\nfor row in output_data:\nbatch_id, score, x_min, y_min, x_max, y_max = row\nif score > threshold: # 过滤低置信度的结果\n# 宽度处理\nx_min = float(x_min) * 1280 / 640\nx_max = float(x_max) * 1280 / 640\nwidth = x_max - x_min\nif width > 330:\ncontinue\nresults.append({\n\"box\": [x_min, float(y_min) * 720 / 640, x_max, float(y_max) * 720 / 640], # 检测框坐标\n\"score\": float(score), # 置信度分数\n\"label\": int(batch_id) # 假设 batch_id 可以作为类别标签\n})\nreturn results\n\nfolder_path = r'D:\\PythonProject\\img_save'\nnewDir = r'D:\\PythonProject\\paddle_image'\n\n获取所有文件列表\nfiles = os.listdir(folder_path)\n\n筛选图片文件\nimage_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp'))]\n\nlabel_data = {\n\"label\": 'null',\n\"x\": -1,\n\"y\": -1,\n\"score\":0.0,\n}\n\nlabels = ('mfzh', 'dgp', 'mxd', 'qmly', 'kbfl', 'slfy', 'lhbl', 'qnngg', 'hyss', 'xljm', 'klbj', 'jsym')\n\n获取输入和输出句柄\ninput_names = predictor.get_input_names()\ninput_tensor1 = predictor.get_input_handle(input_names[0])\nscale_factor=np.array([[1.0,1.0]],dtype=np.float32)\ninput_tensor2 = predictor.get_input_handle(input_names[1])\ninput_tensor2.copy_from_cpu(scale_factor)\noutput_names = predictor.get_output_names()\n\nfor img_file in image_files:\nimg_path = os.path.join(folder_path, img_file)\ninput_data = preprocess(img_path)\n\n# 设置输入数据\ninput_tensor1.copy_from_cpu(input_data)\n# print(output_names)\n\n# 执行推理\npredictor.run()\n\n# 获取输出数据\noutput_tensor = predictor.get_output_handle(output_names[0])\noutput_data = output_tensor.copy_to_cpu()\n\n#解析并打印结果\nresults = parse_output(output_data)\nif len(results) == 0:\n    continue\n\nlabel_data_list = list()\nfor result in results:\n    #print(f\"Class: {result['label']}, Score: {result['score']:.4f}, Box: {result['box']}\")\n    label_data_d = label_data.copy()\n    label_data_d[\"x\"] = int(result['box'][0])\n    label_data_d[\"y\"] = int(result['box'][1])\n    label_data_d[\"label\"] = labels[result['label']]\n    label_data_d[\"score\"] = result[\"score\"]\n    label_data_list.append(label_data_d)\n\ntry:\n    dst = newDir + \"/\" + img_file\n    shutil.copy2(img_path, dst)\nexcept Exception as e:\n    print(f\"复制失败: {img_path}, 错误: {e}\")\n\nprint(img_file)\nprint(label_data_list)\ntime2 = time.time()\nprint(time2 - time1)。\n\n\n使用fastdeploy无法正常识别目标，图片为1280*720。\n733.184082,594.169983, 1563.229370, 719.260498, 0.010073, 2 输出坐标1563都超过了1280",
        "state": "open",
        "user": "hejun126",
        "closed_by": null,
        "created_at": "2025-03-12T14:27:46+00:00",
        "updated_at": "2025-06-10T06:21:44+00:00",
        "closed_at": null,
        "comments_count": [
            "hejun126"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2600,
        "title": "aarch64 RK3588 无法部署fastdeploy1.0.3以上版本",
        "body": "目前需要将pddleOCR转RKNN后在板端使用。按照教程需要用到1.0.3以上版本\n但库中（https://www.paddlepaddle.org.cn/whl/fastdeploy.html）未发现有对应aarch64 1.0.3以上版本\n是否有其他方式可以跑？",
        "state": "open",
        "user": "YuLionel",
        "closed_by": null,
        "created_at": "2025-03-13T06:31:06+00:00",
        "updated_at": "2025-06-10T06:21:45+00:00",
        "closed_at": null,
        "comments_count": [
            "YuLionel"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2603,
        "title": "dynamic_batching 怎么设置呢",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： registry.baidubce.com/paddlepaddle/fastdeploy:1.0.7-gpu-cuda11.4-trt8.5-21.10\n- 【编译命令】\n- 【系统平台】: centos\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\n- 【编译语言】： C++ / Python(3.7或3.8等）\n\n## 问题日志及出现问题的操作流程\n- 服务化，设置的config.pbtxt 设置了max_batch_size 和dynamic_batching，返回只有一个box\n\n",
        "state": "open",
        "user": "PC-god",
        "closed_by": null,
        "created_at": "2025-03-26T03:18:04+00:00",
        "updated_at": "2025-06-10T06:21:46+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2602,
        "title": "Assertion validateCaskKLibSize(buffer.size) failed.",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.3\n- 【系统平台】: LWindows x64(Windows10)\n- 【硬件】：  Nvidia GPU 3060， CUDA 11.1 CUDNN 8.9\n- 【编译语言】： C++ / Python3.8\n\n## 问题日志及出现问题的操作流程\n[FastDeploy][INFO]:  Successfully found CUDA ToolKit from system PATH env -> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(232)::fastdeploy::FDTrtLogger::log  2: [builder.cpp::nvinfer1::builder::createCaskKernelLibraryImpl::137] Error Code 2: Internal Error (Assertion validateCaskKLibSize(buffer.size) failed. )\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(631)::fastdeploy::TrtBackend::CreateTrtEngineFromOnnx\n        Failed to call createInferBuilder().\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(264)::fastdeploy::TrtBackend::InitFromOnnx  Failed to create tensorrt engine.\n[ERROR] fastdeploy/runtime/runtime.cc(382)::fastdeploy::Runtime::CreateTrtBackend       Load model from Paddle failed while initliazing TrtBackend.",
        "state": "closed",
        "user": "eaxts",
        "closed_by": "eaxts",
        "created_at": "2025-03-18T08:32:39+00:00",
        "updated_at": "2025-03-20T02:08:34+00:00",
        "closed_at": "2025-03-20T02:08:32+00:00",
        "comments_count": [
            "eaxts"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2604,
        "title": "小程序替换模型失败",
        "body": "## 环境\n- 【FastDeploy版本】： FastDeploy-develop\n- 【系统平台】: Linux x64(Ubuntu 16.04) \n- 【硬件】： Nvidia GPU 2080TI， CUDA 10.1 CUDNN 7.6\n- 【编译语言】：Python(3.7）\n- 【Paddle环境】：padlepaddle2.5.0，paddlelite2.7，Paddle.js-release-v2.2.3\n\n## 问题日志及出现问题的操作流程\n- 【替换模型后跑不通】\n- - 按照官方给出的步骤在小程序可以跑通ocrXcx示例，检测模型和识别模型都能正常工作(https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/application/js/mini_program/ocrXcx)\n- - paddlejs-converters转换模型有提示错误，如下转换ch_PP-OCRv3_rec模型：\n![Image](https://github.com/user-attachments/assets/504695ca-3b98-4706-bf43-ab1eae4ac902)\n- - 转换模型后，把model.json以及所有chunk_x.dat文件放到OSS链接后替换官方地址，可以正常下载：\n\n![Image](https://github.com/user-attachments/assets/31da3630-0b38-4f18-a6bc-bc47582dc769)\n\n- - 小程序替换识别模型后运行报错：\n\n[127.0.0.1-1743131205658.log](https://github.com/user-attachments/files/19497612/127.0.0.1-1743131205658.log)\n\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\n1，查询了其他issue，如[Web Demo替换模型失败](https://github.com/PaddlePaddle/FastDeploy/issues/597#top)，修改检测模型的infer_shape = [3, 960, 960]之后，检测模型可以正常跑通，但是chunk_1.dat文件有2.25MB，官网只有467KB，可能也是检测模型推理较官方慢的原因？\n2，识别模型修改infer_shape = [3, 48, 320]，修改前后都无法正常推理，且自己转换的识别模型有三个chunk分片，共有10.1MB，官方模型只有一个分片3.38MB\n3，也有issue说是版本问题，使用虚拟环境分别尝试了（py3.7+paddlepaddle2.5.5+paddlelite2.7.1+paddlejsconverter1.0.7）、（py3.7+padlepaddle2.5.0，paddlelite2.7，Paddle.js-release-v2.2.3）、（py3.8+paddlepaddle2.6.0+paddlelite没有源），也都不行\n4，个人猜测，在模型转换时无法使用paddlejslite优化，或者具体的量化步骤缺失，导致上述问题，希望能给出转换模型的详细步骤（paddlepaddle、paddlejslite、paddle.js、pp-OCRV3等等）",
        "state": "open",
        "user": "yinqinggong",
        "closed_by": null,
        "created_at": "2025-03-28T03:02:00+00:00",
        "updated_at": "2025-06-10T06:21:46+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "yinqinggong"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2606,
        "title": "PaddleOCR python版本的DBPostProcess 和 fd.vision.ocr.DBDetectorPostprocessor 为什么存在精度差异",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： 说明具体的版本，如fastdeploy-linux-gpu-0.8.0\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) / Mac OSX arm(12.0) / Mac OSX intel(12.0)\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 3080TI， CUDA 11.2 CUDNN 8.3\n- 【编译语言】： C++ / Python(3.7或3.8等）\n\n## 问题日志及出现问题的操作流程\n- 附上详细的问题日志有助于快速定位分析\n- 【模型跑不通】\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\n- 【模型精度问题】\n- - 先执行`examples`下的部署示例，包括使用examples提供的模型，确认是否可以正确执行\n- - 如若`examples`下的代码可以运行，但自己的模型，或自己的代码不能运行\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\n- 【性能问题】描述清楚对比的方式\n- - 注意性能测试，循环跑N次，取后80%的用时平均（模型启动时，刚开始受限于资源分配，速度会较慢）\n- - FastDeploy的Predict包含模型本身之外的数据前后处理用时\n- - - 提供复现问题的 代码+模型+错误log，供工程师快速定位问题\n",
        "state": "closed",
        "user": "huotong1212",
        "closed_by": "huotong1212",
        "created_at": "2025-04-06T02:38:47+00:00",
        "updated_at": "2025-04-06T02:39:04+00:00",
        "closed_at": "2025-04-06T02:39:04+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2605,
        "title": "paddle框架升级到3.0了，请问fastdeploy什么时候有更新包？",
        "body": null,
        "state": "open",
        "user": "yangliujun01",
        "closed_by": null,
        "created_at": "2025-04-03T03:47:08+00:00",
        "updated_at": "2025-06-10T06:21:46+00:00",
        "closed_at": null,
        "comments_count": [
            "yangliujun01",
            "monkeycc",
            "chunxinQ"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2608,
        "title": "请问如果在rk3576芯片上要怎么部署paddleocr模型？",
        "body": "当前采用rk3568的部署步骤无法在rk3576上部署模型，请问后续有支持rk3576的计划么？\n",
        "state": "open",
        "user": "mixiao-ai",
        "closed_by": null,
        "created_at": "2025-04-08T02:01:10+00:00",
        "updated_at": "2025-06-10T06:21:47+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2609,
        "title": "RK3588 OCR 内存泄露",
        "body": "在对生成的demo文件进行压力测试的时候,出现内存泄露问题.\ninfer.cc修改后的代码:\n```\nint processImages() {\n\n  fastdeploy::RuntimeOption option;\n  fastdeploy::ModelFormat format;\n  int flag = std::atoi(\"1\");\n\n  if (flag == 0) {\n    option.UseCpu();\n    format = fastdeploy::ONNX;\n  } else if (flag == 1) {\n    option.UseRKNPU2();\n    format = fastdeploy::RKNN;\n  }\n\n  std::string det_model_dir = \"./yh/det4_shape_rk3588_unquantized.rknn\";\n  std::string cls_model_dir = \"./yh/cls2_shape_rk3588_unquantized.rknn\";\n  std::string rec_model_dir = \"./yh/rec4_shape_rk3588_unquantized.rknn\";\n  std::string rec_label_file = \"./yh/ppocr_keys_v1.txt\";\n  std::string test_image = \"./1.jpg\";\n  InitAndInfer(det_model_dir, cls_model_dir, rec_model_dir, rec_label_file,\n               test_image, option, format);\n  return 0;\n}\n```\n\n测试test.cpp:\n```\n#include <iostream>\n#include <vector>\n#include <string>\n#include \"infer_demo.h\" // 包含静态库的头文件\n\nint main() {\n    while(1) {\n        processImages();\n    }\n\n    return 0;\n}\n```\n\n头文件:infer_demo.h\n\n\n```\n#ifndef INFER_DEMO_H\n#define INFER_DEMO_H\n\nint processImages();\n\n#endif // INFER_DEMO_H\n```\n\n![Image](https://github.com/user-attachments/assets/ad93e8e9-4e92-449d-8107-6ecde70b8286)\n\n![Image](https://github.com/user-attachments/assets/68c841e8-5d1c-4df7-b46c-498c813fb01b)",
        "state": "open",
        "user": "LDRain777",
        "closed_by": null,
        "created_at": "2025-04-09T08:24:52+00:00",
        "updated_at": "2025-06-10T06:21:47+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2607,
        "title": "PaddleOCR python版本的DBPostProcess 和 fd.vision.ocr.DBDetectorPostprocessor 为什么存在精度差异",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n- 【FastDeploy版本】： fastdeploy-python 1.0.7\n\n## 问题日志及出现问题的操作流程\n其中 DBPostProcess create_operators 是PaddleOCR 2.5 中的代码\n```\nclass DBPostTest(unittest.TestCase):\n    def setUp(self):\n        url = \"localhost:8008\"\n        self.det_runner = SyncGRPCTritonRunner(url, \"v4_det_runtime\", \"1\")\n\n        pre_process_list = [{'DetResizeForTest': {'limit_side_len': 960, 'limit_type': 'max'}}, {\n            'NormalizeImage': {'std': [0.229, 0.224, 0.225], 'mean': [0.485, 0.456, 0.406], 'scale': '1./255.',\n                               'order': 'hwc'}}, {'ToCHWImage': None}, {'KeepKeys': {'keep_keys': ['image', 'shape']}}]\n        self.det_preprocess_op = create_operators(pre_process_list)\n        self.det_post = DBPostProcess(thresh=0.3, box_thresh=0.6, unclip_ratio=1.5)\n\n        self.det_pre_fd = fd.vision.ocr.DBDetectorPreprocessor()\n        self.det_pre_fd.max_side_len = 960\n\n        self.det_post_fd = fd.vision.ocr.DBDetectorPostprocessor()\n        self.det_post_fd.det_db_thresh = 0.3\n        self.det_post_fd.det_db_box_thresh = 0.6\n        self.det_post_fd.det_db_unclip_ratio = 1.5\n\n    def test_db_py(self):\n        image = cv2.imread(\"images/d1.jpg\")\n        output = transform({'image': image}, self.det_preprocess_op)\n        img, shape_list = output\n        print(f\"shape list:{shape_list.tolist()}\")\n        inputs = np.expand_dims(img, axis=0)\n        shape_list = np.expand_dims(shape_list, axis=0)\n        outputs = self.det_runner.Run([inputs])\n        preds = {\n            \"maps\": list(outputs.values())[0]\n        }\n        results = self.det_post(preds, shape_list)\n        for row in results[0][\"points\"]:\n            print(row.tolist())\n\n    def test_db_c(self):\n        image = cv2.imread(\"images/d1.jpg\")\n        inputs, shape_list = self.det_pre_fd.run(image[np.newaxis, :, :, :])\n        print(f\"shape list:{shape_list}\")\n        inputs = inputs[0].numpy()\n        outputs = self.det_runner.Run([inputs])\n        preds = list(outputs.values())[0].copy()\n        results = self.det_post_fd.run([preds,], shape_list)\n        for row in results[0]:\n            print(row)\n```\n输出如下：\n```\npython PaddleOCR\n\nshape_list [1439.0, 1528.0, 0.6226546212647672, 0.6282722513089005]\n\n[[564, 1040], [726, 1023], [730, 1062], [568, 1079]]\n[[708, 1019], [917, 1008], [919, 1048], [710, 1059]]\n[[292, 1001], [537, 994], [538, 1052], [294, 1059]]\n[[1122, 1002], [1316, 999], [1317, 1037], [1123, 1041]]\n[[296, 940], [535, 933], [537, 989], [297, 996]]\n[[959, 942], [1097, 934], [1099, 974], [961, 982]]\n[[658, 929], [847, 921], [849, 971], [660, 979]]\n[[1126, 919], [1264, 913], [1266, 958], [1128, 964]]\n[[314, 891], [532, 874], [536, 926], [318, 942]]\n[[561, 877], [744, 870], [745, 909], [563, 916]]\n[[759, 858], [1058, 849], [1059, 888], [760, 897]]\n[[294, 819], [535, 814], [537, 876], [295, 882]]\n[[563, 792], [735, 792], [735, 830], [563, 830]]\n[[278, 704], [925, 685], [927, 734], [279, 752]]\n[[278, 544], [544, 527], [547, 577], [281, 593]]\n[[856, 466], [1185, 457], [1186, 502], [857, 511]]\n[[281, 465], [664, 452], [666, 502], [283, 514]]\n[[341, 389], [1258, 395], [1257, 450], [340, 443]]\n[[1031, 45], [1197, 45], [1197, 191], [1031, 191]]\n\nfastdeploy\nshape_list [[1528, 1439, 960,               896]]\n              0.6282722513089005  0.6226546212647672\n\n[563, 1040, 725, 1023, 730, 1061, 568, 1079]\n[708, 1018, 916, 1008, 918, 1048, 709, 1058]\n[292, 1000, 536, 994, 537, 1051, 294, 1058]\n[1122, 1002, 1316, 998, 1316, 1037, 1122, 1040]\n[296, 939, 534, 933, 536, 989, 297, 995]\n[959, 941, 1096, 934, 1098, 974, 961, 981]\n[658, 928, 846, 921, 848, 971, 660, 978]\n[1126, 918, 1263, 913, 1265, 958, 1128, 963]\n[313, 891, 531, 873, 536, 925, 318, 942]\n[561, 876, 743, 870, 744, 909, 563, 915]\n[736, 880, 751, 880, 751, 886, 736, 886]\n[759, 857, 1058, 849, 1058, 888, 759, 896]\n[294, 819, 534, 814, 536, 876, 296, 881]\n[563, 791, 735, 791, 735, 830, 563, 830]\n[278, 703, 924, 685, 926, 733, 280, 751]\n[273, 610, 913, 594, 915, 660, 275, 676]\n[276, 544, 544, 526, 547, 576, 280, 594]\n[574, 538, 588, 538, 588, 552, 574, 552]\n[856, 465, 1184, 457, 1185, 502, 857, 510]\n[281, 464, 663, 452, 665, 502, 283, 513]\n[340, 388, 1257, 395, 1257, 449, 340, 443]\n[1031, 44, 1196, 44, 1196, 191, 1031, 191]\n```\n\n经过比对，模型推理后的结果一致，但是后处理的结果不一致，如上，564 和 563 ，这是为什么，是因为c++中的精度差异吗？ 求教",
        "state": "open",
        "user": "huotong1212",
        "closed_by": null,
        "created_at": "2025-04-06T02:46:33+00:00",
        "updated_at": "2025-06-10T06:21:46+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2610,
        "title": "按教程部署完insightface后发现只能使用cpu",
        "body": "## 使用文档\n首先是根据这个文档[InsightFace Python部署示例](https://github.com/PaddlePaddle/FastDeploy/blob/develop/examples/vision/faceid/insightface/rknpu2/python/README_CN.md),根据其环境要求导航到[FastDeploy RKNPU2 导航文档](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rknpu2.md),\n\n## 我的环境\npython是3.8\nrknn版本是2.3.0\n![Image](https://github.com/user-attachments/assets/1ec87d43-37f5-42a4-b3f6-b8e72cfe5a8f)\n\n## 使用情况\n按照教程指令编译成功\n```\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\ncd FastDeploy\n\n# 如果您使用的是develop分支输入以下命令\ngit checkout develop\n\ncd python\nexport ENABLE_ORT_BACKEND=ON\nexport ENABLE_RKNPU2_BACKEND=ON\nexport ENABLE_VISION=ON\n\n# 请根据你的开发版的不同，选择RK3588和RK356X\nexport RKNN2_TARGET_SOC=RK3588\n\n# 如果你的核心板的运行内存大于等于8G，我们建议您执行以下命令进行编译。\npython3 setup.py build\n# 值得注意的是，如果你的核心板的运行内存小于8G，我们建议您执行以下命令进行编译。\npython3 setup.py build -j1\n\npython3 setup.py bdist_wheel\ncd dist\npip3 install fastdeploy_python-0.0.0-cp39-cp39-linux_aarch64.whl\n```\n部分编译参数如下\n\n![Image](https://github.com/user-attachments/assets/612923ec-3256-40a9-9ac6-e6f47c54d1a1)\n\n安装fastdeploy_python后按照文档指令\n```\n#下载部署示例代码\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\ncd examples/vision/faceid/insightface/python/\n\n#下载ArcFace模型文件和测试图片\nwget https://bj.bcebos.com/paddlehub/fastdeploy/ms1mv3_arcface_r100.onnx\nwget https://bj.bcebos.com/paddlehub/fastdeploy/rknpu2/face_demo.zip\nunzip face_demo.zip\n\n# CPU推理\npython infer_arcface.py --model ms1mv3_arcface_r100.onnx \\\n                        --face face_0.jpg \\\n                        --face_positive face_1.jpg \\\n                        --face_negative face_2.jpg \\\n                        --device cpu\n# GPU推理\npython infer_arcface.py --model ms1mv3_arcface_r100.onnx \\\n                        --face face_0.jpg \\\n                        --face_positive face_1.jpg \\\n                        --face_negative face_2.jpg \\\n                        --device gpu\n```\ngpu版本会提示\n```\n/FastDeploy/examples/vision/faceid/insightface/python$ python infer_arcface.py --model ms1mv3_arcface_r100.onnx                         --face face_0.jpg                         --face_positive face_1.jpg                         --face_negative face_2.jpg                         --device gpu\nWARNING:root:The installed fastdeploy-python package is not built with GPU, will force to use CPU. To use GPU, following the commands to install fastdeploy-gpu-python.\nWARNING:root:    ================= Install GPU FastDeploy===============\nWARNING:root:    python -m pip uninstall fastdeploy-python\nWARNING:root:    python -m pip install fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html\n[INFO] fastdeploy/runtime/runtime.cc(326)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\nFaceRecognitionResult: [Dim(512), Min(-2.309219), Max(2.372197), Mean(0.016987)]\nFaceRecognitionResult: [Dim(512), Min(-2.288257), Max(1.995103), Mean(-0.003400)]\nFaceRecognitionResult: [Dim(512), Min(-3.243412), Max(3.875865), Mean(-0.030682)]\nCosine 01:  0.814384554852488\nCosine 02:  -0.059388045136689285\nRuntimeOption(\n  backend : Backend.ORT\t\n  cpu_thread_num : -1\t\n  device : Device.CPU\t\n  device_id : 0\t\n  external_stream : None\t\n  model_file : ms1mv3_arcface_r100.onnx\t\n  model_format : ModelFormat.ONNX\t\n  model_from_memory : False\t\n  openvino_option : <fastdeploy.libs.fastdeploy_main.OpenVINOBackendOption object at 0x7fa003bc30>\t\n  ort_option : <fastdeploy.libs.fastdeploy_main.OrtBackendOption object at 0x7fa0078530>\t\n  paddle_infer_option : <fastdeploy.libs.fastdeploy_main.PaddleBackendOption object at 0x7fa0078530>\t\n  paddle_lite_option : <fastdeploy.libs.fastdeploy_main.LiteBackendOption object at 0x7fa0078530>\t\n  params_file : \t\n  poros_option : <fastdeploy.libs.fastdeploy_main.PorosBackendOption object at 0x7fa0078530>\t\n  trt_option : <fastdeploy.libs.fastdeploy_main.TrtBackendOption object at 0x7fa0078530>\t\n)\n\n```\n输入指令查看npu使用情况确实没变化",
        "state": "open",
        "user": "omaiyiwa",
        "closed_by": null,
        "created_at": "2025-04-10T03:46:54+00:00",
        "updated_at": "2025-06-10T06:21:47+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2611,
        "title": "windows下 c++部署fastdeploy后，测试facelandmark1000，结果完全不对",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： release1.0.7\n- 【编译命令】git clone https://github.com/PaddlePaddle/FastDeploy.git\ncd FastDeploy\nmkdir build && cd build\ncmake .. -G \"Visual Studio 16 2019\" -A x64 \\\n         -DENABLE_ORT_BACKEND=ON \\\n         -DENABLE_PADDLE_BACKEND=ON \\\n         -DENABLE_OPENVINO_BACKEND=ON \\\n         -DENABLE_VISION=ON \\\n         -DCMAKE_INSTALL_PREFIX=\"D:\\code\\LSM\\DemoCOM\\compiled_fastdeploy\"\n\nmsbuild fastdeploy.sln /m /p:Configuration=Debug /p:Platform=x64\nmsbuild INSTALL.vcxproj /m /p:Configuration=Debug /p:Platform=x64\n\n- 【系统平台】: Windows x64\n- 【硬件】： CPU\n- 【编译语言】： C++\n\n## 问题日志及出现问题的操作流程\n- 附上详细的问题日志有助于快速定位分析\n- 【模型跑不通】\n- - 使用FaceLandmark1000.onnx模型，测试代码完全搬运infer.cc，但是运行结果见图片吧\n- - 代码：\n- - void CFaceLandmark::testCam() {\n\n    cv::VideoCapture cap(0);\n    if (!cap.isOpened()) {\n        std::cerr << \"can not open camera.\" << std::endl;\n        return;\n    }\n\n    cv::Mat frame;\n    auto option = fastdeploy::RuntimeOption();\n    if (!CreateRuntimeOption(&option)) {\n        return;\n    }\n\n    std::string modelFile = \"D:\\\\code\\\\xxx\\\\testPro\\\\x64\\\\Release\\\\model\\\\FaceLandmark.onnx\";\n    auto model = fastdeploy::vision::facealign::FaceLandmark1000(modelFile, \"\", option);\n    if (!model.Initialized()) {\n        std::cerr << \"Failed to initialize.\" << std::endl;\n        return;\n    }\n\n    while (1) {\n        cap >> frame;\n        if (frame.empty()) {\n            std::cerr << \"can not read video.\" << std::endl;\n            return;\n        }\n\n        fastdeploy::vision::FaceAlignmentResult res;\n        if (!model.Predict(&frame, &res)) {\n            std::cerr << \"Failed to predict.\" << std::endl;\n            return;\n        }\n\n        auto vis_im = fastdeploy::vision::VisFaceAlignment(frame, res);\n        cv::imshow(\"imagex\", vis_im);\n        cv::waitKey(100);\n    }\n}\n测试结果：\n\n![Image](https://github.com/user-attachments/assets/ff99c34d-6e79-42f9-9388-e220db4fab5b)\n\n",
        "state": "open",
        "user": "ztt19851213",
        "closed_by": null,
        "created_at": "2025-04-17T03:07:13+00:00",
        "updated_at": "2025-06-10T06:21:48+00:00",
        "closed_at": null,
        "comments_count": [
            "ztt19851213",
            "ztt19851213",
            "yangliujun01",
            "ChaoII"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2612,
        "title": "自编译的 fastdeploy_python-1.0.7-py3.10-linux-x86_64 使用 openvino 作为backend 报错, paddle, ort 工作正常",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： 说明具体的版本，\nfastdeploy_python-1.0.7-py3.10-linux-x86_64\n\n\n- 【编译命令】如果您是自行编译的FastDeploy，请说明您的编译方式（参数命令）\n参考:   https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/cpu.md\ngit clone https://github.com/PaddlePaddle/FastDeploy.git\ngit checkout origin/release/1.0.7\n\ncd FastDeploy/python\nexport ENABLE_ORT_BACKEND=ON\nexport ENABLE_PADDLE_BACKEND=ON\nexport ENABLE_OPENVINO_BACKEND=ON\nexport ENABLE_VISION=ON\nexport ENABLE_TEXT=ON\nexport OPENCV_DIRECTORY=/usr/lib/x86_64-linux-gnu/cmake/opencv4\n\npython setup.py build\npython setup.py bdist_wheel\n\n\n\n-- *************FastDeploy Building Summary**********\n\n--   CMake version             : 3.27.2\n--   CMake command             : /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake\n--   System                    : Linux\n--   C++ compiler              : /usr/bin/c++\n--   C++ compiler version      : 12.3.0\n--   CXX flags                 : -Wno-format -g0 -O3\n--   EXE linker flags          : \n--   Shared linker flags       : \n--   Build type                : Release\n--   Compile definitions       : _GLIBCXX_USE_CXX11_ABI=1;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;ENABLE_ORT_BACKEND;ENABLE_PADDLE_BACKEND;ENABLE_OPENVINO_BACKEND;ENABLE_VISION;ENABLE_TEXT;ENABLE_PADDLE2ONNX\n--   CMAKE_PREFIX_PATH         : \n--   CMAKE_INSTALL_PREFIX      : /usr/local\n--   CMAKE_MODULE_PATH         : \n-- \n--   FastDeploy version        : 1.0.7\n--   ENABLE_ORT_BACKEND        : ON\n--   ENABLE_RKNPU2_BACKEND     : OFF\n--   ENABLE_HORIZON_BACKEND    : OFF\n--   ENABLE_SOPHGO_BACKEND     : OFF\n--   ENABLE_PADDLE_BACKEND     : ON\n--   ENABLE_LITE_BACKEND       : OFF\n--   ENABLE_POROS_BACKEND      : OFF\n--   ENABLE_TRT_BACKEND        : OFF\n--   ENABLE_OPENVINO_BACKEND   : ON\n--   ENABLE_BENCHMARK          : OFF\n--   ENABLE_VISION             : ON\n--   ENABLE_TEXT               : ON\n--   ENABLE_ENCRYPTION         : OFF\n--   ENABLE_FLYCV              : OFF\n--   ENABLE_CVCUDA             : OFF\n--   WITH_GPU                  : OFF\n--   WITH_IPU                  : OFF\n--   WITH_OPENCL               : OFF\n--   WITH_TESTING              : OFF\n--   WITH_ASCEND               : OFF\n--   WITH_DIRECTML             : OFF\n--   WITH_TIMVX                : OFF\n--   WITH_KUNLUNXIN            : OFF\n--   WITH_CAPI                 : OFF\n--   WITH_CSHARPAPI            : OFF\n--   ONNXRuntime version       : 1.12.0\n--   Paddle Inference version  : 0.0.0.660f781b77\n--   OpenVINO version          : dev.2023.03.2\n--   Python executable         : /home/test/py310_paddle_cpu/bin/python\n--   Python includes           : /usr/include/python3.10\n\n\n\n- 【系统平台】: Linux x64(Ubuntu 22.04) \n- 【硬件】： 说明具体硬件型号，Xeon-SP  CPU\n- 【编译语言】： Python(3.10）\n    # python list\n\nPython               3.10.12\nopenvino           2024.6.0\nopenvino-dev       2024.6.0\nopenvino-telemetry 2025.1.0\n\n\n\n\n\n## 问题日志及出现问题的操作流程\n\n测试, 参考:\nhttps://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/ocr/PP-OCR/cpu-gpu/python\n看log, openvino 作为backend 已经初始化完成.\n\n\nFastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python# python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image kt.png --device cpu --backend paddle\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nW0419 17:08:07.493630 923328 analysis_config.cc:971] It is detected that mkldnn and memory_optimize_pass are enabled at the same time, but they are not supported yet. Currently, memory_optimize_pass is explicitly disabled\n[INFO] fastdeploy/runtime/runtime.cc(273)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\n[INFO] fastdeploy/runtime/runtime.cc(273)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\n[INFO] fastdeploy/runtime/runtime.cc(273)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::CPU.\ndet boxes: [[6,4],[664,4],[664,18],[6,18]]rec text: KTransformers（KVCache,Al出品）是一个灵活的本地 LLM 推理优化框架，旨在通过高级内 rec score:0.914088 cls label: 0 cls score: 0.940954\ndet boxes: [[6,29],[664,29],[664,44],[6,44]]rec text: 核优化和模型部署/井行策略提升Transformer模型推理性能(GitHub -kvcache- rec score:0.931263 cls label: 0 cls score: 0.967136\ndet boxes: [[6,53],[665,53],[665,68],[6,68]]rec text: ai/ktransformers: A Flexible Framework for Experiencing Cutting-edge LLM rec score:0.914593 cls label: 0 cls score: 0.760472\ndet boxes: [[6,77],[664,78],[664,93],[6,92]]rec text: InferenceOptimizations)。它以注入式设计嵌入在HuggingFaceTransformers接口之 rec score:0.926265 cls label: 0 cls score: 0.960029\ndet boxes: [[7,102],[665,102],[665,117],[7,117]]rec text: 上，用一行代码即可将模型替换为优化版本，提供与HuggingFace接口兼容的使用方式，以及 rec score:0.941941 cls label: 0 cls score: 0.995538\ndet boxes: [[7,127],[665,127],[665,142],[7,142]]rec text: OpenAl APl兼容的REsT接和简易 Web Ul (GitHub - kvcache-ai/ktransformers: A rec score:0.904875 cls label: 0 cls score: 0.932776\ndet boxes: [[6,151],[653,151],[653,166],[6,166]]rec text: Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations). rec score:0.955298 cls label: 0 cls score: 0.802574\n\nFastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python# python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image kt.png --device cpu --backend ort\n[INFO] fastdeploy/runtime/runtime.cc(300)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\n[INFO] fastdeploy/runtime/runtime.cc(300)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\n[INFO] fastdeploy/runtime/runtime.cc(300)::CreateOrtBackend\tRuntime initialized with Backend::ORT in Device::CPU.\ndet boxes: [[6,4],[664,4],[664,18],[6,18]]rec text: KTransformers（KVCache,Al出品）是一个灵活的本地 LLM 推理优化框架，旨在通过高级内 rec score:0.914088 cls label: 0 cls score: 0.940954\ndet boxes: [[6,29],[664,29],[664,44],[6,44]]rec text: 核优化和模型部署/井行策略提升Transformer模型推理性能(GitHub -kvcache- rec score:0.931263 cls label: 0 cls score: 0.967136\ndet boxes: [[6,53],[665,53],[665,68],[6,68]]rec text: ai/ktransformers: A Flexible Framework for Experiencing Cutting-edge LLM rec score:0.914593 cls label: 0 cls score: 0.760473\ndet boxes: [[6,77],[664,78],[664,93],[6,92]]rec text: InferenceOptimizations)。它以注入式设计嵌入在HuggingFaceTransformers接口之 rec score:0.926265 cls label: 0 cls score: 0.960029\ndet boxes: [[7,102],[665,102],[665,117],[7,117]]rec text: 上，用一行代码即可将模型替换为优化版本，提供与HuggingFace接口兼容的使用方式，以及 rec score:0.941942 cls label: 0 cls score: 0.995538\ndet boxes: [[7,127],[665,127],[665,142],[7,142]]rec text: OpenAl APl兼容的REsT接和简易 Web Ul (GitHub - kvcache-ai/ktransformers: A rec score:0.904874 cls label: 0 cls score: 0.932776\ndet boxes: [[6,151],[653,151],[653,166],[6,166]]rec text: Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations). rec score:0.955297 cls label: 0 cls score: 0.802574\n\n\nFastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python# python infer.py --det_model ch_PP-OCRv3_det_infer --cls_model ch_ppocr_mobile_v2.0_cls_infer --rec_model ch_PP-OCRv3_rec_infer --rec_label_file ppocr_keys_v1.txt --image kt.png --device cpu --backend openvino\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(218)::InitFromPaddle\tnumber of streams:1.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(228)::InitFromPaddle\taffinity:YES.\n[INFO] fastdeploy/runtime/backends/openvino/ov_backend.cc(240)::InitFromPaddle\tCompile OpenVINO model on device_name:CPU.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreateOpenVINOBackend\tRuntime initialized with Backend::OPENVINO in Device::CPU.\n**Traceback (most recent call last):\n  File \"/home/test/FastDeploy/examples/vision/ocr/PP-OCR/cpu-gpu/python/infer.py\", line 211, in <module>\n    result = ppocr_v3.predict(im)\n  File \"/home/test/py310_paddle_cpu/lib/python3.10/site-packages/fastdeploy_python-1.0.7-py3.10-linux-x86_64.egg/fastdeploy/vision/ocr/ppocr/__init__.py\", line 958, in predict\n    return self.system_.predict(input_image)\nRuntimeError: Primitive descriptor was not found for node linear_1.tmp_1.**\n\n\n\n\n\n\n\n\n\n",
        "state": "open",
        "user": "jianweimama",
        "closed_by": null,
        "created_at": "2025-04-19T09:15:07+00:00",
        "updated_at": "2025-06-10T06:21:48+00:00",
        "closed_at": null,
        "comments_count": [
            "jianweimama",
            "pioneer12345"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2613,
        "title": "GPU编译错误",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： develop 20250422\n- 【编译命令】\n```\nset ENABLE_ORT_BACKEND=OFF\nset ENABLE_PADDLE_BACKEND=OFF\nset ENABLE_OPENVINO_BACKEND=OFF\nset ENABLE_VISION=ON\nset ENABLE_TEXT=ON\nset ENABLE_TRT_BACKEND=Off\nset WITH_GPU=ON\nset CUDA_DIRECTORY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\nset OPENCV_DIRECTORY=D:\\AI\\opencv\\build\\x64\\vc16\\lib\npython setup.py build\n```\n- 【系统平台】:  Windows x64(Windows11) \n- 【硬件】：  Nvidia GPU 5090D， CUDA 12.8  CUDNN 9.8\n- 【编译语言】： anaconda Python3.10\n\n## 问题日志及出现问题的操作流程\n```\nI:\\AI\\FastDeploy-develop\\python\\setup.py:121: DeprecationWarning: Use shutil.which instead of find_executable\n  CMAKE = find_executable('cmake3') or find_executable('cmake')\nI:\\AI\\FastDeploy-develop\\python\\setup.py:122: DeprecationWarning: Use shutil.which instead of find_executable\n  MAKE = find_executable('make')\nrunning build\nrunning build_py\nrunning create_version\nrunning cmake_build\n-- Building for: Visual Studio 17 2022\nCMake Warning (dev) at CMakeLists.txt:15 (PROJECT):\n  cmake_minimum_required() should be called prior to this top-level project()\n  call.  Please see the cmake-commands(7) manual for usage documentation of\n  both commands.\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- Selecting Windows SDK version 10.0.22621.0 to target Windows 10.0.26100.\n-- The C compiler identification is MSVC 19.43.34809.0\n-- The CXX compiler identification is MSVC 19.43.34809.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: D:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.43.34808/bin/Hostx64/x64/cl.exe - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: D:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.43.34808/bin/Hostx64/x64/cl.exe - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- The CUDA compiler identification is NVIDIA 12.8.93 with host compiler MSVC 19.43.34809.0\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - done\n-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.8/bin/nvcc.exe - skipped\n-- Detecting CUDA compile features\n-- Detecting CUDA compile features - done\n-- CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.8/bin/nvcc.exe, version: NVIDIA 12.8.93\nUsing New Release Strategy - All Arches Packge\n-- CUDA detected: 12.8.93\n-- NVCC_FLAGS_EXTRA:  -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86\nCMake Deprecation Warning at third_party/yaml-cpp/CMakeLists.txt:2 (cmake_minimum_required):\n  Compatibility with CMake < 3.10 will be removed from a future version of\n  CMake.\n\n  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n  to tell CMake that the project requires at least <min> but has been updated\n  to work with policies introduced by <max> or earlier.\n\n\n-- Use the opencv lib specified by user. The OpenCV path: D:/AI/opencv/build/x64/vc16/lib\n-- Found OpenCV: D:/AI/opencv/build (found version \"4.11.0\")\nFASTTOKENIZER_COMPILE_LIB = I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/third_libs/install/fast_tokenizer/lib/core_tokenizers.lib\nCMake Warning (dev) at D:/Program Files/CMake/share/cmake-4.0/Modules/ExternalProject/shared_internal_commands.cmake:1276 (message):\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\n  not set.  The policy's OLD behavior will be used.  When using a URL\n  download, the timestamps of extracted files should preferably be that of\n  the time of extraction, otherwise code that depends on the extracted\n  contents might not be rebuilt if the URL changes.  The OLD behavior\n  preserves the timestamps from the archive instead, but this is usually not\n  what you want.  Update your project to the NEW behavior or specify the\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\n  robustness issue.\nCall Stack (most recent call first):\n  D:/Program Files/CMake/share/cmake-4.0/Modules/ExternalProject.cmake:3076 (_ep_add_download_command)\n  cmake/fast_tokenizer.cmake:110 (ExternalProject_Add)\n  CMakeLists.txt:459 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n--\n-- *************FastDeploy Building Summary**********\n--   CMake version             : 4.0.0\n--   CMake command             : D:/Program Files/CMake/bin/cmake.exe\n--   System                    : Windows\n--   C++ compiler              : D:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.43.34808/bin/Hostx64/x64/cl.exe\n--   C++ standard              :\n--   C++ cuda standard         : 11\n--   C++ compiler version      : 19.43.34809.0\n--   CXX flags                 : /DWIN32 /D_WINDOWS /W3 /GR /EHsc\n--   EXE linker flags          : /machine:x64\n--   Shared linker flags       : /machine:x64\n--   Build type                : Release\n--   Compile definitions       : YAML_CPP_DLL;FASTDEPLOY_LIB;CMAKE_BUILD_TYPE=Release;EIGEN_STRONG_INLINE=inline;WITH_GPU;ENABLE_NVJPEG;ENABLE_VISION;ENABLE_TEXT\n--   CMAKE_PREFIX_PATH         :\n--   CMAKE_INSTALL_PREFIX      : C:/Program Files/fastdeploy\n--   CMAKE_MODULE_PATH         :\n--\n--   FastDeploy version        : 0.0.0\n--   ENABLE_ORT_BACKEND        : OFF\n--   ENABLE_RKNPU2_BACKEND     : OFF\n--   ENABLE_HORIZON_BACKEND    : OFF\n--   ENABLE_SOPHGO_BACKEND     : OFF\n--   ENABLE_PADDLE_BACKEND     : OFF\n--   ENABLE_LITE_BACKEND       : OFF\n--   ENABLE_POROS_BACKEND      : OFF\n--   ENABLE_TRT_BACKEND        : Off\n--   ENABLE_OPENVINO_BACKEND   : OFF\n--   ENABLE_TVM_BACKEND        : OFF\n--   ENABLE_BENCHMARK          : OFF\n--   ENABLE_VISION             : ON\n--   ENABLE_TEXT               : ON\n--   ENABLE_ENCRYPTION         : OFF\n--   ENABLE_FLYCV              : OFF\n--   ENABLE_CVCUDA             : OFF\n--   WITH_GPU                  : ON\n--   WITH_IPU                  : OFF\n--   WITH_OPENCL               : OFF\n--   WITH_TESTING              : OFF\n--   WITH_ASCEND               : OFF\n--   WITH_DIRECTML             : OFF\n--   WITH_TIMVX                : OFF\n--   WITH_KUNLUNXIN            : OFF\n--   WITH_CAPI                 : OFF\n--   WITH_CSHARPAPI            : OFF\n--   CUDA_DIRECTORY            : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.8\n--   TRT_DRECTORY              : I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/UNDEFINED\n--   Python executable         : D:\\anaconda3\\envs\\FastDeploy\\python.exe\n--   Python includes           : D:\\anaconda3\\envs\\FastDeploy\\include\nCMake Warning (dev) at CMakeLists.txt:694 (find_package):\n  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules\n  are removed.  Run \"cmake --help-policy CMP0148\" for policy details.  Use\n  the cmake_policy command to set the policy and suppress this warning.\n\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- Found PythonInterp: D:/anaconda3/envs/FastDeploy/python.exe (found suitable version \"3.10.16\", minimum required is \"3.10\")\nCMake Warning (dev) at CMakeLists.txt:695 (find_package):\n  Policy CMP0148 is not set: The FindPythonInterp and FindPythonLibs modules\n  are removed.  Run \"cmake --help-policy CMP0148\" for policy details.  Use\n  the cmake_policy command to set the policy and suppress this warning.\n\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- Found PythonLibs: D:/anaconda3/envs/FastDeploy/libs/python310.lib (found suitable version \"3.10.16\", minimum required is \"3.10\")\n-- Configuring done (8.2s)\n-- Generating done (0.3s)\nCMake Warning:\n  Manually-specified variables were not used by the project:\n\n    ONNX_NAMESPACE\n    PADDLE2ONNX_URL\n    PADDLEINFERENCE_API_COMPAT_2_4_x\n    PADDLEINFERENCE_API_COMPAT_2_5_x\n    PADDLEINFERENCE_API_COMPAT_DEV\n    PADDLEINFERENCE_API_CUSTOM_OP\n    PADDLEINFERENCE_DIRECTORY\n    PADDLEINFERENCE_URL\n    PADDLELITE_URL\n\n\n-- Build files have been written to: I:/AI/FastDeploy-develop/python/.setuptools-cmake-build\n适用于 .NET Framework MSBuild 版本 17.13.19+0d9f5a35a\n\n  Checking File Globs\n  1>Checking Build System\n  Creating directories for 'extern_fast_tokenizer'\n  Building Custom Rule I:/AI/FastDeploy-develop/CMakeLists.txt\n  Performing download step (download, verify and extract) for 'extern_fast_tokenizer'\n  -- Downloading...\n     dst='I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/third_libs/fast_tokenizer/src/fast_tokenizer-win-x64-\n  1.0.2.zip'\n     timeout='none'\n     inactivity timeout='none'\n  -- Using src='https://bj.bcebos.com/paddlenlp/fast_tokenizer/fast_tokenizer-win-x64-1.0.2.zip'\n  -- Downloading... done\n  -- extracting...\n       src='I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/third_libs/fast_tokenizer/src/fast_tokenizer-win-x6\n  4-1.0.2.zip'\n       dst='I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/third_libs/fast_tokenizer/src/extern_fast_tokenizer\n  '\n  -- extracting... [tar xfz]\n  -- extracting... [analysis]\n  -- extracting... [rename]\n  -- extracting... [clean up]\n  -- extracting... done\n  No update step for 'extern_fast_tokenizer'\n  No patch step for 'extern_fast_tokenizer'\n  No configure step for 'extern_fast_tokenizer'\n  No build step for 'extern_fast_tokenizer'\n  Performing install step for 'extern_fast_tokenizer'\n  Completed 'extern_fast_tokenizer'\n  Building Custom Rule I:/AI/FastDeploy-develop/third_party/yaml-cpp/CMakeLists.txt\ncl : 命令行  warning D9025: 正在重写“/W3”(用“/w”) [I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\n\\yaml-cpp.vcxproj]\n  graphbuilder.cpp\n  Defining YAML_CPP_API for DLL export\n  graphbuilderadapter.cpp\n  Defining YAML_CPP_API for DLL export\n  binary.cpp\n  Defining YAML_CPP_API for DLL export\n  convert.cpp\n  Defining YAML_CPP_API for DLL export\n  depthguard.cpp\n  Defining YAML_CPP_API for DLL export\n  directives.cpp\n  emit.cpp\n  Defining YAML_CPP_API for DLL export\n  emitfromevents.cpp\n  Defining YAML_CPP_API for DLL export\n  emitter.cpp\n  Defining YAML_CPP_API for DLL export\n  emitterstate.cpp\n  Defining YAML_CPP_API for DLL export\n  emitterutils.cpp\n  Defining YAML_CPP_API for DLL export\n  exceptions.cpp\n  Defining YAML_CPP_API for DLL export\n  exp.cpp\n  Defining YAML_CPP_API for DLL export\n  memory.cpp\n  Defining YAML_CPP_API for DLL export\n  node.cpp\n  Defining YAML_CPP_API for DLL export\n  node_data.cpp\n  Defining YAML_CPP_API for DLL export\n  nodebuilder.cpp\n  Defining YAML_CPP_API for DLL export\n  nodeevents.cpp\n  Defining YAML_CPP_API for DLL export\n  null.cpp\n  Defining YAML_CPP_API for DLL export\n  ostream_wrapper.cpp\n  Defining YAML_CPP_API for DLL export\n  正在生成代码...\n  正在编译...\n  parse.cpp\n  Defining YAML_CPP_API for DLL export\n  parser.cpp\n  Defining YAML_CPP_API for DLL export\n  regex_yaml.cpp\n  Defining YAML_CPP_API for DLL export\n  scanner.cpp\n  Defining YAML_CPP_API for DLL export\n  scanscalar.cpp\n  Defining YAML_CPP_API for DLL export\n  scantag.cpp\n  Defining YAML_CPP_API for DLL export\n  scantoken.cpp\n  Defining YAML_CPP_API for DLL export\n  simplekey.cpp\n  Defining YAML_CPP_API for DLL export\n  singledocparser.cpp\n  Defining YAML_CPP_API for DLL export\n  stream.cpp\n  Defining YAML_CPP_API for DLL export\n  tag.cpp\n  Defining YAML_CPP_API for DLL export\n  正在生成代码...\n    正在创建库 I:/AI/FastDeploy-develop/python/.setuptools-cmake-build/third_party/yaml-cpp/Release/yaml-cpp.lib 和对象 I:/AI/F\n  astDeploy-develop/python/.setuptools-cmake-build/third_party/yaml-cpp/Release/yaml-cpp.exp\n  yaml-cpp.vcxproj -> I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_party\\yaml-cpp\\Release\\yaml-cpp.dll\n  Building Custom Rule I:/AI/FastDeploy-develop/CMakeLists.txt\n  Compiling CUDA source file ..\\..\\fastdeploy\\vision\\utils\\yolo_preprocess.cu...\n  Compiling CUDA source file ..\\..\\fastdeploy\\runtime\\backends\\common\\cuda\\adaptive_pool2d_kernel.cu...\n  Compiling CUDA source file ..\\..\\fastdeploy\\vision\\common\\processors\\normalize.cu...\n  Compiling CUDA source file ..\\..\\fastdeploy\\vision\\common\\processors\\normalize_and_permute.cu...\n  Compiling CUDA source file ..\\..\\fastdeploy\\function\\cuda_cast.cu...\n\n  (FastDeploy) I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\C\n  UDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -ccbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSV\n  C\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\AI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12\n  .8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\\third_libs\\install\\fast_tokenizer\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\thi\n  rd_libs\\install\\fast_tokenizer\\third_party\\include\" -ID:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Compu\n  ting Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdeploy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cu\n  dart static -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_\n  52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -genco\n  de arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relax\n  ed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DND\n  EBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJP\n  EG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL\n  _FILE__=\\\"fastdeploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPL\n  OY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TE\n  XT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS\n    /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\cuda_cast.obj \"I:\\AI\\FastDeploy\n  -develop\\fastdeploy\\function\\cuda_cast.cu\"\n\n  (FastDeploy) I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\C\n  UDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -ccbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSV\n  C\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\AI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12\n  .8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\\third_libs\\install\\fast_tokenizer\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\thi\n  rd_libs\\install\\fast_tokenizer\\third_party\\include\" -ID:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Compu\n  ting Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdeploy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cu\n  dart static -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_\n  52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -genco\n  de arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relax\n  ed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DND\n  EBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJP\n  EG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL\n  _FILE__=\\\"fastdeploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL\n  -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -\n  DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nolog\n  o /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\yolo_preprocess.obj \"\n  I:\\AI\\FastDeploy-develop\\fastdeploy\\vision\\utils\\yolo_preprocess.cu\"\n  nvcc fatal   : Unsupported gpu architecture 'compute_35'\n  nvcc fatal   : Unsupported gpu architecture 'compute_35'\n\n  (FastDeploy) I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\C\n  UDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -ccbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSV\n  C\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\AI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12\n  .8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\\third_libs\\install\\fast_tokenizer\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\thi\n  rd_libs\\install\\fast_tokenizer\\third_party\\include\" -ID:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Compu\n  ting Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdeploy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cu\n  dart static -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_\n  52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -genco\n  de arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relax\n  ed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DND\n  EBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJP\n  EG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL\n  _FILE__=\\\"fastdeploy/vision/common/processors/normalize.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CP\n  P_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VI\n  SION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0\n  /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\/fastdeploy/vis\n  ion/common/processors/normalize.cu.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\vision\\common\\processors\\normalize.cu\"\n\n  (FastDeploy) I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\C\n  UDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -ccbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSV\n  C\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\AI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12\n  .8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\\third_libs\\install\\fast_tokenizer\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\thi\n  rd_libs\\install\\fast_tokenizer\\third_party\\include\" -ID:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Compu\n  ting Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdeploy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cu\n  dart static -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_\n  52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -genco\n  de arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relax\n  ed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DND\n  EBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJP\n  EG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL\n  _FILE__=\\\"fastdeploy/vision/common/processors/normalize_and_permute.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEB\n  UG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG\n   -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler\n   \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Release\\/fa\n  stdeploy/vision/common/processors/normalize_and_permute.cu.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\vision\\common\\pro\n  cessors\\normalize_and_permute.cu\"\n  nvcc fatal   : Unsupported gpu architecture 'compute_35'\nD:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.8.targets\n(800,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -cc\nbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\A\nI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_p\narty\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_part\ny\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\includ\ne\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\third_party\\include\" -I\nD:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdepl\noy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencode a\nrch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute\n_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm\n_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /w\nd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -D\nEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_\nINTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/vision/utils/yolo_preprocess.cu\\\"\" -D_WINDLL -D_M\nBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline\n -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfast\ndeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fas\ntdeploy.dir\\Release\\yolo_preprocess.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\vision\\utils\\yolo_preprocess.cu\"”已退出，返回代码为\n 1。 [I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\nD:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.8.targets\n(800,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -cc\nbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\A\nI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_p\narty\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_part\ny\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\includ\ne\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\third_party\\include\" -I\nD:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdepl\noy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencode a\nrch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute\n_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm\n_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /w\nd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -D\nEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_\nINTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/function/cuda_cast.cu\\\"\" -D_WINDLL -D_MBCS -DWIN3\n2 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GP\nU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXP\nORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.di\nr\\Release\\cuda_cast.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\function\\cuda_cast.cu\"”已退出，返回代码为 1。 [I:\\AI\\FastDeploy-deve\nlop\\python\\.setuptools-cmake-build\\fastdeploy.vcxproj]\n  nvcc fatal   : Unsupported gpu architecture 'compute_35'\nD:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.8.targets\n(800,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -cc\nbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\A\nI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_p\narty\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_part\ny\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\includ\ne\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\third_party\\include\" -I\nD:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdepl\noy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencode a\nrch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute\n_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm\n_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /w\nd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -D\nEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_\nINTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/vision/common/processors/normalize.cu\\\"\" -D_WINDL\nL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=\ninline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\"\n-Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\"\n-o fastdeploy.dir\\Release\\/fastdeploy/vision/common/processors/normalize.cu.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\vi\nsion\\common\\processors\\normalize.cu\"”已退出，返回代码为 1。 [I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\fastdeploy.v\ncxproj]\nD:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.8.targets\n(800,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -cc\nbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\A\nI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_p\narty\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_part\ny\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\includ\ne\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\third_party\\include\" -I\nD:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdepl\noy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencode a\nrch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute\n_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm\n_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /w\nd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -D\nEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_\nINTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/vision/common/processors/normalize_and_permute.cu\n\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_ST\nRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\n\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\n\\vc143.pdb\" -o fastdeploy.dir\\Release\\/fastdeploy/vision/common/processors/normalize_and_permute.cu.obj \"I:\\AI\\FastDepl\noy-develop\\fastdeploy\\vision\\common\\processors\\normalize_and_permute.cu\"”已退出，返回代码为 1。 [I:\\AI\\FastDeploy-develop\\python\\\n.setuptools-cmake-build\\fastdeploy.vcxproj]\n\n  (FastDeploy) I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\C\n  UDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -ccbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSV\n  C\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\AI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12\n  .8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_party\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-\n  cmake-build\\third_libs\\install\\fast_tokenizer\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\thi\n  rd_libs\\install\\fast_tokenizer\\third_party\\include\" -ID:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Compu\n  ting Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdeploy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cu\n  dart static -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_\n  52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -genco\n  de arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -w --expt-relax\n  ed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /wd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DND\n  EBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJP\n  EG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL\n  _FILE__=\\\"fastdeploy/runtime/backends/common/cuda/adaptive_pool2d_kernel.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -\n  DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_N\n  VJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcom\n  piler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Release\\vc143.pdb\" -o fastdeploy.dir\\Releas\n  e\\adaptive_pool2d_kernel.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\runtime\\backends\\common\\cuda\\adaptive_pool2d_kernel\n  .cu\"\n  nvcc fatal   : Unsupported gpu architecture 'compute_35'\nD:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 12.8.targets\n(800,9): error MSB3721: 命令“\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcc.exe\"  --use-local-env -cc\nbin \"D:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\bin\\HostX64\\x64\" -x cu   -I\"I:\\A\nI\\FastDeploy-develop\\.\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\" -I\"I:\\AI\\FastDeploy-develop\\third_p\narty\\eigen\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\" -I\"I:\\AI\\FastDeploy-develop\\third_part\ny\\yaml-cpp\\include\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\includ\ne\" -I\"I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\third_libs\\install\\fast_tokenizer\\third_party\\include\" -I\nD:\\AI\\opencv\\build\\include -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\include\"     --keep-dir fastdepl\noy\\x64\\Release  -maxrregcount=0    --machine 64 --compile -cudart static -gencode arch=compute_35,code=sm_35 -gencode a\nrch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute\n_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm\n_80 -gencode arch=compute_86,code=sm_86 -w --expt-relaxed-constexpr --expt-extended-lambda -Xcompiler=\"/EHsc /wd4244 /w\nd4267 /wd4819 /bigobj -Ob2 /wd4251\"   -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -D\nEIGEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_\nINTDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -D\"__REL_FILE__=\\\"fastdeploy/runtime/backends/common/cuda/adaptive_pool2d_kern\nel.cu\\\"\" -D_WINDLL -D_MBCS -DWIN32 -D_WINDOWS -DNDEBUG -DYAML_CPP_DLL -DFASTDEPLOY_LIB -DCMAKE_BUILD_TYPE=Release -DEIG\nEN_STRONG_INLINE=inline -DWITH_GPU -DENABLE_NVJPEG -DENABLE_VISION -DENABLE_TEXT -DBUILD_FASTDEPLOY_PYTHON -D\"CMAKE_INT\nDIR=\\\"Release\\\"\" -Dfastdeploy_EXPORTS -Xcompiler \"/EHsc /W0 /nologo /O2 /FS   /MD /GR\" -Xcompiler \"/Fdfastdeploy.dir\\Re\nlease\\vc143.pdb\" -o fastdeploy.dir\\Release\\adaptive_pool2d_kernel.obj \"I:\\AI\\FastDeploy-develop\\fastdeploy\\runtime\\back\nends\\common\\cuda\\adaptive_pool2d_kernel.cu\"”已退出，返回代码为 1。 [I:\\AI\\FastDeploy-develop\\python\\.setuptools-cmake-build\\fastd\neploy.vcxproj]\nTraceback (most recent call last):\n  File \"I:\\AI\\FastDeploy-develop\\python\\setup.py\", line 445, in <module>\n    setuptools.setup(\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n    return run_commands(dist)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n    dist.run_commands()\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 983, in run_commands\n    self.run_command(cmd)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 136, in run\n    self.run_command(cmd_name)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"I:\\AI\\FastDeploy-develop\\python\\setup.py\", line 308, in run\n    self.run_command('cmake_build')\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"I:\\AI\\FastDeploy-develop\\python\\setup.py\", line 302, in run\n    subprocess.check_call(build_args)\n  File \"D:\\anaconda3\\envs\\FastDeploy\\lib\\subprocess.py\", line 369, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['D:\\\\Program Files\\\\CMake\\\\bin\\\\cmake.exe', '--build', '.', '--config', 'Release', '--', '/maxcpucount:24']' returned non-zero exit status 1.\n```\n",
        "state": "open",
        "user": "monkeycc",
        "closed_by": null,
        "created_at": "2025-04-22T06:41:00+00:00",
        "updated_at": "2025-06-11T11:06:28+00:00",
        "closed_at": null,
        "comments_count": [
            "ChaoII",
            "askxiaozhang",
            "monkeycc",
            "impl1874",
            "impl1874"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2615,
        "title": "c++多线程推理coredump",
        "body": "## 环境\n\n- 【FastDeploy版本】： fastdeploy-linux-gpu-1.0.7\n- 【系统平台】: Linux x64(cenots8) \n- 【硬件】： NVIDIA RTX 4000 SFF Ada， CUDA 11.2 CUDNN 8.2\n- 【编译语言】： C++\n\n## 问题日志及出现问题的操作流程\n- 编译官方测试demo：multi_thread_demo\n  - 运行一个线程推理 ./multi_thread_demo ResNet50_vd_infer ./images/ 1 1 正常输出结果\n  - 运行两个线程推理 ./multi_thread_demo ResNet50_vd_infer ./images/ 1 2  产生: Segmentation fault (core dumped)\n",
        "state": "closed",
        "user": "bert-y",
        "closed_by": "bert-y",
        "created_at": "2025-04-27T02:37:07+00:00",
        "updated_at": "2025-04-28T02:52:57+00:00",
        "closed_at": "2025-04-28T02:52:56+00:00",
        "comments_count": [
            "bert-y"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2618,
        "title": "orin nx 使用tensorrt推理报错",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： 最新版本\n- 【编译命令】\nmkdir build && cd build\ncmake .. \\\n      -DENABLE_ORT_BACKEND=ON \\\n      -DENABLE_TRT_BACKEND=ON \\\n      -DWITH_GPU=OFF \\\n      -DBUILD_ON_JETSON=ON \\\n      -DCMAKE_INSTALL_PREFIX=${PWD}/compiled_fastdeploy_sdk \\\n      -DENABLE_VISION=ON \\\n      -DCMAKE_CXX_STANDARD=17 \\\n      -DENABLE_VISION=ON \\\n      -DOPENCV_DIRECTORY=/usr/lib/cmake/opencv4/\nmake -j4 && make install\n\n- 【系统平台】: arrch64 \n- 【硬件】：\n\n![Image](https://github.com/user-attachments/assets/2ba970b6-fc9b-4dfc-bc0d-785bd3d14a73)\n\n- 【编译语言】： C++\n\n## 问题日志及出现问题的操作流程\n- 附上详细的问题日志有助于快速定位分析\n[ERROR] fastdeploy/runtime/backends/tensorrt/trt_backend.cc(239)::log\t3: [runtime.cpp::~Runtime::344] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::344, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.\n)\n在使用tensorrt推理时，只能够推理一张图片",
        "state": "open",
        "user": "z5013",
        "closed_by": null,
        "created_at": "2025-05-21T06:56:22+00:00",
        "updated_at": "2025-06-10T06:21:49+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2616,
        "title": "Fastdeploy_python初始化cls_model为None之后，推理图片还是会先检测后进行方向旋转然后给识别模型进行识别",
        "body": "\n## 环境\n\n- 【FastDeploy版本】： fastdeploy_gpu_python-1.0.7-cp39-cp39\n- 【编译命令】通过pip安装的\n- 【系统平台】: Linux x64(Ubuntu 22.04) \n- 【硬件】：Nvidia GPU 4060\n- 【编译语言】：Python(3.9）\n\n## ocr初始化模型我是通过该方式初始化的\n···\n        self.ocr = fd.vision.ocr.PPOCRv4(\n            det_model=det_model, cls_model=None, rec_model=rec_model)\n···\n然后推理该图片\n\n![Image](https://github.com/user-attachments/assets/f5292d23-3a1e-47ef-9f2f-2c36dac8eb4b)\n\b推理代码：`self.ocr.predict(image)`\n\n得到结果\ndet boxes: [[154,33],[189,33],[189,53],[154,53]]rec text: ALT rec score:0.999999 \ndet boxes: [[347,32],[409,32],[409,53],[347,53]]rec text: 0-40U/L rec score:0.992021 \ndet boxes: [[253,35],[263,35],[263,52],[253,52]]rec text: 三 rec score:0.660740 \ndet boxes: [[14,34],[140,34],[140,51],[14,51]]rec text: 丙氨酸氨基转移酶 rec score:0.999998 \n\n然后将[[253,35],[263,35],[263,52],[253,52]]手动裁剪出来\n\n![Image](https://github.com/user-attachments/assets/1e35b78d-571b-4e10-b28e-a1bf61dc264c)\n可以看到1这个图片识别效果不好，应该是旋转了。\n然后我自己将裁剪的图片进行单独的rec识别\n结果如下：\n['1', 0.9998661279678345]\n\n该如何解决这个自动旋转的问题？ cls_model为None也会自动旋转，导致识别不对\n",
        "state": "open",
        "user": "askxiaozhang",
        "closed_by": null,
        "created_at": "2025-04-29T06:06:06+00:00",
        "updated_at": "2025-06-10T06:21:49+00:00",
        "closed_at": null,
        "comments_count": [
            "askxiaozhang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2617,
        "title": "ValueError: Unknow elem_type for softmax_2.tmp_0!",
        "body": "PPOCR模型onnx转换成rknn的过程中，报错 Unknow elem_type for softmax_2.tmp_0!,rknn-toolkit2 version: 2.3.2和rknn-toolkit2 version: 2.0.0版本都试了，都是一样的报错，请问怎么解决呢\npython3 convert.py ../model/static_rec_model.onnx rk3568\nI rknn-toolkit2 version: 2.3.2\n--> Config model\ndone\n--> Loading model\nE load_onnx: Unknow elem_type for softmax_2.tmp_0!\nI ===================== WARN(0) =====================\nE rknn-toolkit2 version: 2.3.2\nE load_onnx: Traceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1579, in rknn.api.rknn_base.RKNNBase.load_onnx\n  File \"rknn/api/rknn_base.py\", line 613, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\n  File \"rknn/api/ir_graph.py\", line 84, in rknn.api.ir_graph.IRGraph.__init__\n  File \"rknn/api/ir_graph.py\", line 669, in rknn.api.ir_graph.IRGraph.rebuild\n  File \"rknn/api/rknn_log.py\", line 95, in rknn.api.rknn_log.RKNNLog.e\nValueError: Unknow elem_type for softmax_2.tmp_0!\n\nI ===================== WARN(0) =====================\nE rknn-toolkit2 version: 2.3.2\nTraceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1579, in rknn.api.rknn_base.RKNNBase.load_onnx\n  File \"rknn/api/rknn_base.py\", line 613, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\n  File \"rknn/api/ir_graph.py\", line 84, in rknn.api.ir_graph.IRGraph.__init__\n  File \"rknn/api/ir_graph.py\", line 669, in rknn.api.ir_graph.IRGraph.rebuild\n  File \"rknn/api/rknn_log.py\", line 95, in rknn.api.rknn_log.RKNNLog.e\nValueError: Unknow elem_type for softmax_2.tmp_0!\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"convert.py\", line 68, in <module>\n    ret = rknn.load_onnx(model=model_path)\n  File \"/home/uto/work/ocr/anaconda3/envs/python3.8-tk2/lib/python3.8/site-packages/rknn/api/rknn.py\", line 168, in load_onnx\n    return self.rknn_base.load_onnx(model, inputs, input_size_list, input_initial_val, outputs)\n  File \"rknn/api/rknn_log.py\", line 349, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_log.py\", line 95, in rknn.api.rknn_log.RKNNLog.e\nValueError: Traceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1579, in rknn.api.rknn_base.RKNNBase.load_onnx\n  File \"rknn/api/rknn_base.py\", line 613, in rknn.api.rknn_base.RKNNBase._create_ir_and_inputs_meta\n  File \"rknn/api/ir_graph.py\", line 84, in rknn.api.ir_graph.IRGraph.__init__\n  File \"rknn/api/ir_graph.py\", line 669, in rknn.api.ir_graph.IRGraph.rebuild\n  File \"rknn/api/rknn_log.py\", line 95, in rknn.api.rknn_log.RKNNLog.e\nValueError: Unknow elem_type for softmax_2.tmp_0!",
        "state": "open",
        "user": "chenwei199",
        "closed_by": null,
        "created_at": "2025-05-13T05:39:14+00:00",
        "updated_at": "2025-06-10T06:21:49+00:00",
        "closed_at": null,
        "comments_count": [
            "chenwei199"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2620,
        "title": "GPU多线程使用时，程序卡住随后崩溃。",
        "body": "*********************************************\n温馨提示：根据社区不完全统计，按照模板提问，可以加快回复和解决问题的速度\n*********************************************\n\n## 环境\n\n- 【FastDeploy版本】： fastdeploy-win-x64-gpu-1.0.7\n- 【编译命令】使用下载的预编译库 fastdeploy-win-x64-gpu-1.0.7\n- 【系统平台】: Windows x64(Windows11)\n- 【硬件】： 说明具体硬件型号，如 Nvidia GPU 4070 Laptop， CUDA 11.6 CUDNN 8.2\n- 【编译语言】： C++\n\n## 问题日志及出现问题的操作流程\n参考官方示例`multi_thread.cc`，将代码修改至如下：\n```\n#include <thread>\n#include <future>\n#include \"fastdeploy/vision.h\"\n#ifdef WIN32\nconst char sep = '\\\\';\n#else\nconst char sep = '/';\n#endif\n\n// void Predict(fastdeploy::vision::detection::PPDetBase *model, int thread_id, const std::vector<std::string>& images) {\nvoid Predict(fastdeploy::vision::detection::PPDetBase *model, int thread_id, const std::string& image_file) {\n    // for (auto const &image_file : images) {\n        auto im = cv::imread(image_file);\n\n        fastdeploy::vision::DetectionResult res;\n        if (!model->Predict(im, &res)) {\n            std::cerr << \"Failed to predict.\" << std::endl;\n            return;\n        }\n\n        // print res\n        std::cout << \"Thread Id: \" << thread_id << std::endl;\n        // std::cout << res.Str() << std::endl;\n    // }\n}\n\nvoid GetImageList(std::vector<std::vector<std::string>>* image_list, const std::string& image_file_path, int thread_num){\n    std::vector<cv::String> images;\n    cv::glob(image_file_path, images, false);\n    // number of image files in images folder\n    size_t count = images.size();\n    size_t num = count / thread_num;\n    for (int i = 0; i < thread_num; i++) {\n        std::vector<std::string> temp_list;\n        if (i == thread_num - 1) {\n            for (size_t j = i*num; j < count; j++){\n                temp_list.push_back(images[j]);\n            }\n        } else {\n            for (size_t j = 0; j < num; j++){\n                temp_list.push_back(images[i * num + j]);\n            }\n        }\n        (*image_list)[i] = temp_list;\n    }\n}\n\nvoid GpuInfer(const std::string& model_dir, const std::string& image_file_path, int thread_num) {\n    auto model_file = model_dir + sep + \"model.pdmodel\";\n    auto params_file = model_dir + sep + \"model.pdiparams\";\n    auto config_file = model_dir + sep + \"infer_cfg.yml\";\n    auto option = fastdeploy::RuntimeOption();\n    option.UseGpu();\n    option.UsePaddleBackend();\n    auto model = fastdeploy::vision::detection::PPYOLOE(\n        model_file, params_file, config_file, option);\n    if (!model.Initialized()) {\n        std::cerr << \"Failed to initialize.\" << std::endl;\n        return;\n    }\n\n    std::vector<decltype(model.Clone())> models;\n    for (int i = 0; i < thread_num; ++i) {\n        models.emplace_back(model.Clone());\n    }\n\n    std::vector<std::vector<std::string>> image_list(thread_num);\n    GetImageList(&image_list, image_file_path, thread_num);\n\n    std::thread t1(Predict, models[0].get(), 0, R\"(E:\\project_codes\\paddle_test\\release\\test1.jpg)\");\n    std::thread t2(Predict, models[1].get(), 1, R\"(E:\\project_codes\\paddle_test\\release\\test2.jpg)\");\n    // std::thread t3(Predict, models[2].get(), 2, R\"(E:\\project_codes\\paddle_test\\release\\test3.jpg)\");\n    // std::thread t4(Predict, models[3].get(), 3, R\"(E:\\project_codes\\paddle_test\\release\\test4.jpg)\");\n\n    t1.join();\n    t2.join();\n    // t3.join();\n    // t4.join();\n\n    // auto ret1 = std::async(std::launch::async, Predict, models[0].get(), 0, R\"(E:\\project_codes\\paddle_test\\release\\test.jpg)\");\n    // auto ret2 = std::async(std::launch::async, Predict, models[1].get(), 1, R\"(E:\\project_codes\\paddle_test\\release\\test.jpg)\");\n\n    // ret1.get();\n    // ret2.get();\n\n    // std::vector<std::thread> threads;\n    // for (int i = 0; i < thread_num; ++i) {\n    //     threads.emplace_back(Predict, models[i].get(), i, image_list[i]);\n    // }\n\n    // for (int i = 0; i < thread_num; ++i) {\n    //     threads[i].join();\n    // }\n}\n\nint main(int argc, char **argv) {\n    GpuInfer(R\"(E:\\project_codes\\paddle_test\\release\\ppyoloe_plus_crn_m_80e_coco)\", R\"(E:\\project_codes\\paddle_test\\release\\test1.jpg)\" , 2);\n\n    return 0;\n}\n```\n控制台信息如下：\n\n> 17:11:56: Starting E:\\project_codes\\paddle_test\\release\\paddle_test.exe...\n[INFO] fastdeploy/vision/common/processors/transform.cc(45)::fastdeploy::vision::FuseNormalizeCast\tNormalize and Cast are fused to Normalize in preprocessing pipeline.\n[INFO] fastdeploy/vision/common/processors/transform.cc(93)::fastdeploy::vision::FuseNormalizeHWC2CHW\tNormalize and HWC2CHW are fused to NormalizeAndPermute  in preprocessing pipeline.\n[INFO] fastdeploy/vision/common/processors/transform.cc(159)::fastdeploy::vision::FuseNormalizeColorConvert\tBGR2RGB and NormalizeAndPermute are fused to NormalizeAndPermute with swap_rb=1\n[INFO] fastdeploy/runtime/runtime.cc(273)::fastdeploy::Runtime::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/runtime.cc(384)::fastdeploy::Runtime::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/runtime.cc(384)::fastdeploy::Runtime::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n17:13:23: E:\\project_codes\\paddle_test\\release\\paddle_test.exe 崩溃。\n\n我是将官方示例的模型改为了PPYOLOE，然后作了一些相应的修改，当代码不是：\n```\n std::thread t1(Predict, models[0].get(), 0, R\"(E:\\project_codes\\paddle_test\\release\\test1.jpg)\");\n std::thread t2(Predict, models[1].get(), 1, R\"(E:\\project_codes\\paddle_test\\release\\test2.jpg)\");\n // std::thread t3(Predict, models[2].get(), 2, R\"(E:\\project_codes\\paddle_test\\release\\test3.jpg)\");\n // std::thread t4(Predict, models[3].get(), 3, R\"(E:\\project_codes\\paddle_test\\release\\test4.jpg)\");\n\n t1.join();\n t2.join();\n```\n而是仅开启一个线程时是可以正常运行的：\n```\n std::thread t1(Predict, models[0].get(), 0, R\"(E:\\project_codes\\paddle_test\\release\\test1.jpg)\");\n // std::thread t2(Predict, models[1].get(), 1, R\"(E:\\project_codes\\paddle_test\\release\\test2.jpg)\");\n // std::thread t3(Predict, models[2].get(), 2, R\"(E:\\project_codes\\paddle_test\\release\\test3.jpg)\");\n // std::thread t4(Predict, models[3].get(), 3, R\"(E:\\project_codes\\paddle_test\\release\\test4.jpg)\");\n\n t1.join();\n // t2.join();\n```",
        "state": "open",
        "user": "Yamabukiss",
        "closed_by": null,
        "created_at": "2025-05-29T09:16:51+00:00",
        "updated_at": "2025-06-10T06:21:50+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2619,
        "title": "PP-SHITU 错误",
        "body": "## 环境\n\n- 【FastDeploy版本】： fastdeploy-develop\n- 【系统平台】: Linux x64(Ubuntu 18.04) / Windows x64(Windows10) \n- 【编译语言】： C++\n\n## 问题日志及出现问题的操作流程\n1. 既有example的ppshitu并不完整，缺乏最后的向量索引；\n2. 既有的cmake/faiss中提供的faiss下载地址中，dumpbin /exports \"lib/BLAS.lib\" | findstr \"sgemm_\" 甚至为空，导致链接失败；\n3. 自行编译的faiss独立运行成功，但在同paddlepaddle一同运行时，发生线程死锁无法退出，怀疑是底层线程库冲突，paddlepaddle既有的intel mkl依赖过于陈旧，Intel已不再维护并不提供相关库的下载。",
        "state": "open",
        "user": "ShawnXsw",
        "closed_by": null,
        "created_at": "2025-05-27T14:09:21+00:00",
        "updated_at": "2025-06-10T06:21:50+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "ShawnXsw"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2623,
        "title": "FastDeploy build error",
        "body": "![Image](https://github.com/user-attachments/assets/dbfcd4dd-1126-4be8-af22-9d112f4ffa3c)\n如何fix?",
        "state": "closed",
        "user": "fmiao2372",
        "closed_by": "fmiao2372",
        "created_at": "2025-06-11T03:32:21+00:00",
        "updated_at": "2025-06-11T05:23:42+00:00",
        "closed_at": "2025-06-11T05:23:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 2630
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 2631
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2627,
        "title": "some bugs when enable LLama2-7b with PaddleNLP",
        "body": "1. 分布式环境的初始化\n![Image](https://github.com/user-attachments/assets/b4f8b065-9b52-434b-9d8d-7110c5dbdad3)\n2.  AutoTokenizer的选择和get_pad_id的返回\n![Image](https://github.com/user-attachments/assets/9e29322f-723f-40c9-a91b-fc9346aa3006)\n3. worker最好添加一个参数传给PredictorArgument，同时load完模型后初始化kv cache\n![Image](https://github.com/user-attachments/assets/0cf36496-a418-4589-9d07-0da94bc2f9a9)",
        "state": "open",
        "user": "fmiao2372",
        "closed_by": null,
        "created_at": "2025-06-18T07:19:37+00:00",
        "updated_at": "2025-06-18T07:19:42+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2621,
        "title": "RK3576编译python SDK报错：[rknpu2.cmake] RKNPU_RUNTIME_PATH does not exist.",
        "body": "\n## 环境\n\n- 【FastDeploy版本】： FastDeploy-release-1.0.7.zip\n- 【编译命令】按照https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/build_and_install/rknpu2.md     \n文档所述编译3588的方式，将export RKNN2_TARGET_SOC=RK3588改为 export RKNN2_TARGET_SOC=RK3576，编译后报错，报错如下：\n```\n-- Use the default onnxruntime lib. The ONNXRuntime path: /userdata/sources/fastdeploy/python/.setuptools-cmake-build/third_libs/install/onnxruntime\nCMake Warning (dev) at /usr/share/cmake-3.25/Modules/ExternalProject.cmake:3075 (message):\n  The DOWNLOAD_EXTRACT_TIMESTAMP option was not given and policy CMP0135 is\n  not set.  The policy's OLD behavior will be used.  When using a URL\n  download, the timestamps of extracted files should preferably be that of\n  the time of extraction, otherwise code that depends on the extracted\n  contents might not be rebuilt if the URL changes.  The OLD behavior\n  preserves the timestamps from the archive instead, but this is usually not\n  what you want.  Update your project to the NEW behavior or specify the\n  DOWNLOAD_EXTRACT_TIMESTAMP option with a value of true to avoid this\n  robustness issue.\nCall Stack (most recent call first):\n  /usr/share/cmake-3.25/Modules/ExternalProject.cmake:4185 (_ep_add_download_command)\n  cmake/onnxruntime.cmake:109 (ExternalProject_Add)\n  CMakeLists.txt:220 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\nDecompress file /userdata/sources/fastdeploy/python/.setuptools-cmake-build/rknpu2_runtime-linux-aarch64-1.4.2b0-RK3576.tgz ...\nCMake Error at cmake/rknpu2.cmake:18 (message):\n  [rknpu2.cmake] RKNPU_RUNTIME_PATH does not exist.\nCall Stack (most recent call first):\n  CMakeLists.txt:255 (include)\n\n\n-- Configuring incomplete, errors occurred!\nSee also \"/userdata/sources/fastdeploy/python/.setuptools-cmake-build/CMakeFiles/CMakeOutput.log\".\nTraceback (most recent call last):\n  File \"/userdata/sources/fastdeploy/python/setup.py\", line 440, in <module>\n    setuptools.setup(\n  File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 108, in setup\n    return distutils.core.setup(**attrs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/core.py\", line 185, in setup\n    return run_commands(dist)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n    dist.run_commands()\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n    self.run_command(cmd)\n  File \"/usr/lib/python3/dist-packages/setuptools/dist.py\", line 1213, in run_command\n    super().run_command(command)\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n    cmd_obj.run()\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/command/build.py\", line 132, in run\n    self.run_command(cmd_name)\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib/python3/dist-packages/setuptools/dist.py\", line 1213, in run_command\n    super().run_command(command)\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n    cmd_obj.run()\n  File \"/userdata/sources/fastdeploy/python/setup.py\", line 303, in run\n    self.run_command('cmake_build')\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib/python3/dist-packages/setuptools/dist.py\", line 1213, in run_command\n    super().run_command(command)\n  File \"/usr/lib/python3/dist-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n    cmd_obj.run()\n  File \"/userdata/sources/fastdeploy/python/setup.py\", line 289, in run\n    subprocess.check_call(cmake_args)\n  File \"/usr/lib/python3.11/subprocess.py\", line 413, in check_call\n    raise CalledProcessError(retcode, cmd)\n\n```\n- 【系统平台】:\n No LSB modules are available.\nDistributor ID:\tDebian\nDescription:\tDebian GNU/Linux 12 (bookworm)\nRelease:\t12\nCodename:\tbookworm\n\n- 【硬件】： RK3576\n- 【编译语言】： Python3.11.2\n\n\n",
        "state": "closed",
        "user": "cody111115",
        "closed_by": "cody111115",
        "created_at": "2025-06-02T02:01:23+00:00",
        "updated_at": "2025-06-02T04:58:13+00:00",
        "closed_at": "2025-06-02T04:58:13+00:00",
        "comments_count": [
            "cody111115"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2625,
        "title": "编译报错zlib.error: Error -3 while decompressing data: invalid stored block lengths",
        "body": "以下是编译信息，执行bash build.sh 后：\n```\n###############################################################\nfind python version 3.11\n[init] removing building directory...\nRequirement already satisfied: setuptools_scm in /root/miniforge3/envs/py3.11/lib/python3.11/site-packages (8.3.1)\nRequirement already satisfied: packaging>=20 in /root/miniforge3/envs/py3.11/lib/python3.11/site-packages (from setuptools_scm) (23.2)\nRequirement already satisfied: setuptools in /root/miniforge3/envs/py3.11/lib/python3.11/site-packages (from setuptools_scm) (75.1.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n[init] init success\n\n[build] build and install fastdeploy_base_ops...\n[2025-06-16 15:39:20,262] [    INFO] dist.py:970 - running install\n/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n[2025-06-16 15:39:20,322] [    INFO] easy_install.py:573 - Checking .pth file support in tmp_base/\n[2025-06-16 15:39:20,322] [    INFO] spawn.py:60 - /root/miniforge3/envs/py3.11/bin/python -E -c pass\n[2025-06-16 15:39:20,334] [ WARNING] easy_install.py:627 - TEST FAILED: tmp_base/ does NOT support .pth files\n[2025-06-16 15:39:20,334] [ WARNING] easy_install.py:504 - bad install directory or PYTHONPATH\n\nYou are attempting to install a package to a directory that is not\non PYTHONPATH and which Python does not read \".pth\" files from.  The\ninstallation directory you specified (via --install-dir, --prefix, or\nthe distutils default setting) was:\n\n    tmp_base/\n\nand your PYTHONPATH environment variable currently contains:\n\n    ''\n\nHere are some of your options for correcting the problem:\n\n* You can choose a different installation directory, i.e., one that is\n  on PYTHONPATH or supports .pth files\n\n* You can add the installation directory to the PYTHONPATH environment\n  variable.  (It must then also be on PYTHONPATH whenever you run\n  Python and want to use the package(s) you are installing.)\n\n* You can set up the installation directory to support \".pth\" files by\n  using one of the approaches described here:\n\n  https://setuptools.pypa.io/en/latest/deprecated/easy_install.html#custom-installation-locations\n\n\nPlease make the appropriate changes for your system and try again.\n[2025-06-16 15:39:20,394] [    INFO] dist.py:970 - running bdist_egg\n[2025-06-16 15:39:20,414] [    INFO] dist.py:970 - running egg_info\n[2025-06-16 15:39:20,414] [    INFO] dir_util.py:58 - creating fastdeploy_base_ops.egg-info\n[2025-06-16 15:39:20,421] [    INFO] egg_info.py:648 - writing fastdeploy_base_ops.egg-info/PKG-INFO\n[2025-06-16 15:39:20,421] [    INFO] egg_info.py:282 - writing dependency_links to fastdeploy_base_ops.egg-info/dependency_links.txt\n[2025-06-16 15:39:20,421] [    INFO] egg_info.py:282 - writing top-level names to fastdeploy_base_ops.egg-info/top_level.txt\n[2025-06-16 15:39:20,421] [    INFO] util.py:324 - writing manifest file 'fastdeploy_base_ops.egg-info/SOURCES.txt'\n[06/16/25 15:39:20] ERROR    listing git files failed - pretending   git.py:26\n                             there aren't any                                 \n[2025-06-16 15:39:20,455] [    INFO] sdist.py:202 - reading manifest file 'fastdeploy_base_ops.egg-info/SOURCES.txt'\n[2025-06-16 15:39:20,455] [    INFO] sdist.py:341 - reading manifest template 'MANIFEST.in'\n[2025-06-16 15:39:20,455] [ WARNING] egg_info.py:398 - warning: no files found matching '*.so' under directory 'third_party/x86-simd-sort'\n[2025-06-16 15:39:20,455] [ WARNING] egg_info.py:398 - warning: no files found matching '*.h' under directory 'third_party/x86-simd-sort'\n[2025-06-16 15:39:20,455] [ WARNING] egg_info.py:398 - warning: no files found matching '*.so' under directory 'third_party/xFasterTransformer'\n[2025-06-16 15:39:20,455] [ WARNING] egg_info.py:398 - warning: no files found matching '*.h' under directory 'third_party/xFasterTransformer'\n[2025-06-16 15:39:20,456] [    INFO] util.py:324 - writing manifest file 'fastdeploy_base_ops.egg-info/SOURCES.txt'\n[2025-06-16 15:39:20,456] [    INFO] bdist_egg.py:162 - installing library code to build/fastdeploy_base_ops/bdist.linux-x86_64/egg\n[2025-06-16 15:39:20,456] [    INFO] dist.py:970 - running install_lib\n[2025-06-16 15:39:20,456] [    INFO] dist.py:970 - running build_ext\nCompiling user custom op, it will cost a few seconds.....\n[2025-06-16 15:39:20,466] [    INFO] build_ext.py:530 - building 'fastdeploy_base_ops' extension\n[2025-06-16 15:39:20,466] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include/third_party -I/root/miniforge3/envs/py3.11/include/python3.11 -I/root/miniforge3/envs/py3.11/include/python3.11 -c /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/gpu_ops/get_output.cc -o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output.o -w -DPADDLE_WITH_CUSTOM_KERNEL -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2025-06-16 15:39:20,467] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include/third_party -I/root/miniforge3/envs/py3.11/include/python3.11 -I/root/miniforge3/envs/py3.11/include/python3.11 -c /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/gpu_ops/get_output_msg_with_topk.cc -o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output_msg_with_topk.o -w -DPADDLE_WITH_CUSTOM_KERNEL -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2025-06-16 15:39:20,467] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include/third_party -I/root/miniforge3/envs/py3.11/include/python3.11 -I/root/miniforge3/envs/py3.11/include/python3.11 -c /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/gpu_ops/reset_need_stop_value.cc -o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/reset_need_stop_value.o -w -DPADDLE_WITH_CUSTOM_KERNEL -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2025-06-16 15:39:20,468] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include/third_party -I/root/miniforge3/envs/py3.11/include/python3.11 -I/root/miniforge3/envs/py3.11/include/python3.11 -c /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/gpu_ops/save_with_output_msg.cc -o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/save_with_output_msg.o -w -DPADDLE_WITH_CUSTOM_KERNEL -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2025-06-16 15:39:20,468] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include -I/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/include/third_party -I/root/miniforge3/envs/py3.11/include/python3.11 -I/root/miniforge3/envs/py3.11/include/python3.11 -c /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/gpu_ops/transfer_output.cc -o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/transfer_output.o -w -DPADDLE_WITH_CUSTOM_KERNEL -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/reset_need_stop_value.o is compiled\n/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output_msg_with_topk.o is compiled\n/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output.o is compiled\n/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/transfer_output.o is compiled\n/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/save_with_output_msg.o is compiled\n[2025-06-16 15:39:23,740] [    INFO] spawn.py:60 - g++ -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -fPIC -O2 -isystem /root/miniforge3/envs/py3.11/include -pthread -B /root/miniforge3/envs/py3.11/compiler_compat -shared /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output.o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output_msg_with_topk.o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/reset_need_stop_value.o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/save_with_output_msg.o /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/transfer_output.o -L/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/libs -L/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/base -Wl,-R/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/libs -Wl,-R/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/base -o build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/fastdeploy_base_ops.so -l:libpaddle.so\nRemoved: build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/transfer_output.o\nRemoved: build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/reset_need_stop_value.o\nRemoved: build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output_msg_with_topk.o\nRemoved: build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/get_output.o\nRemoved: build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/save_with_output_msg.o\n[2025-06-16 15:39:24,416] [    INFO] dir_util.py:58 - creating build/fastdeploy_base_ops/bdist.linux-x86_64/egg\n[2025-06-16 15:39:24,416] [    INFO] file_util.py:130 - copying build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/fastdeploy_base_ops.so -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg\n[2025-06-16 15:39:24,416] [    INFO] file_util.py:130 - copying build/fastdeploy_base_ops/lib.linux-x86_64-cpython-311/version.txt -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg\n[2025-06-16 15:39:24,417] [    INFO] bdist_egg.py:178 - creating stub loader for fastdeploy_base_ops.so\nReceived len(custom_op) = 7, using custom operator\n[2025-06-16 15:39:24,420] [    INFO] util.py:477 - byte-compiling build/fastdeploy_base_ops/bdist.linux-x86_64/egg/fastdeploy_base_ops.py to fastdeploy_base_ops.cpython-311.pyc\n[2025-06-16 15:39:24,422] [    INFO] dir_util.py:58 - creating build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] file_util.py:130 - copying fastdeploy_base_ops.egg-info/PKG-INFO -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] file_util.py:130 - copying fastdeploy_base_ops.egg-info/SOURCES.txt -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] file_util.py:130 - copying fastdeploy_base_ops.egg-info/dependency_links.txt -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] file_util.py:130 - copying fastdeploy_base_ops.egg-info/not-zip-safe -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] file_util.py:130 - copying fastdeploy_base_ops.egg-info/top_level.txt -> build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO\n[2025-06-16 15:39:24,422] [    INFO] bdist_egg.py:201 - writing build/fastdeploy_base_ops/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n[2025-06-16 15:39:24,423] [    INFO] dir_util.py:58 - creating dist\n[2025-06-16 15:39:24,423] [    INFO] bdist_egg.py:445 - creating 'dist/fastdeploy_base_ops-0.0.0-py3.11-linux-x86_64.egg' and adding 'build/fastdeploy_base_ops/bdist.linux-x86_64/egg' to it\n[2025-06-16 15:39:24,432] [    INFO] dir_util.py:227 - removing 'build/fastdeploy_base_ops/bdist.linux-x86_64/egg' (and everything under it)\n[2025-06-16 15:39:24,432] [    INFO] easy_install.py:734 - Processing fastdeploy_base_ops-0.0.0-py3.11-linux-x86_64.egg\n[2025-06-16 15:39:24,433] [    INFO] dir_util.py:58 - creating /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/tmp_base/fastdeploy_base_ops-0.0.0-py3.11-linux-x86_64.egg\n[2025-06-16 15:39:24,433] [    INFO] util.py:324 - Extracting fastdeploy_base_ops-0.0.0-py3.11-linux-x86_64.egg to /media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/tmp_base\nTraceback (most recent call last):\n  File \"/media/root/56824d7e-8be1-4ed5-8978-b9d53fb85e1d/yzx/FastDeploy-develop/custom_ops/setup_ops_base.py\", line 19, in <module>\n    setup(\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/utils/cpp_extension/cpp_extension.py\", line 247, in setup\n    setuptools.setup(**attr)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n    return run_commands(dist)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n    dist.run_commands()\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n    self.run_command(cmd)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n    super().run_command(command)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n    cmd_obj.run()\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/install.py\", line 97, in run\n    self.do_egg_install()\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/install.py\", line 158, in do_egg_install\n    cmd.run(show_deprecation=False)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/paddle/utils/cpp_extension/cpp_extension.py\", line 850, in run\n    super().run(*args, **kwargs)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 439, in run\n    self.easy_install(spec, not self.no_deps)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 689, in easy_install\n    return self.install_item(None, spec, tmpdir, deps, True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 737, in install_item\n    dists = self.install_eggs(spec, download, tmpdir)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 897, in install_eggs\n    return [install_dist(dist_filename, tmpdir)]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 981, in install_egg\n    self.execute(\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 337, in execute\n    util.execute(func, args, msg, dry_run=self.dry_run)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/_distutils/util.py\", line 326, in execute\n    func(*args)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/command/easy_install.py\", line 1308, in unpack_and_compile\n    unpack_archive(egg_path, destination, pf)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/archive_util.py\", line 57, in unpack_archive\n    driver(filename, extract_dir, progress_filter)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/archive_util.py\", line 105, in unpack_zipfile\n    _unpack_zipfile_obj(z, extract_dir, progress_filter)\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/site-packages/setuptools/archive_util.py\", line 130, in _unpack_zipfile_obj\n    data = zipfile_obj.read(info.filename)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/zipfile.py\", line 1528, in read\n    return fp.read()\n           ^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/zipfile.py\", line 952, in read\n    buf += self._read1(self.MAX_N)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniforge3/envs/py3.11/lib/python3.11/zipfile.py\", line 1042, in _read1\n    data = self._decompressor.decompress(data, n)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\n[FAIL] build wheel failed\n          please check your code\n##############################################################################\n```\n相关版本：\n安装的nightly build版本\nPython>=3.11\nCUDA>=12.6\nCUDNN>=9\nLinux X64\n安装paddlepaddle3.0没有这个问题",
        "state": "open",
        "user": "Jerryzxxxxx",
        "closed_by": null,
        "created_at": "2025-06-16T07:46:39+00:00",
        "updated_at": "2025-06-19T03:05:16+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "Jerryzxxxxx",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2626,
        "title": "官方有开发计划表么，何时发布2.0的C++库？",
        "body": "很高兴看到官网更新到2.0 。 最新需要升级到50系列显卡，并且要支持tensorRT 。\n1.请问，官网何时支持自行编译Fastdeploy 的C++库？\n   需要修改源码，支持cudnn_conv_algo_search = DEFAULT  [https://github.com/PaddlePaddle/FastDeploy/issues/2484](url)\n\n2.如果，短期不支持的话，是否只需要将“trt_backend.cc” 里面的接口适配 tesnorRT10即可（感谢大语言模型，让我能改得动）？\n\n\n如果只有2的工作量的话，我去改一版本发布到这里。如果工作量更多，请告知我，我考虑直接从onnxruntime开发支持推理的功能。",
        "state": "open",
        "user": "wujingxin521",
        "closed_by": null,
        "created_at": "2025-06-17T11:35:27+00:00",
        "updated_at": "2025-06-19T07:34:21+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "wujingxin521"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2646,
        "title": "按照官方步骤部署ERNIE-4.5-VL-28B-A3B-Paddle报错",
        "body": "安装包：\n\n![Image](https://github.com/user-attachments/assets/bf7d996c-9669-4760-985e-f741596524b0)\n\n![Image](https://github.com/user-attachments/assets/c0946e14-71aa-4e84-8f0e-9064feb39637)\n\n启动脚本：\n\n![Image](https://github.com/user-attachments/assets/29f06a32-2e2c-45ed-aa29-768cfbb1deee)\n\n报错信息：\n\n![Image](https://github.com/user-attachments/assets/f505b590-884a-42e9-806c-0dd519ea1b6c)\n\n目前ERNIE-4.5-VL-28B-A3B-Paddle是不是不支持quantization参数？",
        "state": "closed",
        "user": "yazheng0307",
        "closed_by": "yazheng0307",
        "created_at": "2025-06-30T07:37:07+00:00",
        "updated_at": "2025-07-01T03:47:47+00:00",
        "closed_at": "2025-07-01T03:47:47+00:00",
        "comments_count": [
            "Lmywl",
            "ltd0924",
            "yazheng0307",
            "yazheng0307",
            "yazheng0307",
            "qingyu24",
            "yazheng0307",
            "qingyu24",
            "yazheng0307"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2647,
        "title": "镜像能否往dockerhub推送一份",
        "body": "由于我的网络问题，拉取不到你们文档中镜像仓库的镜像（`ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/fastdeploy-cuda-12.6:2.0.0`），如有余力，希望能在dockerhub维护一份，感谢！",
        "state": "open",
        "user": "Redias",
        "closed_by": null,
        "created_at": "2025-06-30T08:14:41+00:00",
        "updated_at": "2025-07-01T02:46:42+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2649,
        "title": "How to deploy ERNIE-4.5-300B-A47B-Paddle on 16 GPUs",
        "body": "The [guide](https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-Paddle#using-fastdeploy) somehow only mention the method to deploy with 8 x 80G GPUs, however the bf16 version need 2 nodes of 8 x 80G GPUs.",
        "state": "open",
        "user": "prnake",
        "closed_by": null,
        "created_at": "2025-06-30T10:53:26+00:00",
        "updated_at": "2025-07-01T03:46:02+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "prnake",
            "Jiang-Jia-Jun",
            "qingqing01",
            "prnake"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2651,
        "title": "使用官网镜像+操作步骤启动报错",
        "body": "启动命令：python -m fastdeploy.entrypoints.openai.api_server --model baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle --tensor-parallel-size 8 --gpu-memory-utilization 0.9 --max-model-len 131072 --enable-chunked-prefill\n\n启动命令2：python -m fastdeploy.entrypoints.openai.api_server --model baidu/ERNIE-4.5-300B-A47B-FP8-Paddle        --port 8000 --engine-worker-queue-port 8181        --cache-queue-port 8182 --metrics-port 8182        --tensor-parallel-size 8        --max-model-len 32768        --max-num-seqs 32\n\n2个命令用了2个不同的模型，均无法启动；报错类似，都是加载layers失败；\n机器配置：1.5T内存，8*96G H20；\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/engine/../worker/worker_process.py\", line 772, in <module>\n    run_worker_proc()\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/engine/../worker/worker_process.py\", line 753, in run_worker_proc\n    worker_proc.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/engine/../worker/worker_process.py\", line 375, in load_model\n    self.worker.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/worker/gpu_worker.py\", line 163, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/worker/gpu_model_runner.py\", line 595, in load_model\n   self.model = get_model_from_loader(fd_config=self.fd_config)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/model_loader.py\", line 46, in get_model_from_loader\n    model = model_loader.load_model(fd_config)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/model_loader.py\", line 93, in load_model\n    model.set_state_dict(state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/decorator.py\", line 235, in fun\n    return caller(func, *(extras + args), **kw)\n  File \"/usr/local/lib/python3.10/dist-packages/paddle/base/dygraph/base.py\", line 396, in _decorate_function\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/models/ernie4_5_moe.py\", line 727, in set_state_dict\n   self.model.load_state_dict(state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/models/ernie4_5_moe.py\", line 666, in load_state_dict\n    self.hidden_layers[i].load_state_dict(state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/models/ernie4_5_moe.py\", line 583, in load_state_dict\n    self.self_attn.load_state_dict(state_dict)\nFile \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/models/ernie4_5_moe.py\", line 517, in load_state_dict\n    self.qkv_proj.load_state_dict(state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/layers/linear.py\", line 472, in load_state_dict\n    self.load_prequant_weight(state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/layers/linear.py\", line 124, in load_prequant_weight\n    self.quant_method.process_prequanted_weights(self, state_dict)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/model_executor/layers/quantization/weight_only.py\", line 193, in process_prequanted_weights\n    layer.linear_weight.set_value(quant_weight)\n  File \"/usr/local/lib/python3.10/dist-packages/paddleformers/utils/paddle_patch.py\", line 125, in enhance_set_value\n    return origin_set_value(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/decorator.py\", line 235, in fun\n    return caller(func, *(extras + args), **kw)\n  File \"/usr/local/lib/python3.10/dist-packages/paddle/base/wrapped_decorator.py\", line 40, in __impl__\n    return wrapped_func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/paddle/base/framework.py\", line 722, in __impl__\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/paddle/base/dygraph/tensor_patch_methods.py\", line 226, in set_value\n    assert self.shape == list(\nAssertionError: Variable Shape not match, Variable [ qkv_parallel_linear_0.w_1 ] need tensor with shape [1280, 8192] but load set tensor with shape [10240, 1024]",
        "state": "open",
        "user": "icewool",
        "closed_by": null,
        "created_at": "2025-06-30T11:44:50+00:00",
        "updated_at": "2025-07-01T07:58:20+00:00",
        "closed_at": null,
        "comments_count": [
            "chang-wenbin",
            "icewool",
            "chang-wenbin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2653,
        "title": "ERNIE-4.5-21B-A3B abnormal model behavior",
        "body": "As I ran some batch tests locally the model responded strangely. And the model could not output stably when **temperature=0**. I suspected that this was caused by a wrong kv-cache manager or other reasons.\n\nDevice: H100 80GB Driver Version: 535.154.05   CUDA Version: 12.6\n\nDeploy commands:\n```\npython -m fastdeploy.entrypoints.openai.api_server \\\n       --model /models/ERNIE-4.5-21B-A3B-Paddle \\\n       --port 8180 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --max-num-seqs 32\n```\n\nSome of the logs:\n```\n{\"id\": \"chatcmpl-6d725d2c-d673-4387-9daa-8c775d0745af\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"The IEEE Frank Rosenblatt Award in 2010 was given to **John J. Hopfield**. \\n\\nJohn J. Hopfield is a renowned physicist and computer scientist known for his foundational contributions to neural networks and associative memory models. His work laid the groundwork for many modern artificial intelligence applications, and the award recognizes his significant impact on the field.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285094, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 78, \"prompt_tokens\": 23, \"total_tokens\": 101, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-b45423e3ed3fa4c7270ee190cefe3b5f\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Science and technology', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/IEEE_Frank_Rosenblatt_Award', 'https://ieeexplore.ieee.org/author/37271220500', 'https://en.wikipedia.org/wiki/IEEE_Frank_Rosenblatt_Award', 'https://www.nxtbook.com/nxtbooks/ieee/awards_2010/index.php?startid=21#/p/20']}\", \"problem\": \"Who received the IEEE Frank Rosenblatt Award in 2010?\", \"answer\": \"Michio Sugeno\"}, \"index\": 0}\n{\"id\": \"chatcmpl-a32cf0f1-b2d4-4939-8529-a0afe988bee1\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"In 2018, the Oceanography Society's Jerlov Award was given to **Michael Behrenfeld**. He is recognized for his significant contributions to the field of marine phytoplankton ecology, particularly in advancing our understanding of how these microscopic organisms respond to environmental changes. The Jerlov Award is one of the society's highest honors, given biennially to a scientist for outstanding achievements in oceanography.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285094, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 89, \"prompt_tokens\": 26, \"total_tokens\": 115, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-6978aecf9035b54a381f55f0d8eac16f\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Science and technology', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/The_Oceanography_Society', 'https://en.wikipedia.org/wiki/The_Oceanography_Society', 'https://tos.org/jerlov-medal', 'https://www.eurekalert.org/news-releases/490504']}\", \"problem\": \"Who was awarded the Oceanography Society's Jerlov Award in 2018?\", \"answer\": \"Annick Bricaud\"}, \"index\": 1}\n{\"id\": \"chatcmpl-7de592c4-9c3b-4551-baaa-eae4b8aa4801\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"The Leipzig 1877 tournament was organized **in honor of Wilhelm Steinitz**, the first official World Chess Champion. Steinitz, a pioneering chess theorist, had just secured his title by defeating Johannes Zukertort in a match held in London in 1886. The Leipzig tournament (1877) was a prestigious event that showcased the latest developments in chess strategy, with Steinitz himself participating. It aimed to recognize his contributions to the game and solidify his legacy as the founding figure of modern chess.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285094, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 117, \"prompt_tokens\": 21, \"total_tokens\": 138, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-47609191c7530a41c7975c93b21ca70a\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Sports', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/Adolf_Anderssen', 'https://www.chessgames.com/perl/chess.pl?tid=79429', 'https://en.wikipedia.org/wiki/Adolf_Anderssen']}\", \"problem\": \"In whose honor was the Leipzig 1877 tournament organized?\", \"answer\": \"Adolf Anderssen\"}, \"index\": 3}\n{\"id\": \"chatcmpl-d78b2098-7641-4e41-85b1-b9323768a09c\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"ṛ\\nKarl Küchler, who was an Austrian official and art advisor to Empress Elizabeth of Austria, stated that the favorite sculpture at the Achilleion villa in Corfu depicted **Achilles**. This sculpture, created by the Italian sculptor Giuseppe Moretti, portrays Achilles standing triumphantly, often holding a spear or shield, symbolizing his heroism and victory. The Achilleion was built as a retreat for Empress Elizabeth in the late 19th century, and she was particularly fond of this sculpture, which aligned with her interest in Greek mythology and her desire to project an image of power and elegance.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285094, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 133, \"prompt_tokens\": 40, \"total_tokens\": 173, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-27c6780da4842af7ac370fdb71504b67\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Art', 'answer_type': 'Person', 'urls': ['https://www.gutenberg.org/files/60408/60408-h/60408-h.htm\\\\nhttps://en.wikipedia.org/wiki/Achilleion_(Corfu)', 'https://www.gutenberg.org/cache/epub/60408/pg60408-images.html', 'https://archive.org/stream/elizabethempres01burggoog/elizabethempres01burggoog_djvu.txt', 'https://www.habsburger.net/en/chapter/achilleion-corfu-elisabeths-flight-antiquity']}\", \"problem\": \"According to Karl Küchler, what did Empress Elizabeth of Austria's favorite sculpture depict, which was made for her villa Achilleion at Corfu?\", \"answer\": \"Poet Henrich Heine.\"}, \"index\": 4}\n{\"id\": \"chatcmpl-76e7d1ae-92fd-4223-a294-c2f3963a2cf2\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"_________________ euros\\nThe surgeon was ordered to pay **120,000 euros** to Stella Obasanjo's son, Christopher Obasanjo, following his involvement in her death. This judgment was related to the botched liposuction procedure in 2002, which led to her fatal internal injuries.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285095, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 70, \"prompt_tokens\": 34, \"total_tokens\": 104, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-f07fa763a90a90749f5f5164f347a4e7\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Number', 'urls': ['https://en.wikipedia.org/wiki/Stella_Obasanjo#Death', 'https://en.wikipedia.org/wiki/Stella_Obasanjo', 'https://www.independent.co.uk/news/world/africa/surgeon-jailed-over-death-of-first-lady-1791712.html)', 'https://www.abc.net.au/news/2009-09-22/doctor-jailed-over-former-first-ladys-lipo-death/1437416)']}\", \"problem\": \"How much money, in euros, was the surgeon held responsible for Stella Obasanjo's death ordered to pay her son?\", \"answer\": \"120,000\"}, \"index\": 5}\n{\"id\": \"chatcmpl-e38f35d3-5c6f-4dd9-8cb3-5a3f3274ab2a\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"You can find this information by searching for the interview or statement of Barack Obama in Christianity Today. Barack Obama made this statement in a 2008 interview with Christianity Today magazine. In that interview, he emphasized his Christian faith and its central aspects such as the redemptive death and resurrection of Jesus Christ.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285095, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 65, \"prompt_tokens\": 48, \"total_tokens\": 113, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-4461d682b5dc913bd46e2f3334c6a51a\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Date', 'urls': ['https://en.wikipedia.org/wiki/Barack_Obama', 'https://will-lover-32-wikia.fandom.com/wiki/Barack_obama', 'https://people.wikimedia.org/~ori/mod_pagespeed_tests/obama-modpagespeed.html', 'https://www.dreame.com/story/2723094784-beyond-the-crust/0196694272-a-new-passenger.html']}\", \"problem\": \"What were the month and year when Obama told Christianity Today, \\\"I am a Christian, and I am a devout Christian. I believe in the redemptive death and resurrection of Jesus Christ\\\"?\", \"answer\": \"January 2008\"}, \"index\": 6}\n{\"id\": \"chatcmpl-bd0e664d-f0cf-42bd-a36a-c794fbb652d1\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"文才大学，位于马萨诸塞州剑桥市的女子文理学院？\\n\\n其实，位于马萨诸塞州剑桥市的女子文理学院是**Radcliffe College**（拉德克利夫学院）。不过，需要注意的是，Radcliffe College 在 1999 年已正式并入哈佛大学（Harvard University），成为哈佛学院（Harvard College）的一部分，现在不再作为独立的学院存在。但它在历史上是著名的女子文理学院，与哈佛大学紧密相关。\\n\\n如果你是在寻找一个仍然作为独立女子文理学院存在的机构，在马萨诸塞州剑桥市可能没有完全独立的女子文理学院，但附近有一些其他著名的女子学院，如 Wellesley College（韦尔斯利学院），它位于马萨诸塞州的韦尔斯利。\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285094, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 172, \"prompt_tokens\": 25, \"total_tokens\": 197, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-7f9860c99995fe3d945d2767641cae4d\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Geography', 'answer_type': 'Place', 'urls': ['https://en.wikipedia.org/wiki/Radcliffe_College', 'https://en.wikipedia.org/wiki/Radcliffe_College', 'https://www.braingainmag.com/7-historic-liberal-arts-colleges-in-the-us.htm', 'https://thepeoplesarchive.dclibrary.org/repositories/2/resources/2228']}\", \"problem\": \"What's the name of the women's liberal arts college in Cambridge, Massachusetts?\", \"answer\": \"Radcliffe College\"}, \"index\": 2}\n{\"id\": \"chatcmpl-5ab5c22d-ef43-4889-b9b8-472b8c6b434b\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"Chief Justice Mirza Hameedullah Beg was appointed by **President Fakhruddin Ali Ahmed** in 1977. \\n\\nHe served as the Chief Justice of India from January 29, 1977, to February 21, 1978. His appointment came after the previous Chief Justice, A.N. Ray, was superseded by three senior judges in 1973, which led to significant controversy.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285095, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 100, \"prompt_tokens\": 30, \"total_tokens\": 130, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-e3ea29d055182e67257094cb58d2fdc6\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/Mirza_Hameedullah_Beg', 'https://en.wikipedia.org/wiki/Mirza_Hameedullah_Beg', 'https://www.tutorialspoint.com/mirza-hameedullah-beg-former-chief-justice-of-india', 'https://en.wikipedia.org/wiki/List_of_chief_justices_of_India']}\", \"problem\": \"Who appointed the Chief Justice of India, Mirza Hameedullah Beg, in 1977?\", \"answer\": \"Fakhruddin Ali Ahmed\"}, \"index\": 7}\n{\"id\": \"chatcmpl-177c8ada-5bfa-417e-b926-5ad4b4dfc279\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"The former Prime Minister of Iceland who worked as a cabin crew member until 1971 is **Steingrím J. Steinþórsson**. \\n\\nSteingrím J. Steinþórsson served as Prime Minister of Iceland from 1970 to 1971. Before his political career, he worked as a flight attendant with Icelandic Airlines (now Icelandair) until 1971. He later became a prominent politician, representing the Independence Party.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285095, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 105, \"prompt_tokens\": 32, \"total_tokens\": 137, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-aa697484b4440d78f35b0583b170d7cf\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/J%C3%B3hanna_Sigur%C3%B0ard%C3%B3ttir', 'https://en.wikipedia.org/wiki/J%C3%B3hanna_Sigur%C3%B0ard%C3%B3ttir', 'https://www.britannica.com/biography/Johanna-Sigurdardottir', 'https://kids.kiddle.co/J%C3%B3hanna_Sigur%C3%B0ard%C3%B3ttir']}\", \"problem\": \"What is the name of the former Prime Minister of Iceland who worked as a cabin crew member until 1971?\", \"answer\": \"Jóhanna Sigurðardóttir\"}, \"index\": 8}\n{\"id\": \"chatcmpl-950e1658-373f-493e-9180-f514e0211fcb\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"2016\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285097, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 5, \"prompt_tokens\": 37, \"total_tokens\": 42, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-67780f5d4a98bde90689d606fc4cad8b\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Art', 'answer_type': 'Date', 'urls': ['https://www.australianphotography.com/news/monash-gallery-of-art-to-rebrand-as-museum-of-australian-photography', 'https://maph.org.au/about/#:~:text=In%20March%202023%2C%20MGA%20rebranded,how%20you%20can%20be%20involved.', 'https://www.australianphotography.com/news/monash-gallery-of-art-to-rebrand-as-museum-of-australian-photography', 'https://www.monash.vic.gov.au/About-Us/News/Monash-Gallery-of-Art-rebrands-as-MAPh-Museum-of-Australian-Photography']}\", \"problem\": \"In which year did Melbourne's Monash Gallery of Art (MGA) rebrand and become the Museum of Australian Photography (MAPh)?\", \"answer\": \"2023\"}, \"index\": 13}\n{\"id\": \"chatcmpl-32d2b33b-360a-4e88-8f84-cbb35cf2ec9b\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"In the **2010 Champions League final** between **Bayern Munich and Inter Milan**, Inter Milan committed **8 fouls** in the match. Bayern Munich, on the other hand, committed **10 fouls**. \\n\\nThis data reflects the physical nature of the encounter, which was known for its intensity and refereeing challenges. The match ended with Inter Milan winning 2-0 after extra time, with goals from **Diego Milito** and **Wesley Sneijder**.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285096, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 105, \"prompt_tokens\": 36, \"total_tokens\": 141, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-523268ccafa7ac80fe5c97eb5720c9ac\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Sports', 'answer_type': 'Number', 'urls': ['https://www.uefa.com/uefachampionsleague/match/2000488--bayern-vs-inter/', 'https://en.wikipedia.org/wiki/2010_UEFA_Champions_League_final', 'https://www.uefa.com/uefachampionsleague/match/2000488--bayern-vs-inter/', 'https://uk.soccerway.com/matches/2010/05/22/europe/uefa-champions-league/fc-bayern-munchen/fc-internazionale-milano/932705/']}\", \"problem\": \"How many fouls did Inter commit in the Champions League final match between Bayern and Inter on May 23, 2010?\", \"answer\": \"13\"}, \"index\": 10}\n{\"id\": \"chatcmpl-555646ee-c8b9-4d64-91ec-3506ed3dfc8f\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"In the 2019 Lok Sabha elections, **Mehbooba Mufti** contested from the **Anantnag constituency** in Jammu and Kashmir and lost to **Hasnain Masoodi** of the National Conference (NC). \\n\\nThis was a significant election for her as she had previously served as the Chief Minister of Jammu and Kashmir until 2018. However, the Anantnag seat, which is considered a sensitive and volatile region, was won by Masoodi, who ran as the NC candidate.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285096, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 116, \"prompt_tokens\": 32, \"total_tokens\": 148, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-7e449755669de42a5e89f1f53832ff50\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/Mehbooba_Mufti#References', 'https://www.indiatoday.in/elections/lok-sabha-2019/story/j-k-lok-sabha-results-2019-pdp-chief-mehbooba-mufti-loses-anantnag-seat-to-nc-hasnain-masoodi-1533245-2019-05-23', 'https://en.wikipedia.org/wiki/Mehbooba_Mufti#Political_career', 'https://timesofindia.indiatimes.com/elections/lok-sabha-constituencies/jammu-kashmir/anantnag']}\", \"problem\": \"To whom did Mehbooba Mufti Sayed contest the 2019 Lok Sabha elections and lose?\", \"answer\": \"Hasnain Masoodi\"}, \"index\": 9}\n{\"id\": \"chatcmpl-49999d2e-a5bd-42cb-b35a-31e0ca55f4be\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \" embers of the Lego community often refer to parts by their \\\"ID\\\" or \\\"inventory number.\\\" The part with ID gal56 is known as the \\\"Minifigure, Head\\\" with a \\\"Cape Cloth\\\" pattern, specifically the one with the yellow cape with red trim.\\n\\nThis part was introduced as part of the \\\"Pirates of the Caribbean\\\" theme in 2006. Therefore, the Lego part with ID gal56 first released in **2006**.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285096, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 107, \"prompt_tokens\": 22, \"total_tokens\": 129, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-3e6421760c1c75ea1f413980dfeb0f0a\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Other', 'answer_type': 'Date', 'urls': ['https://www.bricklink.com/v2/catalog/catalogitem.page?P=gal56#T=C&C=17', 'https://www.brickowl.com/catalog/lego-galidor-staff']}\", \"problem\": \"What year did the Lego part with ID gal56 first release?\", \"answer\": \"2002\"}, \"index\": 11}\n{\"id\": \"chatcmpl-1beab381-aa41-47ae-95da-110d3fef45cb\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"1998. Koichi Mizushima was awarded the Kato Memorial Prize in 1998 for his significant contributions to the field of materials science, particularly in the discovery and understanding of lithium cobalt oxide ($LiCoO_2$), which is an important material for lithium-ion batteries.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285096, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 64, \"prompt_tokens\": 26, \"total_tokens\": 90, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-1220868d62c3eda3904c0ee76ff9d4c0\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Science and technology', 'answer_type': 'Date', 'urls': ['https://en.wikipedia.org/wiki/Koichi_Mizushima_(scientist)', 'https://www.amprox.com/oxide/koichi-mizushima-scientist/', 'https://en.wikipedia.org/wiki/Koichi_Mizushima_(scientist)']}\", \"problem\": \"In which year did the Japanese scientist Koichi Mizushima receive the Kato Memorial Prize?\", \"answer\": \"1999\"}, \"index\": 12}\n{\"id\": \"chatcmpl-df3144f3-717e-43f8-ab7a-d7b930a4f115\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"The Federal Aviation Administration (FAA) was directed by **the U.S. Coast Guard** to establish a temporary flight restriction zone (TFR) over the Deepwater Horizon operations area. This action was taken to ensure the safety of aerial operations and protect response efforts during the massive oil spill response and cleanup. The TFR was crucial in preventing unauthorized flights that could interfere with the spill response operations.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285097, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 82, \"prompt_tokens\": 48, \"total_tokens\": 130, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-24d55ec2d11f8b8ee163a4795f438dec\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Other', 'urls': ['https://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill', 'https://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill#:~:text=During%20the%20spill%20response%20operations,zone%20over%20the%20operations%20area.', 'https://www.coursehero.com/file/p5j9pch4/169-On-18-May-2010-BP-was-designated-the-lead-Responsible-Party-under-the-Oil/', 'https://www.ensynox.com/the-true-story-of-deepwater-horizon']}\", \"problem\": \"Who requested the Federal Aviation Administration (FAA) implement a 900 sq mi (2,300 km2) temporary flight restriction zone over the operations areas of the Deepwater Horizon?\", \"answer\": \"The Coast Guard\"}, \"index\": 14}\n{\"id\": \"chatcmpl-0ce6af23-16e4-4071-b767-2840f5edb0dc\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"In the rugby match between Spain and Romania that was part of the 2022 Rugby Europe Championship on February 27, 2022, **Mario Martínez** scored all the conversions for Spain. He played a crucial role in the team's performance, kicking all four successful conversions during the match.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285097, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 67, \"prompt_tokens\": 48, \"total_tokens\": 115, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-d571195f487f3d7ab33bf90e2b8004c1\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Sports', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/2022_Rugby_Europe_Championship#Week_3', 'https://all.rugby/match/16767/rugby-europe-championship-2022/spain-romania', 'https://en.wikipedia.org/wiki/2022_Rugby_Europe_Championship']}\", \"problem\": \"What player scored all the conversions for Spain in the rugby match between Spain and Romania that was part of the 2022 Rugby Europe Championship on February 27, 2022?\", \"answer\": \"Manuel Ordas\"}, \"index\": 16}\n{\"id\": \"chatcmpl-0fb9c13a-0a63-4f3f-bffa-e7cd697492a9\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"In **Season 1, Episode 20 (\\\"The Woman in Me\\\")** of *Ally McBeal*, the psychiatrist who prescribes medication for Marie Hanson due to her periodic blackouts is **Dr. Charles Carmichael**. \\n\\nDr. Carmichael is a recurring character in the series, often consulted by Ally and her colleagues for cases involving psychological issues. His expertise in psychiatry is central to Marie's diagnosis and treatment in this episode.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285097, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 99, \"prompt_tokens\": 44, \"total_tokens\": 143, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-d79ed473b4d2cdb5baa6f2decaea9a3f\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'TV shows', 'answer_type': 'Person', 'urls': ['https://allymcbeal.fandom.com/wiki/The_Inmates', 'https://allymcbeal.fandom.com/wiki/The_Inmates#:~:text=Hanson.,Peters%2C%20had%20prescribed%20her%20medication.', 'https://www.imdb.com/title/tt0510352/']}\", \"problem\": \"What is the surname of the psychiatrist who prescribes medication for Marie Hanson for her periodic blackouts in Season 1, Episode 20 of Ally McBeal?\", \"answer\": \"Peters\"}, \"index\": 17}\n{\"id\": \"chatcmpl-7cb3eccf-1fd6-46a8-abaa-8934d972f9f7\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"From 1969 to 1970, **John Gilbert Layton** served as the **director of the Quebec Youth Federation for Socialist Action**. This position was part of his early political career and involvement in Quebec's socialist and progressive movements. Layton later gained national prominence as a leader in Canadian politics, particularly with the New Democratic Party (NDP).\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285098, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 78, \"prompt_tokens\": 31, \"total_tokens\": 109, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-9483888db99224ef991fa1c2a29d4d8d\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Politics', 'answer_type': 'Other', 'urls': ['https://en.wikipedia.org/wiki/Jack_Layton', 'https://en.wikipedia.org/wiki/Jack_Layton#:~:text=In%201969%2C%20he%20was%20appointed,of%20the%20Sigma%20Chi%20fraternity.', 'https://www.laytonlegacy.ca/jack', 'https://www.cbc.ca/news/canada/jack-layton-a-timeline-of-his-accomplishments-1.1118520']}\", \"problem\": \"What position was John Gilbert Layton appointed to in Quebec from 1969 until 1970?\", \"answer\": \" Quebec Youth Parliament prime minister\"}, \"index\": 19}\n{\"id\": \"chatcmpl-837ef944-80bc-4575-a13b-2416fa2606ac\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"The Gerard P. Kuiper Prize for outstanding contributions to the field of planetary science was awarded to **Peter Goldreich** in 2001. Goldreich is renowned for his significant work in planetary dynamics, particularly his research on the gravitational interactions and rotational states of celestial bodies.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285098, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 61, \"prompt_tokens\": 24, \"total_tokens\": 85, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-28a69cc066c79a2c94a219137e93fc25\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Science and technology', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/Gerard_P._Kuiper_Prize', 'https://dps.aas.org/prizes/2001/', 'https://pubs.aip.org/physicstoday/article/54/12/68/411566/AAS-Division-Awards-Announced', 'https://www.geology.pitt.edu/sites/default/files/Newsletter/Alumni%20Newsletter%202000-2001.pdf']}\", \"problem\": \"Who won the Gerard P. Kuiper Prize in 2001?\", \"answer\": \"Bruce W. Hapke\"}, \"index\": 20}\n{\"id\": \"chatcmpl-9ffeccc7-19f8-40fb-8ef4-7cdc0c7aacd8\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"message\": {\"content\": \"**Kiawah Koala (Ina)** won the \\\"Best VTuber Streamer\\\" award at The Streamer Awards in 2022. Known for her vibrant personality and creative content, Ina's unique blend of gaming, singing, and interactivity made her a standout in the VTuber category.\", \"role\": \"assistant\", \"tool_calls\": null, \"reasoning_content\": null}}], \"created\": 1751285099, \"model\": \"ERNIE-4.5-21B-A3B-Paddle\", \"object\": \"chat.completion\", \"usage\": {\"completion_tokens\": 68, \"prompt_tokens\": 32, \"total_tokens\": 100, \"prompt_tokens_details\": {\"cached_tokens\": 0}}, \"model_spec\": {\"api_url\": \"https://ai.local/v1\", \"model_id\": \"ERNIE-4.5-21B-A3B-Paddle\", \"api_key\": \"\"}, \"answer_id\": \"answer-19eab5985739b294a5c49fcfcc5e328b\", \"subset_name\": \"default\", \"raw_input\": {\"metadata\": \"{'topic': 'Other', 'answer_type': 'Person', 'urls': ['https://en.wikipedia.org/wiki/CodeMiko\\\\nhttps://thestreamerawards.com/winners', 'https://thestreamerawards.com/winners', 'https://dotesports.com/streaming/news/all-2022-streamer-award-winners', 'https://www.invenglobal.com/articles/16733/all-the-award-winners-at-the-streamer-awards-2022']}\", \"problem\": \"Which streamer won the \\\"Best VTuber Streamer\\\" award at The Streamer Awards in 2022?\", \"answer\": \"CodeMiko\"}, \"index\": 21}\n```",
        "state": "closed",
        "user": "prnake",
        "closed_by": "prnake",
        "created_at": "2025-06-30T13:22:34+00:00",
        "updated_at": "2025-07-01T03:46:44+00:00",
        "closed_at": "2025-07-01T03:46:44+00:00",
        "comments_count": [
            "gzy19990617"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2655,
        "title": "启动失败",
        "body": "PaddlePaddle/ERNIE-4.5-VL-28B-A3B-PT\n\n 从modelscope下载\n\n安装按官方指导进行\npython3.12\nfastdeploy-gpu           2.0.0\npaddlepaddle-gpu   3.1.0\n\n<img width=\"1410\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3d30cdb3-c399-423a-a9ea-97720b0672a6\" />",
        "state": "open",
        "user": "cole-dda",
        "closed_by": null,
        "created_at": "2025-06-30T15:07:52+00:00",
        "updated_at": "2025-07-01T07:58:04+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2656,
        "title": "Support for CUDA 12.8 / Blackwell SM120",
        "body": "Hi !\nIs there any support to Support for CUDA 12.8 / Blackwell SM120 ?\n\nThanks in advance !",
        "state": "closed",
        "user": "celsowm",
        "closed_by": "ming1753",
        "created_at": "2025-06-30T21:52:52+00:00",
        "updated_at": "2025-07-08T07:21:29+00:00",
        "closed_at": "2025-07-02T07:56:58+00:00",
        "comments_count": [
            "ming1753",
            "fernandaspets"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2658,
        "title": "fastdeploy-2.0.0a0 版本仅兼容 Paddle-3.1 么？",
        "body": "启动环境：天数 B I150 * 8\n软件环境：paddlepaddle-3.0.0+corex.4.3.0.20250616\n执行脚本参考：https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/get_started/installation/iluvatar_gpu.md\n启动报错：\nroot@TG150-worker18:/workspace/workspace# ./run_demo.sh \nDid not find any active Ray processes.\n`--address` is a required flag unless starting a head node with `--head`.\nTraceback (most recent call last):\n  File \"/workspace/workspace/run_demo.py\", line 1, in <module>\n    from fastdeploy import LLM, SamplingParams\n  File \"/usr/local/lib/python3.12/site-packages/fastdeploy/__init__.py\", line 20, in <module>\n    from fastdeploy.entrypoints.llm import LLM\n  File \"/usr/local/lib/python3.12/site-packages/fastdeploy/entrypoints/llm.py\", line 27, in <module>\n    from fastdeploy.engine.engine import LLMEngine\n  File \"/usr/local/lib/python3.12/site-packages/fastdeploy/engine/engine.py\", line 40, in <module>\n    from fastdeploy.output.token_processor import (TokenProcessor,\n  File \"/usr/local/lib/python3.12/site-packages/fastdeploy/output/token_processor.py\", line 22, in <module>\n    from paddlenlp.utils.env import MAX_BSZ, MAX_DRAFT_TOKENS, SPECULATE_MAX_BSZ\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/__init__.py\", line 46, in <module>\n    from . import (\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/data/__init__.py\", line 18, in <module>\n    from .data_collator import *\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/data/data_collator.py\", line 26, in <module>\n    from ..transformers import BertTokenizer\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/transformers/__init__.py\", line 16, in <module>\n    from .configuration_utils import PretrainedConfig\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/transformers/configuration_utils.py\", line 38, in <module>\n    from ..utils import CONFIG_NAME, LEGACY_CONFIG_NAME\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/utils/__init__.py\", line 32, in <module>\n    from .paddle_patch import *\n  File \"/usr/local/lib/python3.12/site-packages/paddlenlp/utils/paddle_patch.py\", line 42, in <module>\n    paddle.float8_e5m2: (paddle.int8, np.float8_e5m2),\n    ^^^^^^^^^^^^^^^^^^\nAttributeError: module 'paddle' has no attribute 'float8_e5m2'\n/usr/local/lib/python3.12/tempfile.py:936: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp3tjmmw3t'>\n  _warnings.warn(warn_message, ResourceWarning)\n/usr/local/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1017816) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  self.pid = os.fork()",
        "state": "closed",
        "user": "YoctoHan",
        "closed_by": "YoctoHan",
        "created_at": "2025-07-01T06:10:41+00:00",
        "updated_at": "2025-07-01T08:13:11+00:00",
        "closed_at": "2025-07-01T08:13:11+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "YoctoHan"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2667,
        "title": "官网docker镜像作离线推理加载模型到94%时失败，可能跟libnvidia-ml相关",
        "body": "官网镜像（ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/fastdeploy-cuda-12.6:2.0.0），使用offline-inference.md中的Sample Code，尝试用0.3B-Paddle和0.3B-Base-Paddle进行离线推理，Loading Layers 94%时出错并状态冻结，可Ctrl-C退出。\n![Image](https://github.com/user-attachments/assets/76649d94-903b-4517-af00-16c966934c7b)\n硬件环境：KH-40000/32x2 + NVidia H800\n附件：log/workerlog.0，看该文件感觉问题跟镜像内libnvidia-ml的配置相关。\n<!-- Failed to upload \"load_layer94_fail_workerlog.0.log\" -->",
        "state": "closed",
        "user": "Wang-Xiaolong",
        "closed_by": "Jiang-Jia-Jun",
        "created_at": "2025-07-01T08:55:44+00:00",
        "updated_at": "2025-07-01T11:23:35+00:00",
        "closed_at": "2025-07-01T11:23:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2660,
        "title": "P800 docker run报错",
        "body": "计划参考https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/get_started/installation/kunlunxin_xpu.md，在p800上部署ERNIE 4.5；\n执行\"docker run --name fastdeploy-xpu --net=host -itd --privileged -v $PWD:/Work -w /Work     ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/fastdeploy-xpu:2.0.0 /bin/bash\"报错：\n\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'\nnvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.\n\n环境信息:\nDriver Version                            : 5.0.21.16\nXPU-RT Version                            : 10.2\n    Firmware Version\n        PBL Version                       : 1.0\n        PCIE Version                      : 2.6\n        SBL Version                       : 1.39\n        ALL Version                       : 1.0.2.6.1.39\n        CPLD Version                      : 1.6",
        "state": "closed",
        "user": "wrennywang",
        "closed_by": "wrennywang",
        "created_at": "2025-07-01T06:54:40+00:00",
        "updated_at": "2025-07-01T09:09:26+00:00",
        "closed_at": "2025-07-01T09:07:53+00:00",
        "comments_count": [
            "hong19860320",
            "wrennywang",
            "wrennywang"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2663,
        "title": "ERNIE-4.5-VL-28B-A3B-Paddle的int4量化加载，4090单卡成功，双卡失败",
        "body": "模型使用ERNIE-4.5-VL-28B-A3B-Paddle，量化参数设置为wint4，使用单机2卡的4090显卡。\n\n启动脚本：\n\n![Image](https://github.com/user-attachments/assets/355036c3-fbda-49af-b6b4-f383e4d09d5b)\n\n问题：Loading Layers完成后，一直卡住不动，同时GPU的显存占用退回0：\n\n![Image](https://github.com/user-attachments/assets/4dc0659e-8e90-4adf-b9f3-8f5abb93b62d)\n\n补充：单卡的4090是可以正常启动运行、调用，显存占用20GB！",
        "state": "open",
        "user": "yazheng0307",
        "closed_by": null,
        "created_at": "2025-07-01T07:20:58+00:00",
        "updated_at": "2025-07-08T05:51:29+00:00",
        "closed_at": null,
        "comments_count": [
            "QuentinJGMace",
            "SongDI911",
            "QuentinJGMace",
            "SongDI911",
            "chatdl",
            "wangjiezju1988"
        ],
        "labels": [
            "ERNIE-45-VL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2668,
        "title": "官网docker镜像作离线推理加载模型到94%时失败，可能跟libnvidia-ml相关",
        "body": "官网镜像（ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/fastdeploy-cuda-12.6:2.0.0），使用offline-inference.md中的Sample Code，尝试用0.3B-Paddle和0.3B-Base-Paddle进行离线推理，Loading Layers 94%时出错并状态冻结，可Ctrl-C退出。\n![Image](https://github.com/user-attachments/assets/76649d94-903b-4517-af00-16c966934c7b)\n硬件环境：KH-40000/32x2 + NVidia H800\n附件：log/workerlog.0，看该文件感觉问题跟镜像内libnvidia-ml的配置相关。\n\n[load_layer94_fail_workerlog.0.log](https://github.com/user-attachments/files/20995403/load_layer94_fail_workerlog.0.log)",
        "state": "open",
        "user": "Wang-Xiaolong",
        "closed_by": null,
        "created_at": "2025-07-01T09:07:12+00:00",
        "updated_at": "2025-07-01T11:09:45+00:00",
        "closed_at": null,
        "comments_count": [
            "Wang-Xiaolong",
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2676,
        "title": "一键编译FastDeploy时报错",
        "body": "报错信息如下：\nTraceback (most recent call last):\n  File \"/home/wyx/FastDeploy/custom_ops/setup_ops_base.py\", line 18, in <module>\n    setup(\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/paddle/utils/cpp_extension/cpp_extension.py\", line 247, in setup\n    setuptools.setup(**attr)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n    return run_commands(dist)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n    dist.run_commands()\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n    self.run_command(cmd)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/dist.py\", line 1104, in run_command\n    super().run_command(command)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n    cmd_obj.run()\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/install.py\", line 109, in run\n    self.do_egg_install()\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/install.py\", line 176, in do_egg_install\n    cmd.run(show_deprecation=False)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/paddle/utils/cpp_extension/cpp_extension.py\", line 850, in run\n    super().run(*args, **kwargs)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 437, in run\n    self.easy_install(spec, not self.no_deps)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 686, in easy_install\n    return self.install_item(None, spec, tmpdir, deps, True)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 736, in install_item\n    dists = self.install_eggs(spec, download, tmpdir)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 896, in install_eggs\n    return [install_dist(dist_filename, tmpdir)]\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 979, in install_egg\n    self.execute(\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 384, in execute\n    util.execute(func, args, msg, dry_run=self.dry_run)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/_distutils/util.py\", line 334, in execute\n    func(*args)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/command/easy_install.py\", line 1308, in unpack_and_compile\n    unpack_archive(egg_path, destination, pf)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/archive_util.py\", line 59, in unpack_archive\n    driver(filename, extract_dir, progress_filter)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/archive_util.py\", line 107, in unpack_zipfile\n    _unpack_zipfile_obj(z, extract_dir, progress_filter)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/site-packages/setuptools/archive_util.py\", line 132, in _unpack_zipfile_obj\n    data = zipfile_obj.read(info.filename)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/zipfile.py\", line 1487, in read\n    return fp.read()\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/zipfile.py\", line 916, in read\n    buf += self._read1(self.MAX_N)\n  File \"/root/anaconda3/envs/wyx/lib/python3.10/zipfile.py\", line 1006, in _read1\n    data = self._decompressor.decompress(data, n)\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\n[FAIL] build wheel failed\n          please check your code\nWARNING: Skipping fastdeploy as it is not installed.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.",
        "state": "open",
        "user": "WYX868",
        "closed_by": null,
        "created_at": "2025-07-02T06:56:28+00:00",
        "updated_at": "2025-07-03T02:04:13+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2672,
        "title": "how to get logprobs when deploy a openai format server",
        "body": "If I need the logprobs value, how can I get that?\nexample \n```python \nimport requests\ndata = {\n                \"model\": \"ernie-4-5-A21B-3B\",\n                \"prompt\": \"hello,\",\n                \"temperature\": 0,\n                \"max_tokens\": 3,\n                \"logprobs\": 1,\n                \"seed\": 1024,\n                \"echo\": True,\n}\nresp = requests.post(url=url, json=data)\n```\n\nbut I get logprobs is null in response.\n",
        "state": "open",
        "user": "linboyang",
        "closed_by": null,
        "created_at": "2025-07-01T13:12:17+00:00",
        "updated_at": "2025-07-02T05:56:46+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2678,
        "title": "ERNIE-4.5-300B-A47B-2Bits-Paddle 双卡部署报错",
        "body": "命令：\n CUDA_VISIBLE_DEVICES=0,1 python -m fastdeploy.entrypoints.openai.api_server --model \"baidu/ERNIE-4.5-300B-A47B-2Bits-Paddle\" --port 9180 --metrics-port 9181 --engine-worker-queue-port 9182 --tensor-parallel-size 2 --max-model-len  32768 --max-num-seqs 128\n报错：\nAssertionError: Variable Shape not match, Variable [ qkv_parallel_linear_0.w_1 ] need tensor with shape [5120, 8192] but load set tensor with shape [10240, 4096]",
        "state": "open",
        "user": "squirrelfish",
        "closed_by": null,
        "created_at": "2025-07-02T07:32:42+00:00",
        "updated_at": "2025-07-02T07:58:50+00:00",
        "closed_at": null,
        "comments_count": [
            "chang-wenbin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2681,
        "title": "FD运行PP-Vehicle模型推理结果与PaddleDetection运行模型推理结果不同",
        "body": "FastDeploy：1.1.0\nDocker：26\nNVIDIA-SMI 470.256.02\nCuda 11.4\n显卡：T4*2\n\n\n首先，我先尝试使用docker镜像的方式将ppyoloe跑通，输出正确目标检测结果，但这个是通用模型，识别部分车辆图片不能正确识别。\n然后，我开始尝试加入[PP-Vehicle模型](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip)，继续使用ppyoloe调用方式请求新的模型进行推理，这两个模型是同一类的，输入输出结果我使用的是相同的方式，推理也返回了结果，但没有能正确识别图片中的车辆。\n最后，我尝试直接使用PaddleDetection调用模型进行推理，可以正确识别图片中的车辆。\n\n我想请教两个问题：\n1、FD支持PP-Vehicle吗？\n2、我现在遇到的问题是模型不匹配导致的吗？或者其他原因？\n\n麻烦各位老师百忙之中能回复一下这个问题，给小弟提供一些思路，最后希望飞桨越来越好。\n\n\n测试所用车辆图片【PaddleDetection的推理结果】\n![Image](https://github.com/user-attachments/assets/dfdb015a-faf4-4bf5-9f80-1ac9707b1db0)\n\nGRPC请求推出代码\n```\nimport numpy as np\nimport cv2\nimport tritonclient.grpc as grpcclient\n\nserver_addr = '192.168.100.2:8701'\n# server_addr = '192.168.8.78:8701'\nclient = grpcclient.InferenceServerClient(server_addr)\n\ninput_size = (640, 640)\nmodel_name = 'ppvehicle'\nmodel_version = '1'\n\n# image = cv2.imread('000000014439.jpg', cv2.IMREAD_COLOR)\nimage = cv2.imread('car.jpg', cv2.IMREAD_COLOR)\n# image = cv2.imread('car2.jpg', cv2.IMREAD_COLOR)\n# image = cv2.imread('img.png', cv2.IMREAD_COLOR)\nim_h, im_w, _ = image.shape\n\n# === 1. 预处理 ===\nscale = min(input_size[0] / im_h, input_size[1] / im_w)\nnh, nw = int(im_h * scale), int(im_w * scale)\n# 缩放图片\nimage_resized = cv2.resize(image, (nw, nh))\n# 补齐不等宽部分\nimage_padded = np.full((input_size[0], input_size[1], 3), 114, dtype=np.uint8)\nimage_padded[:nh, :nw, :] = image_resized\n#保存缩放图片\ncv2.imwrite(f'resize.jpg', image_resized)\n\nimage_input = cv2.cvtColor(image_padded, cv2.COLOR_BGR2RGB)\n# 归一化\nimage_input = image_input / 255.0\nimage_input = np.transpose(image_input, (2, 0, 1)).astype(np.float32)\nimage_input = np.expand_dims(image_input, axis=0)  # [1, 3, 640, 640]\n\n# scale_factor: [1, 2]\nscale_factor = np.array([[scale, scale]], dtype=np.float32)  # [1, 2]\n\nprint(\"image_input.shape:\", image_input.shape)  # (1, 3, 640, 640)\nprint(\"scale_factor.shape:\", scale_factor.shape)  # (1, 2)\n\n# === 2. 构造输入 ===\ninputs = []\ninfer_input = grpcclient.InferInput('image', image_input.shape, 'FP32')\ninfer_input.set_data_from_numpy(image_input)\ninputs.append(infer_input)\n\ninfer_input_sf = grpcclient.InferInput('scale_factor', scale_factor.shape, 'FP32')\ninfer_input_sf.set_data_from_numpy(scale_factor)\ninputs.append(infer_input_sf)\n\noutputs = [\n    grpcclient.InferRequestedOutput('multiclass_nms3_0.tmp_0'),\n    grpcclient.InferRequestedOutput('multiclass_nms3_0.tmp_2')\n]\n\n# === 3. 推理 ===\nresponse = client.infer(model_name, inputs, model_version=model_version, outputs=outputs)\nbboxs = response.as_numpy('multiclass_nms3_0.tmp_0')  # [N, 6] 或 [1, N, 6]，需根据实际情况调整\nbbox_num = response.as_numpy('multiclass_nms3_0.tmp_2')\n\n# === 4. 后处理 ===\nimage_draw = image.copy()\nprint(\"bboxs type:\", type(bboxs))\nprint(\"bboxs shape:\", bboxs.shape)\nprint(\"bbox num:\", bbox_num)\n\nfor det in bboxs:\n    class_id, score, x1, y1, x2, y2 = det\n    if score < 0.8:\n        continue\n    else:\n        print('class_id:{},score:{}'.format(class_id, score))\n    # 还原到原图\n    x1 = int(x1)\n    y1 = int(y1)\n    x2 = int(x2)\n    y2 = int(y2)\n    cv2.rectangle(image_draw, (x1, y1), (x2, y2), (0, 255, 0), 2)\n    cv2.putText(image_draw, f\"ID:{int(class_id)}, {score:.2f}\", (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n                (0, 0, 255), 2)\n\ncv2.imwrite(f'visualized_result.jpg', image_draw)\n\n````\n\ninfer_cfg.yml\n```\nmode: paddle\ndraw_threshold: 0.5\nmetric: COCO\nuse_dynamic_shape: false\narch: YOLO\nmin_subgraph_size: 3\nPreprocess:\n- interp: 2\n  keep_ratio: false\n  target_size:\n  - 640\n  - 640\n  type: Resize\n- type: Permute\nlabel_list:\n- vehicle\n```\n\nconfig.pbtxt\n```\nname: \"ppvehicle\"\nbackend: \"fastdeploy\"\n\n# Input configuration of the model\ninput [\n  {\n    # input name\n    name: \"image\"\n    # input type such as TYPE_FP32、TYPE_UINT8、TYPE_INT8、TYPE_INT16、TYPE_INT32、TYPE_INT64、TYPE_FP16、TYPE_STRING\n    data_type: TYPE_FP32\n    # input shape， The batch dimension is omitted and the actual shape is [batch, c, h, w]\n    dims: [-1, 3, -1, -1 ]\n  },\n  {\n    name: \"scale_factor\"\n    data_type: TYPE_FP32\n    dims: [-1, 2 ]\n  }\n]\n\n# The output of the model is configured in the same format as the input\noutput [\n  {\n    name: \"multiclass_nms3_0.tmp_0\"\n    data_type: TYPE_FP32\n    dims: [-1, -1, 6 ]\n  },\n  {\n    name: \"multiclass_nms3_0.tmp_2\"\n    data_type: TYPE_INT32\n    dims: [-1, -1, 2 ]\n  }\n]\n\n# Number of instances of the model\ninstance_group [\n  {\n    # The number of instances is 1\n    count: 1\n    # Use GPU, CPU inference option is:KIND_CPU\n    kind: KIND_GPU\n    # The instance is deployed on the 0th GPU card\n    gpus: [0, 1]\n  }\n]\n\noptimization {\n  execution_accelerators {\n    # GPU推理配置， 配合KIND_GPU使用\n    gpu_execution_accelerator : [\n      {\n        name : \"paddle\"\n        # 设置推理并行计算线程数为1\n        parameters { key: \"cpu_threads\" value: \"1\" }\n        # 开启mkldnn加速，设置为0关闭mkldnn\n        parameters { key: \"use_mkldnn\" value: \"1\" }\n      }\n    ]\n  }\n}\n```\n\n\n启动日志：\n```\nroot@f699d48b5622:/# fastdeployserver --model-repository=/fastdeploy/models/ --log-verbose 5 --log-info true --log-error true\nI0702 11:54:43.883587 718 metrics.cc:298] Collecting metrics for GPU 0: Tesla T4\nI0702 11:54:43.883898 718 metrics.cc:298] Collecting metrics for GPU 1: Tesla T4\nI0702 11:54:44.124343 718 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fedea000000' with size 268435456\nI0702 11:54:44.125375 718 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\nI0702 11:54:44.125395 718 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\nI0702 11:54:44.217473 718 backend_factory.h:45] Create TritonBackendFactory\nI0702 11:54:44.223514 718 model_repository_manager.cc:726] AsyncLoad() 'ppmatting'\nI0702 11:54:44.223591 718 model_repository_manager.cc:965] TriggerNextAction() 'ppmatting' version 1: 1\nI0702 11:54:44.223637 718 model_repository_manager.cc:1003] Load() 'ppmatting' version 1\nI0702 11:54:44.223670 718 model_repository_manager.cc:1022] loading: ppmatting:1\nI0702 11:54:44.323989 718 model_repository_manager.cc:726] AsyncLoad() 'ppocrv3det'\nI0702 11:54:44.324085 718 model_repository_manager.cc:1082] CreateInferenceBackend() 'ppmatting' version 1\nI0702 11:54:44.324196 718 model_repository_manager.cc:965] TriggerNextAction() 'ppocrv3det' version 1: 1\nI0702 11:54:44.324302 718 model_repository_manager.cc:1003] Load() 'ppocrv3det' version 1\nI0702 11:54:44.324362 718 model_repository_manager.cc:1022] loading: ppocrv3det:1\nI0702 11:54:44.324559 718 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so\nI0702 11:54:44.424709 718 model_repository_manager.cc:726] AsyncLoad() 'ppocrv3rec'\nI0702 11:54:44.424891 718 model_repository_manager.cc:965] TriggerNextAction() 'ppocrv3rec' version 1: 1\nI0702 11:54:44.424885 718 model_repository_manager.cc:1082] CreateInferenceBackend() 'ppocrv3det' version 1\nI0702 11:54:44.424983 718 model_repository_manager.cc:1003] Load() 'ppocrv3rec' version 1\nI0702 11:54:44.425153 718 model_repository_manager.cc:1022] loading: ppocrv3rec:1\nI0702 11:54:44.525539 718 model_repository_manager.cc:726] AsyncLoad() 'ppyoloe'\nI0702 11:54:44.525610 718 model_repository_manager.cc:1082] CreateInferenceBackend() 'ppocrv3rec' version 1\nI0702 11:54:44.525708 718 model_repository_manager.cc:965] TriggerNextAction() 'ppyoloe' version 1: 1\nI0702 11:54:44.525753 718 model_repository_manager.cc:1003] Load() 'ppyoloe' version 1\nI0702 11:54:44.525834 718 model_repository_manager.cc:1022] loading: ppyoloe:1\nI0702 11:54:44.626264 718 model_repository_manager.cc:726] AsyncLoad() 'ppvehicle'\nI0702 11:54:44.626342 718 model_repository_manager.cc:1082] CreateInferenceBackend() 'ppyoloe' version 1\nI0702 11:54:44.626450 718 model_repository_manager.cc:965] TriggerNextAction() 'ppvehicle' version 1: 1\nI0702 11:54:44.626511 718 model_repository_manager.cc:1003] Load() 'ppvehicle' version 1\nI0702 11:54:44.626579 718 model_repository_manager.cc:1022] loading: ppvehicle:1\nI0702 11:54:44.707716 718 fastdeploy_runtime.cc:1321] TRITONBACKEND_Initialize: fastdeploy\nI0702 11:54:44.707762 718 fastdeploy_runtime.cc:1330] Triton TRITONBACKEND API version: 1.6\nI0702 11:54:44.707777 718 fastdeploy_runtime.cc:1335] 'fastdeploy' TRITONBACKEND API version: 1.6\nI0702 11:54:44.707790 718 fastdeploy_runtime.cc:1364] backend configuration:\n{}\nI0702 11:54:44.712480 718 fastdeploy_runtime.cc:1394] TRITONBACKEND_ModelInitialize: ppmatting (version 1)\nI0702 11:54:44.713830 718 model_config_utils.cc:1550] ModelConfig 64-bit fields:\nI0702 11:54:44.713881 718 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\nI0702 11:54:44.713934 718 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\nI0702 11:54:44.713966 718 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\nI0702 11:54:44.713997 718 model_config_utils.cc:1552] \tModelConfig::ensemble_scheduling::step::model_version\nI0702 11:54:44.714030 718 model_config_utils.cc:1552] \tModelConfig::input::dims\nI0702 11:54:44.714062 718 model_config_utils.cc:1552] \tModelConfig::input::reshape::shape\nI0702 11:54:44.714098 718 model_config_utils.cc:1552] \tModelConfig::instance_group::secondary_devices::device_id\nI0702 11:54:44.714121 718 model_config_utils.cc:1552] \tModelConfig::model_warmup::inputs::value::dims\nI0702 11:54:44.714160 718 model_config_utils.cc:1552] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\nI0702 11:54:44.714201 718 model_config_utils.cc:1552] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\nI0702 11:54:44.714219 718 model_config_utils.cc:1552] \tModelConfig::output::dims\nI0702 11:54:44.714244 718 model_config_utils.cc:1552] \tModelConfig::output::reshape::shape\nI0702 11:54:44.714267 718 model_config_utils.cc:1552] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\nI0702 11:54:44.714288 718 model_config_utils.cc:1552] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\nI0702 11:54:44.714310 718 model_config_utils.cc:1552] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\nI0702 11:54:44.714333 718 model_config_utils.cc:1552] \tModelConfig::version_policy::specific::versions\nI0702 11:54:44.714579 718 fastdeploy_runtime.cc:1394] TRITONBACKEND_ModelInitialize: ppocrv3det (version 1)\nI0702 11:54:44.716212 718 fastdeploy_runtime.cc:1394] TRITONBACKEND_ModelInitialize: ppocrv3rec (version 1)\nI0702 11:54:44.717463 718 fastdeploy_runtime.cc:1394] TRITONBACKEND_ModelInitialize: ppyoloe (version 1)\nI0702 11:54:44.718734 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppmatting_0 (GPU device 0)\nI0702 11:54:44.720927 718 backend_model_instance.cc:105] Creating instance ppmatting_0 on GPU 0 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\nI0702 11:54:44.727023 718 model_repository_manager.cc:1082] CreateInferenceBackend() 'ppvehicle' version 1\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\nI0702 11:55:01.865443 718 fastdeploy_runtime.cc:1394] TRITONBACKEND_ModelInitialize: ppvehicle (version 1)\nI0702 11:55:01.865717 718 triton_model_instance.cc:666] Starting backend thread for ppmatting_0 at nice 0 on device 0...\nI0702 11:55:01.867425 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppocrv3det_0 (GPU device 0)\nI0702 11:55:01.868582 718 backend_model_instance.cc:105] Creating instance ppocrv3det_0 on GPU 0 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\nI0702 11:55:02.965936 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppyoloe_0 (GPU device 0)\nI0702 11:55:02.966095 718 triton_model_instance.cc:666] Starting backend thread for ppocrv3det_0 at nice 0 on device 0...\nI0702 11:55:02.966659 718 backend_model_instance.cc:105] Creating instance ppyoloe_0 on GPU 0 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\nI0702 11:55:05.643018 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppocrv3rec_0 (GPU device 0)\nI0702 11:55:05.643198 718 triton_model_instance.cc:666] Starting backend thread for ppyoloe_0 at nice 0 on device 0...\nI0702 11:55:05.643878 718 backend_model_instance.cc:105] Creating instance ppocrv3rec_0 on GPU 0 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\nI0702 11:55:06.653779 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppmatting_0 (GPU device 1)I0702 11:55:06.653917 718 triton_model_instance.cc:666] Starting backend thread for ppocrv3rec_0 at nice 0 on device 0...\n\nI0702 11:55:06.655065 718 backend_model_instance.cc:105] Creating instance ppmatting_0 on GPU 1 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/runtime.cc(409)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(455)::Clone\tThe target device id:1 is different from current device id:0, cannot share memory with current engine.\nI0702 11:55:23.180427 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppvehicle_0 (GPU device 0)\nI0702 11:55:23.180714 718 triton_model_instance.cc:666] Starting backend thread for ppmatting_0 at nice 0 on device 1...\nI0702 11:55:23.181048 718 model_repository_manager.cc:1183] successfully loaded 'ppmatting' version 1\nI0702 11:55:23.181096 718 model_repository_manager.cc:965] TriggerNextAction() 'ppmatting' version 1: 0\nI0702 11:55:23.181133 718 model_repository_manager.cc:980] no next action, trigger OnComplete()\nI0702 11:55:23.181289 718 model_repository_manager.cc:571] VersionStates() 'ppmatting'\nI0702 11:55:23.181374 718 backend_model_instance.cc:105] Creating instance ppvehicle_0 on GPU 0 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[INFO] fastdeploy/runtime/runtime.cc(286)::CreatePaddleBackend\tRuntime initialized with Backend::PDINFER in Device::GPU.\nI0702 11:55:26.066575 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppocrv3det_0 (GPU device 1)\nI0702 11:55:26.067337 718 backend_model_instance.cc:105] Creating instance ppocrv3det_0 on GPU 1 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/runtime.cc(409)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\nI0702 11:55:26.072279 718 triton_model_instance.cc:666] Starting backend thread for ppvehicle_0 at nice 0 on device 0...\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(455)::Clone\tThe target device id:1 is different from current device id:0, cannot share memory with current engine.\nI0702 11:55:27.278718 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppyoloe_0 (GPU device 1)\nI0702 11:55:27.278949 718 triton_model_instance.cc:666] Starting backend thread for ppocrv3det_0 at nice 0 on device 1...\nI0702 11:55:27.279151 718 model_repository_manager.cc:1183] successfully loaded 'ppocrv3det' version 1\nI0702 11:55:27.279174 718 model_repository_manager.cc:965] TriggerNextAction() 'ppocrv3det' version 1: 0\nI0702 11:55:27.279210 718 model_repository_manager.cc:980] no next action, trigger OnComplete()\nI0702 11:55:27.279321 718 model_repository_manager.cc:571] VersionStates() 'ppocrv3det'\nI0702 11:55:27.279596 718 backend_model_instance.cc:105] Creating instance ppyoloe_0 on GPU 1 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/runtime.cc(409)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(455)::Clone\tThe target device id:1 is different from current device id:0, cannot share memory with current engine.\nI0702 11:55:29.848600 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppocrv3rec_0 (GPU device 1)\nI0702 11:55:29.848785 718 triton_model_instance.cc:666] Starting backend thread for ppyoloe_0 at nice 0 on device 1...\nI0702 11:55:29.849062 718 model_repository_manager.cc:1183] successfully loaded 'ppyoloe' version 1\nI0702 11:55:29.849085 718 model_repository_manager.cc:965] TriggerNextAction() 'ppyoloe' version 1: 0\nI0702 11:55:29.849135 718 model_repository_manager.cc:980] no next action, trigger OnComplete()\nI0702 11:55:29.849459 718 backend_model_instance.cc:105] Creating instance ppocrv3rec_0 on GPU 1 (7.5) using artifact ''\n[INFO] fastdeploy/runtime/runtime.cc(409)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(455)::Clone\tThe target device id:1 is different from current device id:0, cannot share memory with current engine.\nI0702 11:55:30.825714 718 fastdeploy_runtime.cc:1433] TRITONBACKEND_ModelInstanceInitialize: ppvehicle_0 (GPU device 1)\nI0702 11:55:30.826028 718 triton_model_instance.cc:666] Starting backend thread for ppocrv3rec_0 at nice 0 on device 1...\nI0702 11:55:30.826240 718 model_repository_manager.cc:1183] successfully loaded 'ppocrv3rec' version 1\nI0702 11:55:30.826296 718 model_repository_manager.cc:965] TriggerNextAction() 'ppocrv3rec' version 1: 0\nI0702 11:55:30.826321 718 model_repository_manager.cc:980] no next action, trigger OnComplete()\nI0702 11:55:30.826417 718 model_repository_manager.cc:571] VersionStates() 'ppocrv3rec'I0702 11:55:30.826451 718 backend_model_instance.cc:105] Creating instance ppvehicle_0 on GPU 1 (7.5) using artifact ''\n\nI0702 11:55:30.826616 718 model_repository_manager.cc:571] VersionStates() 'ppyoloe'\n[INFO] fastdeploy/runtime/runtime.cc(409)::Clone\tRuntime Clone with Backend:: Backend::PDINFER in Device::GPU.\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(29)::BuildOption\tWill inference_precision float32\n[INFO] fastdeploy/runtime/backends/paddle/paddle_backend.cc(61)::BuildOption\tWill use external stream for Paddle Backend.\n[WARNING] fastdeploy/runtime/backends/paddle/paddle_backend.cc(455)::Clone\tThe target device id:1 is different from current device id:0, cannot share memory with current engine.\nI0702 11:55:33.326950 718 triton_model_instance.cc:666] Starting backend thread for ppvehicle_0 at nice 0 on device 1...\nI0702 11:55:33.327212 718 model_repository_manager.cc:1183] successfully loaded 'ppvehicle' version 1\nI0702 11:55:33.327236 718 model_repository_manager.cc:965] TriggerNextAction() 'ppvehicle' version 1: 0\nI0702 11:55:33.327258 718 model_repository_manager.cc:980] no next action, trigger OnComplete()\nI0702 11:55:33.327362 718 model_repository_manager.cc:571] VersionStates() 'ppvehicle'\nI0702 11:55:33.327513 718 model_repository_manager.cc:571] VersionStates() 'ppocrv3rec'\nI0702 11:55:33.327561 718 model_repository_manager.cc:571] VersionStates() 'ppyoloe'\nI0702 11:55:33.327609 718 model_repository_manager.cc:571] VersionStates() 'ppvehicle'\nI0702 11:55:33.327652 718 model_repository_manager.cc:571] VersionStates() 'ppocrv3det'\nI0702 11:55:33.327696 718 model_repository_manager.cc:571] VersionStates() 'ppmatting'\nI0702 11:55:33.327855 718 server.cc:522] \n+------------------+------+\n| Repository Agent | Path |\n+------------------+------+\n+------------------+------+\n\nI0702 11:55:33.328104 718 server.cc:549] \n+------------+---------------------------------------------------------------+--------+\n| Backend    | Path                                                          | Config |\n+------------+---------------------------------------------------------------+--------+\n| fastdeploy | /opt/tritonserver/backends/fastdeploy/libtriton_fastdeploy.so | {}     |\n+------------+---------------------------------------------------------------+--------+\n\nI0702 11:55:33.328262 718 model_repository_manager.cc:547] BackendStates()\nI0702 11:55:33.328427 718 server.cc:592] \n+------------+---------+--------+\n| Model      | Version | Status |\n+------------+---------+--------+\n| ppmatting  | 1       | READY  |\n| ppocrv3det | 1       | READY  |\n| ppocrv3rec | 1       | READY  |\n| ppvehicle  | 1       | READY  |\n| ppyoloe    | 1       | READY  |\n+------------+---------+--------+\n\nI0702 11:55:33.328956 718 tritonserver.cc:1920] \n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Option                           | Value                                                                                                                                                                              |\n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| server_id                        | triton                                                                                                                                                                             |\n| server_version                   | 2.15.0                                                                                                                                                                             |\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statis |\n|                                  | tics                                                                                                                                                                               |\n| model_repository_path[0]         | /fastdeploy/models/                                                                                                                                                                |\n| model_control_mode               | MODE_NONE                                                                                                                                                                          |\n| strict_model_config              | 1                                                                                                                                                                                  |\n| rate_limit                       | OFF                                                                                                                                                                                |\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                          |\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                           |\n| cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                           |\n| response_cache_byte_size         | 0                                                                                                                                                                                  |\n| min_supported_compute_capability | 6.0                                                                                                                                                                                |\n| strict_readiness                 | 1                                                                                                                                                                                  |\n| exit_timeout                     | 30                                                                                                                                                                                 |\n+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nI0702 11:55:33.329700 718 grpc_server.cc:4071] === GRPC KeepAlive Options ===\nI0702 11:55:33.329753 718 grpc_server.cc:4072] keepalive_time_ms: 7200000\nI0702 11:55:33.329797 718 grpc_server.cc:4074] keepalive_timeout_ms: 20000\nI0702 11:55:33.329857 718 grpc_server.cc:4076] keepalive_permit_without_calls: 0\nI0702 11:55:33.329900 718 grpc_server.cc:4078] http2_max_pings_without_data: 2\nI0702 11:55:33.329938 718 grpc_server.cc:4080] http2_min_recv_ping_interval_without_data_ms: 300000\nI0702 11:55:33.329975 718 grpc_server.cc:4083] http2_max_ping_strikes: 2\nI0702 11:55:33.330021 718 grpc_server.cc:4085] ==============================\nI0702 11:55:33.332894 718 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\nI0702 11:55:33.333019 718 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\nI0702 11:55:33.333076 718 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\nI0702 11:55:33.333151 718 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\nI0702 11:55:33.333213 718 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\nI0702 11:55:33.333269 718 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\nI0702 11:55:33.333327 718 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\nI0702 11:55:33.333384 718 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\nI0702 11:55:33.333438 718 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\nI0702 11:55:33.333494 718 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\nI0702 11:55:33.333551 718 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\nI0702 11:55:33.333604 718 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\nI0702 11:55:33.333662 718 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\nI0702 11:55:33.333718 718 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\nI0702 11:55:33.333771 718 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\nI0702 11:55:33.333827 718 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\nI0702 11:55:33.333914 718 grpc_server.cc:416] Thread started for CommonHandler\nI0702 11:55:33.334235 718 grpc_server.cc:3150] New request handler for ModelInferHandler, 1\nI0702 11:55:33.334354 718 grpc_server.cc:2202] Thread started for ModelInferHandler\nI0702 11:55:33.334610 718 grpc_server.cc:3503] New request handler for ModelStreamInferHandler, 3\nI0702 11:55:33.334720 718 grpc_server.cc:2202] Thread started for ModelStreamInferHandler\nI0702 11:55:33.334766 718 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\nI0702 11:55:33.335558 718 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\nI0702 11:55:33.378336 718 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\n```\n\n推理日志：\n```\nI0702 10:26:29.632383 668 grpc_server.cc:3157] Process for ModelInferHandler, rpc_ok=1, 8 step START\nI0702 10:26:29.632492 668 grpc_server.cc:3150] New request handler for ModelInferHandler, 9\nI0702 10:26:29.632562 668 model_repository_manager.cc:615] GetInferenceBackend() 'ppvehicle' version 1\nI0702 10:26:29.632634 668 model_repository_manager.cc:615] GetInferenceBackend() 'ppvehicle' version 1\nI0702 10:26:29.632713 668 infer_request.cc:524] prepared: [0x0x7f9100004700] request id: , model: ppvehicle, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 0, priority: 0, timeout (us): 0\noriginal inputs:\n[0x0x7f91000058f8] input: scale_factor, type: FP32, original shape: [1,2], batch + shape: [1,2], shape: [1,2]\n[0x0x7f9100004d78] input: image, type: FP32, original shape: [1,3,640,640], batch + shape: [1,3,640,640], shape: [1,3,640,640]\noverride inputs:\ninputs:\n[0x0x7f9100004d78] input: image, type: FP32, original shape: [1,3,640,640], batch + shape: [1,3,640,640], shape: [1,3,640,640]\n[0x0x7f91000058f8] input: scale_factor, type: FP32, original shape: [1,2], batch + shape: [1,2], shape: [1,2]\noriginal requested outputs:\nmulticlass_nms3_0.tmp_0\nmulticlass_nms3_0.tmp_2\nrequested outputs:\nmulticlass_nms3_0.tmp_0\nmulticlass_nms3_0.tmp_2\n\nI0702 10:26:29.633259 668 fastdeploy_runtime.cc:1494] model ppvehicle, instance ppvehicle_0, executing 1 requests\nI0702 10:26:29.633347 668 fastdeploy_runtime.cc:974] TRITONBACKEND_ModelExecute: Running ppvehicle_0 with 1 requests\nI0702 10:26:29.709033 668 infer_response.cc:165] add response output: output: multiclass_nms3_0.tmp_0, type: FP32, shape: [95,6]\nI0702 10:26:29.709086 668 grpc_server.cc:2275] GRPC: unable to provide 'multiclass_nms3_0.tmp_0' in GPU, will use CPU\nI0702 10:26:29.709104 668 grpc_server.cc:2286] GRPC: using buffer for 'multiclass_nms3_0.tmp_0', size: 2280, addr: 0x7f9000dc2d90\nI0702 10:26:29.709164 668 pinned_memory_manager.cc:161] pinned memory allocation: size 2280, addr 0x7f9370000090\nI0702 10:26:29.709213 668 infer_response.cc:165] add response output: output: multiclass_nms3_0.tmp_2, type: INT32, shape: [1]\nI0702 10:26:29.709229 668 grpc_server.cc:2275] GRPC: unable to provide 'multiclass_nms3_0.tmp_2' in GPU, will use CPU\nI0702 10:26:29.709253 668 grpc_server.cc:2286] GRPC: using buffer for 'multiclass_nms3_0.tmp_2', size: 4, addr: 0x7f9000db0cf0\nI0702 10:26:29.709273 668 pinned_memory_manager.cc:161] pinned memory allocation: size 4, addr 0x7f9370000980\nI0702 10:26:29.709324 668 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f9370000090\nI0702 10:26:29.709355 668 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f9370000980\nI0702 10:26:29.709373 668 grpc_server.cc:3310] ModelInferHandler::InferResponseComplete, 8 step ISSUED\nI0702 10:26:29.709410 668 grpc_server.cc:2321] GRPC free: size 2280, addr 0x7f9000dc2d90\nI0702 10:26:29.709452 668 grpc_server.cc:2321] GRPC free: size 4, addr 0x7f9000db0cf0\nI0702 10:26:29.709551 668 grpc_server.cc:2879] ModelInferHandler::InferRequestComplete\nI0702 10:26:29.709582 668 grpc_server.cc:3157] Process for ModelInferHandler, rpc_ok=1, 8 step COMPLETE\nI0702 10:26:29.709625 668 grpc_server.cc:2195] Done for ModelInferHandler, 8\n\n```\n",
        "state": "closed",
        "user": "impl1874",
        "closed_by": "impl1874",
        "created_at": "2025-07-02T12:15:04+00:00",
        "updated_at": "2025-07-02T14:10:53+00:00",
        "closed_at": "2025-07-02T14:10:53+00:00",
        "comments_count": [
            "impl1874"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2691,
        "title": "Feature Request: FastDeploy Architecture Overview",
        "body": "Could you give an overview of the FastDeploy architecture and explain how it works?",
        "state": "open",
        "user": "zhenwenDang",
        "closed_by": null,
        "created_at": "2025-07-03T09:30:01+00:00",
        "updated_at": "2025-07-04T06:41:04+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2683,
        "title": "8卡 h200 部署ERNIE-4.5-VL-424B-A47B-Paddle 失败",
        "body": "[Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:317)",
        "state": "open",
        "user": "SongDI911",
        "closed_by": null,
        "created_at": "2025-07-02T12:39:25+00:00",
        "updated_at": "2025-07-08T05:49:46+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "SongDI911",
            "Jiang-Jia-Jun",
            "SongDI911"
        ],
        "labels": [
            "ERNIE-45-VL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2693,
        "title": "python -m fastdeploy.entrypoints.openai.api_server --model /root/fssd/PaddlePaddle/ERNIE-4.5-VL-28B-A3B-Base-Paddle 执行失败",
        "body": "运行python -m fastdeploy.entrypoints.openai.api_server        --model /root/fssd/PaddlePaddle/ERNIE-4.5-VL-28B-A3B-Base-Paddle        --port 8180        --metrics-port 8181        --engine-worker-queue-port 8182        --max-model-len 32768        --max-num-seqs 32        --reasoning-parser ernie-45-vl        --enable-mm直接卡死如下，没有任何显示\nLoading Weights: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:03<00:00,  1.59it/s]\nLoading Layers: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.48it/s]\n\n然后ctrl c终止 报错如下\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/root/fssd/ERNIE-4.5-Base/lib/python3.11/site-packages/fastdeploy/entrypoints/openai/api_server.py\", line 385, in <module>\n    main()\n  File \"/root/fssd/ERNIE-4.5-Base/lib/python3.11/site-packages/fastdeploy/entrypoints/openai/api_server.py\", line 376, in main\n    if load_engine() is None:\n       ^^^^^^^^^^^^^\n  File \"/root/fssd/ERNIE-4.5-Base/lib/python3.11/site-packages/fastdeploy/entrypoints/openai/api_server.py\", line 85, in load_engine\n    if not engine.start(api_server_pid=os.getpid()):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/fssd/ERNIE-4.5-Base/lib/python3.11/site-packages/fastdeploy/engine/engine.py\", line 242, in start\n    self._stop_profile()\n  File \"/root/fssd/ERNIE-4.5-Base/lib/python3.11/site-packages/fastdeploy/engine/engine.py\", line 1152, in _stop_profile\n    time.sleep(1)\nKeyboardInterrupt\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 6 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/worker_healthy_live_signal.10375': [Errno 2] No such file or directory: '/worker_healthy_live_signal.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/exist_swapped_task_signal.10375': [Errno 2] No such file or directory: '/exist_swapped_task_signal.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/exist_task_signal.10375': [Errno 2] No such file or directory: '/exist_task_signal.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/worker_ready_signal.10375': [Errno 2] No such file or directory: '/worker_ready_signal.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/model_weights_status.10375': [Errno 2] No such file or directory: '/model_weights_status.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n/root/fssd/ERNIE-4.5-Base/lib/python3.11/multiprocessing/resource_tracker.py:267: UserWarning: resource_tracker: '/prefill_time_signal.10375': [Errno 2] No such file or directory: '/prefill_time_signal.10375'\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n\n然后查看fastdeploy.log日志\n\nWARNING  2025-07-03 10:26:36,668 10454 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.cpu import failed, it may be not compiled.\nWARNING  2025-07-03 10:26:36,669 10454 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.xpu import failed, it may be not compiled.\nWARNING  2025-07-03 10:26:36,669 10454 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.npu import failed, it may be not compiled.\nINFO     2025-07-03 10:27:47,841 10454 engine_worker_queue.py[line:219] Connected EngineWorkerQueue client_id: 0, number of connected clients: 2\nWARNING  2025-07-03 10:27:49,634 10375 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.cpu import failed, it may be not compiled.\nWARNING  2025-07-03 10:27:49,634 10375 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.xpu import failed, it may be not compiled.\nWARNING  2025-07-03 10:27:49,635 10375 import_ops.py[line:47] Ops of fastdeploy.model_executor.ops.npu import failed, it may be not compiled.\nINFO     2025-07-03 10:30:07,810 10375 zmq_client.py[line:178] Closing ZMQ connection...\nWARNING  2025-07-03 10:30:07,811 10375 zmq_client.py[line:157] Context was terminated\nERROR    2025-07-03 10:30:07,811 10375 engine.py[line:375] Engine stops inserting zmq task into scheduler",
        "state": "open",
        "user": "liuxun0436",
        "closed_by": null,
        "created_at": "2025-07-03T10:33:22+00:00",
        "updated_at": "2025-07-08T05:41:12+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "ERNIE-45-VL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2697,
        "title": "Feature Request: Add Support for max_completion_tokens Parameter (OpenAI API Deprecation)",
        "body": "I'd like to request the addition of a max_completion_tokens parameter to FastDeploy's OpenAI-compatible interfaces, as OpenAI has deprecated the max_tokens parameter in favor of more explicit token control.",
        "state": "open",
        "user": "zhenwenDang",
        "closed_by": null,
        "created_at": "2025-07-03T12:33:12+00:00",
        "updated_at": "2025-07-04T02:12:23+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 2728
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2704,
        "title": "fastdeploy 部署erine-21B",
        "body": "基于fastdeploy-cuda-12.6:2.0.0 容器进行离线部署，安装命令是 python -m fastdeploy.entrypoints.openai.api_server --model xxx --port 8180 \n报以下错误：\ntokenizer,tokenizer_config_file_dir = super().from_pretrained()\ntypeError: cannot unpack non-iterable bool object\n请问如何解决",
        "state": "open",
        "user": "dream-wujianguo",
        "closed_by": null,
        "created_at": "2025-07-04T02:37:49+00:00",
        "updated_at": "2025-07-04T03:21:11+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "dream-wujianguo",
            "Jiang-Jia-Jun",
            "dream-wujianguo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2722,
        "title": "OpenAI接口兼容性不佳以及一些其他问题",
        "body": "1、OpenAI格式的接口无法在Cline中使用；包括VL和300B都无法正常工作；后台五明显报错日志；\n\n2、后台日志过于简陋，无法观测性能指标；还是有什么特殊日志文件？\n    SGlang或VLLM都有显示token生成速度等功能，但是fastdeploy暂时没有找到；\n\n3、并发10个情况下，很容易出现一些莫名其妙的回复：\n如一直输出<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>，查看后台有报错日志：\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/utils.py\", line 62, in format\n    message = super().format(record)\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/output/token_processor.py\", line 153, in process_sampling_results\n    self._process_batch_output()\n  File \"/usr/local/lib/python3.10/dist-packages/fastdeploy/output/token_processor.py\", line 282, in _process_batch_output\n    llm_logger.info(\nMessage: 'recovery stop signal found at task chatcmpl-4d7a5a03-02f4-4cb9-b63e-8d19e5394274-8a49a2d5-7dcc-44ea-96eb-113544957f4d'\nArguments: ('token_ids: [-3]',)\n",
        "state": "open",
        "user": "icewool",
        "closed_by": null,
        "created_at": "2025-07-05T06:46:14+00:00",
        "updated_at": "2025-07-08T02:39:55+00:00",
        "closed_at": null,
        "comments_count": [
            "icewool",
            "Jiang-Jia-Jun",
            "icewool"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2723,
        "title": "ERNIE-4.5-VL-424B-A47B-Paddle加载卡住不动",
        "body": "<img width=\"2576\" height=\"934\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fb8e91f6-12be-464f-a4c4-4b646b1bd98b\" />\n\n```sh\n$python -m fastdeploy.entrypoints.openai.api_server \\\n>     --model ERNIE-4.5-VL-424B-A47B-Paddle \\\n>     --port 8180 \\\n>     --tensor-parallel-size 8 \\\n>     --quantization wint4 \\\n>     --metrics-port 8181 \\\n>     --engine-worker-queue-port 8182 \\\n>     --max-model-len 32768 \\\n>     --max-num-seqs 32 \\\n>     --mm-processor-kwargs '{\"video_max_frames\": 30}' \\\n>     --limit-mm-per-prompt '{\"image\": 10, \"video\": 3}' \\\n>     --reasoning-parser ernie-45-vl \\\n>     --enable-mm\n```\n\n环境：\npaddlepaddle-gpu==3.1.0\nfastdeploy-gpu==2.0.0\n\nERNIE-4.5-VL-28B-A3B-Paddle模型可以起来，换成 ERNIE-4.5-VL-424B-A47B-Paddle模型总是中途卡住，在不同的进度位置\n",
        "state": "open",
        "user": "zjbshk",
        "closed_by": null,
        "created_at": "2025-07-06T03:02:49+00:00",
        "updated_at": "2025-07-08T05:40:56+00:00",
        "closed_at": null,
        "comments_count": [
            "zjbshk",
            "Jiang-Jia-Jun",
            "zjbshk",
            "zjbshk",
            "xiaoxiaohehe001",
            "Jiang-Jia-Jun",
            "zjbshk",
            "zjbshk"
        ],
        "labels": [
            "Feature Request",
            "ERNIE-45-VL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2727,
        "title": "基于FastDeploy运行ernie-4.5-vl，在OpenAI的配置里[enable_thingking]参数不生效",
        "body": "硬件：A100 80G*1\n\n模型：ernie-4.5-vl-28b-a3b\n\n启动方式：\n\n<img width=\"804\" height=\"347\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c511609e-3f5f-4517-b105-52e31340e6a0\" />\n\n推理方式：基于OpenAI的推理方法\n\n```python\n# -*- coding: utf-8 -*-\n# pip install openai\nimport base64\nfrom openai import OpenAI\nimport time\n\nclient = OpenAI(\n    api_key=\"null\",\n    base_url=\"http://0.0.0.0:8180/v1\",\n)\n\n# 本地图像转Base64\nwith open(\"inputs/chart_img_temp/00100.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n\nchat_completion = client.chat.completions.create(\n    # model=\"ernie-4.5-vl-28b-a3b\",\n    model = \"null\",\n    messages=[\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"将图表信息转为表格的HTML格式，不要任何其他解释\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\":  f\"data:image/png;base64,{base64_image}\"\n                }\n            }\n        ]\n    }\n],\n    stream=True,\n    extra_body={\n        \"penalty_score\": 1,\n        \"enable_thinking\": False\n    },\n    max_completion_tokens=2000,\n    temperature=0.2,\n    top_p=0.8,\n    frequency_penalty=0,\n    presence_penalty=0\n)\n\nt1 = time.time()\nusage = 0\nfor chunk in chat_completion:\n    if hasattr(chunk.choices[0].delta, \"reasoning_content\") and chunk.choices[0].delta.reasoning_content:\n        print(chunk.choices[0].delta.reasoning_content, end=\"\", flush=True)\n        usage += len(chunk.choices[0].delta.reasoning_content)\n    else:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n        usage += len(chunk.choices[0].delta.content)\n\nt2 = time.time()\nprint(usage)\nprint(f\"请求时间:{t2-t1}秒,使用量:{usage},每秒输出tokens:{usage/(t2-t1)}\")\n```\n\n问题：无论enable_thinking为True还是False，模型输出总是带思考过程：\n\n<img width=\"1297\" height=\"546\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ee82c5a5-6886-46cb-98f7-fe8485e0c3af\" />\n\n",
        "state": "closed",
        "user": "yazheng0307",
        "closed_by": "yazheng0307",
        "created_at": "2025-07-07T06:28:14+00:00",
        "updated_at": "2025-07-07T07:13:43+00:00",
        "closed_at": "2025-07-07T07:13:43+00:00",
        "comments_count": [
            "Jiang-Jia-Jun",
            "yazheng0307"
        ],
        "labels": [
            "Feature Request"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2739,
        "title": "ERNIE-4.5-VL-28B-A3B-Paddle 加载卡主不动，无论是单卡4090 48b还是双卡4090 48g都不行",
        "body": "ERNIE-4.5-VL-28B-A3B-Paddle 加载卡主不动，无论是单卡4090 48b还是双卡4090 48g都一样卡主不动\n\nfastdeploy 安装脚本\n\n```\npython -m pip install paddlepaddle-gpu==3.1.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n\n# Install stable release(4090)\npython -m pip install fastdeploy-gpu -i https://www.paddlepaddle.org.cn/packages/stable/fastdeploy-gpu-86_89/ --extra-index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n```\n\n\n单卡启动\n```\nCUDA_VISIBLE_DEVICES=6 python -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-28B-A3B-Paddle \\\n       --tensor-parallel-size 1 \\\n       --port 8180 \\\n       --quantization wint4 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n```\n\n双卡启动\n```\nCUDA_VISIBLE_DEVICES=6,7 python -m fastdeploy.entrypoints.openai.api_server \\\n       --model baidu/ERNIE-4.5-VL-28B-A3B-Paddle \\\n       --tensor-parallel-size 2 \\\n       --port 8180 \\\n       --quantization wint4 \\\n       --metrics-port 8181 \\\n       --engine-worker-queue-port 8182 \\\n       --max-model-len 32768 \\\n       --enable-mm \\\n       --reasoning-parser ernie-45-vl \\\n       --max-num-seqs 32\n```\n\n<img width=\"740\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f8daa19a-db6a-47cd-a886-c7178b6750da\" />\n\ncrtl z退出后提示如下：\n\n<img width=\"747\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/34be6b6e-757d-468c-9c89-a02c37cbd5a4\" />\n\n这个问题应该怎么解决吗？",
        "state": "open",
        "user": "wangjiezju1988",
        "closed_by": null,
        "created_at": "2025-07-07T14:12:08+00:00",
        "updated_at": "2025-07-08T09:52:56+00:00",
        "closed_at": null,
        "comments_count": [
            "Jiang-Jia-Jun",
            "wangjiezju1988",
            "Jiang-Jia-Jun",
            "wangjiezju1988",
            "ming1753",
            "wangjiezju1988",
            "ming1753",
            "wangjiezju1988",
            "lm970585581",
            "ming1753",
            "Jiang-Jia-Jun",
            "wangjiezju1988",
            "wangjiezju1988",
            "wangjiezju1988",
            "Jiang-Jia-Jun",
            "wangjiezju1988",
            "Jiang-Jia-Jun",
            "ming1753"
        ],
        "labels": [
            "ERNIE-45-VL"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2752,
        "title": "【Feature】add fd commit/branch info when start server",
        "body": null,
        "state": "open",
        "user": "gzy19990617",
        "closed_by": null,
        "created_at": "2025-07-08T08:53:05+00:00",
        "updated_at": "2025-07-08T12:49:55+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2753,
        "title": "[Feature]support top_k_top_p sampling",
        "body": "我们参考 flashinfer 的实现（感谢），完善了 rejection_top_p_sampling，支持 top_k_top_p sampling，包含 `top_k_first` 和 `joint` 两种顺序\r\n\r\n- `top_k_first` (default)：top_k- > normalize -> top_p\r\n- `joint`：同步进行 top_k 和 top_p",
        "state": "open",
        "user": "Sunny-bot1",
        "closed_by": null,
        "created_at": "2025-07-08T08:55:49+00:00",
        "updated_at": "2025-07-08T13:24:21+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2755,
        "title": "【Sync develop】 add commit info",
        "body": "-",
        "state": "closed",
        "user": "gzy19990617",
        "closed_by": "Jiang-Jia-Jun",
        "created_at": "2025-07-08T09:01:00+00:00",
        "updated_at": "2025-07-08T09:02:50+00:00",
        "closed_at": "2025-07-08T09:02:50+00:00",
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2754,
        "title": "【Sync develop】add commit info",
        "body": "-",
        "state": "closed",
        "user": "gzy19990617",
        "closed_by": "gzy19990617",
        "created_at": "2025-07-08T08:57:33+00:00",
        "updated_at": "2025-07-08T08:58:45+00:00",
        "closed_at": "2025-07-08T08:58:45+00:00",
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2756,
        "title": "dcu adapter ernie45t",
        "body": "dcu adapter ernie45t",
        "state": "open",
        "user": "lifulll",
        "closed_by": null,
        "created_at": "2025-07-08T09:12:12+00:00",
        "updated_at": "2025-07-08T12:39:26+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]",
            "CLAassistant"
        ],
        "labels": [
            "contributor"
        ]
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2757,
        "title": "Clear dead code And supplementary notes",
        "body": "The cleaned files include:\r\n- fastdeploy/model_executor/graph_optimization/*\r\n- fastdeploy/worker/forward_meta.py\r\n- fastdeploy/worker/gpu_model_runner.py\r\n- fastdeploy/worker/gpu_worker.py\r\n- test/layers/test_attention.py\r\n\r\nExecution operations：\r\n1. delete dead code \r\n2. supplementary notes ",
        "state": "open",
        "user": "gongshaotian",
        "closed_by": null,
        "created_at": "2025-07-08T09:57:55+00:00",
        "updated_at": "2025-07-08T13:52:10+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2758,
        "title": "[Feature] support custom all-reduce",
        "body": "Supports custom all-reduce kernel when using TP inference. \r\nYou can add `--enable-custom-all-reduce` to enable the kernel when serving.\r\nYou can add `enable_custom_all_reduce=True` in LLM to enable the kernel when using offline inference.",
        "state": "open",
        "user": "zhink",
        "closed_by": null,
        "created_at": "2025-07-08T10:48:41+00:00",
        "updated_at": "2025-07-08T12:40:50+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2759,
        "title": "[sync fix]",
        "body": null,
        "state": "closed",
        "user": "gzy19990617",
        "closed_by": "Jiang-Jia-Jun",
        "created_at": "2025-07-08T11:28:40+00:00",
        "updated_at": "2025-07-08T11:29:23+00:00",
        "closed_at": "2025-07-08T11:29:23+00:00",
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2760,
        "title": "[Test] Test CI",
        "body": null,
        "state": "open",
        "user": "Jiang-Jia-Jun",
        "closed_by": null,
        "created_at": "2025-07-08T13:32:06+00:00",
        "updated_at": "2025-07-08T13:32:11+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2761,
        "title": "Revert \"[Bug fix] fix attention rank init\"",
        "body": "Reverts PaddlePaddle/FastDeploy#2743",
        "state": "open",
        "user": "RichardWooSJTU",
        "closed_by": null,
        "created_at": "2025-07-08T15:06:55+00:00",
        "updated_at": "2025-07-08T15:07:01+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/FastDeploy",
        "number": 2762,
        "title": "[Bug fix] Fix ep_moe_expert_dispatch_fp8 in EP mode",
        "body": "In EP prefill phase, ep_moe_expert_dispatch_fp8 is used to prepare permute inputs for ffn, fix some params which diff with TP mode.",
        "state": "open",
        "user": "RichardWooSJTU",
        "closed_by": null,
        "created_at": "2025-07-08T15:24:29+00:00",
        "updated_at": "2025-07-08T15:24:35+00:00",
        "closed_at": null,
        "comments_count": [
            "paddle-bot[bot]"
        ],
        "labels": []
    }
]