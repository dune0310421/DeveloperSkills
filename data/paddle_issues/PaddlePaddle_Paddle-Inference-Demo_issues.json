[
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 24,
        "title": "使用paddle_trt预测失败",
        "body": "您好，我在使用paddle_trt推理时遇到如下报错，请问该如何解决?\r\npaddlepaddle-gpu      1.8.2.post107\r\n\r\nEnforceNotMet: \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::string paddle::platform::GetTraceBackString<char const*>(char const*&&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int)\r\n2   paddle::framework::ir::PassRegistry::Get(std::string const&) const\r\n3   paddle::inference::analysis::IRPassManager::CreatePasses(paddle::inference::analysis::Argument*, std::vector<std::string, std::allocator<std::string> > const&)\r\n4   paddle::inference::analysis::IRPassManager::IRPassManager(paddle::inference::analysis::Argument*)\r\n5   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)\r\n6   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\r\n7   paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n8   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n9   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n10  std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n11  std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig>(paddle::AnalysisConfig const&)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: Pass tensorrt_subgraph_pass has not been registered at (/paddle/paddle/fluid/framework/ir/pass.h:201)",
        "state": "closed",
        "user": "DYJNG",
        "closed_by": "DYJNG",
        "created_at": "2020-07-13T03:12:34+00:00",
        "updated_at": "2020-07-13T04:01:08+00:00",
        "closed_at": "2020-07-13T04:01:08+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 30,
        "title": "gpu利用率低",
        "body": "![image](https://user-images.githubusercontent.com/47356938/92453240-97896e00-f1f1-11ea-9f47-c6a567e266bb.png)\r\n1.大图小图都是5085mb;\r\n2.怎样提升GPU 的利用率，来加快推理速度？\r\n谢谢~",
        "state": "closed",
        "user": "Unknown-parameters",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-08T08:39:47+00:00",
        "updated_at": "2024-04-16T08:57:13+00:00",
        "closed_at": "2024-04-16T08:57:13+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 26,
        "title": "Yolo v3示例报错",
        "body": "--- Running analysis [ir_graph_build_pass]\r\nTraceback (most recent call last):\r\n  File \"infer_yolov3.py\", line 64, in <module>\r\n    pred = create_predictor(args) \r\n  File \"infer_yolov3.py\", line 27, in create_predictor\r\n    predictor = create_paddle_predictor(config)\r\npaddle.fluid.core_avx.EnforceNotMet: \r\n\r\n--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > paddle::platform::GetTraceBackString<char const*>(char const*&&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::exception_ptr, char const*, int)\r\n2   paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const\r\n3   std::__1::__function::__func<paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CPUPlace, false, 0ul, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, double>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, int>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, signed char>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, long long> >::operator()(char const*, char const*, int) const::'lambda'(paddle::framework::ExecutionContext const&), std::__1::allocator<paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CPUPlace, false, 0ul, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, float>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, double>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, int>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, signed char>, paddle::operators::LoadCombineOpKernel<paddle::platform::CPUDeviceContext, long long> >::operator()(char const*, char const*, int) const::'lambda'(paddle::framework::ExecutionContext const&)>, void (paddle::framework::ExecutionContext const&)>::operator()(paddle::framework::ExecutionContext const&)\r\n4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const\r\n5   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)\r\n7   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)\r\n8   paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, bool, bool)\r\n9   paddle::inference::LoadPersistables(paddle::framework::Executor*, paddle::framework::Scope*, paddle::framework::ProgramDesc const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)\r\n10  paddle::inference::Load(paddle::framework::Executor*, paddle::framework::Scope*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\n11  paddle::inference::analysis::IrGraphBuildPass::RunImpl(paddle::inference::analysis::Argument*)\r\n12  paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\r\n13  paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n14  paddle::AnalysisPredictor::PrepareProgram(std::__1::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n15  paddle::AnalysisPredictor::Init(std::__1::shared_ptr<paddle::framework::Scope> const&, std::__1::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n16  std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n17  std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig>(paddle::AnalysisConfig const&)\r\n18  void pybind11::cpp_function::initialize<std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> > (*&)(paddle::AnalysisConfig const&), std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> >, paddle::AnalysisConfig const&, pybind11::name, pybind11::scope, pybind11::sibling>(std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> > (*&)(paddle::AnalysisConfig const&), std::__1::unique_ptr<paddle::PaddlePredictor, std::__1::default_delete<paddle::PaddlePredictor> > (*)(paddle::AnalysisConfig const&), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const\r\n19  pybind11::cpp_function::dispatcher(_object*, _object*, _object*)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: OP(LoadCombine) fail to open file ./yolove_infer/__params__, please check whether the model file is complete or damaged. at (/home/teamcity/work/ef54dc8a5b211854/paddle/fluid/operators/load_combine_op.h:46)",
        "state": "closed",
        "user": "AIpioneer",
        "closed_by": "AIpioneer",
        "created_at": "2020-07-15T09:09:31+00:00",
        "updated_at": "2020-07-15T09:10:49+00:00",
        "closed_at": "2020-07-15T09:10:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 20,
        "title": "请问能给出作为输入数据的具体图像样张的处理示例代码吗？",
        "body": "我使用的是PaddleClass训练生成的模型，是否也可以使用这样的C++接口进行部署，如何处理图像数据为输入的vector<float>,是否需要考虑通道顺序等",
        "state": "closed",
        "user": "YasinFu",
        "closed_by": "NHZlX",
        "created_at": "2020-07-07T01:18:33+00:00",
        "updated_at": "2022-07-19T10:18:36+00:00",
        "closed_at": "2020-08-17T08:00:33+00:00",
        "comments_count": [
            "NHZlX",
            "anazh"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 23,
        "title": "”CHECK\" 找不到识别符",
        "body": "先生：\r\n\r\ninference C++中：\r\n识别不了“CHECK”；   找不到；\r\n无论是引用windows 下的预编译库，还是ubuntu 的预编译库；\r\n\r\n请问，该怎么处理这个问题？",
        "state": "closed",
        "user": "Unknown-parameters",
        "closed_by": "Unknown-parameters",
        "created_at": "2020-07-08T07:34:59+00:00",
        "updated_at": "2020-07-09T15:53:56+00:00",
        "closed_at": "2020-07-09T15:53:56+00:00",
        "comments_count": [
            "Unknown-parameters",
            "Unknown-parameters"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 29,
        "title": "KeyError: \"Couldn't find field paddle.framework.proto.OpDesc.output\"",
        "body": "之前python的demo还可以跑，为什么出现了这个错误？\r\n网上查了一下，似乎没有查到解决办法，希望得到大佬的帮助",
        "state": "closed",
        "user": "eucommiaulmoides",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-08-13T06:29:04+00:00",
        "updated_at": "2024-02-06T03:13:36+00:00",
        "closed_at": "2024-02-06T03:13:36+00:00",
        "comments_count": [
            "NHZlX",
            "eucommiaulmoides",
            "NHZlX"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 19,
        "title": "paddle-inference起的推理服务能运行在32位的Windows系统上吗",
        "body": "如题",
        "state": "closed",
        "user": "JserWF",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-07-01T09:16:14+00:00",
        "updated_at": "2024-02-06T03:13:35+00:00",
        "closed_at": "2024-02-06T03:13:35+00:00",
        "comments_count": [
            "NHZlX"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 22,
        "title": "在jetson nano上用paddle1.6.3跑demo的问题",
        "body": "在跑yolov3使用python接口的demo时遇到的问题\r\n由于paddle版本低于你们要求的，而且有API不支持，我自己修改了infer_yolov3.py部分代码。全在main（）里面\r\n代码修改入下：\r\nif __name__ == '__main__':\r\n    args = parse_args()\r\n    predictor = create_predictor(args)\r\n    img_name = 'kite.jpg'\r\n    save_img_name = 'res.jpg'\r\n    im_size = 608\r\n\r\n    img = cv2.imread(img_name)\r\n    data = preprocess(img, im_size)\r\n    im_shape = np.array([im_size, im_size]).reshape((1, 2)).astype(np.int32)\r\n\r\n    img_data = PaddleTensor(data)#1.6.3 supported\r\n    img_shape = PaddleTensor(im_shape)\r\n    result = predictor.run([img_data, img_shape])#1.6.3 supported\r\n    result = result[0]\r\n    result_data = result.as_ndarray()  #return numpy.ndarray\r\n    print(result_data.shape)\r\n    result_data = result_data.tolist()\r\n    print(result_data)\r\n    img = Image.open(img_name).convert('RGB').resize((im_size, im_size))\r\n    draw_bbox(img, result=result_data, save_name=save_img_name)\r\n\r\n在jetson nano结果如下：paddlepaddle-gpu==1.6.3\r\nPlease NOTE: device: 0, CUDA Capability: 53, Driver API Version: 10.0, Runtime API Version: 10.0\r\nI0707 19:41:27.813036 15092 op_compatible_info.cc:201] The default operator required version is missing. Please update the model version.\r\nI0707 19:41:27.813113 15092 analysis_predictor.cc:841] MODEL VERSION: 0.0.0\r\nI0707 19:41:27.813138 15092 analysis_predictor.cc:843] PREDICTOR VERSION: 0.0.0\r\nW0707 19:41:27.813304 15092 analysis_predictor.cc:855]  - Version incompatible (1) batch_norm\r\nW0707 19:41:27.813334 15092 analysis_predictor.cc:855]  - Version incompatible (1) concat\r\nW0707 19:41:27.813356 15092 analysis_predictor.cc:855]  - Version incompatible (1) conv2d\r\nW0707 19:41:27.813376 15092 analysis_predictor.cc:855]  - Version incompatible (1) elementwise_add\r\nW0707 19:41:27.813397 15092 analysis_predictor.cc:855]  - Version incompatible (1) feed\r\nW0707 19:41:27.813417 15092 analysis_predictor.cc:855]  - Version incompatible (1) fetch\r\nW0707 19:41:27.813437 15092 analysis_predictor.cc:855]  - Version incompatible (1) leaky_relu\r\nW0707 19:41:27.813457 15092 analysis_predictor.cc:855]  - Version incompatible (1) multiclass_nms\r\nW0707 19:41:27.813477 15092 analysis_predictor.cc:855]  - Version incompatible (1) nearest_interp\r\nW0707 19:41:27.813495 15092 analysis_predictor.cc:855]  - Version incompatible (1) scale\r\nW0707 19:41:27.813514 15092 analysis_predictor.cc:855]  - Version incompatible (1) transpose2\r\nW0707 19:41:27.813534 15092 analysis_predictor.cc:855]  - Version incompatible (1) yolo_box\r\nW0707 19:41:27.813552 15092 analysis_predictor.cc:144] WARNING: Results may be incorrect! Using same versions between model and lib.\r\n\r\n在使用paddlepaddle1.8.0的虚拟机上运行相同的代码（修改后的），图片预测结果也是错误的，但没有报model和lib版本不一的错误。最后预测结果也不正确，请问这个是因为什么原因啊？\r\n个人感觉是不支持算子的原因？有什么在nano上比较好的解决方法吗？\r\n\r\n谢谢大佬",
        "state": "closed",
        "user": "jedibobo",
        "closed_by": "jedibobo",
        "created_at": "2020-07-08T04:04:12+00:00",
        "updated_at": "2020-07-13T08:15:00+00:00",
        "closed_at": "2020-07-13T08:15:00+00:00",
        "comments_count": [
            "NHZlX",
            "jedibobo",
            "jedibobo",
            "NHZlX",
            "jedibobo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 17,
        "title": "win10使用RT报错",
        "body": "![image](https://user-images.githubusercontent.com/7700464/82880354-e415aa00-9f70-11ea-963c-d678fcf25d91.png)\r\n环境： paddlepaddle-gpu-1.8.1.post107\r\ncuda:9.0\r\n系统：win10\r\npython:3.7\r\n执行这句的时候报错\r\n config.enable_tensorrt_engine(workspace_size = 1<<30,\r\n            max_batch_size=1, min_subgraph_size=5,\r\n            precision_mode=AnalysisConfig.Precision.Float32,\r\n            use_static=False, use_calib_mode=False)\r\n\r\n错误信息：\r\nError: Pass tensorrt_subgraph_pass has not been registered at (D:\\1.8.1\\paddle\\paddle/fluid/framework/ir/pass.h:201)\r\n关掉RT不报错",
        "state": "closed",
        "user": "aixier",
        "closed_by": "NHZlX",
        "created_at": "2020-05-26T08:53:09+00:00",
        "updated_at": "2020-07-10T09:17:03+00:00",
        "closed_at": "2020-07-10T09:17:03+00:00",
        "comments_count": [
            "jiweibo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 31,
        "title": "这里说的 paddle inference 是指用 paddlepaddle/serving 吗？",
        "body": "RT，多谢。",
        "state": "closed",
        "user": "chengweiv5",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-10T06:59:47+00:00",
        "updated_at": "2024-04-16T08:57:14+00:00",
        "closed_at": "2024-04-16T08:57:14+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 33,
        "title": "推理Demo代码问题",
        "body": "Paddle-Inference-Demo/python/resnet50/img_preprocess.py \r\n里面第24行笔误：img = img[int(h_start):int(h_end), int(w_start):int(w_end), :]w_start:w_end, :]\r\n![image](https://user-images.githubusercontent.com/50802787/93709852-66575900-fb74-11ea-84af-287f646d67e2.png)\r\n",
        "state": "closed",
        "user": "Sharpiless",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-20T11:06:55+00:00",
        "updated_at": "2024-04-16T08:57:15+00:00",
        "closed_at": "2024-04-16T08:57:15+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 34,
        "title": "多路视频流部署加速问题",
        "body": "想咨询一下，我们想在一个nvidia-agx上跑两路视频流，有相关多路视频流部署及优化的文档或案例供参考吗，非常感谢",
        "state": "closed",
        "user": "liangruofei",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-10-17T03:19:55+00:00",
        "updated_at": "2024-04-16T08:57:17+00:00",
        "closed_at": "2024-04-16T08:57:17+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 32,
        "title": "Jetson Xavier NX",
        "body": "是否有Jetson NX版本的Paddle Inference？",
        "state": "closed",
        "user": "cszyx666",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-09-15T07:29:57+00:00",
        "updated_at": "2024-02-06T03:13:37+00:00",
        "closed_at": "2024-02-06T03:13:36+00:00",
        "comments_count": [
            "cszyx666",
            "Fitz-Fitz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 38,
        "title": "jetson nano加载TRT推理报错",
        "body": "使用jetpack4.4\r\n模型为PPYOLO\r\n以下是predictor部分设置代码\r\n    def create_predictor(model, params):\r\n            config = AnalysisConfig(model, params)\r\n\r\n            config.enable_use_gpu(100, 0)\r\n             # config.switch_ir_optim(True)\r\n             config.enable_memory_optim()\r\n             config.switch_use_feed_fetch_ops(False)\r\n\r\n    # 开启TensorRT预测，精度为fp32\r\n             config.enable_tensorrt_engine(\r\n                                workspace_size = 1<<30,\r\n                                max_batch_size=1, \r\n                                min_subgraph_size=5,\r\n                                precision_mode=AnalysisConfig.Precision.Float32,\r\n                                use_static=False, \r\n                                use_calib_mode=False\r\n                                )\r\n    \r\n\r\n                 # config.switch_specify_input_names(True)\r\n             predictor = create_paddle_predictor(config)\r\n\r\n    return predictor\r\n\r\n报错内容:\r\n\r\n         W1106 13:08:45.822986 14852 analysis_predictor.cc:578] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI1106 13:08:45.823392 14852 analysis_predictor.cc:139] Profiler is deactivated, and no profiling report will be generated.\r\nI1106 13:08:45.967123 14852 analysis_predictor.cc:952] MODEL VERSION: 1.8.5\r\nI1106 13:08:45.967211 14852 analysis_predictor.cc:954] PREDICTOR VERSION: 2.0.0\r\nI1106 13:08:45.968782 14852 analysis_predictor.cc:449] TensorRT subgraph engine is enabled\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [conv_affine_channel_fuse_pass]\r\n--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]\r\n--- Running IR pass [shuffle_channel_detect_pass]\r\n--- Running IR pass [quant_conv2d_dequant_fuse_pass]\r\n--- Running IR pass [delete_quant_dequant_op_pass]\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v2]\r\n--- Running IR pass [skip_layernorm_fuse_pass]\r\n--- Running IR pass [conv_bn_fuse_pass]\r\nI1106 13:08:49.006705 14852 graph_pattern_detector.cc:100] ---  detected 73 subgraphs\r\n--- Running IR pass [fc_fuse_pass]\r\n--- Running IR pass [tensorrt_subgraph_pass]\r\nI1106 13:08:49.315248 14852 tensorrt_subgraph_pass.cc:115] ---  detect a sub-graph with 28 nodes\r\nI1106 13:08:49.320538 14852 tensorrt_subgraph_pass.cc:321] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nI1106 13:09:46.427413 14852 tensorrt_subgraph_pass.cc:115] ---  detect a sub-graph with 25 nodes\r\nI1106 13:09:46.443456 14852 tensorrt_subgraph_pass.cc:321] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nI1106 13:09:57.191118 14852 tensorrt_subgraph_pass.cc:115] ---  detect a sub-graph with 13 nodes\r\nI1106 13:09:57.195003 14852 tensorrt_subgraph_pass.cc:321] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\n/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import fnmatch, glob, traceback, errno, sys, atexit, locale, imp\r\nTraceback (most recent call last):\r\n  File \"inference_paddle/inference.py\", line 176, in <module>\r\n    '/home/zdhsyb/inference_paddle/inference_model/best_model/__params__'\r\n  File \"inference_paddle/inference.py\", line 95, in create_predictor\r\n    predictor = create_paddle_predictor(config)\r\nRuntimeError: parallel_for failed: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device\r\n\r\n\r\n   ",
        "state": "closed",
        "user": "watcheras",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-06T05:26:04+00:00",
        "updated_at": "2024-04-16T08:57:18+00:00",
        "closed_at": "2024-04-16T08:57:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 42,
        "title": "Error when using Paddle-inference",
        "body": "--------------------------------------------\r\nC++ Call Stacks (More useful to developers):\r\n--------------------------------------------\r\n0   std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > paddle::platform::GetTraceBackString<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, int)\r\n1   paddle::platform::EnforceNotMet::EnforceNotMet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, int)\r\n2   paddle::operators::ConvOp::GetExpectedKernelType(paddle::framework::ExecutionContext const&) const\r\n3   paddle::framework::OperatorWithKernel::ChooseKernel(paddle::framework::RuntimeContext const&, paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const\r\n5   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const\r\n6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)\r\n7   paddle::framework::NaiveExecutor::Run()\r\n8   paddle::AnalysisPredictor::ZeroCopyRun()\r\n\r\n------------------------------------------\r\nPython Call Stacks (More useful to users):\r\n------------------------------------------\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py\", line 2938, in conv2d\r\n    \"data_format\": data_format,\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/mobilenet_v3.py\", line 201, in _conv_bn_layer\r\n    bias_attr=False)\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/mobilenet_v3.py\", line 368, in __call__\r\n    name='conv1')\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/detection/yolo_v3.py\", line 502, in build_net\r\n    feats = self.backbone(image)\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/ppyolo.py\", line 171, in build_net\r\n    model_out = model.build_net(inputs)\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/load_model.py\", line 66, in load_model\r\n    mode='test')\r\n  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/command.py\", line 156, in main\r\n    model = pdx.load_model(args.model_dir, fixed_input_shape)\r\n  File \"/opt/conda/envs/python35-paddle120-env/bin/paddlex\", line 10, in <module>\r\n    sys.exit(main())\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nError: input and filter data type should be consistent\r\n  [Hint: Expected input_data_type == filter_data_type, but received input_data_type:2 != filter_data_type:5.] at (/home/paddle/github/paddle/paddle/fluid/operators/conv_op.cc:172)\r\n  [operator < conv2d_fusion > error]\r\n",
        "state": "closed",
        "user": "Fitz-Fitz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-14T11:42:11+00:00",
        "updated_at": "2024-04-16T08:57:20+00:00",
        "closed_at": "2024-04-16T08:57:20+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 40,
        "title": "基于websocket服务器的部署方案有吗，实时返回这种，谢谢",
        "body": null,
        "state": "closed",
        "user": "Sunyingbin",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-11T01:38:30+00:00",
        "updated_at": "2024-04-16T08:57:19+00:00",
        "closed_at": "2024-04-16T08:57:19+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 35,
        "title": "使用paddle源码编译c++库时出错。",
        "body": "报错内容：\r\n/home/huyutao/paddle/paddle/fluid/framework/unused_var_check.cc: In function ‘void paddle::framework::CheckUnusedVar(const paddle::framework::OperatorBase&, const paddle::framework::Scope&)’:\r\n/home/huyutao/paddle/paddle/fluid/framework/unused_var_check.cc:82:57: error: converting to ‘std::unordered_set<std::basic_string<char> >’ from initializer list would use explicit constructor ‘std::unordered_set<_Value, _Hash, _Pred, _Alloc>::unordered_set(std::unordered_set<_Value, _Hash, _Pred, _Alloc>::size_type, const hasher&, const key_equal&, const allocator_type&) [with _Value = std::basic_string<char>; _Hash = std::hash<std::basic_string<char> >; _Pred = std::equal_to<std::basic_string<char> >; _Alloc = std::allocator<std::basic_string<char> >; std::unordered_set<_Value, _Hash, _Pred, _Alloc>::size_type = long unsigned int; std::unordered_set<_Value, _Hash, _Pred, _Alloc>::hasher = std::hash<std::basic_string<char> >; std::unordered_set<_Value, _Hash, _Pred, _Alloc>::key_equal = std::equal_to<std::basic_string<char> >; std::unordered_set<_Value, _Hash, _Pred, _Alloc>::allocator_type = std::allocator<std::basic_string<char> >]’\r\n   std::unordered_set<std::string> no_need_buffer_ins = {};\r\n                                                         ^\r\nmake[2]: *** [paddle/fluid/framework/CMakeFiles/unused_var_check.dir/unused_var_check.cc.o] Error 1\r\nmake[1]: *** [paddle/fluid/framework/CMakeFiles/unused_var_check.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n\r\n\r\n编译选项：cmake -DFLUID_INFERENCE_INSTALL_DIR=/home/huyutao/paddle/libs -DCMAKE_BUILD_TYPE=Release -DWITH_PYTHON=OFF -DON_INFER=ON -DWITH_GPU=ONWITH_MKL=OFF -DWITH_MKLDNN=OFF -DWITH_XBYAK=ON -DWITH_NV_JETSON=OFF .. && make && make inference_lib_dist\r\n\r\n环境：\r\nGIT COMMIT ID: 1e01335e195d993f3c5c97bed3a15a6f9170acea\r\nWITH_MKL: ON\r\nWITH_MKLDNN: OFF\r\nWITH_GPU: ONWITH_MKL=OFF\r\nCUDA version: 9.2\r\nCUDNN version: v7.6\r\nCXX compiler version: 4.9.2",
        "state": "closed",
        "user": "huyutao3550346",
        "closed_by": "huyutao3550346",
        "created_at": "2020-10-21T01:56:28+00:00",
        "updated_at": "2020-10-22T01:14:06+00:00",
        "closed_at": "2020-10-22T01:14:06+00:00",
        "comments_count": [
            "huyutao3550346"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 41,
        "title": "在nano上跑官方demo的infer_yolov3无法使用GPU加速",
        "body": "use_gpu=1，加载模型全部正常，但是最后报错如下\r\n\r\nW1112 08:38:37,097999 16867 devlce context.cc:237] Please NOTE: device:  0,  CUDA capability: 53 ,Driver API Version : 10.2, Runtime API Version: 10.0\r\nW1112 08:38:37.226699 16867 devlce context.cc:245] devlce:  0, cuDNN Version: 8.0.\r\nW1112 08:38:43.420408 16867 init.cc:209] Warntng: PaddlePaddle catches a fatlure signal, it may not work properly\r\nW1112 08:38:43.420493 16867 init.cc:211] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle\r\nW1112 08:38:43.420511 16867 init.cc:214] The detail failure signal is:\r\n\r\nW1112 08:38:43.420527 16867 init.cc:217] *** Aborted at 1605188323 (unix time) try \"date -d @1605188323\" Lf you are using GNU date ***\r\nW1112 08:38:43.426568 16867 init.cc:217] PC:  @                          0x0(unknown)\r\nW1112 08:38:43 448750 16867 init.cc:217] *** SIGSECV (@0x0) received by PID 16867 (TID 0x7fa0dfd010) from PID 0; stack trace: ***\r\nW1112 08:38:43.455078 16867 init.cc:217]        @          0x7fa0e046c0 ([vdso]+0x6bf) \r\nSegmentation fault (core dumped)",
        "state": "closed",
        "user": "Fitz-Fitz",
        "closed_by": "Fitz-Fitz",
        "created_at": "2020-11-12T14:23:46+00:00",
        "updated_at": "2020-11-16T06:44:02+00:00",
        "closed_at": "2020-11-16T06:44:02+00:00",
        "comments_count": [
            "cryoco",
            "Fitz-Fitz",
            "cryoco",
            "Fitz-Fitz",
            "Fitz-Fitz",
            "cryoco",
            "Fitz-Fitz",
            "cryoco",
            "Fitz-Fitz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 43,
        "title": "无法使用GPU",
        "body": "环境：jetson nano，装的jetpack4.3 ，python3.6.9，cuda10，cudnn7.6，paddle推理库是0.0.0的。跑自己模型的时候使用GPU报错如下：\r\n\r\nW1115 15:58:03.358932  9830 device_context.cc:265] Please NOTE: device: 0, CUDA Capability: 53, Driver API Version: 10.0, Runtime API Version: 10.0\r\nW1115 15:58:03.604619  9830 device_context.cc:273] device: 0, cuDNN Version: 7.6.\r\nW1115 15:58:46.768879  9830 operator.cc:187] elementwise_mul raises an exception thrust::system::system_error, parallel_for failed: too many resources requested for launch\r\nTraceback (most recent call last):\r\n  File \"infer_yolov3.py\", line 85, in <module>\r\n    result = run(pred, [data, im_shape])\r\n  File \"infer_yolov3.py\", line 39, in run\r\n    predictor.zero_copy_run()\r\nRuntimeError: parallel_for failed: too many resources requested for launch\r\n\r\n不使用GPU则无报错",
        "state": "closed",
        "user": "Fitz-Fitz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-11-15T08:06:36+00:00",
        "updated_at": "2024-04-16T08:57:21+00:00",
        "closed_at": "2024-04-16T08:57:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 56,
        "title": "tensorRT想加载已优化的模型无效",
        "body": "将EnableTensorRtEngine中的use_static置成true，得到优化后的模型，请问怎么调用，尝试了多种方法均无效",
        "state": "closed",
        "user": "A1exy",
        "closed_by": "A1exy",
        "created_at": "2020-12-09T11:30:39+00:00",
        "updated_at": "2020-12-22T02:19:37+00:00",
        "closed_at": "2020-12-22T02:19:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 86,
        "title": "Jetson nano pip3 install paddlepaddle-gpu==2.0.0报错",
        "body": "根据官网上命令下载：\r\n![image](https://user-images.githubusercontent.com/58903762/108865137-dc814f80-762d-11eb-91fb-9a3fb3c8052a.png)\r\n\r\n报错：\r\n![image](https://user-images.githubusercontent.com/58903762/108864972-aa6fed80-762d-11eb-9b9a-37d440ce7bd7.png)\r\n",
        "state": "closed",
        "user": "Irvingao",
        "closed_by": "Irvingao",
        "created_at": "2021-02-23T15:20:39+00:00",
        "updated_at": "2021-03-21T09:42:27+00:00",
        "closed_at": "2021-03-21T09:42:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 103,
        "title": "旧版本文档",
        "body": "请问一下，1.8版本的文档在哪查看呢，我切换到release1.8打开的还是2.0的文档",
        "state": "closed",
        "user": "HelloAI-twj",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-10T16:29:37+00:00",
        "updated_at": "2024-04-16T08:57:22+00:00",
        "closed_at": "2024-04-16T08:57:22+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 104,
        "title": "旧版本文档",
        "body": "请问一下，1.8版本的文档在哪查看呢，我切换到release1.8打开的还是2.0的文档",
        "state": "closed",
        "user": "HelloAI-twj",
        "closed_by": "HelloAI-twj",
        "created_at": "2021-03-10T16:29:47+00:00",
        "updated_at": "2021-03-11T01:33:18+00:00",
        "closed_at": "2021-03-11T01:33:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 72,
        "title": "运行golang的示例出错",
        "body": "# _/root/go_projects/PaddleInference/paddle\r\n../paddle/common.go:20:11: fatal error: paddle_c_api.h: No such file or directory\r\n // #include <paddle_c_api.h>\r\n           ^~~~~~~~~~~~~~~~\r\ncompilation terminated.",
        "state": "closed",
        "user": "mervyn81",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2020-12-30T06:34:00+00:00",
        "updated_at": "2024-02-06T03:13:37+00:00",
        "closed_at": "2024-02-06T03:13:37+00:00",
        "comments_count": [
            "xox9001"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 74,
        "title": "Python如何生成TensorRT的校准表",
        "body": " - PaddlePaddle 1.8.5\r\n - windows 10\r\n\r\n我在这个文档上看到将Float32的模型转换成Int8的模型的介绍，请问如何使用Python实现这个生成校准表呢？\r\nhttps://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/performance_improving/inference_improving/paddle_tensorrt_infer.html#a-name-paddle-trt-int8-paddle-trt-int8-a\r\n\r\nhttps://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/ff8091c4ce87c591a7d89b95fa1e1f47ebcc2a5d/python/paddle_trt/infer_trt_ernie.py#L17-L22",
        "state": "closed",
        "user": "yeyupiaoling",
        "closed_by": "yeyupiaoling",
        "created_at": "2020-12-31T09:01:40+00:00",
        "updated_at": "2021-01-06T08:15:19+00:00",
        "closed_at": "2021-01-06T08:15:19+00:00",
        "comments_count": [
            "yeyupiaoling"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 105,
        "title": "建议更新文档",
        "body": "[预测实例C++](https://paddle-inference.readthedocs.io/en/latest/quick_start/cpp_demo.html)\r\n\r\ngithub最新代码库，包含的文件和说明文档已经不一样了",
        "state": "closed",
        "user": "duohappy",
        "closed_by": "duohappy",
        "created_at": "2021-03-13T02:22:10+00:00",
        "updated_at": "2021-04-06T09:35:38+00:00",
        "closed_at": "2021-04-06T09:35:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 106,
        "title": "ernie-varlen  运行报错 TensorRT's tensor input requires at least 2 dimensions, but input read_file_0.tmp_0 has 1 dims.",
        "body": "运行 demo  ernie-varlen \r\nhttps://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/ernie-varlen\r\n\r\nLinux环境：\r\nCentOS Linux release 7.4.1708 (Core) \r\nLinux version 3.10.0-693.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) ) #1 SMP Tue Aug 22 21:09:27 UTC 2017\r\n\r\n前向库：\r\nhttps://paddle-inference-lib.bj.bcebos.com/2.0.0-rc0-gpu-cuda10-cudnn7-avx-mkl/paddle_inference.tgz\r\n模型：\r\ndemo页面自带的 ernie_model_4.tar.gz\r\nTR:\r\nTensorRT-6.0.1.5.CentOS-7.6.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz\r\n编译通过，运行模型出现如下错误：\r\n\r\n./build/ernie_varlen_test --model_dir=./ernie_model_4\r\n\r\n报错：\r\n\r\n> I0316 20:46:29.055225 163963 tensorrt_subgraph_pass.cc:118] ---  detect a sub-graph with 81 nodes\r\n\r\nW0316 20:46:29.058228 163963 tensorrt_subgraph_pass.cc:293] The Paddle lib links the 6015 version TensorRT, make sure the runtime TensorRT you are using is no less than this version, otherwise, there might be Segfault!\r\nI0316 20:46:29.058288 163963 tensorrt_subgraph_pass.cc:329] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nI0316 20:46:29.536973 163963 op_converter.h:187] trt input [matmul_0.tmp_0] dynamic shape info not set, please check and retry.\r\nterminate called after throwing an instance of 'paddle::platform::EnforceNotMet'\r\n  what():  \r\n\r\n>   --------------------------------------\r\n> C++ Traceback (most recent call last):\r\n>   --------------------------------------\r\n0   paddle_infer::CreatePredictor(paddle::AnalysisConfig const&)\r\n1   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n2   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n3   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n4   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n5   paddle::AnalysisPredictor::OptimizeInferenceProgram()\r\n6   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)\r\n7   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)\r\n8   paddle::inference::analysis::IRPassManager::Apply(std::unique_ptr<paddle::framework::ir::Graph, std::default_delete<paddle::framework::ir::Graph> >)\r\n9   paddle::framework::ir::Pass::Apply(paddle::framework::ir::Graph*) const\r\n10  paddle::inference::analysis::TensorRtSubgraphPass::ApplyImpl(paddle::framework::ir::Graph*) const\r\n11  paddle::inference::analysis::TensorRtSubgraphPass::CreateTensorRTOp(paddle::framework::ir::Node*, paddle::framework::ir::Graph*, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> >*) const\r\n12  paddle::inference::tensorrt::OpConverter::ConvertBlockToTRTEngine(paddle::framework::BlockDesc*, paddle::framework::Scope const&, std::vector<std::string, std::allocator<std::string> > const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, paddle::inference::tensorrt::TensorRTEngine*)\r\n13  paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)\r\n14  paddle::platform::GetCurrentTraceBackString()\r\n> ----------------------\r\n> Error Message Summary:\r\n> ----------------------\r\n> InvalidArgumentError: TensorRT's tensor input requires at least 2 dimensions, but input read_file_0.tmp_0 has 1 dims.\r\n  [Hint: Expected shape.size() > 1UL, but received shape.size():1 <= 1UL:1.] (at /paddle/paddle/fluid/inference/tensorrt/engine.h:78)\r\n",
        "state": "closed",
        "user": "ustcdane",
        "closed_by": "ustcdane",
        "created_at": "2021-03-17T02:24:31+00:00",
        "updated_at": "2021-03-17T02:50:11+00:00",
        "closed_at": "2021-03-17T02:49:46+00:00",
        "comments_count": [
            "ustcdane"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 114,
        "title": "使用paddlepaddle的预测库，运行C++ ResNet50图像分类样例，关于TensorRT的使用报错Load symbol getPluginRegistry failed!",
        "body": "在win10系统上使用cuda10.0_cudnn7_avx_mkl_trt6版本的paddlepaddle的预测库，CUDA version:10.0， CUDNN version:7.4。\r\n\r\n设置compile.sh中的USE_TENSORRT=OFF。编译成功，也可以生成可执行文件。但是运行的时候，报错如下：\r\n\r\nYou are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nE0402 09:31:04.486455 19108 port.h:50] Load symbol getPluginRegistry failed.\r\n\r\n如果设置compile.sh中的USE_TENSORRT=ON。编译成功，可以生成可执行文件，并成功运行。\r\n\r\n使用cuda10.0_cudnn7_avx_mkl_trt6版本的paddlepaddle的预测库，编译的时候必须设置USE_TENSORRT=ON吗，TENSORRT的选项不是可选择的吗？",
        "state": "closed",
        "user": "Marium504",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-02T02:06:09+00:00",
        "updated_at": "2024-04-16T08:57:22+00:00",
        "closed_at": "2024-04-16T08:57:22+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 115,
        "title": "能否提供c#用例",
        "body": null,
        "state": "closed",
        "user": "ghost",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-09T06:05:16+00:00",
        "updated_at": "2024-04-16T08:57:23+00:00",
        "closed_at": "2024-04-16T08:57:23+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 112,
        "title": "paddle_inference开启多线程，推理速度反而更慢，推理速度与多线程数量呈反比",
        "body": null,
        "state": "closed",
        "user": "wsy1991",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-03-26T02:16:02+00:00",
        "updated_at": "2024-02-06T03:13:38+00:00",
        "closed_at": "2024-02-06T03:13:38+00:00",
        "comments_count": [
            "wsy1991",
            "Shixiaowei02"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 116,
        "title": "能否来个FasterRCNN的目标检测代码示例吗",
        "body": "能否来个FasterRCNN的目标检测代码示例",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-11T02:33:52+00:00",
        "updated_at": "2024-04-16T08:57:24+00:00",
        "closed_at": "2024-04-16T08:57:24+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 121,
        "title": "paddle-trt中的names与ernie model中的什么对应呢?",
        "body": "请问设置的输入names，与ernie paddle2.0代码中如何对应呢?\r\nnames = [\r\n        \"placeholder_0\", \"placeholder_1\", \"placeholder_2\", \"stack_0.tmp_0\"\r\n    ]\r\n我用ernie的训练了一个Classification，不知道应该如何做输入。\r\nhttps://github.com/PaddlePaddle/ERNIE/blob/develop/demo/finetune_classifier.py    \r\n",
        "state": "closed",
        "user": "lonelydancer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-19T12:01:50+00:00",
        "updated_at": "2024-04-16T08:57:25+00:00",
        "closed_at": "2024-04-16T08:57:25+00:00",
        "comments_count": [
            "CCZGG",
            "lonelydancer"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 119,
        "title": "InvalidArgumentError: The input's dimension of Operator(Conv2DFusion) is expected to be 4. But received: input's dimension = 2, shape = [1, 2].",
        "body": "When I run this：\r\n\r\n> python infer_yolov3.py --model_file=./yolov3_infer/__model__ --params_file=./yolov3_infer/__params__ --use_gpu=1\r\n\r\nthe cmd show：\r\n>W0413 15:44:13.015229 17512 analysis_predictor.cc:677] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\nI0413 15:44:13.015229 17512 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_graph_clean_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [is_test_pass]e[0m\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [conv_affine_channel_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_bn_fuse_pass]e[0m\r\nI0413 15:44:13.586211 17512 graph_pattern_detector.cc:101] ---  detected 72 subgraphs\r\ne[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]e[0m\r\ne[32m--- Running IR pass [squeeze2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [reshape2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [flatten2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [map_matmul_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m\r\nI0413 15:44:13.997112 17512 graph_pattern_detector.cc:101] ---  detected 75 subgraphs\r\ne[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0m\r\ne[32m--- Running IR pass [runtime_context_cache_pass]e[0m\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\nI0413 15:44:14.019053 17512 ir_params_sync_among_devices_pass.cc:45] Sync params from CPU to GPU\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0413 15:44:14.172643 17512 memory_optimize_pass.cc:200] Cluster name : elementwise_add_20.tmp_0  size: 4096\r\nI0413 15:44:14.172643 17512 memory_optimize_pass.cc:200] Cluster name : elementwise_add_14.tmp_0  size: 2048\r\nI0413 15:44:14.172643 17512 memory_optimize_pass.cc:200] Cluster name : elementwise_add_21.tmp_0  size: 4096\r\nI0413 15:44:14.172643 17512 memory_optimize_pass.cc:200] Cluster name : leaky_relu_51.tmp_0  size: 4096\r\nI0413 15:44:14.173640 17512 memory_optimize_pass.cc:200] Cluster name : leaky_relu_45.tmp_0  size: 4096\r\nI0413 15:44:14.173640 17512 memory_optimize_pass.cc:200] Cluster name : image  size: 12\r\nI0413 15:44:14.173640 17512 memory_optimize_pass.cc:200] Cluster name : elementwise_add_10.tmp_0  size: 1024\r\nI0413 15:44:14.173640 17512 memory_optimize_pass.cc:200] Cluster name : transpose_0.tmp_0  size: 12\r\nI0413 15:44:14.174638 17512 memory_optimize_pass.cc:200] Cluster name : transpose_1.tmp_0  size: 12\r\nI0413 15:44:14.174638 17512 memory_optimize_pass.cc:200] Cluster name : im_size  size: 8\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0413 15:44:14.209545 17512 analysis_predictor.cc:598] ======= optimize end =======\r\nI0413 15:44:14.209545 17512 naive_executor.cc:107] ---  skip [feed], feed -> im_size\r\nI0413 15:44:14.210542 17512 naive_executor.cc:107] ---  skip [feed], feed -> image\r\nI0413 15:44:14.212536 17512 naive_executor.cc:107] ---  skip [elementwise_add_20.tmp_0], fetch -> fetch\r\nW0413 15:44:14.265396 17512 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 10.2, Runtime API Version: 10.2\r\nW0413 15:44:14.279357 17512 device_context.cc:372] device: 0, cuDNN Version: 7.6.\r\nTraceback (most recent call last):\r\n  File \"infer_yolov3.py\", line 91, in <module>\r\n    result = run(pred, [im_shape, data, scale_factor])\r\n  File \"infer_yolov3.py\", line 40, in run\r\n    predictor.run()\r\nValueError: In user code:\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\r\n    attrs=kwargs.get(\"attrs\", None))\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\r\n    return self.main_program.current_block().append_op(*args, **kwargs)\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py\", line 2938, in conv2d\r\n    \"data_format\": data_format,\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/darknet.py\", line 68, in _conv_norm\r\n    bias_attr=False)\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/darknet.py\", line 154, in __call__\r\n    name=self.prefix_name + \"yolo_input\")\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/nets/detection/yolo_v3.py\", line 507, in build_net\r\n    feats = self.backbone(image)\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/ppyolo.py\", line 175, in build_net\r\n    model_out = model.build_net(inputs)\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/load_model.py\", line 82, in load_model\r\n    mode='test')\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/command.py\", line 158, in main\r\n    model = pdx.load_model(args.model_dir, fixed_input_shape)\r\n\r\n    File \"/opt/conda/envs/python35-paddle120-env/bin/paddlex\", line 10, in <module>\r\n    sys.exit(main())\r\n\r\n\r\n    InvalidArgumentError: The input's dimension of Operator(Conv2DFusion) is expected to be 4. But received: input's dimension = 2, shape = [1, 2].\r\n      [Hint: Expected in_dims.size() == 4U, but received in_dims.size():2 != 4U:4.] (at D:\\v2.0.1\\paddle\\paddle\\fluid\\operators\\fused\\conv_fusion_op.cc:77)\r\n      [operator < conv2d_fusion > error]\r\n\r\nI don't know why and how can I solve this problem?",
        "state": "closed",
        "user": "HWH-2019",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-13T07:54:32+00:00",
        "updated_at": "2024-02-06T03:13:39+00:00",
        "closed_at": "2024-02-06T03:13:39+00:00",
        "comments_count": [
            "jiamingming-004",
            "jiamingming-004",
            "jiamingming-004"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 132,
        "title": "python/yolov3/infer_yolov3.py  能查看推测时间或者速度吗",
        "body": "python/yolov3/infer_yolov3.py  能查看推测时间或者速度吗",
        "state": "closed",
        "user": "Williamlizl",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-19T02:02:43+00:00",
        "updated_at": "2024-04-16T08:57:27+00:00",
        "closed_at": "2024-04-16T08:57:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 141,
        "title": "examples/text_classification/pretrained_models/deploy/python/predict.py 这里的这里有个变量是没有定义的，报错",
        "body": "`  def convert_example(example,\r\n                      tokenizer,\r\n                      label_list,\r\n                      max_seq_length=512,\r\n                      is_test=False):\r\n      text = example\r\n      encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\r\n      input_ids = encoded_inputs[\"input_ids\"]\r\n      segment_ids = encoded_inputs[\"token_type_ids\"]\r\n  \r\n      if not is_test:\r\n          # create label maps\r\n          label_map = {}\r\n          for (i, l) in enumerate(label_list):\r\n              label_map[l] = i\r\n  \r\n          label = label_map[label]        ！！！！！！！这里的label没有定义！！！！！！！！\r\n          label = np.array([label], dtype=\"int64\")\r\n          return input_ids, segment_ids, label\r\n      else:\r\n          return input_ids, segment_ids`\r\n\r\n\r\n",
        "state": "closed",
        "user": "CCZGG",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-26T02:30:28+00:00",
        "updated_at": "2024-04-16T08:57:27+00:00",
        "closed_at": "2024-04-16T08:57:27+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 122,
        "title": "paddle_trt 2.0版本用model跑不起来；在1.8版本代码下能跑通",
        "body": "RT，model采用的是:\r\nwget https://paddle-inference-dist.bj.bcebos.com/inference_demo/Ernie_inference_model.gz\r\n报错\r\nTraceback (most recent call last):\r\n  File \"infer_trt_ernie.py\", line 107, in <module>\r\n    pred = init_predictor(args)\r\n  File \"infer_trt_ernie.py\", line 50, in init_predictor\r\n    predictor = create_predictor(config)\r\nValueError: (InvalidArgument) some trt inputs dynamic shape info not set, check the INFO log above for more details.\r\n  [Hint: Expected all_dynamic_shape_set == true, but received all_dynamic_shape_set:0 != true:1.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:221)",
        "state": "closed",
        "user": "lonelydancer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-04-20T09:16:05+00:00",
        "updated_at": "2024-04-16T08:57:26+00:00",
        "closed_at": "2024-04-16T08:57:26+00:00",
        "comments_count": [
            "lonelydancer"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 143,
        "title": "ernie-varlen 运行报错 Segmentation fault (core dumped)",
        "body": "运行 demo ernie-varlen\r\nhttps://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/ernie-varlen\r\n\r\nLinux环境：\r\nUbuntu18.04\r\ncuda10.2 \r\ncudnn8.1\r\n\r\n模型：\r\ndemo页面自带的 ernie_model_4.tar.gz\r\nTR:\r\nTensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn8.1\r\n编译通过，运行模型出现如下错误：\r\n\r\n./build/ernie_varlen_test --model_dir=./ernie_model_4\r\n\r\n报错：\r\nYou are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0528 13:57:40.816888 19597 analysis_predictor.cc:139] Profiler is deactivated, and no profiling report will be generated.\r\nI0528 13:57:40.850455 19597 analysis_predictor.cc:474] TensorRT subgraph engine is enabled\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [conv_affine_channel_fuse_pass]\r\n--- Running IR pass [adaptive_pool2d_convert_global_pass]\r\n--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]\r\n--- Running IR pass [shuffle_channel_detect_pass]\r\n--- Running IR pass [quant_conv2d_dequant_fuse_pass]\r\n--- Running IR pass [delete_quant_dequant_op_pass]\r\n--- Running IR pass [delete_quant_dequant_filter_op_pass]\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\r\nI0528 13:57:41.143846 19597 graph_pattern_detector.cc:101] ---  detected 1 subgraphs\r\nI0528 13:57:41.145031 19597 graph_pattern_detector.cc:101] ---  detected 1 subgraphs\r\nI0528 13:57:41.150530 19597 graph_pattern_detector.cc:101] ---  detected 25 subgraphs\r\n--- Running IR pass [multihead_matmul_fuse_pass_v2]\r\nI0528 13:57:41.282735 19597 graph_pattern_detector.cc:101] ---  detected 12 subgraphs\r\n--- Running IR pass [skip_layernorm_fuse_pass]\r\nI0528 13:57:41.389559 19597 graph_pattern_detector.cc:101] ---  detected 24 subgraphs\r\n--- Running IR pass [unsqueeze2_eltwise_fuse_pass]\r\n--- Running IR pass [conv_bn_fuse_pass]\r\n--- Running IR pass [squeeze2_matmul_fuse_pass]\r\n--- Running IR pass [reshape2_matmul_fuse_pass]\r\n--- Running IR pass [flatten2_matmul_fuse_pass]\r\n--- Running IR pass [map_matmul_to_mul_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\nI0528 13:57:41.401916 19597 graph_pattern_detector.cc:101] ---  detected 12 subgraphs\r\nI0528 13:57:41.406260 19597 graph_pattern_detector.cc:101] ---  detected 26 subgraphs\r\n--- Running IR pass [conv_elementwise_add_fuse_pass]\r\n--- Running IR pass [tensorrt_subgraph_pass]\r\nI0528 13:57:41.419596 19597 tensorrt_subgraph_pass.cc:126] ---  detect a sub-graph with 82 nodes\r\nW0528 13:57:41.422353 19597 tensorrt_subgraph_pass.cc:304] The Paddle lib links the 7134 version TensorRT, make sure the runtime TensorRT you are using is no less than this version, otherwise, there might be Segfault!\r\nI0528 13:57:41.439488 19597 tensorrt_subgraph_pass.cc:345] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.\r\nSegmentation fault (core dumped)\r\n\r\n",
        "state": "closed",
        "user": "xiaoranchenwai",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-28T06:15:54+00:00",
        "updated_at": "2024-04-16T08:57:28+00:00",
        "closed_at": "2024-04-16T08:57:28+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 144,
        "title": "找不到CMakeLists.txt文件",
        "body": "在 https://paddle-inference.readthedocs.io/en/release-v2.1/demo_tutorial/cuda_windows_demo.html 示例中，在 windows 下编译 x86_linux_demo 时用到了 CMakeLists.txt 文件，但是下载的文件夹中并没有该文件，导致无法编译。",
        "state": "closed",
        "user": "FroyoZzz",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-01T01:57:17+00:00",
        "updated_at": "2024-04-16T08:57:29+00:00",
        "closed_at": "2024-04-16T08:57:29+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 142,
        "title": "请问这个trt部署的时候，有提供序列化的功能吗，还是每次都需要进行优化",
        "body": null,
        "state": "closed",
        "user": "liwang54321",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-05-27T02:11:42+00:00",
        "updated_at": "2024-02-06T03:13:40+00:00",
        "closed_at": "2024-02-06T03:13:40+00:00",
        "comments_count": [
            "YasinFu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 147,
        "title": "编译win10平台源码没找到libpaddle_inference.lib",
        "body": "根据源码编译教程编译win10平台源码，文档说明编译成功的项目结构如下：\r\n```\r\nbuild/paddle_inference_install_dir\r\n├── CMakeCache.txt\r\n├── paddle\r\n│   ├── include\r\n│   │   ├── paddle_anakin_config.h\r\n│   │   ├── paddle_analysis_config.h\r\n│   │   ├── paddle_api.h\r\n│   │   ├── paddle_inference_api.h\r\n│   │   ├── paddle_mkldnn_quantizer_config.h\r\n│   │   └── paddle_pass_builder.h\r\n│   └── lib\r\n│       ├── libpaddle_inference.a (Linux)\r\n│       ├── libpaddle_inference.so (Linux)\r\n│       └── libpaddle_inference.lib (Windows)\r\n├── third_party\r\n│   ├── boost\r\n│   │   └── boost\r\n│   ├── eigen3\r\n│   │   ├── Eigen\r\n│   │   └── unsupported\r\n│   └── install\r\n│       ├── gflags\r\n│       ├── glog\r\n│       ├── mkldnn\r\n│       ├── mklml\r\n│       ├── protobuf\r\n│       ├── xxhash\r\n│       └── zlib\r\n└── version.txt\r\n```\r\n问题1：但是我编译出来的paddle/lib文件夹中没有这个文件libpaddle_inference.lib (Windows)，但是有一个叫paddle_fluid.dll和.lib的文件，这个paddle_fluid就是libpaddle_inference吗？\r\n\r\n编译指令如下：\r\n```\r\ncmake .. -G \"Visual Studio 14 2015\" -A x64 -T host=x64 -DWITH_GPU=ON -DWITH_TESTING=OFF -DON_INFER=ON -DCMAKE_BUILD_TYPE=Release -DPY_VERSION=3  -DWITH_TENSORRT=ON -DTENSORRT_ROOT=\"E:\\\\TensorRT-7.0.0.11\" -DWITH_NCCL=OFF -DCUDA_ARCH_NAME=Turing\r\n```\r\n\r\n问题2：还有编译Paddle-Inference-Demo的paddle_infer_demo项目提示如下，使用的是自己编译出来的预测库。libmklml_intel这个文件在预测库里没有找到。编译出来实则是叫mklml吗？\r\n![image](https://user-images.githubusercontent.com/7108023/122321351-4ebbce00-cf56-11eb-86a5-280646363180.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "a2824256",
        "closed_by": "a2824256",
        "created_at": "2021-06-17T02:28:23+00:00",
        "updated_at": "2021-06-18T02:06:46+00:00",
        "closed_at": "2021-06-18T02:06:46+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 150,
        "title": "python 在for循环中赋值预测结果不正确",
        "body": "# 在fro循环中执行预测结果不正确\r\n## 具体情况\r\n predictor.run第一次循环可以正确运行，在控制台输出`I0624 15:48:29.592471   560 device_context.cc:598] oneDNN v2.2.1`，并且`probs`结果正确，在第二次for循环的时候，predictor.run()运行过后控制台没有输出类似`I0624 15:48:29.592471   560 device_context.cc:598] oneDNN v2.2.1`这样的结果，并且`probs`结果时不正确的，想问下这种情况是什么原因，具体怎么解决。\r\n\r\n## 部分代码\r\n\r\n\r\n### 1. 主要预测部分\r\nreuslt_all_text是一个字符串列表\r\n\r\n```python\r\n   for index in range(0,len(result_all_text)) :\r\n        data=result_all_text[index]\r\n        result=  _predict_text([data],predict)\r\n        print(\"预测内容：{}\\n 预测结果:{}\\n\".format(data,result))\r\n```\r\n\r\n\r\n### 2.`_predict_text()`代码\r\n`ModelPredcit`是我自己简单封装的一个类，方便调用\r\n```python\r\ndef _predict_text(text_list:list,predict:ModelPredict):\r\n    predict.set_input(text_list)\r\n    result=predict.predict_and_get_output()\r\n    return result\r\n```\r\n### 3. 类内部封装代码\r\n下面的三个函数都是封装在`ModelPredcit`类里面的\r\n\r\n#### 3.1 `set_input`\r\n把字符串列表转换成模型可以接受的格式\r\n```python\r\n    def set_input(self,text_list):\r\n        input_segment_tuple_list=[]\r\n        for text in text_list:\r\n            input_ids,segment_ids=self.convert_example(text)\r\n            input_segment_tuple_list.append((input_ids,segment_ids))\r\n        batchify_fn = lambda samples, fn=Tuple(Pad(axis=0, pad_val=self.tokenizer.pad_token_id),\r\n                                               Pad(axis=0, pad_val=self.tokenizer.pad_token_id)): fn(samples)\r\n        input_ids, segment_ids = batchify_fn(input_segment_tuple_list)\r\n\r\n\r\n        # 获取输入的名称\r\n        input_names = self.predictor.get_input_names()\r\n        input_ids_tensor = self.predictor.get_input_handle(input_names[0])\r\n        segment_ids_tensor=self.predictor.get_input_handle(input_names[1])\r\n\r\n        # 设置输入\r\n\r\n        input_ids_tensor.reshape(input_ids.shape)\r\n        segment_ids_tensor.reshape(segment_ids.shape)\r\n        input_ids_tensor.copy_from_cpu(np.array(input_ids))\r\n        segment_ids_tensor.copy_from_cpu(np.array(segment_ids))\r\n```\r\n#### 3.2 `convert_example`\r\n使用paddlenlp 转换字符串为tokenizer\r\n```python\r\n    def convert_example(self,text):\r\n        encoded_inputs=self.tokenizer.encode(text=text,max_seq_len=256)\r\n        input_ids=encoded_inputs[\"input_ids\"]\r\n        segment_ids=encoded_inputs[\"token_type_ids\"]\r\n        return input_ids,segment_ids\r\n```\r\n#### 3.3 `predict_and_get_outpu`\r\n执行预测并输出结果\r\n```python\r\n\r\n    def predict_and_get_output(self)->List:\r\n        self.predictor.run()\r\n        # 获取输出\r\n        output_names = self.predictor.get_output_names()\r\n        output_handle = self.predictor.get_output_handle(output_names[0])\r\n        output_data = output_handle.copy_to_cpu() # numpy.ndarray类型\r\n        probs=functional.softmax(paddle.to_tensor(output_data)).numpy().tolist()\r\n        return probs\r\n```\r\n## 使用的版本\r\n```python\r\nIn [1]: import paddle\r\n\r\nIn [2]: import paddlenlp\r\n\r\nIn [3]: print(paddle.__version__)\r\n2.1.0\r\n\r\nIn [4]: print(paddlenlp.__version__)\r\n2.0.3\r\n```\r\n",
        "state": "closed",
        "user": "ZKBL",
        "closed_by": "ZKBL",
        "created_at": "2021-06-24T08:14:22+00:00",
        "updated_at": "2021-06-24T08:26:58+00:00",
        "closed_at": "2021-06-24T08:26:58+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 152,
        "title": "paddle inference对所有含有transformer模型都能自动加速吗？还是要开发算子相关代码。",
        "body": "你好，我在使用plato模型，请问如果我使用paddle serving，里面的paddle_inference模块会自动做加速吗?\r\n还是需要额外的开发量。",
        "state": "closed",
        "user": "lonelydancer",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-29T09:43:08+00:00",
        "updated_at": "2024-04-16T08:57:31+00:00",
        "closed_at": "2024-04-16T08:57:31+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 151,
        "title": "加载model.pdmodel和model.pdparams提示(InvalidArgument) Deserialize to tensor failed, maybe the loaded file is not a paddle model(expected file format: 0, but 2828338304 found).",
        "body": "paddle预测库是自己编译的，分支是2.0-rc1，编译流程记录在这个[博客文章](https://blog.csdn.net/a2824256/article/details/117806304?spm=1001.2014.3001.5501)\r\nPaddlePaddle/model/paddleCV分支是2.0-beta\r\n\r\n将PaddlePaddle/model/paddleCV的human_pose_estimation项目的权重加载到model之后通过以下两行代码导出静态图模型\r\n\r\n```python\r\n# 在该项目test.py的87行后加入以下代码，87行前已载入预训练参数\r\n# 第87行代码\r\ntest_exe = fluid.ParallelExecutor(\r\n        use_cuda=True if args.use_gpu else False,\r\n        main_program=fluid.default_main_program().clone(for_test=True),\r\n        loss_name=None)\r\n# 添加的代码\r\npaddle.save(fluid.default_main_program(), \"temp/model.pdmodel\")\r\npaddle.save(fluid.default_main_program().state_dict(), \"temp/model.pdparams\")\r\nexit()\r\n```\r\n得到model.pdmodel和model.pdparams文件\r\n\r\n使用如下C++代码加载模型\r\n```c\r\n#include \"paddle/include/paddle_inference_api.h\"\r\n\r\n#include <chrono>\r\n#include <iostream>\r\n#include <memory>\r\n#include <numeric>\r\n\r\n#include <gflags/gflags.h>\r\n#include <glog/logging.h>\r\n\r\nusing paddle_infer::Config;\r\nusing paddle_infer::Predictor;\r\nusing paddle_infer::CreatePredictor;\r\nusing paddle_infer::PrecisionType;\r\n\r\nDEFINE_string(model_file, \"\", \"Directory of the inference model.\");\r\nDEFINE_string(params_file, \"\", \"Directory of the inference model.\");\r\nDEFINE_string(model_dir, \"\", \"Directory of the inference model.\");\r\nDEFINE_int32(batch_size, 1, \"Directory of the inference model.\");\r\n\r\nint main(int argc, char *argv[]) {\r\n\tgoogle::ParseCommandLineFlags(&argc, &argv, true);\r\n\tpaddle_infer::Config config;\r\n\tif (FLAGS_model_dir == \"\") {\r\n\t\tstd::cout << \"SetModel\" << std::endl;\r\n\t\tconfig.SetModel(FLAGS_model_file, FLAGS_params_file);\r\n\t}\r\n\telse {\r\n\t\tconfig.SetModel(FLAGS_model_dir); // Load no-combined model\r\n\t}\r\n\tconfig.EnableUseGpu(500, 0);\r\n\tconfig.SwitchIrOptim(true);\r\n\tconfig.EnableMemoryOptim();\r\n\tstd::shared_ptr<paddle_infer::Predictor> predictor = paddle_infer::CreatePredictor(config);\r\n\tauto input_names = predictor->GetInputNames();\r\n\tauto input_t = predictor->GetInputHandle(input_names[0]);\r\n        // 对应模型输入3x348x348\r\n\tstd::vector<int> input_shape = { 1, 3, 348, 348 };\r\n\tstd::vector<float> input_data(1 * 3 * 348 * 348, 1);\r\n\tinput_t->Reshape(input_shape);\r\n\tinput_t->CopyFromCpu(input_data.data());\r\n\r\n\treturn 0;\r\n}\r\n```\r\n提示以下信息\r\n![image](https://user-images.githubusercontent.com/7108023/123592512-df7c8e80-d81f-11eb-8c24-307d29ad9151.png)\r\n",
        "state": "closed",
        "user": "a2824256",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-06-28T06:53:22+00:00",
        "updated_at": "2024-04-16T08:57:30+00:00",
        "closed_at": "2024-04-16T08:57:30+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 158,
        "title": "paddleinference 2.1.1 C++预测问题",
        "body": "官方给出的预测示例都是用vector<float>给出如下图，现在有几个问题想了解一下：\r\n![image](https://user-images.githubusercontent.com/44053467/125578495-7a499426-9ee8-474e-9315-1b3287c4c3a5.png)\r\n\r\n1、opencv 的Mat类型的数据如何在预测库中使用；\r\n2、在使用前需不需要convertTo(im, CV_32FC3, 1 / 255.0)；\r\n3、在restnet50 demo中需不需要进行resize(im, im,Size(224,224))；\r\n这是我写的代码示例图，我不知道要不要resize要不要connvertto，但是无论我怎么去调整使用，都没办法得到准确的结果，比如画面中能够一只狗，预测的不是狗，换一张图可能结果也是类似的乱的。\r\n![image](https://user-images.githubusercontent.com/44053467/125578624-1c9899f0-4ee0-42d5-a369-2d8347e1db93.png)\r\n",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-07-14T07:09:14+00:00",
        "updated_at": "2024-04-16T08:57:32+00:00",
        "closed_at": "2024-04-16T08:57:32+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 160,
        "title": "C++ Traceback  Segmentation fault",
        "body": "numpy                             1.21.1\r\nopencv-python                     4.5.3.56\r\npackaging                         21.0\r\npaddlepaddle-gpu                  2.1.1\r\npaddleslim                        2.1.0\r\npaddlex                           2.0.0rc4\r\n\r\npython3.8 linux cuda11.2\r\n----------------------\r\n\r\n\r\npython infer_resnet.py --model_file=./resnet50/inference.pdmodel --params_file=./resnet50/inference.pdiparams --use_gpu=1\r\ninfer_resnet.py:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\r\n  if args.model_dir is not \"\":\r\nW0807 09:36:34.106942 12056 analysis_predictor.cc:715] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [is_test_pass]\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [conv_affine_channel_fuse_pass]\r\n--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]\r\n--- Running IR pass [conv_bn_fuse_pass]\r\nI0807 09:36:34.281066 12056 graph_pattern_detector.cc:91] ---  detected 53 subgraphs\r\n--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\r\n--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\r\n--- Running IR pass [multihead_matmul_fuse_pass_v2]\r\n--- Running IR pass [squeeze2_matmul_fuse_pass]\r\n--- Running IR pass [reshape2_matmul_fuse_pass]\r\nI0807 09:36:34.303220 12056 graph_pattern_detector.cc:91] ---  detected 1 subgraphs\r\n--- Running IR pass [flatten2_matmul_fuse_pass]\r\n--- Running IR pass [map_matmul_to_mul_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\nI0807 09:36:34.304824 12056 graph_pattern_detector.cc:91] ---  detected 1 subgraphs\r\n--- Running IR pass [fc_elementwise_layernorm_fuse_pass]\r\n--- Running IR pass [conv_elementwise_add_act_fuse_pass]\r\nI0807 09:36:34.314018 12056 graph_pattern_detector.cc:91] ---  detected 33 subgraphs\r\n--- Running IR pass [conv_elementwise_add2_act_fuse_pass]\r\nI0807 09:36:34.322469 12056 graph_pattern_detector.cc:91] ---  detected 16 subgraphs\r\n--- Running IR pass [conv_elementwise_add_fuse_pass]\r\nI0807 09:36:34.325784 12056 graph_pattern_detector.cc:91] ---  detected 4 subgraphs\r\n--- Running IR pass [transpose_flatten_concat_fuse_pass]\r\n--- Running IR pass [runtime_context_cache_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\nI0807 09:36:34.328943 12056 ir_params_sync_among_devices_pass.cc:45] Sync params from CPU to GPU\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\nI0807 09:36:34.364941 12056 memory_optimize_pass.cc:199] Cluster name : inputs  size: 602112\r\nI0807 09:36:34.364956 12056 memory_optimize_pass.cc:199] Cluster name : relu_1.tmp_0  size: 3211264\r\nI0807 09:36:34.364959 12056 memory_optimize_pass.cc:199] Cluster name : batch_norm_12.tmp_3  size: 401408\r\nI0807 09:36:34.364962 12056 memory_optimize_pass.cc:199] Cluster name : batch_norm_4.tmp_2  size: 3211264\r\nI0807 09:36:34.364964 12056 memory_optimize_pass.cc:199] Cluster name : relu_0.tmp_0  size: 3211264\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI0807 09:36:34.386811 12056 analysis_predictor.cc:636] ======= optimize end =======\r\nI0807 09:36:34.386862 12056 naive_executor.cc:98] ---  skip [feed], feed -> inputs\r\nI0807 09:36:34.389506 12056 naive_executor.cc:98] ---  skip [batch_norm_4.tmp_2], fetch -> fetch\r\nW0807 09:36:34.405136 12056 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 10.2\r\nW0807 09:36:34.412685 12056 device_context.cc:422] device: 0, cuDNN Version: 8.1.\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::framework::SignalHandle(char const*, int)\r\n1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Segmentation fault` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1628300196 (unix time) try \"date -d @1628300196\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGSEGV (@0x0) received by PID 12056 (TID 0x7f23e6a2c100) from PID 0 ***]\r\n\r\n段错误\r\n",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-08-07T01:40:01+00:00",
        "updated_at": "2024-04-16T08:57:33+00:00",
        "closed_at": "2024-04-16T08:57:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 169,
        "title": "预测时参数无法加载",
        "body": "您好，我在Jeston Nano上跑yolov3的示例预测代码时，遇到了参数无法加载的问题。进程一直卡在Sync params from CPU to GPU,但是我查看内存使用情况，4.1G的Memory只使用了2.6G，然后一直不变，请问一下这是什么原因呢",
        "state": "closed",
        "user": "HelloAI-twj",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-20T11:12:18+00:00",
        "updated_at": "2024-04-16T08:57:34+00:00",
        "closed_at": "2024-04-16T08:57:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 176,
        "title": "python的demo文件夹下没有ernie的推理demo",
        "body": "python文件下ernie的demo呢？不是说有的？",
        "state": "closed",
        "user": "tianchiguaixia",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-12T05:42:39+00:00",
        "updated_at": "2024-04-16T08:57:34+00:00",
        "closed_at": "2024-04-16T08:57:34+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 170,
        "title": "jetson nx上部署paddleseg框架下biesenetV2算法训练自己的数据集产生的模型，推理速度太慢.预测程序用的是paddleseg中的C++部署示例，",
        "body": "![image](https://user-images.githubusercontent.com/71338215/134484572-4882e93c-c196-49b7-8dd9-292bacdfaa77.png)\r\n这是我参考的例程，\r\n这是我的推理时间：\r\n![image](https://user-images.githubusercontent.com/71338215/134484701-1283d3d7-4360-488c-8b59-e7f28c149d8f.png)\r\n以下是全部代码：\r\n\r\n#include <algorithm>\r\n#include <chrono>\r\n#include <iostream>\r\n#include <fstream>\r\n#include <numeric>\r\n#include <time.h>\r\n#include <gflags/gflags.h>\r\n#include <glog/logging.h>\r\nusing namespace std;\r\n\r\n#include \"paddle/include/paddle_inference_api.h\"\r\n#include \"yaml-cpp/yaml.h\"\r\n#include \"opencv2/core.hpp\"\r\n#include \"opencv2/imgproc.hpp\"\r\n#include \"opencv2/highgui.hpp\"\r\n\r\n\r\nDEFINE_string(model_dir, \"\", \"Directory of the inference model. \"\r\n                             \"It constains deploy.yaml and infer models\");\r\nDEFINE_string(img_path, \"\", \"Path of the test image.\");\r\nDEFINE_bool(use_cpu, false, \"Wether use CPU. Default: use GPU.\");\r\nDEFINE_bool(use_trt, false, \"Wether enable TensorRT when use GPU. Defualt: false.\");\r\nDEFINE_bool(use_mkldnn, false, \"Wether enable MKLDNN when use CPU. Defualt: false.\");\r\nDEFINE_string(save_dir, \"\", \"Directory of the output image.\");\r\n\r\ntypedef struct YamlConfig {\r\n  std::string model_file;\r\n  std::string params_file;\r\n  bool is_normalize;\r\n}YamlConfig;\r\n\r\nYamlConfig load_yaml(const std::string& yaml_path) {\r\n  YAML::Node node = YAML::LoadFile(yaml_path);\r\n  std::string model_file = node[\"Deploy\"][\"model\"].as<std::string>();\r\n  std::string params_file = node[\"Deploy\"][\"params\"].as<std::string>();\r\n  bool is_normalize = false;\r\n  if (node[\"Deploy\"][\"transforms\"] &&\r\n    node[\"Deploy\"][\"transforms\"][0][\"type\"].as<std::string>() == \"Normalize\") {\r\n      is_normalize = true;\r\n  }\r\n\r\n  YamlConfig yaml_config = {model_file, params_file, is_normalize};\r\n  return yaml_config;\r\n}\r\n\r\nstd::shared_ptr<paddle_infer::Predictor> create_predictor(const YamlConfig& yaml_config) {\r\n  std::string& model_dir = FLAGS_model_dir;\r\n\r\n  paddle_infer::Config infer_config;\r\n  infer_config.SetModel(model_dir + \"/\" + yaml_config.model_file,\r\n                  model_dir + \"/\" + yaml_config.params_file);\r\n  infer_config.EnableMemoryOptim();\r\n\r\n  if (FLAGS_use_cpu) {\r\n    LOG(INFO) << \"Use CPU\";\r\n    if (FLAGS_use_mkldnn) {\r\n      // TODO(jc): fix the bug\r\n      //infer_config.EnableMKLDNN();\r\n      infer_config.SetCpuMathLibraryNumThreads(5);\r\n    }\r\n  } else {\r\n    LOG(INFO) << \"Use GPU\";\r\n    infer_config.EnableUseGpu(500, 0);\r\n    if (FLAGS_use_trt) {\r\n      infer_config.EnableTensorRtEngine(1 << 30, 1, 1,\r\n        paddle_infer::PrecisionType::kFloat32, false, false);\r\n    }\r\n  }\r\n\r\n  auto predictor = paddle_infer::CreatePredictor(infer_config);\r\n  return predictor;\r\n}\r\n\r\nvoid hwc_img_2_chw_data(const cv::Mat& hwc_img, float* data) {\r\n  int rows = hwc_img.rows;\r\n  int cols = hwc_img.cols;\r\n  int chs = hwc_img.channels();\r\n  for (int i = 0; i < chs; ++i) {\r\n    cv::extractChannel(hwc_img, cv::Mat(rows, cols, CV_32FC1, data + i * rows * cols), i);\r\n  }\r\n}\r\n\r\ncv::Mat read_process_image(bool is_normalize) {\r\n  cv::Mat img = cv::imread(FLAGS_img_path, cv::IMREAD_COLOR);\r\n  cv::cvtColor(img, img, cv::COLOR_BGR2RGB);\r\n  if (is_normalize) {\r\n    img.convertTo(img, CV_32F, 1.0 / 255, 0);\r\n    img = (img - 0.5) / 0.5;\r\n  }\r\n  return img;\r\n}\r\n\r\n\r\nint main(int argc, char *argv[]) {\r\n  google::ParseCommandLineFlags(&argc, &argv, true);\r\n  if (FLAGS_model_dir == \"\") {\r\n    LOG(FATAL) << \"The model_dir should not be empty.\";\r\n  }\r\n\r\n  // Load yaml\r\n  std::string yaml_path = FLAGS_model_dir + \"/deploy.yaml\";\r\n  YamlConfig yaml_config = load_yaml(yaml_path);\r\n\r\n  // Prepare data\r\n  cv::Mat img = read_process_image(yaml_config.is_normalize);\r\n  int rows = img.rows;\r\n  int cols = img.cols;\r\n  int chs = img.channels();\r\n  std::vector<float> input_data(1 * 3 * 1080 *1920, 0.0f);\r\n  hwc_img_2_chw_data(img, input_data.data());\r\n\r\n  // Create predictor\r\n  auto predictor = create_predictor(yaml_config);\r\n\r\n  // Set input\r\n  auto input_names = predictor->GetInputNames();\r\n  auto input_t = predictor->GetInputHandle(input_names[0]);\r\n  std::vector<int> input_shape = {1, 3, 1080, 1920};\r\n  input_t->Reshape(input_shape);\r\n  input_t->CopyFromCpu(input_data.data());\r\n\r\n  // Run\r\n  clock_t start,end;\r\n  start=clock();\r\n  predictor->Run();\r\n  end=clock();\r\n  cout << \"The run time is:\" << (double)(end-start)/CLOCKS_PER_SEC << \"s\" << endl;\r\n  // Get output\r\n  auto output_names = predictor->GetOutputNames();\r\n  auto output_t = predictor->GetOutputHandle(output_names[0]);\r\n  std::vector<int> output_shape = output_t->shape();  // n * h * w\r\n  int out_num = std::accumulate(output_shape.begin(), output_shape.end(), 1,\r\n                                std::multiplies<int>());\r\n  std::vector<int64_t> out_data(out_num);\r\n  output_t->CopyToCpu(out_data.data());\r\n\r\n  // Get pseudo image\r\n  std::vector<uint8_t> out_data_u8(out_num);\r\n  for (int i = 0; i < out_num; i++) {\r\n    out_data_u8[i] = static_cast<uint8_t>(out_data[i]);\r\n  }\r\n  cv::Mat out_gray_img(output_shape[1], output_shape[2], CV_8UC1, out_data_u8.data());\r\n  cv::Mat out_eq_img;\r\n  cv::equalizeHist(out_gray_img, out_eq_img);\r\n  cv::imwrite(\"out_img.jpg\", out_eq_img);\r\n  \r\n  LOG(INFO) << \"Finish\";\r\n}\r\n paddle 版本2.1.2：\r\n![image](https://user-images.githubusercontent.com/71338215/134484945-9dd18176-9116-4548-b7c9-e194f17300c0.png)\r\nnx 环境：\r\n![image](https://user-images.githubusercontent.com/71338215/134485007-6d422285-77d7-41af-93c3-ce22856a3660.png)\r\n输入图像大小为1080*1920。",
        "state": "closed",
        "user": "huangwan-jiayi",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-09-23T09:33:14+00:00",
        "updated_at": "2025-03-25T06:44:35+00:00",
        "closed_at": "2025-03-25T06:44:35+00:00",
        "comments_count": [
            "funny000"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 175,
        "title": "C++预测demo执行报错：\"./build/yolov3_test: error while loading shared libraries: libdnnl.so.2\"",
        "body": "环境：Ubuntu18.04\r\nCUDA：10.2\r\ncudnn：8.1\r\nTensorRT：7.2.3\r\nPaddle inference：2.1.1预测库（编译好的）\r\n\r\n---\r\n编译成功但是执行时报错：不存在libdnnl链接库，但是该链接库就在mkldnn目录下。\r\n\r\n![image](https://user-images.githubusercontent.com/58903762/136894461-966f4cc4-fc1d-495f-aecb-ba2f6cb90296.png)\r\n\r\n`libdnnl`路径：\r\n\r\n![image](https://user-images.githubusercontent.com/58903762/136913451-1c279a82-eee0-4705-993f-dda0587e84d3.png)\r\n\r\n\r\n",
        "state": "closed",
        "user": "Irvingao",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-12T05:01:16+00:00",
        "updated_at": "2024-11-29T11:17:11+00:00",
        "closed_at": "2024-02-06T03:13:41+00:00",
        "comments_count": [
            "StephensonDing",
            "cerasumat"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 179,
        "title": "C#部署inference结果不一致",
        "body": "同一个inference在C#部署环境下与paddle环境下运行的结果不一致，请问这种情况是什么原因导致的？",
        "state": "closed",
        "user": "Hold-on-li",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-10-22T01:02:58+00:00",
        "updated_at": "2024-04-16T08:57:35+00:00",
        "closed_at": "2024-04-16T08:57:35+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 188,
        "title": "win10  paddle2.0 生成vs解决方案报错：",
        "body": "按照官网的方法，cmake 的源目录为：Paddle-Inference-Demo-master/c++/x86_linux_demo\r\n\r\n然后点击Configure\r\n报错：\r\nCMake Error: The source directory \"D:/qing/paddleWork/inference/Paddle-Inference-Demo-master/c++/x86_linux_demo\" does not appear to contain CMakeLists.txt.\r\nSpecify --help for usage, or press the help button on the CMake GUI.",
        "state": "closed",
        "user": "qing130",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-16T01:00:48+00:00",
        "updated_at": "2024-04-16T08:57:36+00:00",
        "closed_at": "2024-04-16T08:57:36+00:00",
        "comments_count": [
            "qing130",
            "qing130",
            "qing130",
            "qing130",
            "AlisaChen98",
            "qing130",
            "qing130"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 189,
        "title": "教程中的两个windows部署示例有何区别？",
        "body": "第一个部署示例：https://paddle-inference.readthedocs.io/en/latest/demo_tutorial/x86_windows_demo.html\r\n第二个部署示例：https://paddle-inference.readthedocs.io/en/latest/demo_tutorial/cuda_windows_demo.html#c\r\n\r\n是否是第一个例子只跑CPU？",
        "state": "closed",
        "user": "qing130",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2021-11-16T01:24:34+00:00",
        "updated_at": "2024-02-06T03:13:42+00:00",
        "closed_at": "2024-02-06T03:13:42+00:00",
        "comments_count": [
            "BeCoolMaker"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 214,
        "title": "paddle2.0部署预测时单条数据有结果，多条数据出core",
        "body": "使用的是mxps框架，部署预测服务时predict出core\r\n\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55\r\n#1  0x00007f94baf8f2d9 in __GI_abort () at abort.c:89\r\n#2  0x00007f94bb2c9963 in ?? () from /opt/compiler/gcc-8.2/lib/libstdc++.so.6\r\n#3  0x00007f94bb2cf9a6 in ?? () from /opt/compiler/gcc-8.2/lib/libstdc++.so.6\r\n#4  0x00007f94bb2cf9e1 in std::terminate() () from /opt/compiler/gcc-8.2/lib/libstdc++.so.6\r\n#5  0x00007f94bb2cfc13 in __cxa_throw () from /opt/compiler/gcc-8.2/lib/libstdc++.so.6\r\n#6  0x00007f94f8385745 in paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&) [clone .cold.1673] () from ./extend/lib/libpaddle_inference.so\r\n#7  0x00007f94f897ab80 in paddle::framework::NaiveExecutor::Run() () from ./extend/lib/libpaddle_inference.so\r\n#8  0x00007f94f86d5867 in paddle::AnalysisPredictor::Run(std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> > const&, std::vector<paddle::PaddleTensor, std::allocator<paddle::PaddleTensor> >*, int) () from ./extend/lib/libpaddle_inference.so\r\n#9  0x00000000013215cf in ie::PaddleFluidExPredictor::predict (this=0x8269b10, inputs=..., outputs=...)\r\n    at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/bits/unique_ptr.h:342\r\n#10 0x0000000000d073d4 in mxps::PredictorWorker::handle_task (this=0x9a159c0, context=0x25900280, agent=0x28370300, tm=...)\r\n    at baidu/ps/mxps/src/module/mxps/predict/predictor_accessor.cpp:138\r\n#11 0x0000000000c9898d in std::__invoke_impl<int, int (mxps::PredictorWorker::*&)(baidu::envoy::Context*, baidu::envoy::Agent*, mxps::TimerRecorder), mxps::PredictorWorker*, baidu::envoy::Context*&, mxps::PredictAgent*&, mxps::TimerRecorder> (__f=<optimized out>,\r\n    __f=<optimized out>, __t=<optimized out>) at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/bits/invoke.h:89\r\n#12 std::__invoke<int (mxps::PredictorWorker::*&)(baidu::envoy::Context*, baidu::envoy::Agent*, mxps::TimerRecorder), mxps::PredictorWorker*, baidu::envoy::Context*&, mxps::PredictAgent*&, mxps::TimerRecorder> (__fn=<optimized out>)\r\n    at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/bits/invoke.h:95\r\n#13 std::_Bind<int (mxps::PredictorWorker::*(std::_Placeholder<1>, baidu::envoy::Context*, mxps::PredictAgent*, std::_Placeholder<2>))(baidu::envoy::Context*, baidu::envoy::Agent*, mxps::TimerRecorder)>::__call<int, mxps::PredictorWorker*&&, mxps::TimerRecorder&&, 0ul, 1ul, 2ul, 3ul>(std::tuple<mxps::PredictorWorker*&&, mxps::TimerRecorder&&>&&, std::_Index_tuple<0ul, 1ul, 2ul, 3ul>) (\r\n    __args=..., this=<optimized out>) at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/functional:400\r\n#14 std::_Bind<int (mxps::PredictorWorker::*(std::_Placeholder<1>, baidu::envoy::Context*, mxps::PredictAgent*, std::_Placeholder<2>))(baidu::envoy::Context*, baidu::envoy::Agent*, mxps::TimerRecorder)>::operator()<mxps::PredictorWorker*, mxps::TimerRecorder, int>(mxps::PredictorWorker*&&, mxps::TimerRecorder&&) (this=<optimized out>)\r\n    at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/functional:484\r\n#15 std::_Function_handler<void (mxps::PredictorWorker*, mxps::TimerRecorder), std::_Bind<int (mxps::PredictorWorker::*(std::_Placeholder<1>, baidu::envoy::Context*, mxps::PredictAgent*, std::_Placeholder<2>))(baidu::envoy::Context*, baidu::envoy::Agent*, mxps::TimerRecorder)> >::_M_invoke(std::_Any_data const&, mxps::PredictorWorker*&&, mxps::TimerRecorder&&) (__functor=...,\r\n    __args#0=<optimized out>, __args#1=...) at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/bits/std_function.h:297\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#16 0x0000000000d05d1e in std::function<void (mxps::PredictorWorker*, mxps::TimerRecorder)>::operator()(mxps::PredictorWorker*, mxps::TimerRecorder) const (__args#1=..., __args#0=<optimized out>, this=0x7f93f3fd9ec0)\r\n    at /home/opt/compiler/gcc-8.2/gcc-8.2/include/c++/8.2.0/bits/std_function.h:682\r\n#17 mxps::PredictorWorker::run_task (this=0x9a159c0) at baidu/ps/mxps/src/module/mxps/predict/predictor_accessor.cpp:106\r\n#18 0x00007f94bb2f916f in ?? () from /opt/compiler/gcc-8.2/lib/libstdc++.so.6\r\n#19 0x00007f9531419da4 in start_thread (arg=<optimized out>) at pthread_create.c:333\r\n#20 0x00007f94bb05a32d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109",
        "state": "closed",
        "user": "jemmryx",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-20T03:22:02+00:00",
        "updated_at": "2024-04-16T08:57:37+00:00",
        "closed_at": "2024-04-16T08:57:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 213,
        "title": "paddle_inference部署demo没有完整的流程体现",
        "body": "1、paddle_inference c++ 部署均是直接赋值模拟数据，对于opencv加载图片，前后处理，赋值等等操作不明确，对于新手直接劝退。\r\n2、paddle_inference c++，python 部署demo可以考虑将典型的模型的全流程放一起，比如识别restnet，检测yolo系列 ，RCNN系列，picodet，分割BiseNet，Unet，自然语言LSTM等等系列模型的全流程，新用户看几个，从模型结构分析，模型输出解码等例子就可以明白整体流程如何。\r\n3、模型部署是一大头，模型部署也是paddlepaddle的核心和优势，这一点如果劝退一波人可能不太好。其实对于其它套件paddledet，paddlesg等可以在里面找到部署的代码，但是里面的全流程封装比较高，文件零散，对于新人思路比较杂乱，对于进阶可以，但是对于入门就不太友好。",
        "state": "closed",
        "user": "ChaoII",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-10T02:58:16+00:00",
        "updated_at": "2024-02-06T03:13:43+00:00",
        "closed_at": "2024-02-06T03:13:43+00:00",
        "comments_count": [
            "h1034021643",
            "h1034021643",
            "gg22mm",
            "qinxue123321"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 212,
        "title": "动态链接库(DLL)初始化例程失败 Can not import avx core while this file exists paddle\\fluid\\core_avx.pyd",
        "body": "paddlepaddle_gpu-2.2.1.post112-cp38-cp38-win_amd64.whl\r\nI7 CPU 支持 avx\r\npython 3.8\r\n\r\n```python\r\nimport argparse\r\nimport numpy as np\r\n\r\n# 引用 paddle inference 预测库\r\nimport paddle.inference as paddle_infer\r\n\r\ndef main():\r\n    args = parse_args()\r\n\r\n    # 创建 config\r\n    config = paddle_infer.Config(args.model_file, args.params_file)\r\n\r\n    # 根据 config 创建 predictor\r\n    predictor = paddle_infer.create_predictor(config)\r\n\r\n    # 获取输入的名称\r\n    input_names = predictor.get_input_names()\r\n    input_handle = predictor.get_input_handle(input_names[0])\r\n\r\n    # 设置输入\r\n    fake_input = np.random.randn(args.batch_size, 3, 318, 318).astype(\"float32\")\r\n    input_handle.reshape([args.batch_size, 3, 318, 318])\r\n    input_handle.copy_from_cpu(fake_input)\r\n\r\n    # 运行predictor\r\n    predictor.run()\r\n\r\n    # 获取输出\r\n    output_names = predictor.get_output_names()\r\n    output_handle = predictor.get_output_handle(output_names[0])\r\n    output_data = output_handle.copy_to_cpu() # numpy.ndarray类型\r\n    print(\"Output data size is {}\".format(output_data.size))\r\n    print(\"Output data shape is {}\".format(output_data.shape))\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_file\", type=str, help=\"model filename\")\r\n    parser.add_argument(\"--params_file\", type=str, help=\"parameter filename\")\r\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size\")\r\n    return parser.parse_args()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n```\r\npython demo2.py --model_file F:/paddle2/paddle_model/model.pdmodel --params_file \r\nF:/paddle2/paddle_model/model.pdiparams --batch_size 2\r\n\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW0105 17:48:08.006067 62532 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.\r\nThe TensorRT that Paddle depends on is not configured correctly.\r\n  Suggestions:\r\n  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.\r\n  2. Configure environment variables as follows:\r\n  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`\r\n  - Windows: set PATH by `set PATH=XXX;%PATH%`\r\n  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`\r\nE0105 17:48:08.007102 62532 port.h:50] Load symbol getPluginRegistry failed.\r\n\r\nError: Can not import avx core while this file exists: E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\fluid\\core_avx.pyd\r\n\r\nTraceback (most recent call last):\r\n  File \"demo2.py\", line 5, in <module>\r\n    import paddle.inference as paddle_infer\r\n  File \"E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\__init__.py\", line 25, in <module>\r\n    from .fluid import monkey_patch_variable\r\n  File \"E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\fluid\\__init__.py\", line 36, in <module>\r\n    from . import framework\r\n  File \"E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\fluid\\framework.py\", line 37, in <module>\r\n    from . import core\r\n  File \"E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\fluid\\core.py\", line 294, in <module>\r\n    raise e\r\n  File \"E:\\anaconda3\\envs\\paddleinference\\lib\\site-packages\\paddle\\fluid\\core.py\", line 256, in <module>\r\n    from .core_avx import *\r\nImportError: DLL load failed while importing core_avx: 动态链接库(DLL)初始化例程失败。\r\n```",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-05T09:52:55+00:00",
        "updated_at": "2024-02-06T03:13:43+00:00",
        "closed_at": "2024-02-06T03:13:43+00:00",
        "comments_count": [
            "simple123456T"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 217,
        "title": "如何复现Paddle-Inference的benchmark?",
        "body": "https://paddle-inference.readthedocs.io/en/latest/benchmark/benchmark.html\r\n\r\nlatency和qps是怎么测出来的,请问有相关代码吗",
        "state": "closed",
        "user": "Water2style",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-01-26T09:09:23+00:00",
        "updated_at": "2024-04-16T08:57:38+00:00",
        "closed_at": "2024-04-16T08:57:38+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 223,
        "title": "ValueError: (InvalidArgument) Pass tensorrt_subgraph_pass has not been registered. Please use the paddle inference library compiled with tensorrt or disable the tensorrt engine in inference configuration!",
        "body": "在运行时遇到这个问题该如何解决",
        "state": "closed",
        "user": "bairdxiong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-03-15T13:06:54+00:00",
        "updated_at": "2024-02-06T03:13:44+00:00",
        "closed_at": "2024-02-06T03:13:44+00:00",
        "comments_count": [
            "BeCoolMaker",
            "bairdxiong",
            "BeCoolMaker"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 237,
        "title": "c++在线部署输入的特征中有单值特征和多值特征（如用户序列）的时候，比如一个batch中，每一行的数据格式如[a1,a2,a3,[b1,b2,b3]],  其中b为用户序列，如何设置输入tensor的shape，进行预测，demo中没有",
        "body": null,
        "state": "closed",
        "user": "tianrenheyi09",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-22T09:08:06+00:00",
        "updated_at": "2024-04-16T08:57:39+00:00",
        "closed_at": "2024-04-16T08:57:39+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 226,
        "title": "win平台CPU部署工程生成失败，可能是预测库版本不对？",
        "body": "编译器：MSVC 2022 \r\n环境：windows10\r\nPaddle版本：2.2.2\r\n运行环境：release x64\r\npaddle-inference版本：[c++ cpu_avx_mkl2.2.1](https://paddle-inference.readthedocs.io/en/latest/user_guides/download_lib.html）\r\n\r\n报错信息：\r\n![Uploading 批注 2022-03-26 151901.png…]()\r\n",
        "state": "closed",
        "user": "BeCoolMaker",
        "closed_by": "BeCoolMaker",
        "created_at": "2022-03-26T07:22:47+00:00",
        "updated_at": "2022-04-08T01:15:01+00:00",
        "closed_at": "2022-04-08T01:15:01+00:00",
        "comments_count": [
            "BeCoolMaker",
            "BeCoolMaker"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 239,
        "title": "编译linux x_86出错",
        "body": null,
        "state": "closed",
        "user": "qdd1234",
        "closed_by": "qdd1234",
        "created_at": "2022-04-25T12:02:45+00:00",
        "updated_at": "2022-04-25T12:02:59+00:00",
        "closed_at": "2022-04-25T12:02:59+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 242,
        "title": "关于利用mkldnn进行推断时出现的错误",
        "body": "在模型配置TIPC测试的时候出现以下错误，其他测试运行正常，但是在enable_mkldnn的时候报错。\r\n主要错误信息如下：\r\nRuntimeError: In user code:\r\n\r\n    File \"test_tipc/configs/dsin/to_static.py\", line 102, in <module>\r\n      main(args)\r\n    File \"test_tipc/configs/dsin/to_static.py\", line 97, in main\r\n      save_jit_model(dy_model, model_save_path, prefix='tostatic')\r\n    File \"/home/aistudio/PaddleRec_DSIN-master/tools/utils/save_load.py\", line 38, in save_jit_model\r\n      paddle.jit.save(net, model_prefix)\r\n    File \"<decorator-gen-101>\", line 2, in save\r\n      \r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/jit.py\", line 744, in save\r\n      inner_input_spec)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 517, in concrete_program_specify_input_spec\r\n      *desired_input_spec)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 427, in get_concrete_program\r\n      concrete_program, partial_program_layer = self._program_cache[cache_key]\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 723, in __getitem__\r\n      self._caches[item] = self._build_once(item)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 714, in _build_once\r\n      **cache_key.kwargs)\r\n    File \"<decorator-gen-99>\", line 2, in from_func_spec\r\n      \r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\", line 25, in __impl__\r\n      return wrapped_func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\", line 51, in __impl__\r\n      return func(*args, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py\", line 662, in from_func_spec\r\n      outputs = static_func(*inputs)\r\n    File \"/home/aistudio/PaddleRec_DSIN-master/models/rank/dsin/net.py\", line 326, in forward\r\n      lstm_input[0], lstm_input[1], lstm_input[2], lstm_input[3],\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 917, in __call__\r\n      return self._dygraph_call_func(*inputs, **kwargs)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 907, in _dygraph_call_func\r\n      outputs = self.forward(*inputs, **kwargs)\r\n    File \"/home/aistudio/PaddleRec_DSIN-master/models/rank/dsin/sequence_layers.py\", line 89, in forward\r\n      querys = paddle.tile(querys, [1, keys_length, 1])\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py\", line 347, in __impl__\r\n      attrs={'axis': axis})\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 3184, in append_op\r\n      attrs=kwargs.get(\"attrs\", None))\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\", line 2224, in __init__\r\n      for frame in traceback.extract_stack():\r\n\r\n    NotFoundError: The variable Y is not found when promote complex types.\r\n      [Hint: var should not be null.] (at /paddle/paddle/fluid/framework/operator.cc:1653)\r\n      [operator < elementwise_sub > error]\r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 1, in <module>\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\r\n      exitcode = _main(fd)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\r\n      self = reduction.pickle.load(from_parent)\r\n    File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/synchronize.py\", line 110, in __setstate__\r\n      self._semlock = _multiprocessing.SemLock._rebuild(*state)\r\n  FileNotFoundError: [Errno 2] No such file or directory\r\n请问该如何解决这个错误。",
        "state": "closed",
        "user": "Li-fAngyU",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-04T14:57:23+00:00",
        "updated_at": "2024-04-16T08:57:40+00:00",
        "closed_at": "2024-04-16T08:57:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 244,
        "title": "模型部署相关疑惑",
        "body": "您好，直接使用 老照片上色的demo，发现效果很是不错，想将其转为C++代码，但我在转onnx时发现缺少model.pdmodel文件，\r\n而再寻到paddle-inference-demo，发现均无该实例。求教下一步该如何转",
        "state": "closed",
        "user": "tongchangD",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-10T09:41:48+00:00",
        "updated_at": "2024-04-16T08:57:41+00:00",
        "closed_at": "2024-04-16T08:57:41+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 293,
        "title": "多个模型串联如何进行部署",
        "body": null,
        "state": "closed",
        "user": "janelu9",
        "closed_by": "janelu9",
        "created_at": "2022-06-08T03:02:10+00:00",
        "updated_at": "2022-06-12T14:41:31+00:00",
        "closed_at": "2022-06-12T14:41:22+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 240,
        "title": "编译linux x86 demo报错",
        "body": "您好，我在使用linux x86 demo时候遇到如下错误，但是我的paddle_inference目录下的mkl有libiomp5.so,这该怎么解决\r\n./build/model_test: error while loading shared libraries: libiomp5.so: cannot open shared object file: No such file or directory",
        "state": "closed",
        "user": "qdd1234",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-04-25T12:04:32+00:00",
        "updated_at": "2024-02-06T03:13:45+00:00",
        "closed_at": "2024-02-06T03:13:45+00:00",
        "comments_count": [
            "ANDROIDTODO"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 280,
        "title": "cmake生成vs2022工程，编译报错，无法解析的外部符号 OrtGetApiBase",
        "body": "1、环境：Win10 64位\r\n2、使用Paddle Inference 的c++ sdk进行推理部署\r\n3、用cmake生成vs2022工程，打开工程进行编译，直接报错，链接失败，详细检查过vs的各种配置，发现没有啥问题\r\n\r\n有大神知道是怎么回事吗？谢谢了\r\n\r\n![UUI_{VQEH_$UEQ(4XC5F458](https://user-images.githubusercontent.com/4141550/170921736-95d1e086-e36b-413f-8cc0-1628e3bd729b.png)\r\n",
        "state": "closed",
        "user": "dogpandacat",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-05-30T05:18:44+00:00",
        "updated_at": "2024-02-06T03:13:46+00:00",
        "closed_at": "2024-02-06T03:13:46+00:00",
        "comments_count": [
            "heliqi",
            "ANDROIDTODO"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 331,
        "title": "paddle inference for tensorrt8 无可支持paddlepaddle镜像",
        "body": "rt，目前看到的paddle官方镜像都是装的cudnn 8.1，但tensorrt 8 需要cudnn8.2",
        "state": "closed",
        "user": "frankxyy",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-24T14:31:50+00:00",
        "updated_at": "2024-04-16T08:57:42+00:00",
        "closed_at": "2024-04-16T08:57:42+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 332,
        "title": "Paddle-Inference 是否能按照PaddleDetection,PaddleSeg,PaddleX的模型来",
        "body": "Paddle-Inference 是否能按照PaddleDetection,PaddleSeg,PaddleX,PaddleClas,PaddleOCR的模型来\r\n\r\n直接加载模型目录就行",
        "state": "closed",
        "user": "monkeycc",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-25T01:22:53+00:00",
        "updated_at": "2024-04-16T08:57:43+00:00",
        "closed_at": "2024-04-16T08:57:43+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 319,
        "title": "Windows编译安装错误",
        "body": "1. 编译seq_cls_infer示例工程，采用CMake 生成文件，并下载对应依赖后，（utf8proc）静态库部分符号无法定位；\r\n2. \r\n![image](https://user-images.githubusercontent.com/36881814/177093836-373c5b53-c985-42b8-b195-3ba2459050ab.png)\r\n",
        "state": "closed",
        "user": "Qinhan-Luo",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-04T06:23:34+00:00",
        "updated_at": "2024-04-16T08:57:41+00:00",
        "closed_at": "2024-04-16T08:57:41+00:00",
        "comments_count": [
            "Qinhan-Luo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 335,
        "title": "GPU 下运行 resent50 图像分类样例，使用 Trt dynamic shape 运行样例（以 Fp32 为例）报以下错误。",
        "body": "[] trt input [inputs] dynamic shape info not set, please check and retry.\r\nTraceback (most recent call last):\r\n  File \"infer_resnet.py\", line 135, in <module>\r\n    pred = init_predictor(args)\r\n  File \"infer_resnet.py\", line 64, in init_predictor\r\n    predictor = create_predictor(config)\r\nValueError: (InvalidArgument) some trt inputs dynamic shape info not set, check the INFO log above for more details.\r\n  [Hint: Expected all_dynamic_shape_set == true, but received all_dynamic_shape_set:0 != true:1.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:308)\r\n\r\n这种情况如何解决？",
        "state": "closed",
        "user": "EmmonsCurse",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-27T09:23:42+00:00",
        "updated_at": "2024-04-16T08:57:44+00:00",
        "closed_at": "2024-04-16T08:57:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 316,
        "title": "Windows 运行CPU ResNet50 图像分类样例，使用 OnnxRuntime 运行样例段错误",
        "body": "Hi,\r\n我按照demo在windows上编译好了ResNet50可执行程序后，出现段错误\r\n$ ./resnet50_test.exe --model_file=resnet50/inference.pdmodel --params_file resnet50/inference.pdiparams --use_ort=1\r\nThe given version [10] is not supported, only version 1 to 7 is supported in this build.\r\n[Paddle2ONNX] Use opset_version = 11 for ONNX export.\r\n[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\r\nSegmentation fault\r\n\r\n我使用的paddle inference 版本：2.3.0 https://paddle-inference-lib.bj.bcebos.com/2.3.0/cxx_c/Windows/CPU/x86-64_vs2017_avx_mkl/paddle_inference.zip\r\n请问这个是什么问题呢？\r\n\r\n使用oneDNN 运行样例，可以正常运行",
        "state": "closed",
        "user": "ANDROIDTODO",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-06-22T09:56:27+00:00",
        "updated_at": "2024-02-06T03:13:47+00:00",
        "closed_at": "2024-02-06T03:13:47+00:00",
        "comments_count": [
            "heliqi",
            "ANDROIDTODO",
            "heliqi",
            "ANDROIDTODO",
            "heliqi",
            "ANDROIDTODO",
            "heliqi"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 336,
        "title": "急！在官网上下载的部署whl文件。适配python3.6的whl中部分依赖却要求3.7版本的python才能安装，而在升级python3.7后，原本的whl就提示无法使用",
        "body": "以下是在nano中安装paddlepaddle_gpu-2.3.0-cp36-cp36m-linux_aarch64.whl所提示的错误，一个适配python3.6的文件结果竟然有python3.7才能安装的依赖包？\r\n![JAYC~0X`3XQOP_1@RJ}$(92](https://user-images.githubusercontent.com/79301727/181226664-0ab43511-3d0f-447d-940c-72d103525de6.jpg)而我在升级到3.7之后，原本下载的文件已经没办法使用了！\r\n![5X~`4S5~%523B)RM%ROKF@A](https://user-images.githubusercontent.com/79301727/181227170-2a128021-0f57-4d47-9931-3f9d15916116.jpg)\r\n希望官方尽快解决这个问题，非常急！\r\n\r\n",
        "state": "closed",
        "user": "wzh326",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-27T10:38:16+00:00",
        "updated_at": "2024-04-16T08:57:45+00:00",
        "closed_at": "2024-04-16T08:57:45+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 338,
        "title": "神奇的新问题！我jetpack版本是4.6，pip安装适配4.6的whl结果显示不适用于本设备，pip安装适配4.6.1的反而不报错？",
        "body": "如下图所示：\r\n为什么会出现这种奇怪的情况呢？\r\n![D{0 KWN6`5~4UJB$ZHK}T4P](https://user-images.githubusercontent.com/79301727/181448124-841a1767-a14b-4f98-97f8-0397278de03e.jpg)\r\n![SG(37U3FT0F F1I@NZ{ NLQ](https://user-images.githubusercontent.com/79301727/181448131-efc3ff0e-e8a9-4a60-8a6e-75c98aebb3e3.jpg)\r\n\r\n",
        "state": "closed",
        "user": "wzh326",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-07-28T07:37:42+00:00",
        "updated_at": "2024-04-16T08:57:46+00:00",
        "closed_at": "2024-04-16T08:57:46+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 340,
        "title": "Paddle Inference  我加群了好多次都，没不同意的吗",
        "body": "QQ: 378537908  找到拉我一把",
        "state": "closed",
        "user": "gg22mm",
        "closed_by": "gg22mm",
        "created_at": "2022-07-29T09:16:55+00:00",
        "updated_at": "2022-08-20T03:19:52+00:00",
        "closed_at": "2022-08-20T03:19:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 341,
        "title": "运行自己的图片格式就报错了",
        "body": "`\r\nimport argparse\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt \r\nimport paddle # 后面的示例代码默认已导入 paddle 模块\r\n\r\n (1) 引用 paddle inference 推理库\r\nimport paddle.inference as paddle_infer\r\n\r\nclass MyClass:\r\n    model_file = './resnet50/inference.pdmodel'\r\n    params_file = './resnet50/inference.pdiparams'\r\n    batch_size = 1   \r\nargs = MyClass() # 实例化类 - 获得参数\r\n\r\n\r\n (2) 创建配置对象，并根据需求配置\r\nconfig = paddle_infer.Config(args.model_file, args.params_file)\r\n\r\n (3) 根据Config创建推理对象\r\npredictor = paddle_infer.create_predictor(config)\r\n\r\n (4) 设置\"推理库\" 模型输入 Tensor 方法\r\n\r\n1、打开图片\r\nmypic = r'./1.jpg'    #要分割的图片\r\ncvImageRead = cv2.imread(mypic)  # BGR # Read image\r\ncvImageRead.shape #(500, 375, 3)\r\n\r\n2、把chanl放在最前面\r\nimg = cvImageRead[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\r\nimg.shape #(3, 256, 192)\r\n\r\n 3、图片转成tensor  - paddle 方式\r\nimg_tensor = paddle.to_tensor(img)\r\nimg_tensor = img_tensor.unsqueeze(0) #tensor: Tensor(shape=[1, 3, 349, 337], [[[[246, 245, 249, ..., 241, 239, 243],\r\nimg_tensor.shape #[1, 3, 349, 337]\r\n\r\ninput_names = predictor.get_input_names() #(4) 设置\"推理库\" 模型输入 Tensor 方法\r\nfor i, name in enumerate(input_names):\r\n    \r\n    # paddle-inference 接收 tensor 输入对象\r\n    input_tensor = predictor.get_input_handle(name) # \"推理库\" 模型的 输入 Tensor 方法,下面要用到\r\n    input_tensor.reshape( img_tensor.shape )       #input_tensor.reshape( [1, 3, 349, 337] )\r\n    input_tensor.copy_from_cpu(img_tensor.numpy())        #图片的输入   \r\n    #input_tensor.copy_from_cpu(image)\r\n\r\n (5) 执行推理\r\npredictor.run()\r\n\r\n\r\n (6) 获得推理结果  获取输出\r\noutput_names = predictor.get_output_names()\r\noutput_handle = predictor.get_output_handle(output_names[0])\r\noutput_data = output_handle.copy_to_cpu() # numpy.ndarray类型\r\nprint(\"Output data size is {}\".format(output_data.size))\r\nprint(\"Output data shape is {}\".format(output_data.shape))\r\n\r\n\r\n#image.shape                            #(1, 3, 318, 318)\r\n#img_tensor.numpy().shape      #(1, 3, 349, 337)\r\n#img_tensor.numpy()                #array([[[[246, 245, 249, ..., 241, 239, 243],\r\n#image                                    #array([[[[-1.2639091 , -0.20949996,  0.1612321 , ..., -0.29188657,\r\n\r\n\r\n\r\n---------------------------- 为什么报下面的错？  ----------------------------\r\n\r\n InvalidArgumentError: input and filter data type should be consistent, but received input data type is int and filter type is float\r\n      [Hint: Expected input_data_type == filter_data_type, but received input_data_type:2 != filter_data_type:5.] (at C:/home/workspace/Paddle_release/paddle/fluid/operators/conv_op.cc:185)\r\n      [operator < conv2d > error]\r\n\r\n\r\n\r\n\r\n`",
        "state": "closed",
        "user": "gg22mm",
        "closed_by": "gg22mm",
        "created_at": "2022-07-30T08:35:14+00:00",
        "updated_at": "2022-07-30T11:46:20+00:00",
        "closed_at": "2022-07-30T11:46:20+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 354,
        "title": "什么时候推理出支持 cuda11.6 的 paddle inference 库？",
        "body": "什么时候推出支持 cuda11.6 的 paddle inference 库？目前最高只支持到 cuda11.2\r\n![image](https://user-images.githubusercontent.com/50402380/186322603-216e9e46-1e9f-4af4-81ea-2e5605156e72.png)\r\n",
        "state": "closed",
        "user": "Shrinco",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-08-24T03:32:17+00:00",
        "updated_at": "2024-02-06T02:35:58+00:00",
        "closed_at": "2024-02-06T02:35:58+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 345,
        "title": "Paddle_Inference下C++推理PPHuman模型问题",
        "body": "参见已提连接，麻烦给下意见：\r\n\r\nhttps://github.com/PaddlePaddle/PaddleDetection/issues/6642",
        "state": "closed",
        "user": "danve-fan",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-16T01:53:56+00:00",
        "updated_at": "2024-04-16T08:57:48+00:00",
        "closed_at": "2024-04-16T08:57:47+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 343,
        "title": "yolov7推断速度相对torch增加了30倍 开启tensorrt时报错",
        "body": "```\r\nimport paddle\r\nimport cv2\r\nimport paddle.inference as paddle_infer\r\nimport numpy as np\r\nimport time\r\nfrom ops import multiclass_nms\r\nfrom paddle.inference import PrecisionType\r\nconfig = paddle_infer.Config(\"pd_model2/inference_model/model.pdmodel\",\"pd_model2/inference_model/model.pdiparams\")\r\nconfig.enable_memory_optim()\r\nconfig.enable_use_gpu(1000, 0)\r\nconfig.enable_tensorrt_engine(workspace_size=1 << 30,\r\n                                max_batch_size=1,\r\n                                min_subgraph_size=5,\r\n                                precision_mode=PrecisionType.Float32,\r\n                                use_static=False,\r\n                                use_calib_mode=False)\r\npredictor = paddle_infer.create_predictor(config)\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-e69dc2abcffd> in <module>\r\n     15                                 use_static=False,\r\n     16                                 use_calib_mode=False)\r\n---> 17 predictor = paddle_infer.create_predictor(config)\r\n     18 \r\n     19 input_names = predictor.get_input_names()\r\n\r\nValueError: (InvalidArgument) Pass preln_embedding_eltwise_layernorm_fuse_pass has not been registered.\r\n  [Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:242)\r\n```",
        "state": "closed",
        "user": "janelu9",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-07T03:57:36+00:00",
        "updated_at": "2024-04-16T08:57:47+00:00",
        "closed_at": "2024-04-16T08:57:47+00:00",
        "comments_count": [
            "janelu9",
            "janelu9",
            "wjj19950828",
            "janelu9",
            "janelu9",
            "wjj19950828",
            "janelu9",
            "janelu9",
            "wjj19950828",
            "janelu9",
            "janelu9"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 355,
        "title": "端到端性能",
        "body": "您好，看到官网提供的benchmark都是推理的时间，请问有端到端的性能结果吗？\r\n\r\n毕竟实际使用时包括前后处理，这块的时间估计比推理时间大很多。\r\n\r\n谢谢！",
        "state": "closed",
        "user": "XiaoPengZong",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-08-25T02:17:39+00:00",
        "updated_at": "2024-04-16T08:57:48+00:00",
        "closed_at": "2024-04-16T08:57:48+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 356,
        "title": "变化检测如何传参",
        "body": "我想用paddle inference 的C++ API来做变化检测的推理，变化检测的输入是两张图片，不知道该怎么传参，没找到示例。",
        "state": "closed",
        "user": "julinfn",
        "closed_by": "julinfn",
        "created_at": "2022-08-26T14:29:22+00:00",
        "updated_at": "2022-08-28T01:27:14+00:00",
        "closed_at": "2022-08-28T01:27:14+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 348,
        "title": "Linux 运行 CPU 运行 YOLOv3 图像检测样例，使用 OnnxRuntime 运行样例报错",
        "body": "Hi:\r\n\r\n1. infer_yolov3.py 代码部分并无 OnnxRuntime 支持\r\n2. 执行 `python infer_yolov3.py --model_file=./yolov3_r50vd_dcn_270e_coco/model.pdmodel --params_file=./yolov3_r50vd_dcn_270e_coco/model.pdiparams --use_onnxruntime=1\r\n[ERROR] Cannot found attribute iou_aware in op: yolo_box\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n1   paddle::CheckConvertToONNX(paddle::AnalysisConfig const&)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Process abort signal` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1660880571 (unix time) try \"date -d @1660880571\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGABRT (@0x65b0) received by PID 26032 (TID 0x7fdd24cc5740) from PID 26032 ***]\r\n\r\nAborted (core dumped)`\r\n添加完善后报错，是否为 yolov3 模型并不支持 OnnxRuntime 后端推理？",
        "state": "open",
        "user": "EmmonsCurse",
        "closed_by": null,
        "created_at": "2022-08-19T05:04:44+00:00",
        "updated_at": "2024-03-20T03:31:11+00:00",
        "closed_at": null,
        "comments_count": [
            "funny000",
            "EmmonsCurse"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 367,
        "title": "新提交的文档如何格式化",
        "body": "hi, 我看到向这个 repo 提交新的 PR 时会启动 CI 做一些文档检查 \r\n\r\n`deadlink-check` and `spell-check`\r\n\r\n请问在写这些文档时如何进行格式化呢? 请问有相关的工具/说明吗?",
        "state": "closed",
        "user": "gglin001",
        "closed_by": "gglin001",
        "created_at": "2022-10-10T09:13:19+00:00",
        "updated_at": "2023-02-10T14:06:21+00:00",
        "closed_at": "2023-02-10T14:06:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 372,
        "title": "在python中inference如何使用多个gpu的进行预测",
        "body": "在python中inference如何使用多个gpu的进行预测",
        "state": "closed",
        "user": "zhenzi0322",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2022-10-20T01:34:33+00:00",
        "updated_at": "2024-04-16T08:57:49+00:00",
        "closed_at": "2024-04-16T08:57:49+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 373,
        "title": "能支持下rust绑定吗",
        "body": "能支持下rust绑定吗，出个demo，应该调用c++库就行了",
        "state": "closed",
        "user": "godsoul",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-10-21T07:59:14+00:00",
        "updated_at": "2024-02-05T10:47:54+00:00",
        "closed_at": "2024-02-05T10:47:53+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 407,
        "title": "Paddle训练UNet转ONNX后在Windows上推理速度变慢",
        "body": "我训练完2分类的UNet模型后转为onnx格式，推理速度变慢了很多\r\n# 导出为onnx\r\ndef paddle_onnx(params_path, onnx_path):\r\n    model = UNet(2, 3)\r\n    model.set_state_dict(paddle.load(params_path))\r\n    model.eval()\r\n    input_spec = paddle.static.InputSpec(shape=[None, 3, 512, 512], dtype='float32', name='image')\r\n    paddle.onnx.export(model, onnx_path, input_spec=[input_spec], opset_version=12)\r\n\r\n# 测试\r\nnet = UNet(2, 3)\r\nnet.set_state_dict(paddle.load(\"F:\\AI\\onnx_test\\pd_unet.pdparams\"))\r\nnet.eval()\r\n\r\nmodel = InferenceSession('F:\\AI\\onnx_test\\pd_unet.onnx')\r\nx = np.random.random((1, 3, 512, 512)).astype('float32')\r\nd0 = model.run(output_names=None, input_feed={'image': x})\r\n\r\nstart = time.time()\r\nd0 = model.run(output_names=None, input_feed={'image': x})\r\nend = time.time()\r\nprint('onnx predict time: %.04f s' % (end - start))\r\n\r\nstart = time.time()\r\npaddle_outs = net(paddle.to_tensor(x))\r\nend = time.time()\r\nprint('paddle predict time: %.04f s' % (end - start))\r\n\r\n# 测试结果\r\nonnx predict time: 13.4033 s\r\nW1207 17:16:43.671676 17220 gpu_resources.cc:201] WARNING: device: . The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.0, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\r\npaddle predict time: 5.4835 s",
        "state": "closed",
        "user": "zhiminwang1",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-12-07T09:42:56+00:00",
        "updated_at": "2024-02-05T10:47:21+00:00",
        "closed_at": "2024-02-05T10:47:21+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 379,
        "title": "windows 运行 CPU 运行 YOLOv3 图像检测样例，使用 OnnxRuntime 运行样例报错",
        "body": "windows 运行 CPU c++运行 YOLOv3 图像检测样例，使用 OnnxRuntime 运行样例报错\r\n[ERROR] Cannot found attribute iou_aware in op: yolo_box",
        "state": "closed",
        "user": "zly19540609",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-11-05T03:00:25+00:00",
        "updated_at": "2024-02-05T10:49:37+00:00",
        "closed_at": "2024-02-05T10:49:37+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 417,
        "title": "在jatson xarier nx上，采用Jetpack5.0.2：nv-jetson-cuda11.4-cudnn8.4.1-trt8.4.1-jetpack5.0.2-xavier 预测库，trt_int8 报如下错误：",
        "body": null,
        "state": "closed",
        "user": "zhizhongqu",
        "closed_by": "zhizhongqu",
        "created_at": "2023-01-18T06:02:23+00:00",
        "updated_at": "2023-01-18T06:09:40+00:00",
        "closed_at": "2023-01-18T06:09:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 413,
        "title": "运行完没有结果也没有报错",
        "body": "根据 https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/python/cpu/resnet50 ，换成成了自己训练的模型和图片，跑完没结果呢.....\r\n\r\n```\r\n--- Running analysis [ir_graph_build_pass]\r\n--- Running analysis [ir_graph_clean_pass]\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running IR pass [simplify_with_basic_ops_pass]\r\n--- Running IR pass [layer_norm_fuse_pass]\r\n---    Fused 0 subgraphs into layer_norm op.\r\n--- Running IR pass [attention_lstm_fuse_pass]\r\n--- Running IR pass [seqconv_eltadd_relu_fuse_pass]\r\n--- Running IR pass [seqpool_cvm_concat_fuse_pass]\r\n--- Running IR pass [mul_lstm_fuse_pass]\r\n--- Running IR pass [fc_gru_fuse_pass]\r\n---    fused 0 pairs of fc gru patterns\r\n--- Running IR pass [mul_gru_fuse_pass]\r\n--- Running IR pass [seq_concat_fc_fuse_pass]\r\n--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]\r\n--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]\r\n--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]\r\n--- Running IR pass [matmul_v2_scale_fuse_pass]\r\n--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]\r\nI1220 08:09:16.387981 21846 fuse_pass_base.cc:59] ---  detected 1 subgraphs\r\n--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]\r\n--- Running IR pass [matmul_scale_fuse_pass]\r\n--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]\r\n--- Running IR pass [fc_fuse_pass]\r\nI1220 08:09:16.390695 21846 fuse_pass_base.cc:59] ---  detected 1 subgraphs\r\n--- Running IR pass [repeated_fc_relu_fuse_pass]\r\n--- Running IR pass [squared_mat_sub_fuse_pass]\r\n--- Running IR pass [conv_bn_fuse_pass]\r\nI1220 08:09:16.409376 21846 fuse_pass_base.cc:59] ---  detected 14 subgraphs\r\n--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\r\n--- Running IR pass [conv_transpose_bn_fuse_pass]\r\n--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]\r\n--- Running IR pass [is_test_pass]\r\n--- Running IR pass [constant_folding_pass]\r\n--- Running IR pass [runtime_context_cache_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [memory_optimize_pass]\r\nI1220 08:09:16.427167 21846 memory_optimize_pass.cc:219] Cluster name : batch_norm_25.tmp_0  size: 2048\r\nI1220 08:09:16.427177 21846 memory_optimize_pass.cc:219] Cluster name : x  size: 602112\r\nI1220 08:09:16.427196 21846 memory_optimize_pass.cc:219] Cluster name : hardswish_30.tmp_0  size: 1605632\r\nI1220 08:09:16.427199 21846 memory_optimize_pass.cc:219] Cluster name : depthwise_conv2d_2.tmp_0  size: 802816\r\nI1220 08:09:16.427202 21846 memory_optimize_pass.cc:219] Cluster name : batch_norm_2.tmp_2  size: 1605632\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI1220 08:09:16.439443 21846 analysis_predictor.cc:1314] ======= optimize end =======\r\nI1220 08:09:16.440057 21846 naive_executor.cc:110] ---  skip [feed], feed -> x\r\nI1220 08:09:16.440650 21846 naive_executor.cc:110] ---  skip [softmax_1.tmp_0], fetch -> fetch\r\n```",
        "state": "closed",
        "user": "sunzhaoyang",
        "closed_by": "sunzhaoyang",
        "created_at": "2022-12-20T08:12:45+00:00",
        "updated_at": "2022-12-20T08:23:33+00:00",
        "closed_at": "2022-12-20T08:23:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 380,
        "title": "Paddle-inference推理开发时出现版本错误，请问这里的版本指的是什么。",
        "body": "**环境** win11，vs2019，使用的是windows下c++的预测库[cpu_avx_mkl（2.3.2版本）](https://paddle-inference-lib.bj.bcebos.com/2.3.2/cxx_c/Windows/CPU/x86-64_avx-mkl-vs2017/paddle_inference.zip)\r\n**CmakeList设置：**\r\n`\r\noption(WITH_MKL        \"Compile demo with MKL/OpenBlas support, default use MKL.\"       ON)\r\noption(WITH_GPU        \"Compile demo with GPU/CPU, default use CPU.\"                    OFF)\r\noption(WITH_STATIC_LIB \"Compile demo with static/shared library, default use static.\"   ON)\r\noption(USE_TENSORRT \"Compile demo with TensorRT.\"   OFF)\r\noption(WITH_ROCM \"Compile demo with rocm.\" OFF)\r\n`\r\n**出现错误：**\r\nThe given version [11] is not supported, only version 1 to 7 is supported in this build.\r\n报错位置：\r\n`paddle_infer::Config infer_config;`",
        "state": "closed",
        "user": "xuxinda",
        "closed_by": "xuxinda",
        "created_at": "2022-11-06T10:20:06+00:00",
        "updated_at": "2022-11-21T09:57:53+00:00",
        "closed_at": "2022-11-21T09:57:53+00:00",
        "comments_count": [
            "xuxinda"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 408,
        "title": "请问使用C++ paddle inference推理，是否产生类似.engine的文件，这样下次使用不需要再经历.pdmodel模型转换的过程？",
        "body": "如题",
        "state": "closed",
        "user": "clw5180",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-12-08T10:43:36+00:00",
        "updated_at": "2024-02-05T10:44:02+00:00",
        "closed_at": "2024-02-05T10:44:02+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 412,
        "title": "部署怎样和c++ opencv结合",
        "body": "不是cmake那种编译，是在visualstudio2019上 opencv3.4.6读取图片后，不到怎么把Mat转成input_tensor，还望大佬给点建议",
        "state": "closed",
        "user": "qinxue123321",
        "closed_by": "vivienfanghuagood",
        "created_at": "2022-12-09T12:24:01+00:00",
        "updated_at": "2024-02-05T10:43:51+00:00",
        "closed_at": "2024-02-05T10:43:51+00:00",
        "comments_count": [
            "BrilliantYuKaimin"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 418,
        "title": "在jatson xarier nx上，采用Jetpack5.0.2：nv-jetson-cuda11.4-cudnn8.4.1-trt8.4.1-jetpack5.0.2-xavier 预测库，trt_int8 报错",
        "body": "预测脚本： python3 infer_resnet.py --model_file=./resnet50/inference.pdmodel --params_file=./resnet50/inference.pdiparams --run_mode=trt_int8\r\n\r\n生成校准表时报错如下：\r\nI0117 21:55:26.689865 23205 engine.cc:675] ====== engine info ======\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  (InvalidArgument) thread local var predictor_id_per_thread must be initialized to >= 0, but now predictor_id_per_thread = -1\r\n  [Hint: Expected predictor_id_per_thread > -1, but received predictor_id_per_thread:-1 <= -1:-1.] (at /home/paddle/data/xly/workspace/24116/Paddle/paddle/fluid/inference/tensorrt/engine.h:298)\r\n\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::operators::TensorRTEngineOp::RunCalibration(paddle::framework::Scope const&, phi::Place const&) const::{lambda()#1}::operator()() const\r\n1   paddle::operators::TensorRTEngineOp::PrepareTRTEngine(paddle::framework::Scope const&, paddle::inference::tensorrt::TensorRTEngine*) const\r\n2   paddle::inference::tensorrt::OpConverter::ConvertBlockToTRTEngine(paddle::framework::BlockDesc*, paddle::framework::Scope const&, std::vector<std::string, std::allocator<std::string > > const&, std::unordered_set<std::string, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > > const&, paddle::inference::tensorrt::TensorRTEngine*)\r\n3   paddle::inference::tensorrt::TensorRTEngine::FreezeNetwork()\r\n4   paddle::inference::tensorrt::TensorRTEngine::GetEngineInfo()\r\n5   paddle::inference::tensorrt::TensorRTEngine::context()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Process abort signal` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1674021326 (unix time) try \"date -d @1674021326\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGABRT (@0x3e800005a85) received by PID 23173 (TID 0xffff31ccf1e0) from PID 23173 ***]\r\n",
        "state": "closed",
        "user": "zhizhongqu",
        "closed_by": "zhizhongqu",
        "created_at": "2023-01-18T06:05:00+00:00",
        "updated_at": "2023-01-18T06:10:01+00:00",
        "closed_at": "2023-01-18T06:10:01+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "error": "'NoneType' object has no attribute 'repository'",
        "issue_number": 421
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 420,
        "title": "官方windows c++&c推理库应用程序无法正常启动(0xc000007b)",
        "body": "\r\nCPU版本也是这个问题\r\nPD_Config* config = PD_ConfigCreate();\r\n就这一行代码只要用了推理库就0xC000007b\r\n注释掉就好了\r\n\r\nwin10 \r\nvs2019(v142)",
        "state": "closed",
        "user": "Synmul",
        "closed_by": "Synmul",
        "created_at": "2023-02-17T04:12:41+00:00",
        "updated_at": "2023-02-19T12:37:44+00:00",
        "closed_at": "2023-02-19T12:37:44+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 419,
        "title": "在jetson xavier nx上，采用Jetpack5.0.2：nv-jetson-cuda11.4-cudnn8.4.1-trt8.4.1-jetpack5.0.2-xavier推理库，trt_int8报错",
        "body": "预测脚本：\r\npython3 infer_resnet.py --model_file=./resnet50/inference.pdmodel --params_file=./resnet50/inference.pdiparams --run_mode=trt_int8\r\n\r\n报错如下：\r\nI0117 21:55:26.689865 23205 engine.cc:675] ====== engine info ======\r\nterminate called after throwing an instance of 'phi::enforce::EnforceNotMet'\r\n  what():  (InvalidArgument) thread local var predictor_id_per_thread must be initialized to >= 0, but now predictor_id_per_thread = -1\r\n  [Hint: Expected predictor_id_per_thread > -1, but received predictor_id_per_thread:-1 <= -1:-1.] (at /home/paddle/data/xly/workspace/24116/Paddle/paddle/fluid/inference/tensorrt/engine.h:298)\r\n\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle::operators::TensorRTEngineOp::RunCalibration(paddle::framework::Scope const&, phi::Place const&) const::{lambda()#1}::operator()() const\r\n1   paddle::operators::TensorRTEngineOp::PrepareTRTEngine(paddle::framework::Scope const&, paddle::inference::tensorrt::TensorRTEngine*) const\r\n2   paddle::inference::tensorrt::OpConverter::ConvertBlockToTRTEngine(paddle::framework::BlockDesc*, paddle::framework::Scope const&, std::vector<std::string, std::allocator<std::string > > const&, std::unordered_set<std::string, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > > const&, paddle::inference::tensorrt::TensorRTEngine*)\r\n3   paddle::inference::tensorrt::TensorRTEngine::FreezeNetwork()\r\n4   paddle::inference::tensorrt::TensorRTEngine::GetEngineInfo()\r\n5   paddle::inference::tensorrt::TensorRTEngine::context()\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nFatalError: `Process abort signal` is detected by the operating system.\r\n  [TimeInfo: *** Aborted at 1674021326 (unix time) try \"date -d @1674021326\" if you are using GNU date ***]\r\n  [SignalInfo: *** SIGABRT (@0x3e800005a85) received by PID 23173 (TID 0xffff31ccf1e0) from PID 23173 ***]\r\n",
        "state": "closed",
        "user": "zhizhongqu",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2023-01-18T06:08:43+00:00",
        "updated_at": "2025-04-15T06:42:23+00:00",
        "closed_at": "2025-04-15T06:42:23+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": [
            "bug"
        ]
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 428,
        "title": "The given version [11] is not supported, only version 1 to 10 is supported in this build，有大佬知道原因吗？",
        "body": "![image](https://user-images.githubusercontent.com/97105652/224339808-1b002e8e-32ac-4e34-a159-152aefcbc38c.png)\r\n在Windows11系统上部署CPU版本，下载的CPU版本Paddle_Inference。使用VS2022进行部署，显示上述报错，有大佬知道原因吗？",
        "state": "closed",
        "user": "MiShiDeHaiLuo",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-03-10T14:22:36+00:00",
        "updated_at": "2024-02-05T10:43:22+00:00",
        "closed_at": "2024-02-05T10:43:22+00:00",
        "comments_count": [
            "flytocc"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 427,
        "title": "Paddle-Inference目前有计划double类型吗?",
        "body": "Paddle-Inference目前有计划double类型吗?",
        "state": "closed",
        "user": "ccsuzzh",
        "closed_by": "ccsuzzh",
        "created_at": "2023-03-08T08:34:35+00:00",
        "updated_at": "2023-10-31T09:47:55+00:00",
        "closed_at": "2023-04-25T03:35:13+00:00",
        "comments_count": [
            "GoXian",
            "ccsuzzh",
            "GoXian",
            "ccsuzzh",
            "GoXian",
            "ccsuzzh",
            "GoXian"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 444,
        "title": "能否提供ppocrv3的paddleinference示例代码",
        "body": "单个模型的推理我能看明白，但是两个甚至三个模型串联怎么做，每一个都要单独起吗？我的代码水平不高，希望官方大大可以提供ocr模型串联的示例代码，谢谢",
        "state": "closed",
        "user": "iloveming",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-05-16T07:25:33+00:00",
        "updated_at": "2024-02-05T10:41:52+00:00",
        "closed_at": "2024-02-05T10:41:52+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 432,
        "title": "smoke trt报错",
        "body": "SMOKE用tensorrt推理时出现如下报错\r\n```\r\nE0322 21:59:19.028708  4500 helper.h:114] 4: Output tensor tmp_21795 of type Float produced from output of incompatible type Bool\r\nE0322 21:59:19.028708  4500 helper.h:114] 4: Output tensor tmp_21795 of type Float produced from output of incompatible type Bool\r\nE0322 21:59:19.028708  4500 helper.h:114] 4: Output tensor tmp_21795 of type Float produced from output of incompatible type Bool\r\nE0322 21:59:19.028708  4500 helper.h:114] 4: Output tensor tmp_21795 of type Float produced from output of incompatible type Bool\r\nW0322 21:59:42.592433  4500 helper.h:110] Skipping tactic 0 due to Myelin error: Mismatched type for tensor tmp_21795', f32 vs. expected type:b.\r\nE0322 21:59:42.593430  4500 helper.h:114] 10: [optimizer.cpp::nvinfer1::builder::`anonymous-namespace'::LeafCNode::computeCosts::2033] Error Code 10: Internal Error (Could not find any implementation for node {ForeignNode[equal (Output: tmp_21795)]}.)\r\nE0322 21:59:42.593430  4500 helper.h:114] 2: [builder.cpp::nvinfer1::builder::Builder::buildSerializedNetwork::619] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\r\n```",
        "state": "closed",
        "user": "Lyric0620",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-03-22T14:04:31+00:00",
        "updated_at": "2024-02-05T10:41:33+00:00",
        "closed_at": "2024-02-05T10:41:33+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 430,
        "title": "使用paddle inference创建预测器时显示无法读取模型文件",
        "body": "问题：使用paddle inference创建预测器时显示无法读取模型文件\r\n\r\n报警信息： \r\n![屏幕截图 2023-03-18 122328](https://user-images.githubusercontent.com/125656584/226084780-027308ba-c9dd-4c89-9fb7-c0027ad92b61.png)\r\n\r\n代码如下：\r\n![屏幕截图 2023-03-18 122415](https://user-images.githubusercontent.com/125656584/226084798-6f6200c2-ec78-42bc-9952-d0aab95088a3.png)\r\n\r\n提问之前我已确认：\r\n①python文件路径的三种写法均已尝试但依然报错\r\n②使用模型张量分析代码确认文件可访问\r\n③模型是在百度飞桨模型库下载的模型，根据paddle inference文档，paddle inference支持百度飞浆上面的所有模型\r\n④使用PaddleDetection脚本确认过模型可以运行，只是在使用paddle inference时无法创建\r\n\r\n",
        "state": "closed",
        "user": "amiscolo",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-03-18T04:31:24+00:00",
        "updated_at": "2024-02-05T10:43:06+00:00",
        "closed_at": "2024-02-05T10:43:06+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 438,
        "title": "可否不依赖完整的Paddle",
        "body": "- 考虑到对整个Paddle库的依赖，感觉PaddleInference使用不太友好。\r\n- [onnxruntime](https://pypi.org/project/onnxruntime/#files)推理引擎最大只有6.6M\r\n- [paddlepaddle](https://pypi.org/project/paddlepaddle/#files)最小却有47M\r\n- 是否可以考虑单独出一个推理版的paddle inference呢？",
        "state": "closed",
        "user": "SWHL",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-04-14T01:08:15+00:00",
        "updated_at": "2024-02-05T10:41:23+00:00",
        "closed_at": "2024-02-05T10:41:23+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 449,
        "title": "为啥一个推理库要这么多的第三方依赖",
        "body": "如题，是不是太复杂了点，paddleinference为啥要mkl， mkldnn， onnxruntime，glog，gflags有必要弄这么多吗",
        "state": "closed",
        "user": "holyYodu",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-06-03T02:34:18+00:00",
        "updated_at": "2024-02-05T10:41:09+00:00",
        "closed_at": "2024-02-05T10:41:09+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 473,
        "title": "cenos7 C++编译链接libpaddle_inference.so提示符号未找到",
        "body": "paddle官网下载X86cpu的预编译包，下载resnet50的测试demo，按照教程在centos中编译resnet50的demo，提示“undefined reference to 'paddle::AnalysisConfig::SetModel(std::string const&, std::string const&)”，同样的操作在ubuntu环境中能正常编过。\r\n是不是官方库只支持ubuntu不支持centos7？\r\n用resnet50这个测试模型的demo或者我这边自己写的简单调用一两个padderinference的接口的testmain，都是同样的现象，符号确实在库里面能找到，否则ubuntu也不能编译通过，centos7中链接找的路径用多种方式确认过是这个库，跟ubuntu同一个路径（docker镜像指向同一份文件），怀疑是centos7未识别出这个库，问了公司内部用paddle的团队，他们对ubuntu和centos7这个区别也没说出什么原因，gcc8.2也试过。\r\n\r\n[root@lyb-ubuntu resnet50]# /opt/compiler/gcc-8.2/bin/g++ -o test_main test_main.cpp -I /workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/paddle/include/ -L/workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/paddle/lib/ -L/workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/third_party/install/paddle2onnx/lib/ -L/workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/third_party/install/onnxruntime/lib -L/workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/third_party/install/mkldnn/lib/ -L /workspace/demo/Paddle-Inference-Demo/c++/lib/paddle_inference/third_party/install/mklml/lib -lpaddle_inference -lpaddle2onnx -lonnxruntime -l:libdnnl.so.2 -l:libmkldnn.so.0 -liomp5 /opt/compiler/gcc-8.2/lib/gcc/x86_64-pc-linux-gnu/8.2.0/…/…/…/…/x86_64-pc-linux-gnu/bin/ld: /tmp/ccf40nhB.o: in function main': test_main.cpp:(.text+0x77): undefined reference to paddle::AnalysisConfig::AnalysisConfig(std::string const&, std::string const&)’ /opt/compiler/gcc-8.2/lib/gcc/x86_64-pc-linux-gnu/8.2.0/…/…/…/…/x86_64-pc-linux-gnu/bin/ld: test_main.cpp:(.text+0xf8): undefined reference to paddle_infer::Predictor::GetInputNames()' /opt/compiler/gcc-8.2/lib/gcc/x86_64-pc-linux-gnu/8.2.0/../../../../x86_64-pc-linux-gnu/bin/ld: test_main.cpp:(.text+0x133): undefined reference to paddle_infer::Predictor::GetInputHandle(std::string const&)’ collect2: error: ld returned 1 exit status",
        "state": "closed",
        "user": "xiaohu990",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-11-02T13:20:51+00:00",
        "updated_at": "2024-02-05T10:40:32+00:00",
        "closed_at": "2024-02-05T10:40:32+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 474,
        "title": "为什么，我使用多流推理的速度没有提升，反而降低了",
        "body": null,
        "state": "closed",
        "user": "mikegong7799",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-11-17T03:58:20+00:00",
        "updated_at": "2024-02-05T10:40:20+00:00",
        "closed_at": "2024-02-05T10:40:20+00:00",
        "comments_count": [
            "vivienfanghuagood",
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 498,
        "title": "bot-test-assign",
        "body": "ignore",
        "state": "closed",
        "user": "Ray961123",
        "closed_by": "Ray961123",
        "created_at": "2024-02-07T06:35:12+00:00",
        "updated_at": "2024-02-07T06:36:06+00:00",
        "closed_at": "2024-02-07T06:36:06+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 497,
        "title": "test-bot-assign-ignore",
        "body": "test-bot-assign-ignore",
        "state": "closed",
        "user": "Ray961123",
        "closed_by": "Ray961123",
        "created_at": "2024-02-07T06:23:41+00:00",
        "updated_at": "2024-02-07T06:34:40+00:00",
        "closed_at": "2024-02-07T06:34:40+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 496,
        "title": "test-bot-ignore",
        "body": "test-bot-ignore",
        "state": "closed",
        "user": "Ray961123",
        "closed_by": "Ray961123",
        "created_at": "2024-02-07T05:52:18+00:00",
        "updated_at": "2024-02-07T06:23:18+00:00",
        "closed_at": "2024-02-07T06:23:18+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 499,
        "title": "test",
        "body": "test",
        "state": "closed",
        "user": "yuanlehome",
        "closed_by": "yuanlehome",
        "created_at": "2024-02-07T06:42:23+00:00",
        "updated_at": "2024-02-07T06:47:56+00:00",
        "closed_at": "2024-02-07T06:47:56+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 475,
        "title": "C++推理resnet50,为什么使用GPU反而比CPU更慢呢",
        "body": "配置都是默认\r\n### CPU\r\noneDNN v2.7.3\r\nI1124 15:51:22.883041 29500 resnet50_test.cc:76] run avg time is 100.07 ms\r\nI1124 15:51:22.883064 29500 resnet50_test.cc:91] 0 : 0\r\nI1124 15:51:22.883070 29500 resnet50_test.cc:91] 100 : 2.04159e-37\r\nI1124 15:51:22.883072 29500 resnet50_test.cc:91] 200 : 2.12378e-33\r\nI1124 15:51:22.883074 29500 resnet50_test.cc:91] 300 : 0\r\nI1124 15:51:22.883075 29500 resnet50_test.cc:91] 400 : 1.6849e-35\r\nI1124 15:51:22.883077 29500 resnet50_test.cc:91] 500 : 0\r\nI1124 15:51:22.883078 29500 resnet50_test.cc:91] 600 : 1.05766e-19\r\nI1124 15:51:22.883080 29500 resnet50_test.cc:91] 700 : 2.04091e-23\r\nI1124 15:51:22.883082 29500 resnet50_test.cc:91] 800 : 3.8525e-25\r\nI1124 15:51:22.883086 29500 resnet50_test.cc:91] 900 : 1.52391e-30\r\n\r\n### GPU\r\n\r\ndevice: 0, GPU Compute Capability: 8.6, Driver API Version: 11.8\r\n\r\nI1124 15:52:20.147627 30276 resnet50_test.cc:110] run avg time is 895.09 ms\r\nI1124 15:52:20.147650 30276 resnet50_test.cc:125] 0 : 2.76056e-43\r\nI1124 15:52:20.147655 30276 resnet50_test.cc:125] 100 : 2.07783e-37\r\nI1124 15:52:20.147657 30276 resnet50_test.cc:125] 200 : 2.15253e-33\r\nI1124 15:52:20.147658 30276 resnet50_test.cc:125] 300 : 5.29831e-42\r\nI1124 15:52:20.147660 30276 resnet50_test.cc:125] 400 : 1.71532e-35\r\nI1124 15:52:20.147661 30276 resnet50_test.cc:125] 500 : 7.00649e-45\r\nI1124 15:52:20.147662 30276 resnet50_test.cc:125] 600 : 1.07957e-19\r\nI1124 15:52:20.147665 30276 resnet50_test.cc:125] 700 : 2.06934e-23\r\nI1124 15:52:20.147666 30276 resnet50_test.cc:125] 800 : 3.8847e-25\r\nI1124 15:52:20.147667 30276 resnet50_test.cc:125] 900 : 1.55243e-30",
        "state": "closed",
        "user": "HonLZL",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-11-24T08:53:43+00:00",
        "updated_at": "2024-02-05T10:39:24+00:00",
        "closed_at": "2024-02-05T10:39:24+00:00",
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 480,
        "title": "关于paddle inference的精度调试工具",
        "body": "您好，我这边看到关于paddle inference的精度调试工具目前的Demo是基于trt的，请问是否支持paddle inference呢？基于Paddle inference的Demo又应该如何修改呢？https://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/master/python/advanced/tensorrt_precision_debug/README.md",
        "state": "closed",
        "user": "tianyuzhou668",
        "closed_by": "vivienfanghuagood",
        "created_at": "2023-12-18T03:20:20+00:00",
        "updated_at": "2024-02-05T10:39:09+00:00",
        "closed_at": "2024-02-05T10:39:09+00:00",
        "comments_count": [
            "vivienfanghuagood",
            "ming1753"
        ],
        "labels": [
            "question"
        ]
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 503,
        "title": "bot test assign",
        "body": "请忽略",
        "state": "closed",
        "user": "Ray961123",
        "closed_by": "Ray961123",
        "created_at": "2024-03-08T05:39:58+00:00",
        "updated_at": "2024-03-08T05:45:25+00:00",
        "closed_at": "2024-03-08T05:45:25+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 502,
        "title": "yolov3 c++gpu  liunx tensorrt推理",
        "body": "输入图像的  input_im input_im_shape 是什么 ，有知道怎么正确输入图像的嘛",
        "state": "closed",
        "user": "mypy98",
        "closed_by": "vivienfanghuagood",
        "created_at": "2024-03-06T11:01:08+00:00",
        "updated_at": "2024-04-12T07:15:48+00:00",
        "closed_at": "2024-04-12T07:15:48+00:00",
        "comments_count": [
            "zhink"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 513,
        "title": "【bug report】option(USE_CPP_STANDARD \"Compile demo with cpp standrad support, default use -std=c++11.\" \"-std=c++11\") 无效设置",
        "body": "关联 PR: https://github.com/PaddlePaddle/Paddle-Inference-Demo/pull/505\r\n\r\n在 CMake 中，option 命令用于定义一个**布尔类型**的选项，其语法为 option(<option_variable> <help_string> [<initial_value>])。其中<initial_value>是该选项的初始值，如果没有显式设置该选项，则会采用这个默认值\r\n参考：https://cmake.org/cmake/help/latest/command/option.html\r\n<img width=\"845\" alt=\"image\" src=\"https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/49938469/0dd23d73-3056-47d5-a327-30e5a0099ee5\">\r\n\r\n\r\noption 命令预期的是一个布尔类型的选项，而不是一个字符串类型的选项，所以无法通过 option 命令将 USE_CPP_STANDARD 选项定义为一个带有默认值 “-std=c++11“ 的字符串，CUSTOM_OPERATOR_FILES 与 CUSTOM_PASS_FILES 同样不适用。\r\n\r\n可参考如下修改：\r\n```\r\nif(NOT DEFINED USE_CPP_STANDARD)\r\n    set(USE_CPP_STANDARD \"-std=c++11\")\r\nendif()\r\n```\r\n\r\n当前 c++ case 均无法正常编译，望调整。@yuanlehome \r\n<img width=\"988\" alt=\"image\" src=\"https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/49938469/21899b0b-d162-4daa-babf-53c0e73c2efe\">\r\n",
        "state": "closed",
        "user": "EmmonsCurse",
        "closed_by": "EmmonsCurse",
        "created_at": "2024-03-18T13:52:59+00:00",
        "updated_at": "2024-03-29T03:15:51+00:00",
        "closed_at": "2024-03-20T02:06:16+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 504,
        "title": "编译c++的resnet50推理代码，提示缺少libphi.so文件，请问可以在哪下载？下的linux推理包里面没有带libphi.so",
        "body": null,
        "state": "closed",
        "user": "funny000",
        "closed_by": "carryyu",
        "created_at": "2024-03-08T08:35:49+00:00",
        "updated_at": "2024-07-31T06:07:31+00:00",
        "closed_at": "2024-03-15T06:35:27+00:00",
        "comments_count": [
            "funny000",
            "carryyu",
            "carryyu",
            "sdcb",
            "magicleo"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 517,
        "title": "issue test",
        "body": "issue test",
        "state": "closed",
        "user": "EmmonsCurse",
        "closed_by": "vivienfanghuagood",
        "created_at": "2024-04-11T14:25:32+00:00",
        "updated_at": "2024-04-12T06:56:33+00:00",
        "closed_at": "2024-04-12T06:56:33+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 506,
        "title": "编译resnet50时出错，缺少dnnl.dll,但是下载的预测库里本身就没有",
        "body": "![1](https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/149128242/64f1273e-345a-4851-a13d-a193a8fc6423)\r\n![2](https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/149128242/8bbbe0ae-1c3e-404c-9206-0fb0fc8592e6)\r\n",
        "state": "closed",
        "user": "psychedelicosisyphus",
        "closed_by": "vivienfanghuagood",
        "created_at": "2024-03-12T06:50:17+00:00",
        "updated_at": "2024-04-12T07:15:25+00:00",
        "closed_at": "2024-04-12T07:15:25+00:00",
        "comments_count": [
            "xiaoxiaohehe001"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 511,
        "title": "paddlepaddle-gpu1.7.2支持Paddle-Inference吗",
        "body": "import paddle.inference as paddle_infer\r\n报错：没有名称为 'inference' 的模块",
        "state": "closed",
        "user": "zdxzheng",
        "closed_by": "yuanlehome",
        "created_at": "2024-03-15T07:02:41+00:00",
        "updated_at": "2024-03-29T06:16:43+00:00",
        "closed_at": "2024-03-29T06:16:43+00:00",
        "comments_count": [
            "yuanlehome"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 512,
        "title": "在ubuntu20环境下g++9.4编译了2.3和2.6，用的resnet50 demo，编译通过，但是缺少so文件，把这些文件加入到环境变量也不行，请问是什么原因？",
        "body": null,
        "state": "closed",
        "user": "funny000",
        "closed_by": "vivienfanghuagood",
        "created_at": "2024-03-17T09:08:55+00:00",
        "updated_at": "2024-06-07T06:39:28+00:00",
        "closed_at": "2024-06-07T06:39:28+00:00",
        "comments_count": [
            "bukejiyu",
            "vivienfanghuagood",
            "funny000"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 514,
        "title": "无法打开输入文件“D:\\Paddle-Inference-Demo\\c++\\lib\\paddle_inference\\third_party\\install\\onn xruntime\\lib\\onnxruntime.lib",
        "body": "预测库中缺少这个文件，windows版本。\r\n![1](https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/149128242/d38f51ce-abd5-4b40-8e19-99056f731d3c)\r\n",
        "state": "closed",
        "user": "psychedelicosisyphus",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-03-19T10:44:10+00:00",
        "updated_at": "2025-03-25T06:44:35+00:00",
        "closed_at": "2025-03-25T06:44:35+00:00",
        "comments_count": [
            "EmmonsCurse"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 518,
        "title": "issue test [do not close]",
        "body": "issue test [do not close]",
        "state": "closed",
        "user": "EmmonsCurse",
        "closed_by": "EmmonsCurse",
        "created_at": "2024-04-12T14:23:34+00:00",
        "updated_at": "2024-07-04T12:37:55+00:00",
        "closed_at": "2024-07-04T12:37:55+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 519,
        "title": "在win10系统下使用PaddleInference2.5编译ppyoloe_crn_l,出现如下问题, 请问如何解决? ",
        "body": "\r\nppyoloe_crn_l.exe --model_file ppyoloe_crn_l_300e_coco/model.pdmodel --params_file ppyoloe_crn_l_300e_coco/model.pdiparams\r\n\r\n\r\n\r\nD:\\000-AI\\paddle\\Deploy\\2.5\\Paddle-Inference-Demo-master\\c++\\gpu\\ppyoloe_crn_l\\build\\Release>ppyoloe_crn_l.exe --model_file ppyoloe_crn_l_300e_coco/model.pdmodel --params_file ppyoloe_crn_l_300e_coco/model.pdiparams\r\ne[1me[35m--- Running analysis [ir_graph_build_pass]e[0m\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0505 13:45:31.034536  5320 executor.cc:187] Old Executor is Running.\r\ne[1me[35m--- Running analysis [ir_analysis_pass]e[0m\r\ne[32m--- Running IR pass [map_op_to_another_pass]e[0m\r\ne[32m--- Running IR pass [identity_scale_op_clean_pass]e[0m\r\ne[32m--- Running IR pass [is_test_pass]e[0m\r\ne[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m\r\ne[32m--- Running IR pass [delete_quant_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [delete_weight_dequant_linear_op_pass]e[0m\r\ne[32m--- Running IR pass [constant_folding_pass]e[0m\r\ne[32m--- Running IR pass [silu_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_bn_fuse_pass]e[0m\r\nI0505 13:45:31.640825  5320 fuse_pass_base.cc:59] ---  detected 78 subgraphs\r\ne[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m\r\ne[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]e[0m\r\ne[32m--- Running IR pass [vit_attention_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_encoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [multi_devices_fused_multi_transformer_decoder_fuse_qkv_pass]e[0m\r\ne[32m--- Running IR pass [fuse_multi_transformer_layer_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]e[0m\r\ne[32m--- Running IR pass [matmul_scale_fuse_pass]e[0m\r\ne[32m--- Running IR pass [multihead_matmul_fuse_pass_v3]e[0m\r\ne[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]e[0m\r\ne[32m--- Running IR pass [fc_fuse_pass]e[0m\r\ne[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0m\r\nI0505 13:45:34.308128  5320 fuse_pass_base.cc:59] ---  detected 9 subgraphs\r\ne[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m\r\nI0505 13:45:34.541483  5320 fuse_pass_base.cc:59] ---  detected 118 subgraphs\r\ne[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0m\r\ne[32m--- Running IR pass [conv2d_fusion_layout_transfer_pass]e[0m\r\ne[32m--- Running IR pass [transfer_layout_elim_pass]e[0m\r\ne[32m--- Running IR pass [auto_mixed_precision_pass]e[0m\r\ne[32m--- Running IR pass [inplace_op_var_pass]e[0m\r\ne[1me[35m--- Running analysis [save_optimized_model_pass]e[0m\r\nW0505 13:45:34.565424  5320 save_optimized_model_pass.cc:28] save_optim_cache_model is turned off, skip save_optimized_model_pass\r\ne[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m\r\nI0505 13:45:34.566417  5320 ir_params_sync_among_devices_pass.cc:51] Sync params from CPU to GPU\r\ne[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m\r\ne[1me[35m--- Running analysis [inference_op_replace_pass]e[0m\r\ne[1me[35m--- Running analysis [memory_optimize_pass]e[0m\r\nI0505 13:45:34.726987  5320 memory_optimize_pass.cc:222] Cluster name : tmp_2  size: 26214400\r\nI0505 13:45:34.726987  5320 memory_optimize_pass.cc:222] Cluster name : batch_norm_2.tmp_2  size: 26214400\r\nI0505 13:45:34.726987  5320 memory_optimize_pass.cc:222] Cluster name : image  size: 4915200\r\nI0505 13:45:34.727985  5320 memory_optimize_pass.cc:222] Cluster name : sigmoid_2.tmp_0  size: 26214400\r\nI0505 13:45:34.727985  5320 memory_optimize_pass.cc:222] Cluster name : batch_norm_48.tmp_2  size: 1228800\r\nI0505 13:45:34.727985  5320 memory_optimize_pass.cc:222] Cluster name : tmp_0  size: 13107200\r\nI0505 13:45:34.728982  5320 memory_optimize_pass.cc:222] Cluster name : elementwise_add_0  size: 4915200\r\nI0505 13:45:34.728982  5320 memory_optimize_pass.cc:222] Cluster name : tmp_7  size: 4915200\r\nI0505 13:45:34.728982  5320 memory_optimize_pass.cc:222] Cluster name : elementwise_add_16  size: 614400\r\nI0505 13:45:34.728982  5320 memory_optimize_pass.cc:222] Cluster name : pool2d_5.tmp_0  size: 768\r\nI0505 13:45:34.729979  5320 memory_optimize_pass.cc:222] Cluster name : scale_factor  size: 8\r\nI0505 13:45:34.729979  5320 memory_optimize_pass.cc:222] Cluster name : shape_2.tmp_0_slice_0  size: 4\r\ne[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m\r\nI0505 13:45:34.970336  5320 analysis_predictor.cc:1660] ======= optimize end =======\r\nI0505 13:45:34.971334  5320 naive_executor.cc:164] ---  skip [feed], feed -> scale_factor\r\nI0505 13:45:34.973328  5320 naive_executor.cc:164] ---  skip [feed], feed -> image\r\nI0505 13:45:34.984299  5320 naive_executor.cc:164] ---  skip [gather_nd_0.tmp_0], fetch -> fetch\r\nI0505 13:45:34.984299  5320 naive_executor.cc:164] ---  skip [multiclass_nms3_0.tmp_2], fetch -> fetch\r\nW0505 13:45:34.987293  5320 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.2, Runtime API Version: 11.8\r\nW0505 13:45:34.991281  5320 gpu_resources.cc:149] device: 0, cuDNN Version: 8.6.\r\n\r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\nNot support stack backtrace yet.\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: The axis is expected to be in range of [-1, 1), but got 1\r\n  [Hint: Expected axis_value >= -rank && axis_value < rank == true, but received axis_value >= -rank && axis_value < rank:0 != true:1.] (at ..\\paddle\\phi\\infermeta\\unary.cc:3567)",
        "state": "closed",
        "user": "dict1234",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-05T05:48:25+00:00",
        "updated_at": "2025-05-13T06:48:51+00:00",
        "closed_at": "2025-05-13T06:48:51+00:00",
        "comments_count": [
            "kangguangli",
            "dict1234",
            "kangguangli",
            "kangguangli",
            "lizexu123",
            "dict1234",
            "dict1234",
            "lizexu123",
            "kangguangli",
            "dict1234",
            "kangguangli"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 521,
        "title": "No such file or directory",
        "body": "error while loading shared libraries: libpaddle2onnx.so.1.0.0rc2: cannot open shared object file: No such file or directory\r\n",
        "state": "closed",
        "user": "linlongrd",
        "closed_by": "paddle-bot[bot]",
        "created_at": "2024-05-09T07:54:08+00:00",
        "updated_at": "2025-05-27T06:43:24+00:00",
        "closed_at": "2025-05-27T06:43:24+00:00",
        "comments_count": [
            "linlongrd",
            "bukejiyu"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 523,
        "title": "cmake 编译过程中提示缺少  libphi.a",
        "body": "描述：jetson nano 下使用make从源码编译，cmake 构建成功后，使用make -j4编译，提示 缺少 libphi.a文件；\r\n问题： 原因，如何解决？\r\n日志如下：，环境 如下，cmakelists.txt 见附件\r\n![27](https://github.com/PaddlePaddle/Paddle-Inference-Demo/assets/35392697/459795cf-accb-4ccb-accd-af89b199228f)\r\n\r\n[ 11%] Building CUDA object paddle/phi/CMakeFiles/phi.dir/kernels/sparse/gpu/softmax_kernel.cu.o\r\n[ 11%] Building CXX object paddle/phi/CMakeFiles/phi.dir/kernels/cpu/activation_grad_kernel.cc.o\r\n[ 11%] Building CXX object paddle/phi/CMakeFiles/phi.dir/kernels/cpu/warprnnt_kernel.cc.o\r\n[ 11%] Linking CXX static library libphi.a\r\n[ 42%] Built target phi\r\n[ 42%] Built target string_helper\r\n[ 42%] Built target pretty_log\r\nmake[2]: *** No rule to make target 'paddle/phi/libphi.a', needed by 'paddle/phi/kernels/funcs/jit/jit_ker              nel_benchmark'.  Stop.\r\nCMakeFiles/Makefile2:5471: recipe for target 'paddle/phi/kernels/funcs/jit/CMakeFiles/jit_kernel_benchmark              .dir/all' failed\r\nmake[1]: *** [paddle/phi/kernels/funcs/jit/CMakeFiles/jit_kernel_benchmark.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 42%] Built target pybind_util\r\nmake[2]: *** No rule to make target 'paddle/phi/libphi.a', needed by 'paddle/phi/tools/print_phi_kernels'.                Stop.\r\nCMakeFiles/Makefile2:5510: recipe for target 'paddle/phi/tools/CMakeFiles/print_phi_kernels.dir/all' faile              d\r\nmake[1]: *** [paddle/phi/tools/CMakeFiles/print_phi_kernels.dir/all] Error 2\r\n[ 42%] Built target enforce\r\nMakefile:148: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\n\r\n更多细节：\r\n硬件环境：\r\n\tcuda\t10.2.89\r\n\tcudnn\t8.2.1\r\n\tjetpack\t4.6.4\r\n\ttensorRT\t8.x （32G 的是8.2.1，单独安装见3.1） \r\n\tsudo find / -name \"libnvinfer.so\" /usr/lib/aarch64-linux-gnu/libnvinfer.so\r\n\tubuntu\t18.4 LTS\r\n\tGPU架构\tmaxwell （Jetson Nano | NVIDIA Developer）\r\n\tpython\t3.9.11\r\n\tpaddlepaddle\t2.5.0\r\n\tpaddleDetection\t2.6.1\r\n\tgcc\t7.5 --> 转 8.2\r\n\tgfortran :\r\n\tcmake\t3.10 ->3.16 /usr/local/ --> 3.19.8 (必须，见4.1）\r\n\tnvidia-docker\tDocker version 20.10.21, build 20.10.21-0ubuntu1~18.04.3\r\n\tnccl= 无\r\n\r\n\r\nCMakeLists.txt 见附件\r\n[CMakeLists.txt](https://github.com/user-attachments/files/16026471/CMakeLists.txt)\r\n\r\n\t",
        "state": "open",
        "user": "wsdxyz",
        "closed_by": null,
        "created_at": "2024-06-28T08:17:14+00:00",
        "updated_at": "2024-07-13T02:43:36+00:00",
        "closed_at": null,
        "comments_count": [
            "zoooo0820",
            "vivienfanghuagood",
            "wsdxyz"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 530,
        "title": "使用paddle_inference在c++中调用官方例程报错",
        "body": "首先我下载了官方所给的resunet模型的参数和模型文件：inference.pdiparams和inference.pdiparams.info和inference.pdmodel，然后调用了官方的例程，一直在“auto input_names = predictor->GetInputNames();”这一步代码出错，显示内存已被破坏，后来经过调试发现config中根本就没读取到，\r\n代码如下：    // 字符串 prog_file 为 Combine 模型文件所在路径\r\n    std::string prog_file = \"./resunet/inference.pdmodel\";\r\n    // 字符串 params_file 为 Combine 模型参数文件所在路径\r\n    std::string params_file = \"./resunet/inference.pdiparams\";\r\n    // 设置推理模型路径，即为本小节第2步中下载的模型\r\n    // 根据模型文件和参数文件构造 Config 对象\r\n    paddle_infer::Config config(prog_file, params_file);\r\n\r\n    // 不启用 GPU 和 MKLDNN 推理\r\n    config.DisableGpu();\r\n    config.EnableMKLDNN();\r\n\r\n    // 开启 内存/显存 复用\r\n    config.EnableMemoryOptim();\r\n\r\n    auto predictor = paddle_infer::CreatePredictor(config);\r\n    // 获取输入 Tensor\r\n    auto input_names = predictor->GetInputNames();\r\n    auto input_tensor = predictor->GetInputHandle(input_names[0]);\r\n\r\n    // 设置输入 Tensor 的维度信息\r\n    std::vector<int> INPUT_SHAPE = { 1, 3, 224, 224 };\r\n    input_tensor->Reshape(INPUT_SHAPE);\r\n\r\n    // 准备输入数据\r\n    int input_size = 1 * 3 * 224 * 224;\r\n    std::vector<float> input_data(input_size, 1);\r\n    // 设置输入 Tensor 数据\r\n    input_tensor->CopyFromCpu(input_data.data());\r\n##以下是config的参数截图，可以发现根本就没读取的到文件，但是我的文件是确实存在的\r\n![image](https://github.com/user-attachments/assets/a041cf98-c8f7-4a94-afd2-06d02a9d332d)\r\n",
        "state": "open",
        "user": "mfq2003",
        "closed_by": null,
        "created_at": "2024-08-01T01:35:34+00:00",
        "updated_at": "2025-03-04T02:54:14+00:00",
        "closed_at": null,
        "comments_count": [
            "mfq2003",
            "Wangzheee",
            "mfq2003",
            "Wangzheee",
            "StarTwinkled"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 532,
        "title": "Paddle-Inference 资源加载速度过慢",
        "body": "请问在windows10上，采用paddle inference对PaddleOCR做GPU版本的C++部署，链接了cuda和tensorrt，在加载资源这个过程中速度非常慢，是因为什么原因导致的呢？",
        "state": "open",
        "user": "onenerve",
        "closed_by": null,
        "created_at": "2024-08-22T06:34:03+00:00",
        "updated_at": "2024-10-17T01:10:38+00:00",
        "closed_at": null,
        "comments_count": [
            "lizexu123",
            "Shawn-Tao"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 535,
        "title": "jetson部署paddleocr<fused_conv2d_add_act>算子不支持",
        "body": "有老哥给了这个https://github.com/PaddlePaddle/Paddle/issues/60540  解决方案，cudnn换成8.5版本的，但是\r\n![微信图片_20240906194633](https://github.com/user-attachments/assets/dd591a37-2bd1-4891-b285-f077a3f174d0)\r\npaddle inference的wheel包只支持8.4的cudnn，无解。另外我用的是nvidia/cuda:11.4.3-cudnn8-runtime-ubuntu20.04这个镜像，这里cudnn是8.2.4的，希望工作人员能尽快更新这个镜像版本的whl包支持！",
        "state": "open",
        "user": "NyquistBodeTu",
        "closed_by": null,
        "created_at": "2024-09-06T11:53:01+00:00",
        "updated_at": "2024-10-12T02:54:45+00:00",
        "closed_at": null,
        "comments_count": [
            "vivienfanghuagood",
            "EmmonsCurse"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 531,
        "title": "Paddle-Inference-Demo，这行代码错了吧",
        "body": "https://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/cbf3a528eedf876d2b59409c406ef64aee29772f/python/advanced/multi_thread/threads_demo.py#L104\r\n\r\n是不是应该是：\r\nfor t in threads：\r\n     t.join()\r\n",
        "state": "open",
        "user": "lovychen",
        "closed_by": null,
        "created_at": "2024-08-19T02:22:44+00:00",
        "updated_at": "2024-08-19T03:04:55+00:00",
        "closed_at": null,
        "comments_count": [
            "vivienfanghuagood"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 541,
        "title": " 从飞桨镜像库拉取编译镜像报错，镜像不存在",
        "body": "[https://www.paddlepaddle.org.cn/inference/v2.6/guides/hardware_support/cpu_phytium_cn.html](url)\r\n第一步，调用：\r\n`docker pull registry.baidubce.com/device/paddle-cpu:kylinv10-aarch64-gcc73`\r\n报错：\r\nError response from daemon: manifest for registry.baidubce.com/device/paddle-cpu:kylinv10-aarch64-gcc73 not found: manifest unknown: manifest unknown\r\n\r\n尝试申威、兆芯的安装均可正常拉取镜像，飞腾/鲲鹏拉取镜像失败 ",
        "state": "open",
        "user": "huameinan219",
        "closed_by": null,
        "created_at": "2024-11-13T12:22:02+00:00",
        "updated_at": "2025-07-08T07:24:07+00:00",
        "closed_at": null,
        "comments_count": [
            "vivienfanghuagood",
            "LeeCASC"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 542,
        "title": "用最新的develop分支，执行c++ -> cpu -> yoloV3样例报错",
        "body": "以下是配置项：\r\n![image](https://github.com/user-attachments/assets/bec2d4e1-f1bf-4e03-9198-fa56e65b9e0f)\r\n\r\n以下是错误堆栈：\r\n![image](https://github.com/user-attachments/assets/9b20bced-31df-4bbb-8d7e-67e11ccf8d74)\r\n\r\n文字信息：\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI1130 22:23:30.442376 14563 analysis_predictor.cc:2142] MKLDNN is enabled\r\nI1130 22:23:30.442462 14563 analysis_predictor.cc:2259] Ir optimization is turned off, no ir pass will be executed.\r\n--- Running analysis [ir_graph_build_pass]\r\nI1130 22:23:30.449360 14563 executor.cc:183] Old Executor is Running.\r\n--- Running analysis [ir_analysis_pass]\r\n--- Running analysis [ir_params_sync_among_devices_pass]\r\n--- Running analysis [adjust_cudnn_workspace_size_pass]\r\n--- Running analysis [inference_op_replace_pass]\r\n--- Running analysis [save_optimized_model_pass]\r\n--- Running analysis [ir_graph_to_program_pass]\r\nI1130 22:23:30.606269 14563 analysis_predictor.cc:2348] ======= ir optimization completed =======\r\nterminate called after throwing an instance of 'common::enforce::EnforceNotMet'\r\n  what():  \r\n\r\n--------------------------------------\r\nC++ Traceback (most recent call last):\r\n--------------------------------------\r\n0   paddle_infer::CreatePredictor(paddle::AnalysisConfig const&)\r\n1   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)\r\n2   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)\r\n3   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n4   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)\r\n5   paddle::TranslateLegacyProgramToProgram(paddle::framework::ProgramDesc const&)\r\n6   paddle::translator::ProgramTranslator::Translate()\r\n7   paddle::translator::ProgramTranslator::TranslateBlock(paddle::framework::BlockDesc const&, unsigned long, unsigned long, paddle::translator::TranslationContext*, pir::Block*)\r\n8   paddle::translator::ProgramTranslator::TranslateGeneralOperation(paddle::framework::OpDesc const*, paddle::translator::TranslationContext*, pir::Block*)\r\n9   paddle::translator::OpTranscriber::operator()(pir::IrContext*, paddle::translator::TranslationContext*, paddle::framework::OpDesc const&, pir::Block*)\r\n10  pir::Operation::Create(std::vector<pir::Value, std::allocator<pir::Value> > const&, std::unordered_map<std::string, pir::Attribute, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, pir::Attribute> > > const&, std::vector<pir::Type, std::allocator<pir::Type> > const&, pir::OpInfo, unsigned long, std::vector<pir::Block*, std::allocator<pir::Block*> > const&, bool)\r\n11  pir::Op<paddle::dialect::YoloBoxOp, paddle::dialect::InferMetaInterface, pir::InferSymbolicShapeInterface, paddle::dialect::OpYamlInfoInterface, paddle::dialect::GetKernelTypeForVarInterface>::VerifySigInvariants(pir::Operation*)\r\n12  paddle::dialect::YoloBoxOp::VerifySig()\r\n13  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)\r\n\r\n----------------------\r\nError Message Summary:\r\n----------------------\r\nInvalidArgumentError: Type of attribute: iou_aware_factor is not pir::FloatAttribute.\r\n  [Hint: Expected attributes.at(\"iou_aware_factor\").isa<pir::FloatAttribute>() == true, but received attributes.at(\"iou_aware_factor\").isa<pir::FloatAttribute>():0 != true:1.] (at /paddle/build/paddle/fluid/pir/dialect/operator/ir/pd_op4.cc:30369)\r\n\r\nAborted",
        "state": "open",
        "user": "cerasumat",
        "closed_by": null,
        "created_at": "2024-11-30T14:32:48+00:00",
        "updated_at": "2024-12-03T09:29:44+00:00",
        "closed_at": null,
        "comments_count": [
            "zoooo0820",
            "cerasumat"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 564,
        "title": "arch64的docker镜像全都拉不下来",
        "body": "https://www.paddlepaddle.org.cn/inference/v2.4/guides/hardware_support/xpu_kunlun_cn.html\n\ndocker pull registry.baidubce.com/device/paddle-xpu:kylinv10-aarch64-gcc73\nError response from daemon: manifest for registry.baidubce.com/device/paddle-xpu:kylinv10-aarch64-gcc73 not found: manifest unknown: manifest unknown\n\n镜像拉不下来，看到issues里貌似24年底就有人提到过，arch64相关镜像一直没有，以后也不会再更新了吗",
        "state": "open",
        "user": "LeeCASC",
        "closed_by": null,
        "created_at": "2025-07-08T07:28:21+00:00",
        "updated_at": "2025-07-08T07:28:25+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/Paddle-Inference-Demo",
        "number": 544,
        "title": "c++版本的推理结果与python版本的推理结果不符",
        "body": "使用2.6/3.0版本的paddle inference预编译库，官方yolov8预训练模型，对示例图片进行推理，输出的bbox总是有很多不正常的结果（均出现在左下角，面积很小但置信度很高），如下贴图。\r\n该问题与模型无关，我使用自己训练的模型（python推理没这个问题），对其它图片预测，也是在左下角区域出现很多错误的置信度极高的bbox。\r\n请问可能是什么原因导致？\r\n错误的bbox截图如下：\r\n![image](https://github.com/user-attachments/assets/8d6f0993-947f-40a1-a09a-48d62a982120)\r\n![image](https://github.com/user-attachments/assets/0f7ae18b-ebf1-45b5-ba88-a8aeeb76366f)\r\n\r\n\r\n### 复现环境 Environment\r\n\r\n\tOS:Ubuntu22.04\r\n\tPaddleInference: 2.6.1/3.0\r\n\tGCC: 11\r\n\tOpenCV: 4\r\n        CPU推理，开了mklnn\r\n\r\n---\r\n补充下，不是绘图的问题。\r\n在Predict方法里，从predictor_->Run();预测之后，开始postprocess，在如下计算rect坐标前数据就已经不对了。\r\n![image](https://github.com/user-attachments/assets/c5d00dff-f273-4d88-bfd8-22edf9f40c3f)\r\n\r\n\r\n",
        "state": "open",
        "user": "cerasumat",
        "closed_by": null,
        "created_at": "2024-12-06T01:47:57+00:00",
        "updated_at": "2024-12-17T12:24:01+00:00",
        "closed_at": null,
        "comments_count": [
            "vivienfanghuagood",
            "cerasumat",
            "cerasumat",
            "vivienfanghuagood",
            "vivienfanghuagood",
            "cerasumat"
        ],
        "labels": []
    }
]